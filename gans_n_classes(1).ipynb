{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gans_n_classes.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvCMx6WZrec9"
      },
      "source": [
        "import os, math\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from tensorflow.keras.datasets import cifar10, mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, BatchNormalization \n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Activation\n",
        "from tensorflow.keras.layers import Reshape, Conv2DTranspose, LeakyReLU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.losses import mse, SparseCategoricalCrossentropy"
      ],
      "execution_count": 21,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df9LBqdbriOh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "72917cd0-a999-4fa4-c8e5-c45da4f2d723"
      },
      "source": [
        "(x_train, y_train),(x_test, _) = mnist.load_data()\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceVV7TVkmAYs"
      },
      "source": [
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_train = x_train / 255"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW9SJ9vprkAr",
        "outputId": "4194ee5d-74a2-4122-f76b-8b1113fd1883"
      },
      "source": [
        "x_train.max()"
      ],
      "execution_count": 78,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 78
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJserdiD0huu"
      },
      "source": [
        "image_size = x_train[0].shape[1]\n",
        "image_size"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyFFv7MotmPM"
      },
      "source": [
        "class ConvTransBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides):\n",
        "        super().__init__()\n",
        "        self.bn = BatchNormalization()  # check training flag\n",
        "        self.act = Activation(activation='relu')\n",
        "        self.conv2D_trans = Conv2DTranspose(filters=filters,\n",
        "                                kernel_size=kernel_size,\n",
        "                                strides=strides,\n",
        "                                padding='same')\n",
        "        \n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.bn(inputs)\n",
        "        x = self.act(x)\n",
        "        return self.conv2D_trans(x)\n"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "INY9a1B4z7Wn"
      },
      "source": [
        "noise = np.random.uniform(0, 1, size=[10000, 100])\n"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "g5VoZvOg0vtf"
      },
      "source": [
        "# model = Sequential()\n",
        "# model.add(Input(noise.shape[1]))\n",
        "# model.add(Dense(14*14*32, activation=\"relu\"))\n",
        "# model.add(Reshape([14, 14, 32]))\n",
        "# model.add(ConvTransBlock(32, 5, 2))\n",
        "# model.add(ConvTransBlock(16, 5, 1))\n",
        "# model.add(Conv2DTranspose(1, 5, padding=\"same\", activation=\"sigmoid\"))\n"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H2Duk3Xc2l0w"
      },
      "source": [
        "# model.compile(loss=\"MSE\", metrics=[\"MAE\"])\n",
        "# model.fit(noise, x_train.reshape(-1, 28, 28, 1)[:10000])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRIsDGBlxYOA"
      },
      "source": [
        "class Generator(tf.keras.models.Model):\n",
        "    def __init__(self, filters, kernel_size, resize_img):\n",
        "        super().__init__()\n",
        "        self.dense1 = Dense(resize_img * resize_img * filters[0])\n",
        "        self.reshape = Reshape([resize_img, resize_img, filters[0]])\n",
        "        self.conv2dtrans = []\n",
        "        for i, _filter in enumerate(filters):\n",
        "            if i <= 1:\n",
        "                strides = 2\n",
        "            else:\n",
        "                strides = 1\n",
        "            self.conv2dtrans.append(ConvTransBlock(_filter, kernel_size, strides))\n",
        "        \n",
        "        self.act = Activation(\"sigmoid\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.reshape(x)\n",
        "        for conv in self.conv2dtrans:\n",
        "            x = conv(x)\n",
        "        return self.act(x)\n",
        "\n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6avJKo9G50jp",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 162
        },
        "outputId": "27626f79-6040-4b5d-e3c8-8db817c75c41"
      },
      "source": [
        "# gen = Generator(gen_layers_filter, gen_kernel_size, gen_resize_img)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-8-a0d34591d352>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgen\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGenerator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgen_layers_filter\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_kernel_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgen_resize_img\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'gen_layers_filter' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iIWu8eEf8dTx"
      },
      "source": [
        "# gen.compile(loss=\"mse\", metrics=[\"mae\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YU1UT74M847B"
      },
      "source": [
        "# gen.fit(noise, x_train.reshape(-1, 28, 28, 1)[:10000], batch_size=64)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUfyrKaXi7hx"
      },
      "source": [
        "def leaky_conv(filters, kernel_size, strides):\n",
        "    return Sequential([LeakyReLU(alpha=0.2),\n",
        "                       Conv2D(filters=filters,\n",
        "                       kernel_size=kernel_size,\n",
        "                       strides=strides,\n",
        "                       padding='same')])\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QZYAmEkxkrXU"
      },
      "source": [
        "# model = Sequential()\n",
        "# model.add(Input(x_train.shape[1:]))\n",
        "# model.add(leaky_conv(32, 5, 2))\n",
        "# model.add(leaky_conv(64, 5, 2))\n",
        "# model.add(Flatten())\n",
        "# model.add(Dense(10))\n",
        "# model.add(Activation(\"softmax\"))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CVF6HoMij1ot",
        "outputId": "d75060cb-5006-4225-c55e-cb868bd5bccc"
      },
      "source": [
        "# model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
        "# model.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_10 (Sequential)   (None, 14, 14, 32)        832       \n",
            "_________________________________________________________________\n",
            "sequential_11 (Sequential)   (None, 7, 7, 64)          51264     \n",
            "_________________________________________________________________\n",
            "flatten (Flatten)            (None, 3136)              0         \n",
            "_________________________________________________________________\n",
            "dense_1 (Dense)              (None, 10)                31370     \n",
            "_________________________________________________________________\n",
            "activation_2 (Activation)    (None, 10)                0         \n",
            "=================================================================\n",
            "Total params: 83,466\n",
            "Trainable params: 83,466\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dDFwZx5CmnvE"
      },
      "source": [
        "# model.fit(x_train, y_train)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuHfmqqgl1UE"
      },
      "source": [
        "class Discriminator(tf.keras.models.Model):\n",
        "    def __init__(self, filters, kernel_size):\n",
        "        super().__init__()\n",
        "        self.leaky_convs = []\n",
        "        for i, _filter in enumerate(filters):\n",
        "            if i < len(filters) - 1:\n",
        "                strides = 2\n",
        "            else:\n",
        "                strides = 1\n",
        "            self.leaky_convs.append(leaky_conv(_filter, kernel_size, strides))\n",
        "        self.flat = Flatten()\n",
        "        self.dense = Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for conv in self.leaky_convs:\n",
        "            x = conv(x)\n",
        "        x = self.flat(x)\n",
        "        return self.dense(x)"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cVKLSZL_ozgy"
      },
      "source": [
        "# disc = Discriminator(disc_layers_filters, disc_kernel_size)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eS9yU7CAo5sY"
      },
      "source": [
        "# disc.compile(loss=\"categorical_crossentropy\", metrics=[\"mae\"])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fZISAcJnpxrx"
      },
      "source": [
        "# disc.fit(x_train, np.where(y_train < 5, 1, 0))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWwkky2Hp413",
        "outputId": "4c4e0b19-e892-4505-b72d-2b077636df74"
      },
      "source": [
        "# disc.summary()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"discriminator_2\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "sequential_12 (Sequential)   (32, 14, 14, 32)          832       \n",
            "_________________________________________________________________\n",
            "sequential_13 (Sequential)   (32, 7, 7, 64)            51264     \n",
            "_________________________________________________________________\n",
            "sequential_14 (Sequential)   (32, 4, 4, 128)           204928    \n",
            "_________________________________________________________________\n",
            "sequential_15 (Sequential)   (32, 4, 4, 256)           819456    \n",
            "_________________________________________________________________\n",
            "flatten_1 (Flatten)          multiple                  0         \n",
            "_________________________________________________________________\n",
            "dense_2 (Dense)              multiple                  4097      \n",
            "=================================================================\n",
            "Total params: 1,080,577\n",
            "Trainable params: 1,080,577\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mprUKHR-qyH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f37f5bd-39d6-4d57-fe5b-249c5fd68bf0"
      },
      "source": [
        "latent_size = 100\n",
        "batch_size = 64\n",
        "train_steps = 40000\n",
        "disc_lr = 2e-4\n",
        "disc_decay = 6e-8\n",
        "gen_lr = disc_lr/2\n",
        "gen_decay = disc_decay/2\n",
        "\n",
        "disc_lr, disc_decay, gen_lr, gen_decay"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0002, 6e-08, 0.0001, 3e-08)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYGp4FOZ8_lI"
      },
      "source": [
        "disc_kernel_size = 5\n",
        "disc_layers_filters = [32, 64, 128, 256]"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNBiGfK4s5v4"
      },
      "source": [
        "gen_resize_img = image_size // 4\n",
        "gen_kernel_size = 5\n",
        "gen_layers_filter = [128, 64, 32, 1]"
      ],
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80EQpJupsYsu"
      },
      "source": [
        "gen = Generator(gen_layers_filter, gen_kernel_size, gen_resize_img)"
      ],
      "execution_count": 82,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsGHWgavtkcd"
      },
      "source": [
        "disc = Discriminator(disc_layers_filters, disc_kernel_size)\n",
        "disc.compile(loss='binary_crossentropy',\n",
        "             optimizer=RMSprop(learning_rate=disc_lr, decay=disc_decay),\n",
        "             metrics=['accuracy'])\n"
      ],
      "execution_count": 83,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dQJ7JIUuAF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9cdc9a58-2847-4505-8b55-fd6705c18e3c"
      },
      "source": [
        "def build_gan():\n",
        "    # disc.trainable = False\n",
        "    gan = Sequential(name='gan_model')\n",
        "    gan.add(Input([100,]))\n",
        "    gan.add(gen)\n",
        "    gan.add(disc)\n",
        "    gan.summary()\n",
        "    gan.compile(loss='binary_crossentropy',\n",
        "                optimizer=RMSprop(learning_rate=gen_lr, decay=gen_decay),\n",
        "                metrics=['accuracy'])            \n",
        "    return gan   \n",
        "\n",
        "\n",
        "gan = build_gan()\n"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"gan_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "generator_5 (Generator)      (None, 28, 28, 1)         1301505   \n",
            "_________________________________________________________________\n",
            "discriminator_5 (Discriminat (None, 1)                 1080577   \n",
            "=================================================================\n",
            "Total params: 2,382,082\n",
            "Trainable params: 2,381,378\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diElXoXzFgLe",
        "outputId": "74c71007-8202-434a-ad4d-f5f591aa0e6c"
      },
      "source": [
        "for layer in gan.layers:\n",
        "    print(layer.trainable)"
      ],
      "execution_count": 85,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9zEUeIpNjsK"
      },
      "source": [
        "def plot_images(generator,\n",
        "                noise_input,\n",
        "                noise_label=None,\n",
        "                noise_codes=None,\n",
        "                show=False,\n",
        "                step=0,\n",
        "                model_name=\"gan\"):\n",
        "    \"\"\"Generate fake images and plot them\n",
        "\n",
        "    For visualization purposes, generate fake images\n",
        "    then plot them in a square grid\n",
        "\n",
        "    # Arguments\n",
        "        generator (Model): The Generator Model for \n",
        "            fake images generation\n",
        "        noise_input (ndarray): Array of z-vectors\n",
        "        show (bool): Whether to show plot or not\n",
        "        step (int): Appended to filename of the save images\n",
        "        model_name (string): Model name\n",
        "\n",
        "    \"\"\"\n",
        "    # os.makedirs(model_name, exist_ok=True)\n",
        "    # filename = os.path.join(model_name, \"%05d.png\" % step)\n",
        "    rows = int(math.sqrt(noise_input.shape[0]))\n",
        "    if noise_label is not None:\n",
        "        noise_input = [noise_input, noise_label]\n",
        "        if noise_codes is not None:\n",
        "            noise_input += noise_codes\n",
        "\n",
        "    images = generator.predict(noise_input)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    num_images = images.shape[0]\n",
        "    image_size = images.shape[1]\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(rows, rows, i + 1)\n",
        "        image = np.reshape(images[i], [image_size, image_size])\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.axis('off')\n",
        "    # plt.savefig(filename)\n",
        "    if show:\n",
        "        \n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close('all')"
      ],
      "execution_count": 86,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbex8Ww8H03-"
      },
      "source": [
        "# Train the Discriminator and Adversarial Networks\n",
        "def train(models, x_train):\n",
        "    generator, discriminator, gan = models \n",
        "    # batch_size, latent_size, train_step, model_name = params\n",
        "    save_interval = 500\n",
        "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size]) \n",
        "    train_size = x_train.shape[0]\n",
        "\n",
        "    for i in range(train_steps):\n",
        "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
        "        real_images = x_train[rand_indexes]\n",
        "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "        fake_images = generator.predict(noise)        \n",
        "        x = np.concatenate((real_images, fake_images)) # same size\n",
        "        y = np.ones([2 * batch_size, 1])\n",
        "        y[batch_size:, :] = 0.0\n",
        "        # experimental\n",
        "        y[: batch_size] = 0.9\n",
        "        discriminator.trainable = True\n",
        "        loss, acc = discriminator.train_on_batch(x, y)\n",
        "        # log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
        "        log = f'{i}: [discriminator loss: {loss}, acc: {acc}]'\n",
        "        discriminator.trainable = False\n",
        "\n",
        "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "        y = np.ones([batch_size, 1])        \n",
        "        loss, acc = gan.train_on_batch(noise, y)\n",
        "        log = \"%s [gan loss: %f, acc: %f]\" % (log, loss, acc)\n",
        "        print(log)\n",
        "        if (i + 1) % save_interval == 0:\n",
        "            # plot generator images on a periodic basis\n",
        "            plot_images(generator,\n",
        "                        noise_input=noise_input,\n",
        "                        show=True,\n",
        "                        step=(i + 1),\n",
        "                        model_name='gan_model')\n",
        "    \n",
        "        "
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3_7RVfcXIbaR",
        "outputId": "ee027e28-5497-43b0-971b-7e2f01f06926"
      },
      "source": [
        "train(models=[gen, disc, gan], x_train=x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: [discriminator loss: 0.6997567415237427, acc: 0.0] [gan loss: 0.987655, acc: 0.000000]\n",
            "1: [discriminator loss: 0.6570408940315247, acc: 0.5] [gan loss: 0.911752, acc: 0.000000]\n",
            "2: [discriminator loss: 0.6047424077987671, acc: 0.5] [gan loss: 0.809514, acc: 0.000000]\n",
            "3: [discriminator loss: 0.5252009034156799, acc: 0.5] [gan loss: 0.539394, acc: 1.000000]\n",
            "4: [discriminator loss: 0.41239386796951294, acc: 0.5] [gan loss: 0.114349, acc: 1.000000]\n",
            "5: [discriminator loss: 0.29888567328453064, acc: 0.5] [gan loss: 0.034962, acc: 1.000000]\n",
            "6: [discriminator loss: 0.2233809530735016, acc: 0.5] [gan loss: 0.000591, acc: 1.000000]\n",
            "7: [discriminator loss: 0.287579208612442, acc: 0.5] [gan loss: 0.133725, acc: 1.000000]\n",
            "8: [discriminator loss: 0.25668153166770935, acc: 0.5] [gan loss: 0.000573, acc: 1.000000]\n",
            "9: [discriminator loss: 0.18306869268417358, acc: 0.5] [gan loss: 0.000804, acc: 1.000000]\n",
            "10: [discriminator loss: 0.17841333150863647, acc: 0.5] [gan loss: 0.000481, acc: 1.000000]\n",
            "11: [discriminator loss: 0.1753663420677185, acc: 0.5] [gan loss: 0.000423, acc: 1.000000]\n",
            "12: [discriminator loss: 0.17203328013420105, acc: 0.5] [gan loss: 0.000500, acc: 1.000000]\n",
            "13: [discriminator loss: 0.17138050496578217, acc: 0.5] [gan loss: 0.000286, acc: 1.000000]\n",
            "14: [discriminator loss: 0.17084547877311707, acc: 0.5] [gan loss: 0.000285, acc: 1.000000]\n",
            "15: [discriminator loss: 0.16788536310195923, acc: 0.5] [gan loss: 0.000321, acc: 1.000000]\n",
            "16: [discriminator loss: 0.17039963603019714, acc: 0.5] [gan loss: 0.000185, acc: 1.000000]\n",
            "17: [discriminator loss: 0.1694534718990326, acc: 0.5] [gan loss: 0.000179, acc: 1.000000]\n",
            "18: [discriminator loss: 0.16770389676094055, acc: 0.5] [gan loss: 0.000172, acc: 1.000000]\n",
            "19: [discriminator loss: 0.16658005118370056, acc: 0.5] [gan loss: 0.000133, acc: 1.000000]\n",
            "20: [discriminator loss: 0.16721883416175842, acc: 0.5] [gan loss: 0.000185, acc: 1.000000]\n",
            "21: [discriminator loss: 0.1680019646883011, acc: 0.5] [gan loss: 0.000059, acc: 1.000000]\n",
            "22: [discriminator loss: 0.1704929769039154, acc: 0.5] [gan loss: 0.000350, acc: 1.000000]\n",
            "23: [discriminator loss: 0.17486684024333954, acc: 0.5] [gan loss: 0.000012, acc: 1.000000]\n",
            "24: [discriminator loss: 0.19114618003368378, acc: 0.5] [gan loss: 0.000879, acc: 1.000000]\n",
            "25: [discriminator loss: 0.18524415791034698, acc: 0.5] [gan loss: 0.000021, acc: 1.000000]\n",
            "26: [discriminator loss: 0.1906691938638687, acc: 0.5] [gan loss: 0.000672, acc: 1.000000]\n",
            "27: [discriminator loss: 0.1800617128610611, acc: 0.5] [gan loss: 0.000034, acc: 1.000000]\n",
            "28: [discriminator loss: 0.19214311242103577, acc: 0.5] [gan loss: 0.000804, acc: 1.000000]\n",
            "29: [discriminator loss: 0.1708720177412033, acc: 0.5] [gan loss: 0.000161, acc: 1.000000]\n",
            "30: [discriminator loss: 0.16947080194950104, acc: 0.5] [gan loss: 0.000470, acc: 1.000000]\n",
            "31: [discriminator loss: 0.1663089245557785, acc: 0.5] [gan loss: 0.000216, acc: 1.000000]\n",
            "32: [discriminator loss: 0.16587689518928528, acc: 0.5] [gan loss: 0.000331, acc: 1.000000]\n",
            "33: [discriminator loss: 0.16530905663967133, acc: 0.5] [gan loss: 0.000218, acc: 1.000000]\n",
            "34: [discriminator loss: 0.16518369317054749, acc: 0.5] [gan loss: 0.000290, acc: 1.000000]\n",
            "35: [discriminator loss: 0.16605019569396973, acc: 0.5] [gan loss: 0.000127, acc: 1.000000]\n",
            "36: [discriminator loss: 0.16619375348091125, acc: 0.5] [gan loss: 0.000461, acc: 1.000000]\n",
            "37: [discriminator loss: 0.16865414381027222, acc: 0.5] [gan loss: 0.000042, acc: 1.000000]\n",
            "38: [discriminator loss: 0.17747990787029266, acc: 0.5] [gan loss: 0.001143, acc: 1.000000]\n",
            "39: [discriminator loss: 0.18147361278533936, acc: 0.5] [gan loss: 0.000020, acc: 1.000000]\n",
            "40: [discriminator loss: 0.19387730956077576, acc: 0.5] [gan loss: 0.001124, acc: 1.000000]\n",
            "41: [discriminator loss: 0.16970396041870117, acc: 0.5] [gan loss: 0.000167, acc: 1.000000]\n",
            "42: [discriminator loss: 0.17070157825946808, acc: 0.5] [gan loss: 0.000766, acc: 1.000000]\n",
            "43: [discriminator loss: 0.1662568747997284, acc: 0.5] [gan loss: 0.000239, acc: 1.000000]\n",
            "44: [discriminator loss: 0.1670059859752655, acc: 0.5] [gan loss: 0.000595, acc: 1.000000]\n",
            "45: [discriminator loss: 0.16657602787017822, acc: 0.5] [gan loss: 0.000137, acc: 1.000000]\n",
            "46: [discriminator loss: 0.1676599681377411, acc: 0.5] [gan loss: 0.000582, acc: 1.000000]\n",
            "47: [discriminator loss: 0.1667485535144806, acc: 0.5] [gan loss: 0.000093, acc: 1.000000]\n",
            "48: [discriminator loss: 0.17023572325706482, acc: 0.5] [gan loss: 0.000764, acc: 1.000000]\n",
            "49: [discriminator loss: 0.17012721300125122, acc: 0.5] [gan loss: 0.000046, acc: 1.000000]\n",
            "50: [discriminator loss: 0.17723442614078522, acc: 0.5] [gan loss: 0.000974, acc: 1.000000]\n",
            "51: [discriminator loss: 0.16967006027698517, acc: 0.5] [gan loss: 0.000078, acc: 1.000000]\n",
            "52: [discriminator loss: 0.17442305386066437, acc: 0.5] [gan loss: 0.000999, acc: 1.000000]\n",
            "53: [discriminator loss: 0.17360515892505646, acc: 0.5] [gan loss: 0.000035, acc: 1.000000]\n",
            "54: [discriminator loss: 0.18281523883342743, acc: 0.5] [gan loss: 0.000991, acc: 1.000000]\n",
            "55: [discriminator loss: 0.16719090938568115, acc: 0.5] [gan loss: 0.000184, acc: 1.000000]\n",
            "56: [discriminator loss: 0.16669003665447235, acc: 0.5] [gan loss: 0.000445, acc: 1.000000]\n",
            "57: [discriminator loss: 0.16524964570999146, acc: 0.5] [gan loss: 0.000146, acc: 1.000000]\n",
            "58: [discriminator loss: 0.16564427316188812, acc: 0.5] [gan loss: 0.000356, acc: 1.000000]\n",
            "59: [discriminator loss: 0.16485503315925598, acc: 0.5] [gan loss: 0.000107, acc: 1.000000]\n",
            "60: [discriminator loss: 0.16622833907604218, acc: 0.5] [gan loss: 0.000411, acc: 1.000000]\n",
            "61: [discriminator loss: 0.16670957207679749, acc: 0.5] [gan loss: 0.000043, acc: 1.000000]\n",
            "62: [discriminator loss: 0.1714058816432953, acc: 0.5] [gan loss: 0.000593, acc: 1.000000]\n",
            "63: [discriminator loss: 0.17012500762939453, acc: 0.5] [gan loss: 0.000024, acc: 1.000000]\n",
            "64: [discriminator loss: 0.1779891401529312, acc: 0.5] [gan loss: 0.000754, acc: 1.000000]\n",
            "65: [discriminator loss: 0.17003801465034485, acc: 0.5] [gan loss: 0.000040, acc: 1.000000]\n",
            "66: [discriminator loss: 0.17272305488586426, acc: 0.5] [gan loss: 0.000524, acc: 1.000000]\n",
            "67: [discriminator loss: 0.16940400004386902, acc: 0.5] [gan loss: 0.000035, acc: 1.000000]\n",
            "68: [discriminator loss: 0.17113152146339417, acc: 0.5] [gan loss: 0.000365, acc: 1.000000]\n",
            "69: [discriminator loss: 0.16720116138458252, acc: 0.5] [gan loss: 0.000051, acc: 1.000000]\n",
            "70: [discriminator loss: 0.1693982481956482, acc: 0.5] [gan loss: 0.000375, acc: 1.000000]\n",
            "71: [discriminator loss: 0.1658525913953781, acc: 0.5] [gan loss: 0.000057, acc: 1.000000]\n",
            "72: [discriminator loss: 0.16565775871276855, acc: 0.5] [gan loss: 0.000206, acc: 1.000000]\n",
            "73: [discriminator loss: 0.1648043990135193, acc: 0.5] [gan loss: 0.000045, acc: 1.000000]\n",
            "74: [discriminator loss: 0.16642802953720093, acc: 0.5] [gan loss: 0.000250, acc: 1.000000]\n",
            "75: [discriminator loss: 0.170291006565094, acc: 0.5] [gan loss: 0.000009, acc: 1.000000]\n",
            "76: [discriminator loss: 0.1815541833639145, acc: 0.5] [gan loss: 0.000644, acc: 1.000000]\n",
            "77: [discriminator loss: 0.16972461342811584, acc: 0.5] [gan loss: 0.000030, acc: 1.000000]\n",
            "78: [discriminator loss: 0.17162634432315826, acc: 0.5] [gan loss: 0.000319, acc: 1.000000]\n",
            "79: [discriminator loss: 0.16561713814735413, acc: 0.5] [gan loss: 0.000055, acc: 1.000000]\n",
            "80: [discriminator loss: 0.1651197075843811, acc: 0.5] [gan loss: 0.000140, acc: 1.000000]\n",
            "81: [discriminator loss: 0.16476334631443024, acc: 0.5] [gan loss: 0.000039, acc: 1.000000]\n",
            "82: [discriminator loss: 0.165519580245018, acc: 0.5] [gan loss: 0.000156, acc: 1.000000]\n",
            "83: [discriminator loss: 0.16539542376995087, acc: 0.5] [gan loss: 0.000024, acc: 1.000000]\n",
            "84: [discriminator loss: 0.16645020246505737, acc: 0.5] [gan loss: 0.000172, acc: 1.000000]\n",
            "85: [discriminator loss: 0.16695396602153778, acc: 0.5] [gan loss: 0.000011, acc: 1.000000]\n",
            "86: [discriminator loss: 0.17000772058963776, acc: 0.5] [gan loss: 0.000190, acc: 1.000000]\n",
            "87: [discriminator loss: 0.16584432125091553, acc: 0.5] [gan loss: 0.000017, acc: 1.000000]\n",
            "88: [discriminator loss: 0.16675281524658203, acc: 0.5] [gan loss: 0.000132, acc: 1.000000]\n",
            "89: [discriminator loss: 0.16562245786190033, acc: 0.5] [gan loss: 0.000013, acc: 1.000000]\n",
            "90: [discriminator loss: 0.16692236065864563, acc: 0.5] [gan loss: 0.000147, acc: 1.000000]\n",
            "91: [discriminator loss: 0.1686156988143921, acc: 0.5] [gan loss: 0.000005, acc: 1.000000]\n",
            "92: [discriminator loss: 0.17761217057704926, acc: 0.5] [gan loss: 0.000363, acc: 1.000000]\n",
            "93: [discriminator loss: 0.16765758395195007, acc: 0.5] [gan loss: 0.000017, acc: 1.000000]\n",
            "94: [discriminator loss: 0.16751033067703247, acc: 0.5] [gan loss: 0.000121, acc: 1.000000]\n",
            "95: [discriminator loss: 0.16489681601524353, acc: 0.5] [gan loss: 0.000021, acc: 1.000000]\n",
            "96: [discriminator loss: 0.1651579886674881, acc: 0.5] [gan loss: 0.000067, acc: 1.000000]\n",
            "97: [discriminator loss: 0.16472946107387543, acc: 0.5] [gan loss: 0.000013, acc: 1.000000]\n",
            "98: [discriminator loss: 0.16475121676921844, acc: 0.5] [gan loss: 0.000050, acc: 1.000000]\n",
            "99: [discriminator loss: 0.16431401669979095, acc: 0.5] [gan loss: 0.000009, acc: 1.000000]\n",
            "100: [discriminator loss: 0.16520777344703674, acc: 0.5] [gan loss: 0.000054, acc: 1.000000]\n",
            "101: [discriminator loss: 0.165127694606781, acc: 0.5] [gan loss: 0.000006, acc: 1.000000]\n",
            "102: [discriminator loss: 0.1688465029001236, acc: 0.5] [gan loss: 0.000130, acc: 1.000000]\n",
            "103: [discriminator loss: 0.16944190859794617, acc: 0.5] [gan loss: 0.000003, acc: 1.000000]\n",
            "104: [discriminator loss: 0.17765118181705475, acc: 0.5] [gan loss: 0.000308, acc: 1.000000]\n",
            "105: [discriminator loss: 0.16748479008674622, acc: 0.5] [gan loss: 0.000006, acc: 1.000000]\n",
            "106: [discriminator loss: 0.1676885336637497, acc: 0.5] [gan loss: 0.000057, acc: 1.000000]\n",
            "107: [discriminator loss: 0.16421958804130554, acc: 0.5] [gan loss: 0.000013, acc: 1.000000]\n",
            "108: [discriminator loss: 0.16404294967651367, acc: 0.5] [gan loss: 0.000031, acc: 1.000000]\n",
            "109: [discriminator loss: 0.16372007131576538, acc: 0.5] [gan loss: 0.000011, acc: 1.000000]\n",
            "110: [discriminator loss: 0.16352063417434692, acc: 0.5] [gan loss: 0.000018, acc: 1.000000]\n",
            "111: [discriminator loss: 0.16425038874149323, acc: 0.5] [gan loss: 0.000006, acc: 1.000000]\n",
            "112: [discriminator loss: 0.16542214155197144, acc: 0.5] [gan loss: 0.000053, acc: 1.000000]\n",
            "113: [discriminator loss: 0.16572503745555878, acc: 0.5] [gan loss: 0.000004, acc: 1.000000]\n",
            "114: [discriminator loss: 0.16801071166992188, acc: 0.5] [gan loss: 0.000086, acc: 1.000000]\n",
            "115: [discriminator loss: 0.165578231215477, acc: 0.5] [gan loss: 0.000003, acc: 1.000000]\n",
            "116: [discriminator loss: 0.16630548238754272, acc: 0.5] [gan loss: 0.000049, acc: 1.000000]\n",
            "117: [discriminator loss: 0.1669454574584961, acc: 0.5] [gan loss: 0.000002, acc: 1.000000]\n",
            "118: [discriminator loss: 0.17042353749275208, acc: 0.5] [gan loss: 0.000105, acc: 1.000000]\n",
            "119: [discriminator loss: 0.1654461771249771, acc: 0.5] [gan loss: 0.000004, acc: 1.000000]\n",
            "120: [discriminator loss: 0.1663675457239151, acc: 0.5] [gan loss: 0.000046, acc: 1.000000]\n",
            "121: [discriminator loss: 0.16542235016822815, acc: 0.5] [gan loss: 0.000003, acc: 1.000000]\n",
            "122: [discriminator loss: 0.1663881242275238, acc: 0.5] [gan loss: 0.000043, acc: 1.000000]\n",
            "123: [discriminator loss: 0.16465182602405548, acc: 0.5] [gan loss: 0.000004, acc: 1.000000]\n",
            "124: [discriminator loss: 0.16523770987987518, acc: 0.5] [gan loss: 0.000027, acc: 1.000000]\n",
            "125: [discriminator loss: 0.16533184051513672, acc: 0.5] [gan loss: 0.000002, acc: 1.000000]\n",
            "126: [discriminator loss: 0.1659298688173294, acc: 0.5] [gan loss: 0.000031, acc: 1.000000]\n",
            "127: [discriminator loss: 0.16474053263664246, acc: 0.5] [gan loss: 0.000003, acc: 1.000000]\n",
            "128: [discriminator loss: 0.16623368859291077, acc: 0.5] [gan loss: 0.000049, acc: 1.000000]\n",
            "129: [discriminator loss: 0.1655665785074234, acc: 0.5] [gan loss: 0.000002, acc: 1.000000]\n",
            "130: [discriminator loss: 0.16630256175994873, acc: 0.5] [gan loss: 0.000043, acc: 1.000000]\n",
            "131: [discriminator loss: 0.16493481397628784, acc: 0.5] [gan loss: 0.000002, acc: 1.000000]\n",
            "132: [discriminator loss: 0.166584312915802, acc: 0.5] [gan loss: 0.000040, acc: 1.000000]\n",
            "133: [discriminator loss: 0.16547097265720367, acc: 0.5] [gan loss: 0.000001, acc: 1.000000]\n",
            "134: [discriminator loss: 0.16604307293891907, acc: 0.5] [gan loss: 0.000028, acc: 1.000000]\n",
            "135: [discriminator loss: 0.16604341566562653, acc: 0.5] [gan loss: 0.000001, acc: 1.000000]\n",
            "136: [discriminator loss: 0.1698896884918213, acc: 0.5] [gan loss: 0.000162, acc: 1.000000]\n",
            "137: [discriminator loss: 0.16371887922286987, acc: 0.5] [gan loss: 0.000001, acc: 1.000000]\n",
            "138: [discriminator loss: 0.16356173157691956, acc: 0.5] [gan loss: 0.000004, acc: 1.000000]\n",
            "139: [discriminator loss: 0.1635882407426834, acc: 0.5] [gan loss: 0.000002, acc: 1.000000]\n",
            "140: [discriminator loss: 0.16376978158950806, acc: 0.5] [gan loss: 0.000008, acc: 1.000000]\n",
            "141: [discriminator loss: 0.16374383866786957, acc: 0.5] [gan loss: 0.000002, acc: 1.000000]\n",
            "142: [discriminator loss: 0.1644679456949234, acc: 0.5] [gan loss: 0.000013, acc: 1.000000]\n",
            "143: [discriminator loss: 0.16515018045902252, acc: 0.5] [gan loss: 0.000001, acc: 1.000000]\n",
            "144: [discriminator loss: 0.1680164337158203, acc: 0.5] [gan loss: 0.000075, acc: 1.000000]\n",
            "145: [discriminator loss: 0.16701263189315796, acc: 0.5] [gan loss: 0.000001, acc: 1.000000]\n",
            "146: [discriminator loss: 0.17223955690860748, acc: 0.5] [gan loss: 0.000343, acc: 1.000000]\n",
            "147: [discriminator loss: 0.16325341165065765, acc: 0.5] [gan loss: 0.000001, acc: 1.000000]\n",
            "148: [discriminator loss: 0.16346292197704315, acc: 0.5] [gan loss: 0.000002, acc: 1.000000]\n",
            "149: [discriminator loss: 0.1636238992214203, acc: 0.5] [gan loss: 0.000001, acc: 1.000000]\n",
            "150: [discriminator loss: 0.16423587501049042, acc: 0.5] [gan loss: 0.000006, acc: 1.000000]\n",
            "151: [discriminator loss: 0.1652577519416809, acc: 0.5] [gan loss: 0.000001, acc: 1.000000]\n",
            "152: [discriminator loss: 0.16745440661907196, acc: 0.5] [gan loss: 0.000036, acc: 1.000000]\n",
            "153: [discriminator loss: 0.1660972684621811, acc: 0.5] [gan loss: 0.000002, acc: 1.000000]\n",
            "154: [discriminator loss: 0.16620603203773499, acc: 0.5] [gan loss: 0.000024, acc: 1.000000]\n",
            "155: [discriminator loss: 0.16506868600845337, acc: 0.5] [gan loss: 0.000003, acc: 1.000000]\n",
            "156: [discriminator loss: 0.1655704826116562, acc: 0.5] [gan loss: 0.000036, acc: 1.000000]\n",
            "157: [discriminator loss: 0.16379594802856445, acc: 0.5] [gan loss: 0.000005, acc: 1.000000]\n",
            "158: [discriminator loss: 0.16387499868869781, acc: 0.5] [gan loss: 0.000019, acc: 1.000000]\n",
            "159: [discriminator loss: 0.1638101041316986, acc: 0.5] [gan loss: 0.000004, acc: 1.000000]\n",
            "160: [discriminator loss: 0.1651184856891632, acc: 0.5] [gan loss: 0.000068, acc: 1.000000]\n",
            "161: [discriminator loss: 0.16388346254825592, acc: 0.5] [gan loss: 0.000002, acc: 1.000000]\n",
            "162: [discriminator loss: 0.16488808393478394, acc: 0.5] [gan loss: 0.000052, acc: 1.000000]\n",
            "163: [discriminator loss: 0.1662495732307434, acc: 0.5] [gan loss: 0.000001, acc: 1.000000]\n",
            "164: [discriminator loss: 0.18624348938465118, acc: 0.5] [gan loss: 0.047592, acc: 1.000000]\n",
            "165: [discriminator loss: 0.1653929501771927, acc: 0.5] [gan loss: 0.000024, acc: 1.000000]\n",
            "166: [discriminator loss: 0.1638166308403015, acc: 0.5] [gan loss: 0.000033, acc: 1.000000]\n",
            "167: [discriminator loss: 0.16344158351421356, acc: 0.5] [gan loss: 0.000027, acc: 1.000000]\n",
            "168: [discriminator loss: 0.1632152646780014, acc: 0.5] [gan loss: 0.000029, acc: 1.000000]\n",
            "169: [discriminator loss: 0.1631470024585724, acc: 0.5] [gan loss: 0.000027, acc: 1.000000]\n",
            "170: [discriminator loss: 0.16311143338680267, acc: 0.5] [gan loss: 0.000025, acc: 1.000000]\n",
            "171: [discriminator loss: 0.16309837996959686, acc: 0.5] [gan loss: 0.000025, acc: 1.000000]\n",
            "172: [discriminator loss: 0.1630305051803589, acc: 0.5] [gan loss: 0.000023, acc: 1.000000]\n",
            "173: [discriminator loss: 0.16313958168029785, acc: 0.5] [gan loss: 0.000028, acc: 1.000000]\n",
            "174: [discriminator loss: 0.16482886672019958, acc: 0.5] [gan loss: 0.000035, acc: 1.000000]\n",
            "175: [discriminator loss: 0.1652568280696869, acc: 0.5] [gan loss: 0.000077, acc: 1.000000]\n",
            "176: [discriminator loss: 0.16398122906684875, acc: 0.5] [gan loss: 0.000044, acc: 1.000000]\n",
            "177: [discriminator loss: 0.16378329694271088, acc: 0.5] [gan loss: 0.000073, acc: 1.000000]\n",
            "178: [discriminator loss: 0.1632663458585739, acc: 0.5] [gan loss: 0.000057, acc: 1.000000]\n",
            "179: [discriminator loss: 0.16321304440498352, acc: 0.5] [gan loss: 0.000069, acc: 1.000000]\n",
            "180: [discriminator loss: 0.16310612857341766, acc: 0.5] [gan loss: 0.000062, acc: 1.000000]\n",
            "181: [discriminator loss: 0.16334469616413116, acc: 0.5] [gan loss: 0.000103, acc: 1.000000]\n",
            "182: [discriminator loss: 0.16339629888534546, acc: 0.5] [gan loss: 0.000065, acc: 1.000000]\n",
            "183: [discriminator loss: 0.16393999755382538, acc: 0.5] [gan loss: 0.000190, acc: 1.000000]\n",
            "184: [discriminator loss: 0.16496680676937103, acc: 0.5] [gan loss: 0.000059, acc: 1.000000]\n",
            "185: [discriminator loss: 0.16667839884757996, acc: 0.5] [gan loss: 0.000642, acc: 1.000000]\n",
            "186: [discriminator loss: 0.1667046993970871, acc: 0.5] [gan loss: 0.000124, acc: 1.000000]\n",
            "187: [discriminator loss: 0.16713230311870575, acc: 0.5] [gan loss: 0.001175, acc: 1.000000]\n",
            "188: [discriminator loss: 0.1647592931985855, acc: 0.5] [gan loss: 0.000273, acc: 1.000000]\n",
            "189: [discriminator loss: 0.16468650102615356, acc: 0.5] [gan loss: 0.000833, acc: 1.000000]\n",
            "190: [discriminator loss: 0.16383644938468933, acc: 0.5] [gan loss: 0.000262, acc: 1.000000]\n",
            "191: [discriminator loss: 0.16391058266162872, acc: 0.5] [gan loss: 0.000711, acc: 1.000000]\n",
            "192: [discriminator loss: 0.1635822206735611, acc: 0.5] [gan loss: 0.000269, acc: 1.000000]\n",
            "193: [discriminator loss: 0.1636747121810913, acc: 0.5] [gan loss: 0.000763, acc: 1.000000]\n",
            "194: [discriminator loss: 0.16325005888938904, acc: 0.5] [gan loss: 0.000307, acc: 1.000000]\n",
            "195: [discriminator loss: 0.16369076073169708, acc: 0.5] [gan loss: 0.000853, acc: 1.000000]\n",
            "196: [discriminator loss: 0.16364091634750366, acc: 0.5] [gan loss: 0.000267, acc: 1.000000]\n",
            "197: [discriminator loss: 0.16477598249912262, acc: 0.5] [gan loss: 0.002927, acc: 1.000000]\n",
            "198: [discriminator loss: 0.1638483703136444, acc: 0.5] [gan loss: 0.000121, acc: 1.000000]\n",
            "199: [discriminator loss: 0.1646885871887207, acc: 0.5] [gan loss: 0.001680, acc: 1.000000]\n",
            "200: [discriminator loss: 0.16492922604084015, acc: 0.5] [gan loss: 0.000093, acc: 1.000000]\n",
            "201: [discriminator loss: 0.16729623079299927, acc: 0.5] [gan loss: 0.003874, acc: 1.000000]\n",
            "202: [discriminator loss: 0.16501060128211975, acc: 0.5] [gan loss: 0.000055, acc: 1.000000]\n",
            "203: [discriminator loss: 0.16663353145122528, acc: 0.5] [gan loss: 0.013950, acc: 1.000000]\n",
            "204: [discriminator loss: 0.16380995512008667, acc: 0.5] [gan loss: 0.000032, acc: 1.000000]\n",
            "205: [discriminator loss: 0.16379143297672272, acc: 0.5] [gan loss: 0.000170, acc: 1.000000]\n",
            "206: [discriminator loss: 0.16411583125591278, acc: 0.5] [gan loss: 0.000062, acc: 1.000000]\n",
            "207: [discriminator loss: 0.16486603021621704, acc: 0.5] [gan loss: 0.000428, acc: 1.000000]\n",
            "208: [discriminator loss: 0.16519728302955627, acc: 0.5] [gan loss: 0.000080, acc: 1.000000]\n",
            "209: [discriminator loss: 0.16501468420028687, acc: 0.5] [gan loss: 0.000936, acc: 1.000000]\n",
            "210: [discriminator loss: 0.16432547569274902, acc: 0.5] [gan loss: 0.000189, acc: 1.000000]\n",
            "211: [discriminator loss: 0.16466766595840454, acc: 0.5] [gan loss: 0.001524, acc: 1.000000]\n",
            "212: [discriminator loss: 0.16362620890140533, acc: 0.5] [gan loss: 0.000219, acc: 1.000000]\n",
            "213: [discriminator loss: 0.1637939214706421, acc: 0.5] [gan loss: 0.001103, acc: 1.000000]\n",
            "214: [discriminator loss: 0.1637706607580185, acc: 0.5] [gan loss: 0.000230, acc: 1.000000]\n",
            "215: [discriminator loss: 0.16445639729499817, acc: 0.5] [gan loss: 0.002473, acc: 1.000000]\n",
            "216: [discriminator loss: 0.16384845972061157, acc: 0.5] [gan loss: 0.000107, acc: 1.000000]\n",
            "217: [discriminator loss: 0.16418036818504333, acc: 0.5] [gan loss: 0.002046, acc: 1.000000]\n",
            "218: [discriminator loss: 0.16450223326683044, acc: 0.5] [gan loss: 0.000092, acc: 1.000000]\n",
            "219: [discriminator loss: 0.16760876774787903, acc: 0.5] [gan loss: 0.834363, acc: 0.000000]\n",
            "220: [discriminator loss: 0.1645282655954361, acc: 0.5] [gan loss: 0.000203, acc: 1.000000]\n",
            "221: [discriminator loss: 0.16515018045902252, acc: 0.5] [gan loss: 0.002201, acc: 1.000000]\n",
            "222: [discriminator loss: 0.16436293721199036, acc: 0.5] [gan loss: 0.001026, acc: 1.000000]\n",
            "223: [discriminator loss: 0.1641695499420166, acc: 0.5] [gan loss: 0.002689, acc: 1.000000]\n",
            "224: [discriminator loss: 0.16360826790332794, acc: 0.5] [gan loss: 0.001529, acc: 1.000000]\n",
            "225: [discriminator loss: 0.16351637244224548, acc: 0.5] [gan loss: 0.003706, acc: 1.000000]\n",
            "226: [discriminator loss: 0.1635010540485382, acc: 0.5] [gan loss: 0.001946, acc: 1.000000]\n",
            "227: [discriminator loss: 0.1637735813856125, acc: 0.5] [gan loss: 0.008448, acc: 1.000000]\n",
            "228: [discriminator loss: 0.16482290625572205, acc: 0.5] [gan loss: 0.002125, acc: 1.000000]\n",
            "229: [discriminator loss: 0.16628873348236084, acc: 0.5] [gan loss: 0.403438, acc: 1.000000]\n",
            "230: [discriminator loss: 0.16606636345386505, acc: 0.5] [gan loss: 2.701641, acc: 0.000000]\n",
            "231: [discriminator loss: 0.1636105179786682, acc: 0.5] [gan loss: 0.010656, acc: 1.000000]\n",
            "232: [discriminator loss: 0.1635756492614746, acc: 0.5] [gan loss: 0.004749, acc: 1.000000]\n",
            "233: [discriminator loss: 0.1639336496591568, acc: 0.5] [gan loss: 0.021003, acc: 1.000000]\n",
            "234: [discriminator loss: 0.16407591104507446, acc: 0.5] [gan loss: 0.006176, acc: 1.000000]\n",
            "235: [discriminator loss: 0.16485480964183807, acc: 0.5] [gan loss: 0.068737, acc: 1.000000]\n",
            "236: [discriminator loss: 0.16434532403945923, acc: 0.5] [gan loss: 0.010331, acc: 1.000000]\n",
            "237: [discriminator loss: 0.16451530158519745, acc: 0.5] [gan loss: 0.090405, acc: 1.000000]\n",
            "238: [discriminator loss: 0.16431379318237305, acc: 0.5] [gan loss: 0.010694, acc: 1.000000]\n",
            "239: [discriminator loss: 0.16502626240253448, acc: 0.5] [gan loss: 0.316957, acc: 1.000000]\n",
            "240: [discriminator loss: 0.16436073184013367, acc: 0.5] [gan loss: 0.010589, acc: 1.000000]\n",
            "241: [discriminator loss: 0.1651429980993271, acc: 0.5] [gan loss: 1.308226, acc: 0.000000]\n",
            "242: [discriminator loss: 0.16489440202713013, acc: 0.5] [gan loss: 1.241015, acc: 0.000000]\n",
            "243: [discriminator loss: 0.16390228271484375, acc: 0.5] [gan loss: 0.077496, acc: 1.000000]\n",
            "244: [discriminator loss: 0.1642177253961563, acc: 0.5] [gan loss: 0.016594, acc: 1.000000]\n",
            "245: [discriminator loss: 0.1654493361711502, acc: 0.5] [gan loss: 0.197165, acc: 1.000000]\n",
            "246: [discriminator loss: 0.16531844437122345, acc: 0.5] [gan loss: 0.017459, acc: 1.000000]\n",
            "247: [discriminator loss: 0.1672711819410324, acc: 0.5] [gan loss: 1.924434, acc: 0.000000]\n",
            "248: [discriminator loss: 0.16486383974552155, acc: 0.5] [gan loss: 0.238336, acc: 1.000000]\n",
            "249: [discriminator loss: 0.16381332278251648, acc: 0.5] [gan loss: 0.178895, acc: 1.000000]\n",
            "250: [discriminator loss: 0.1631745547056198, acc: 0.5] [gan loss: 0.062104, acc: 1.000000]\n",
            "251: [discriminator loss: 0.16359685361385345, acc: 0.5] [gan loss: 0.304348, acc: 1.000000]\n",
            "252: [discriminator loss: 0.1685393899679184, acc: 0.5] [gan loss: 0.008111, acc: 1.000000]\n",
            "253: [discriminator loss: 0.3410843312740326, acc: 0.5] [gan loss: 7.264035, acc: 0.000000]\n",
            "254: [discriminator loss: 0.16490623354911804, acc: 0.5] [gan loss: 1.381535, acc: 0.000000]\n",
            "255: [discriminator loss: 0.18218111991882324, acc: 0.5] [gan loss: 0.272569, acc: 1.000000]\n",
            "256: [discriminator loss: 0.17064239084720612, acc: 0.5] [gan loss: 0.191277, acc: 1.000000]\n",
            "257: [discriminator loss: 0.16738858819007874, acc: 0.5] [gan loss: 0.162347, acc: 1.000000]\n",
            "258: [discriminator loss: 0.16606861352920532, acc: 0.5] [gan loss: 0.139557, acc: 1.000000]\n",
            "259: [discriminator loss: 0.16525375843048096, acc: 0.5] [gan loss: 0.122058, acc: 1.000000]\n",
            "260: [discriminator loss: 0.16483484208583832, acc: 0.5] [gan loss: 0.107568, acc: 1.000000]\n",
            "261: [discriminator loss: 0.16498982906341553, acc: 0.5] [gan loss: 0.095813, acc: 1.000000]\n",
            "262: [discriminator loss: 0.16458283364772797, acc: 0.5] [gan loss: 0.086391, acc: 1.000000]\n",
            "263: [discriminator loss: 0.1643298715353012, acc: 0.5] [gan loss: 0.077833, acc: 1.000000]\n",
            "264: [discriminator loss: 0.16667336225509644, acc: 0.5] [gan loss: 0.070757, acc: 1.000000]\n",
            "265: [discriminator loss: 0.1657504141330719, acc: 0.5] [gan loss: 0.071776, acc: 1.000000]\n",
            "266: [discriminator loss: 0.16490347683429718, acc: 0.5] [gan loss: 0.065977, acc: 1.000000]\n",
            "267: [discriminator loss: 0.16445621848106384, acc: 0.5] [gan loss: 0.062713, acc: 1.000000]\n",
            "268: [discriminator loss: 0.16429631412029266, acc: 0.5] [gan loss: 0.057944, acc: 1.000000]\n",
            "269: [discriminator loss: 0.1648940145969391, acc: 0.5] [gan loss: 0.052726, acc: 1.000000]\n",
            "270: [discriminator loss: 0.16876724362373352, acc: 0.5] [gan loss: 0.048865, acc: 1.000000]\n",
            "271: [discriminator loss: 0.1682683378458023, acc: 0.5] [gan loss: 0.064990, acc: 1.000000]\n",
            "272: [discriminator loss: 0.16439007222652435, acc: 0.5] [gan loss: 0.056733, acc: 1.000000]\n",
            "273: [discriminator loss: 0.16473272442817688, acc: 0.5] [gan loss: 0.053044, acc: 1.000000]\n",
            "274: [discriminator loss: 0.16416974365711212, acc: 0.5] [gan loss: 0.048855, acc: 1.000000]\n",
            "275: [discriminator loss: 0.16411735117435455, acc: 0.5] [gan loss: 0.047861, acc: 1.000000]\n",
            "276: [discriminator loss: 0.16425712406635284, acc: 0.5] [gan loss: 0.044938, acc: 1.000000]\n",
            "277: [discriminator loss: 0.16412845253944397, acc: 0.5] [gan loss: 0.044916, acc: 1.000000]\n",
            "278: [discriminator loss: 0.16464191675186157, acc: 0.5] [gan loss: 0.041104, acc: 1.000000]\n",
            "279: [discriminator loss: 0.1651405245065689, acc: 0.5] [gan loss: 0.046347, acc: 1.000000]\n",
            "280: [discriminator loss: 0.1644037365913391, acc: 0.5] [gan loss: 0.047043, acc: 1.000000]\n",
            "281: [discriminator loss: 0.16722126305103302, acc: 0.5] [gan loss: 0.038467, acc: 1.000000]\n",
            "282: [discriminator loss: 0.1697750985622406, acc: 0.5] [gan loss: 0.105564, acc: 1.000000]\n",
            "283: [discriminator loss: 0.17365865409374237, acc: 0.5] [gan loss: 0.273899, acc: 1.000000]\n",
            "284: [discriminator loss: 0.21507585048675537, acc: 0.5] [gan loss: 5.059610, acc: 0.000000]\n",
            "285: [discriminator loss: 0.20575472712516785, acc: 0.5] [gan loss: 0.803642, acc: 0.000000]\n",
            "286: [discriminator loss: 0.1980956345796585, acc: 0.5] [gan loss: 0.144301, acc: 1.000000]\n",
            "287: [discriminator loss: 0.19559255242347717, acc: 0.5] [gan loss: 0.397600, acc: 1.000000]\n",
            "288: [discriminator loss: 0.16730782389640808, acc: 0.5] [gan loss: 0.253701, acc: 1.000000]\n",
            "289: [discriminator loss: 0.167218416929245, acc: 0.5] [gan loss: 0.174457, acc: 1.000000]\n",
            "290: [discriminator loss: 0.16776098310947418, acc: 0.5] [gan loss: 0.185798, acc: 1.000000]\n",
            "291: [discriminator loss: 0.1663873940706253, acc: 0.5] [gan loss: 0.149184, acc: 1.000000]\n",
            "292: [discriminator loss: 0.16835732758045197, acc: 0.5] [gan loss: 0.124191, acc: 1.000000]\n",
            "293: [discriminator loss: 0.16952203214168549, acc: 0.5] [gan loss: 0.193232, acc: 1.000000]\n",
            "294: [discriminator loss: 0.16673776507377625, acc: 0.5] [gan loss: 0.135779, acc: 1.000000]\n",
            "295: [discriminator loss: 0.16725991666316986, acc: 0.5] [gan loss: 0.119143, acc: 1.000000]\n",
            "296: [discriminator loss: 0.1675606071949005, acc: 0.5] [gan loss: 0.164580, acc: 1.000000]\n",
            "297: [discriminator loss: 0.177911639213562, acc: 0.5] [gan loss: 0.070514, acc: 1.000000]\n",
            "298: [discriminator loss: 0.1902582198381424, acc: 0.5] [gan loss: 2.156631, acc: 0.000000]\n",
            "299: [discriminator loss: 0.19293975830078125, acc: 0.5] [gan loss: 2.406112, acc: 0.000000]\n",
            "300: [discriminator loss: 0.17129381000995636, acc: 0.5] [gan loss: 0.346985, acc: 1.000000]\n",
            "301: [discriminator loss: 0.17034175992012024, acc: 0.5] [gan loss: 0.363181, acc: 1.000000]\n",
            "302: [discriminator loss: 0.1698175072669983, acc: 0.5] [gan loss: 0.467751, acc: 1.000000]\n",
            "303: [discriminator loss: 0.17082427442073822, acc: 0.5] [gan loss: 0.367140, acc: 1.000000]\n",
            "304: [discriminator loss: 0.17210672795772552, acc: 0.5] [gan loss: 0.452672, acc: 1.000000]\n",
            "305: [discriminator loss: 0.17310570180416107, acc: 0.5] [gan loss: 0.772299, acc: 0.000000]\n",
            "306: [discriminator loss: 0.1888413280248642, acc: 0.5] [gan loss: 0.964779, acc: 0.000000]\n",
            "307: [discriminator loss: 0.22218109667301178, acc: 0.5] [gan loss: 1.905551, acc: 0.000000]\n",
            "308: [discriminator loss: 0.31965160369873047, acc: 0.5] [gan loss: 0.416222, acc: 1.000000]\n",
            "309: [discriminator loss: 0.3802827000617981, acc: 0.5] [gan loss: 1.350501, acc: 0.000000]\n",
            "310: [discriminator loss: 0.21015320718288422, acc: 0.5] [gan loss: 0.574991, acc: 1.000000]\n",
            "311: [discriminator loss: 0.19189900159835815, acc: 0.5] [gan loss: 0.530156, acc: 1.000000]\n",
            "312: [discriminator loss: 0.18850389122962952, acc: 0.5] [gan loss: 0.443185, acc: 1.000000]\n",
            "313: [discriminator loss: 0.19406987726688385, acc: 0.5] [gan loss: 0.365645, acc: 1.000000]\n",
            "314: [discriminator loss: 0.19398510456085205, acc: 0.5] [gan loss: 0.376551, acc: 1.000000]\n",
            "315: [discriminator loss: 0.17636613547801971, acc: 0.5] [gan loss: 0.290411, acc: 1.000000]\n",
            "316: [discriminator loss: 0.18502019345760345, acc: 0.5] [gan loss: 0.295617, acc: 1.000000]\n",
            "317: [discriminator loss: 0.18068574368953705, acc: 0.5] [gan loss: 0.204330, acc: 1.000000]\n",
            "318: [discriminator loss: 0.19353161752223969, acc: 0.5] [gan loss: 0.265171, acc: 1.000000]\n",
            "319: [discriminator loss: 0.19647316634655, acc: 0.5] [gan loss: 0.163135, acc: 1.000000]\n",
            "320: [discriminator loss: 0.19572865962982178, acc: 0.5] [gan loss: 0.257667, acc: 1.000000]\n",
            "321: [discriminator loss: 0.1973419189453125, acc: 0.5] [gan loss: 0.134809, acc: 1.000000]\n",
            "322: [discriminator loss: 0.21665632724761963, acc: 0.5] [gan loss: 0.296769, acc: 1.000000]\n",
            "323: [discriminator loss: 0.18439078330993652, acc: 0.5] [gan loss: 0.194930, acc: 1.000000]\n",
            "324: [discriminator loss: 0.1897972673177719, acc: 0.5] [gan loss: 0.135474, acc: 1.000000]\n",
            "325: [discriminator loss: 0.19179710745811462, acc: 0.5] [gan loss: 0.189167, acc: 1.000000]\n",
            "326: [discriminator loss: 0.17988792061805725, acc: 0.5] [gan loss: 0.121609, acc: 1.000000]\n",
            "327: [discriminator loss: 0.17911331355571747, acc: 0.5] [gan loss: 0.156913, acc: 1.000000]\n",
            "328: [discriminator loss: 0.17417436838150024, acc: 0.5] [gan loss: 0.138299, acc: 1.000000]\n",
            "329: [discriminator loss: 0.17410808801651, acc: 0.5] [gan loss: 0.118276, acc: 1.000000]\n",
            "330: [discriminator loss: 0.18986442685127258, acc: 0.5] [gan loss: 0.065351, acc: 1.000000]\n",
            "331: [discriminator loss: 0.19964267313480377, acc: 0.5] [gan loss: 0.382663, acc: 1.000000]\n",
            "332: [discriminator loss: 0.3549520969390869, acc: 0.5] [gan loss: 1.547825, acc: 0.000000]\n",
            "333: [discriminator loss: 0.270859032869339, acc: 0.5] [gan loss: 0.274201, acc: 1.000000]\n",
            "334: [discriminator loss: 0.42350199818611145, acc: 0.5] [gan loss: 0.508236, acc: 1.000000]\n",
            "335: [discriminator loss: 0.20577022433280945, acc: 0.5] [gan loss: 0.301955, acc: 1.000000]\n",
            "336: [discriminator loss: 0.19060875475406647, acc: 0.5] [gan loss: 0.218591, acc: 1.000000]\n",
            "337: [discriminator loss: 0.17870227992534637, acc: 0.5] [gan loss: 0.174983, acc: 1.000000]\n",
            "338: [discriminator loss: 0.17819061875343323, acc: 0.5] [gan loss: 0.115556, acc: 1.000000]\n",
            "339: [discriminator loss: 0.17829637229442596, acc: 0.5] [gan loss: 0.110108, acc: 1.000000]\n",
            "340: [discriminator loss: 0.19175802171230316, acc: 0.5] [gan loss: 0.053222, acc: 1.000000]\n",
            "341: [discriminator loss: 0.21895278990268707, acc: 0.5] [gan loss: 0.161811, acc: 1.000000]\n",
            "342: [discriminator loss: 0.19669857621192932, acc: 0.5] [gan loss: 0.057474, acc: 1.000000]\n",
            "343: [discriminator loss: 0.21017877757549286, acc: 0.5] [gan loss: 0.149091, acc: 1.000000]\n",
            "344: [discriminator loss: 0.19351723790168762, acc: 0.5] [gan loss: 0.077887, acc: 1.000000]\n",
            "345: [discriminator loss: 0.18281134963035583, acc: 0.5] [gan loss: 0.089107, acc: 1.000000]\n",
            "346: [discriminator loss: 0.18857507407665253, acc: 0.5] [gan loss: 0.044173, acc: 1.000000]\n",
            "347: [discriminator loss: 0.1999305933713913, acc: 0.5] [gan loss: 0.325745, acc: 1.000000]\n",
            "348: [discriminator loss: 0.4549015164375305, acc: 0.5] [gan loss: 1.222742, acc: 0.000000]\n",
            "349: [discriminator loss: 0.6164525747299194, acc: 0.5] [gan loss: 0.235365, acc: 1.000000]\n",
            "350: [discriminator loss: 0.6047145128250122, acc: 0.1953125] [gan loss: 0.839222, acc: 0.000000]\n",
            "351: [discriminator loss: 0.28670817613601685, acc: 0.5] [gan loss: 0.595638, acc: 1.000000]\n",
            "352: [discriminator loss: 0.2597322463989258, acc: 0.5] [gan loss: 0.330114, acc: 1.000000]\n",
            "353: [discriminator loss: 0.25191426277160645, acc: 0.5] [gan loss: 0.527130, acc: 1.000000]\n",
            "354: [discriminator loss: 0.23045045137405396, acc: 0.5] [gan loss: 0.413067, acc: 1.000000]\n",
            "355: [discriminator loss: 0.23697882890701294, acc: 0.5] [gan loss: 0.277444, acc: 1.000000]\n",
            "356: [discriminator loss: 0.24449782073497772, acc: 0.5] [gan loss: 0.269238, acc: 1.000000]\n",
            "357: [discriminator loss: 0.23381543159484863, acc: 0.5] [gan loss: 0.544638, acc: 1.000000]\n",
            "358: [discriminator loss: 0.2658613622188568, acc: 0.5] [gan loss: 0.346585, acc: 1.000000]\n",
            "359: [discriminator loss: 0.26249319314956665, acc: 0.5] [gan loss: 0.326737, acc: 1.000000]\n",
            "360: [discriminator loss: 0.2577711045742035, acc: 0.5] [gan loss: 0.842036, acc: 0.015625]\n",
            "361: [discriminator loss: 0.3286498188972473, acc: 0.5] [gan loss: 0.498274, acc: 1.000000]\n",
            "362: [discriminator loss: 0.25111326575279236, acc: 0.5] [gan loss: 0.728348, acc: 0.328125]\n",
            "363: [discriminator loss: 0.33776232600212097, acc: 0.5] [gan loss: 0.188344, acc: 1.000000]\n",
            "364: [discriminator loss: 0.3278461694717407, acc: 0.5] [gan loss: 1.517361, acc: 0.000000]\n",
            "365: [discriminator loss: 0.28849101066589355, acc: 0.5] [gan loss: 0.636033, acc: 0.718750]\n",
            "366: [discriminator loss: 0.3282041549682617, acc: 0.5] [gan loss: 0.695854, acc: 0.500000]\n",
            "367: [discriminator loss: 0.2693314254283905, acc: 0.5] [gan loss: 0.205941, acc: 1.000000]\n",
            "368: [discriminator loss: 0.2765826880931854, acc: 0.5] [gan loss: 1.088631, acc: 0.000000]\n",
            "369: [discriminator loss: 0.3058466911315918, acc: 0.5] [gan loss: 0.143327, acc: 1.000000]\n",
            "370: [discriminator loss: 0.433828204870224, acc: 0.5] [gan loss: 1.209057, acc: 0.000000]\n",
            "371: [discriminator loss: 0.2708037793636322, acc: 0.5] [gan loss: 0.192068, acc: 1.000000]\n",
            "372: [discriminator loss: 0.30330517888069153, acc: 0.5] [gan loss: 1.049398, acc: 0.000000]\n",
            "373: [discriminator loss: 0.2499256283044815, acc: 0.5] [gan loss: 0.557058, acc: 0.984375]\n",
            "374: [discriminator loss: 0.24619898200035095, acc: 0.5] [gan loss: 0.488638, acc: 1.000000]\n",
            "375: [discriminator loss: 0.2561706006526947, acc: 0.5] [gan loss: 0.840878, acc: 0.015625]\n",
            "376: [discriminator loss: 0.34334301948547363, acc: 0.5] [gan loss: 0.356657, acc: 1.000000]\n",
            "377: [discriminator loss: 0.3674657940864563, acc: 0.5] [gan loss: 2.864369, acc: 0.000000]\n",
            "378: [discriminator loss: 0.2958964705467224, acc: 0.5] [gan loss: 0.812179, acc: 0.015625]\n",
            "379: [discriminator loss: 0.4394529461860657, acc: 0.5] [gan loss: 3.280712, acc: 0.000000]\n",
            "380: [discriminator loss: 0.4271639883518219, acc: 0.5] [gan loss: 0.075032, acc: 1.000000]\n",
            "381: [discriminator loss: 0.5787309408187866, acc: 0.5] [gan loss: 0.360802, acc: 1.000000]\n",
            "382: [discriminator loss: 0.317170649766922, acc: 0.5] [gan loss: 0.750998, acc: 0.265625]\n",
            "383: [discriminator loss: 0.26401811838150024, acc: 0.5] [gan loss: 0.586768, acc: 0.906250]\n",
            "384: [discriminator loss: 0.25910407304763794, acc: 0.5] [gan loss: 0.697556, acc: 0.468750]\n",
            "385: [discriminator loss: 0.2842574715614319, acc: 0.5] [gan loss: 0.652298, acc: 0.796875]\n",
            "386: [discriminator loss: 0.2535668611526489, acc: 0.5] [gan loss: 0.329862, acc: 1.000000]\n",
            "387: [discriminator loss: 0.2673557996749878, acc: 0.5] [gan loss: 1.119067, acc: 0.000000]\n",
            "388: [discriminator loss: 0.2562963664531708, acc: 0.5] [gan loss: 0.306083, acc: 1.000000]\n",
            "389: [discriminator loss: 0.27275341749191284, acc: 0.5] [gan loss: 0.338981, acc: 1.000000]\n",
            "390: [discriminator loss: 0.2568422853946686, acc: 0.5] [gan loss: 0.494244, acc: 1.000000]\n",
            "391: [discriminator loss: 0.27836185693740845, acc: 0.5] [gan loss: 1.468701, acc: 0.000000]\n",
            "392: [discriminator loss: 0.3092498183250427, acc: 0.5] [gan loss: 1.520781, acc: 0.000000]\n",
            "393: [discriminator loss: 0.28523436188697815, acc: 0.5] [gan loss: 0.122070, acc: 1.000000]\n",
            "394: [discriminator loss: 0.287977933883667, acc: 0.5] [gan loss: 1.649068, acc: 0.000000]\n",
            "395: [discriminator loss: 0.2556194067001343, acc: 0.5] [gan loss: 0.372603, acc: 1.000000]\n",
            "396: [discriminator loss: 0.28389760851860046, acc: 0.5] [gan loss: 3.718137, acc: 0.000000]\n",
            "397: [discriminator loss: 0.2769591808319092, acc: 0.5] [gan loss: 0.018251, acc: 1.000000]\n",
            "398: [discriminator loss: 0.43035778403282166, acc: 0.5] [gan loss: 1.216447, acc: 0.000000]\n",
            "399: [discriminator loss: 0.3553760349750519, acc: 0.5] [gan loss: 3.618076, acc: 0.000000]\n",
            "400: [discriminator loss: 0.23304671049118042, acc: 0.5] [gan loss: 0.034137, acc: 1.000000]\n",
            "401: [discriminator loss: 0.33339861035346985, acc: 0.5] [gan loss: 0.806053, acc: 0.250000]\n",
            "402: [discriminator loss: 0.2896086275577545, acc: 0.5] [gan loss: 2.667423, acc: 0.000000]\n",
            "403: [discriminator loss: 0.2163471132516861, acc: 0.5] [gan loss: 0.231767, acc: 1.000000]\n",
            "404: [discriminator loss: 0.2624845504760742, acc: 0.5] [gan loss: 1.182117, acc: 0.000000]\n",
            "405: [discriminator loss: 0.2576152980327606, acc: 0.5] [gan loss: 1.800272, acc: 0.000000]\n",
            "406: [discriminator loss: 0.2803863286972046, acc: 0.5] [gan loss: 0.141185, acc: 1.000000]\n",
            "407: [discriminator loss: 0.2575613558292389, acc: 0.5] [gan loss: 0.386679, acc: 1.000000]\n",
            "408: [discriminator loss: 0.2532891035079956, acc: 0.5] [gan loss: 4.171023, acc: 0.000000]\n",
            "409: [discriminator loss: 0.23537208139896393, acc: 0.5] [gan loss: 0.014928, acc: 1.000000]\n",
            "410: [discriminator loss: 0.34154245257377625, acc: 0.5] [gan loss: 1.958211, acc: 0.000000]\n",
            "411: [discriminator loss: 0.24295882880687714, acc: 0.5] [gan loss: 0.808254, acc: 0.187500]\n",
            "412: [discriminator loss: 0.2911146879196167, acc: 0.5] [gan loss: 4.269304, acc: 0.000000]\n",
            "413: [discriminator loss: 0.2346070557832718, acc: 0.5] [gan loss: 0.019594, acc: 1.000000]\n",
            "414: [discriminator loss: 0.3173854947090149, acc: 0.5] [gan loss: 1.152191, acc: 0.000000]\n",
            "415: [discriminator loss: 0.25403130054473877, acc: 0.5] [gan loss: 3.616901, acc: 0.000000]\n",
            "416: [discriminator loss: 0.22876957058906555, acc: 0.5] [gan loss: 0.044689, acc: 1.000000]\n",
            "417: [discriminator loss: 0.29051563143730164, acc: 0.5] [gan loss: 1.845733, acc: 0.000000]\n",
            "418: [discriminator loss: 0.2601690888404846, acc: 0.5] [gan loss: 2.957749, acc: 0.000000]\n",
            "419: [discriminator loss: 0.21813885867595673, acc: 0.5] [gan loss: 0.406588, acc: 0.968750]\n",
            "420: [discriminator loss: 0.2555645704269409, acc: 0.5] [gan loss: 2.304364, acc: 0.000000]\n",
            "421: [discriminator loss: 0.22690120339393616, acc: 0.5] [gan loss: 0.717472, acc: 0.515625]\n",
            "422: [discriminator loss: 0.2709318697452545, acc: 0.5] [gan loss: 1.894483, acc: 0.000000]\n",
            "423: [discriminator loss: 0.24944859743118286, acc: 0.5] [gan loss: 1.379461, acc: 0.000000]\n",
            "424: [discriminator loss: 0.3376758098602295, acc: 0.4921875] [gan loss: 7.554686, acc: 0.000000]\n",
            "425: [discriminator loss: 1.1317728757858276, acc: 0.5] [gan loss: 0.025570, acc: 1.000000]\n",
            "426: [discriminator loss: 1.0189390182495117, acc: 0.0078125] [gan loss: 0.193963, acc: 1.000000]\n",
            "427: [discriminator loss: 0.5009609460830688, acc: 0.40625] [gan loss: 0.514271, acc: 0.968750]\n",
            "428: [discriminator loss: 0.3520904779434204, acc: 0.5] [gan loss: 0.876406, acc: 0.015625]\n",
            "429: [discriminator loss: 0.3363119065761566, acc: 0.5] [gan loss: 1.343205, acc: 0.000000]\n",
            "430: [discriminator loss: 0.31473642587661743, acc: 0.5] [gan loss: 1.588018, acc: 0.000000]\n",
            "431: [discriminator loss: 0.3540741503238678, acc: 0.5] [gan loss: 2.386463, acc: 0.000000]\n",
            "432: [discriminator loss: 0.3155896067619324, acc: 0.5] [gan loss: 2.035534, acc: 0.000000]\n",
            "433: [discriminator loss: 0.31907108426094055, acc: 0.5] [gan loss: 1.982486, acc: 0.000000]\n",
            "434: [discriminator loss: 0.34043094515800476, acc: 0.5] [gan loss: 2.179767, acc: 0.000000]\n",
            "435: [discriminator loss: 0.3544999957084656, acc: 0.5] [gan loss: 1.565958, acc: 0.000000]\n",
            "436: [discriminator loss: 0.40340858697891235, acc: 0.484375] [gan loss: 2.000373, acc: 0.000000]\n",
            "437: [discriminator loss: 0.28880807757377625, acc: 0.5] [gan loss: 0.908668, acc: 0.046875]\n",
            "438: [discriminator loss: 0.3633418083190918, acc: 0.484375] [gan loss: 2.577395, acc: 0.000000]\n",
            "439: [discriminator loss: 0.2589816749095917, acc: 0.5] [gan loss: 0.679265, acc: 0.609375]\n",
            "440: [discriminator loss: 0.3312729597091675, acc: 0.5] [gan loss: 2.496190, acc: 0.000000]\n",
            "441: [discriminator loss: 0.25079119205474854, acc: 0.5] [gan loss: 0.579303, acc: 0.859375]\n",
            "442: [discriminator loss: 0.28652235865592957, acc: 0.5] [gan loss: 1.613173, acc: 0.000000]\n",
            "443: [discriminator loss: 0.2575344145298004, acc: 0.5] [gan loss: 1.002397, acc: 0.000000]\n",
            "444: [discriminator loss: 0.2676592469215393, acc: 0.5] [gan loss: 1.618165, acc: 0.000000]\n",
            "445: [discriminator loss: 0.23323196172714233, acc: 0.5] [gan loss: 0.711189, acc: 0.421875]\n",
            "446: [discriminator loss: 0.27238380908966064, acc: 0.5] [gan loss: 1.687005, acc: 0.000000]\n",
            "447: [discriminator loss: 0.2327749878168106, acc: 0.5] [gan loss: 0.634650, acc: 0.750000]\n",
            "448: [discriminator loss: 0.2493901401758194, acc: 0.5] [gan loss: 1.555989, acc: 0.000000]\n",
            "449: [discriminator loss: 0.23344427347183228, acc: 0.5] [gan loss: 0.529628, acc: 0.953125]\n",
            "450: [discriminator loss: 0.24432025849819183, acc: 0.5] [gan loss: 1.295158, acc: 0.000000]\n",
            "451: [discriminator loss: 0.22338752448558807, acc: 0.5] [gan loss: 0.854370, acc: 0.015625]\n",
            "452: [discriminator loss: 0.23793122172355652, acc: 0.5] [gan loss: 0.692089, acc: 0.625000]\n",
            "453: [discriminator loss: 0.24096250534057617, acc: 0.5] [gan loss: 1.502743, acc: 0.000000]\n",
            "454: [discriminator loss: 0.22650586068630219, acc: 0.5] [gan loss: 0.372874, acc: 1.000000]\n",
            "455: [discriminator loss: 0.23281130194664001, acc: 0.5] [gan loss: 0.420309, acc: 0.984375]\n",
            "456: [discriminator loss: 0.25918835401535034, acc: 0.5] [gan loss: 2.723729, acc: 0.000000]\n",
            "457: [discriminator loss: 0.26259130239486694, acc: 0.5] [gan loss: 1.389575, acc: 0.000000]\n",
            "458: [discriminator loss: 0.25112980604171753, acc: 0.5] [gan loss: 2.025450, acc: 0.000000]\n",
            "459: [discriminator loss: 0.2872883081436157, acc: 0.5] [gan loss: 4.039926, acc: 0.000000]\n",
            "460: [discriminator loss: 0.2534103989601135, acc: 0.5] [gan loss: 0.107897, acc: 1.000000]\n",
            "461: [discriminator loss: 0.4184006452560425, acc: 0.4296875] [gan loss: 5.065451, acc: 0.000000]\n",
            "462: [discriminator loss: 0.27014267444610596, acc: 0.5] [gan loss: 0.969913, acc: 0.000000]\n",
            "463: [discriminator loss: 0.38468673825263977, acc: 0.4921875] [gan loss: 5.282593, acc: 0.000000]\n",
            "464: [discriminator loss: 0.2280164361000061, acc: 0.5] [gan loss: 1.661536, acc: 0.000000]\n",
            "465: [discriminator loss: 0.33290016651153564, acc: 0.5] [gan loss: 4.602015, acc: 0.000000]\n",
            "466: [discriminator loss: 0.20823167264461517, acc: 0.5] [gan loss: 2.521150, acc: 0.000000]\n",
            "467: [discriminator loss: 0.26424309611320496, acc: 0.5] [gan loss: 3.805008, acc: 0.000000]\n",
            "468: [discriminator loss: 0.2269296497106552, acc: 0.5] [gan loss: 2.814235, acc: 0.000000]\n",
            "469: [discriminator loss: 0.25735360383987427, acc: 0.5] [gan loss: 4.091675, acc: 0.000000]\n",
            "470: [discriminator loss: 0.21148645877838135, acc: 0.5] [gan loss: 2.871698, acc: 0.000000]\n",
            "471: [discriminator loss: 0.2697332203388214, acc: 0.5] [gan loss: 5.338984, acc: 0.000000]\n",
            "472: [discriminator loss: 0.2181011140346527, acc: 0.5] [gan loss: 1.314849, acc: 0.000000]\n",
            "473: [discriminator loss: 0.43581804633140564, acc: 0.4921875] [gan loss: 6.432414, acc: 0.000000]\n",
            "474: [discriminator loss: 0.3082974851131439, acc: 0.5] [gan loss: 0.598547, acc: 0.953125]\n",
            "475: [discriminator loss: 0.7134153842926025, acc: 0.03125] [gan loss: 4.913766, acc: 0.000000]\n",
            "476: [discriminator loss: 0.28085458278656006, acc: 0.5] [gan loss: 1.533464, acc: 0.000000]\n",
            "477: [discriminator loss: 0.41671866178512573, acc: 0.4921875] [gan loss: 3.754336, acc: 0.000000]\n",
            "478: [discriminator loss: 0.2628568410873413, acc: 0.5] [gan loss: 1.647605, acc: 0.000000]\n",
            "479: [discriminator loss: 0.33091288805007935, acc: 0.5] [gan loss: 3.153425, acc: 0.000000]\n",
            "480: [discriminator loss: 0.2445400506258011, acc: 0.5] [gan loss: 1.853064, acc: 0.000000]\n",
            "481: [discriminator loss: 0.284702330827713, acc: 0.5] [gan loss: 2.686160, acc: 0.000000]\n",
            "482: [discriminator loss: 0.27668437361717224, acc: 0.5] [gan loss: 2.110040, acc: 0.000000]\n",
            "483: [discriminator loss: 0.34573519229888916, acc: 0.484375] [gan loss: 4.139538, acc: 0.000000]\n",
            "484: [discriminator loss: 0.3022022247314453, acc: 0.5] [gan loss: 0.272979, acc: 1.000000]\n",
            "485: [discriminator loss: 0.5770379900932312, acc: 0.234375] [gan loss: 8.114834, acc: 0.000000]\n",
            "486: [discriminator loss: 0.4565593898296356, acc: 0.5] [gan loss: 0.246879, acc: 1.000000]\n",
            "487: [discriminator loss: 0.447196900844574, acc: 0.46875] [gan loss: 1.733612, acc: 0.000000]\n",
            "488: [discriminator loss: 0.4321599006652832, acc: 0.453125] [gan loss: 4.530465, acc: 0.000000]\n",
            "489: [discriminator loss: 0.23102396726608276, acc: 0.5] [gan loss: 1.641440, acc: 0.000000]\n",
            "490: [discriminator loss: 0.45155274868011475, acc: 0.4375] [gan loss: 5.583611, acc: 0.000000]\n",
            "491: [discriminator loss: 0.2417605072259903, acc: 0.5] [gan loss: 1.167720, acc: 0.000000]\n",
            "492: [discriminator loss: 0.5729877948760986, acc: 0.203125] [gan loss: 6.966667, acc: 0.000000]\n",
            "493: [discriminator loss: 0.3157498836517334, acc: 0.5] [gan loss: 0.774662, acc: 0.265625]\n",
            "494: [discriminator loss: 0.828413724899292, acc: 0.015625] [gan loss: 5.855590, acc: 0.000000]\n",
            "495: [discriminator loss: 0.3425971269607544, acc: 0.5] [gan loss: 1.096182, acc: 0.000000]\n",
            "496: [discriminator loss: 0.6452924013137817, acc: 0.09375] [gan loss: 3.755621, acc: 0.000000]\n",
            "497: [discriminator loss: 0.3246817886829376, acc: 0.5] [gan loss: 1.199706, acc: 0.000000]\n",
            "498: [discriminator loss: 0.4606338143348694, acc: 0.421875] [gan loss: 3.330817, acc: 0.000000]\n",
            "499: [discriminator loss: 0.2988325357437134, acc: 0.5] [gan loss: 1.746081, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "500: [discriminator loss: 0.34555554389953613, acc: 0.5] [gan loss: 2.512326, acc: 0.000000]\n",
            "501: [discriminator loss: 0.3207136392593384, acc: 0.5] [gan loss: 1.352039, acc: 0.000000]\n",
            "502: [discriminator loss: 0.33812612295150757, acc: 0.5] [gan loss: 2.369819, acc: 0.000000]\n",
            "503: [discriminator loss: 0.2564980387687683, acc: 0.5] [gan loss: 1.275039, acc: 0.000000]\n",
            "504: [discriminator loss: 0.28584718704223633, acc: 0.5] [gan loss: 1.690533, acc: 0.000000]\n",
            "505: [discriminator loss: 0.2709035575389862, acc: 0.5] [gan loss: 1.184524, acc: 0.000000]\n",
            "506: [discriminator loss: 0.2860843539237976, acc: 0.5] [gan loss: 1.461654, acc: 0.000000]\n",
            "507: [discriminator loss: 0.256675124168396, acc: 0.5] [gan loss: 0.646904, acc: 0.718750]\n",
            "508: [discriminator loss: 0.2898266911506653, acc: 0.5] [gan loss: 1.431739, acc: 0.000000]\n",
            "509: [discriminator loss: 0.25048232078552246, acc: 0.5] [gan loss: 0.349962, acc: 1.000000]\n",
            "510: [discriminator loss: 0.27312129735946655, acc: 0.5] [gan loss: 1.190628, acc: 0.000000]\n",
            "511: [discriminator loss: 0.28777170181274414, acc: 0.5] [gan loss: 1.339719, acc: 0.000000]\n",
            "512: [discriminator loss: 0.28653261065483093, acc: 0.5] [gan loss: 1.161257, acc: 0.000000]\n",
            "513: [discriminator loss: 0.3751574158668518, acc: 0.5] [gan loss: 3.008877, acc: 0.000000]\n",
            "514: [discriminator loss: 0.3189503848552704, acc: 0.5] [gan loss: 0.454995, acc: 1.000000]\n",
            "515: [discriminator loss: 0.4038379192352295, acc: 0.46875] [gan loss: 6.028353, acc: 0.000000]\n",
            "516: [discriminator loss: 0.31237855553627014, acc: 0.5] [gan loss: 0.241769, acc: 1.000000]\n",
            "517: [discriminator loss: 0.4543665647506714, acc: 0.4375] [gan loss: 4.029559, acc: 0.000000]\n",
            "518: [discriminator loss: 0.37346288561820984, acc: 0.5] [gan loss: 1.153894, acc: 0.000000]\n",
            "519: [discriminator loss: 0.5289162397384644, acc: 0.3125] [gan loss: 5.114318, acc: 0.000000]\n",
            "520: [discriminator loss: 0.35626041889190674, acc: 0.5] [gan loss: 0.288013, acc: 1.000000]\n",
            "521: [discriminator loss: 0.5062832832336426, acc: 0.2890625] [gan loss: 3.320253, acc: 0.000000]\n",
            "522: [discriminator loss: 0.31849348545074463, acc: 0.5] [gan loss: 1.343890, acc: 0.000000]\n",
            "523: [discriminator loss: 0.4718747138977051, acc: 0.390625] [gan loss: 4.009212, acc: 0.000000]\n",
            "524: [discriminator loss: 0.33062586188316345, acc: 0.5] [gan loss: 1.093412, acc: 0.000000]\n",
            "525: [discriminator loss: 0.6488589644432068, acc: 0.09375] [gan loss: 5.533916, acc: 0.000000]\n",
            "526: [discriminator loss: 0.4129604995250702, acc: 0.5] [gan loss: 0.659087, acc: 0.687500]\n",
            "527: [discriminator loss: 0.6902388334274292, acc: 0.0234375] [gan loss: 3.388837, acc: 0.000000]\n",
            "528: [discriminator loss: 0.33973613381385803, acc: 0.5] [gan loss: 1.775749, acc: 0.000000]\n",
            "529: [discriminator loss: 0.4441418945789337, acc: 0.5] [gan loss: 2.618869, acc: 0.000000]\n",
            "530: [discriminator loss: 0.39627066254615784, acc: 0.5] [gan loss: 1.868336, acc: 0.000000]\n",
            "531: [discriminator loss: 0.4310992360115051, acc: 0.5] [gan loss: 2.786421, acc: 0.000000]\n",
            "532: [discriminator loss: 0.324184387922287, acc: 0.5] [gan loss: 2.042371, acc: 0.000000]\n",
            "533: [discriminator loss: 0.37103503942489624, acc: 0.5] [gan loss: 2.399192, acc: 0.000000]\n",
            "534: [discriminator loss: 0.3405250012874603, acc: 0.5] [gan loss: 2.013143, acc: 0.000000]\n",
            "535: [discriminator loss: 0.34491854906082153, acc: 0.5] [gan loss: 2.560878, acc: 0.000000]\n",
            "536: [discriminator loss: 0.3044750392436981, acc: 0.5] [gan loss: 2.110145, acc: 0.000000]\n",
            "537: [discriminator loss: 0.3062216639518738, acc: 0.5] [gan loss: 2.653682, acc: 0.000000]\n",
            "538: [discriminator loss: 0.2771400809288025, acc: 0.5] [gan loss: 2.347868, acc: 0.000000]\n",
            "539: [discriminator loss: 0.2897730767726898, acc: 0.5] [gan loss: 2.289479, acc: 0.000000]\n",
            "540: [discriminator loss: 0.3075109124183655, acc: 0.5] [gan loss: 2.794029, acc: 0.000000]\n",
            "541: [discriminator loss: 0.28755149245262146, acc: 0.5] [gan loss: 2.153826, acc: 0.000000]\n",
            "542: [discriminator loss: 0.3126510977745056, acc: 0.5] [gan loss: 3.431456, acc: 0.000000]\n",
            "543: [discriminator loss: 0.24838532507419586, acc: 0.5] [gan loss: 1.951066, acc: 0.000000]\n",
            "544: [discriminator loss: 0.3368784189224243, acc: 0.5] [gan loss: 4.603930, acc: 0.000000]\n",
            "545: [discriminator loss: 0.2743992209434509, acc: 0.5] [gan loss: 1.534038, acc: 0.000000]\n",
            "546: [discriminator loss: 0.41337910294532776, acc: 0.4921875] [gan loss: 5.178455, acc: 0.000000]\n",
            "547: [discriminator loss: 0.29751306772232056, acc: 0.5] [gan loss: 1.299489, acc: 0.000000]\n",
            "548: [discriminator loss: 0.5573481321334839, acc: 0.3125] [gan loss: 5.002048, acc: 0.000000]\n",
            "549: [discriminator loss: 0.2359088957309723, acc: 0.5] [gan loss: 2.076249, acc: 0.000000]\n",
            "550: [discriminator loss: 0.30598169565200806, acc: 0.5] [gan loss: 2.966640, acc: 0.000000]\n",
            "551: [discriminator loss: 0.2695160508155823, acc: 0.5] [gan loss: 2.624594, acc: 0.000000]\n",
            "552: [discriminator loss: 0.3197185695171356, acc: 0.5] [gan loss: 3.756891, acc: 0.000000]\n",
            "553: [discriminator loss: 0.24802176654338837, acc: 0.5] [gan loss: 2.151549, acc: 0.000000]\n",
            "554: [discriminator loss: 0.3792368769645691, acc: 0.5] [gan loss: 5.392988, acc: 0.000000]\n",
            "555: [discriminator loss: 0.26365023851394653, acc: 0.5] [gan loss: 1.643121, acc: 0.000000]\n",
            "556: [discriminator loss: 0.48236021399497986, acc: 0.453125] [gan loss: 6.127572, acc: 0.000000]\n",
            "557: [discriminator loss: 0.2828809916973114, acc: 0.5] [gan loss: 1.842684, acc: 0.000000]\n",
            "558: [discriminator loss: 0.5376206040382385, acc: 0.2421875] [gan loss: 6.021017, acc: 0.000000]\n",
            "559: [discriminator loss: 0.26577556133270264, acc: 0.5] [gan loss: 2.139177, acc: 0.000000]\n",
            "560: [discriminator loss: 0.33189764618873596, acc: 0.5] [gan loss: 3.494001, acc: 0.000000]\n",
            "561: [discriminator loss: 0.2485816925764084, acc: 0.5] [gan loss: 2.494425, acc: 0.000000]\n",
            "562: [discriminator loss: 0.29220423102378845, acc: 0.5] [gan loss: 3.189638, acc: 0.000000]\n",
            "563: [discriminator loss: 0.2539263963699341, acc: 0.5] [gan loss: 2.419118, acc: 0.000000]\n",
            "564: [discriminator loss: 0.2901170253753662, acc: 0.5] [gan loss: 3.390637, acc: 0.000000]\n",
            "565: [discriminator loss: 0.2405930608510971, acc: 0.5] [gan loss: 2.594420, acc: 0.000000]\n",
            "566: [discriminator loss: 0.33069169521331787, acc: 0.5] [gan loss: 3.971091, acc: 0.000000]\n",
            "567: [discriminator loss: 0.23660752177238464, acc: 0.5] [gan loss: 2.070340, acc: 0.000000]\n",
            "568: [discriminator loss: 0.3310857117176056, acc: 0.5] [gan loss: 4.282588, acc: 0.000000]\n",
            "569: [discriminator loss: 0.23561573028564453, acc: 0.5] [gan loss: 2.062150, acc: 0.000000]\n",
            "570: [discriminator loss: 0.3153950273990631, acc: 0.5] [gan loss: 3.534080, acc: 0.000000]\n",
            "571: [discriminator loss: 0.2274387925863266, acc: 0.5] [gan loss: 2.328108, acc: 0.000000]\n",
            "572: [discriminator loss: 0.3172604441642761, acc: 0.5] [gan loss: 5.384390, acc: 0.000000]\n",
            "573: [discriminator loss: 0.23840409517288208, acc: 0.5] [gan loss: 1.673830, acc: 0.000000]\n",
            "574: [discriminator loss: 0.35267195105552673, acc: 0.5] [gan loss: 4.802776, acc: 0.000000]\n",
            "575: [discriminator loss: 0.20472019910812378, acc: 0.5] [gan loss: 2.290245, acc: 0.000000]\n",
            "576: [discriminator loss: 0.282675564289093, acc: 0.5] [gan loss: 4.454582, acc: 0.000000]\n",
            "577: [discriminator loss: 0.2128792554140091, acc: 0.5] [gan loss: 1.922604, acc: 0.000000]\n",
            "578: [discriminator loss: 0.3313021659851074, acc: 0.5] [gan loss: 5.519583, acc: 0.000000]\n",
            "579: [discriminator loss: 0.22226542234420776, acc: 0.5] [gan loss: 1.844247, acc: 0.000000]\n",
            "580: [discriminator loss: 0.3157934546470642, acc: 0.5] [gan loss: 4.021712, acc: 0.000000]\n",
            "581: [discriminator loss: 0.21926690638065338, acc: 0.5] [gan loss: 1.669378, acc: 0.000000]\n",
            "582: [discriminator loss: 0.3305807113647461, acc: 0.5] [gan loss: 5.104249, acc: 0.000000]\n",
            "583: [discriminator loss: 0.1902610957622528, acc: 0.5] [gan loss: 2.261482, acc: 0.000000]\n",
            "584: [discriminator loss: 0.25680091977119446, acc: 0.5] [gan loss: 2.999467, acc: 0.000000]\n",
            "585: [discriminator loss: 0.2229735553264618, acc: 0.5] [gan loss: 2.196384, acc: 0.000000]\n",
            "586: [discriminator loss: 0.24575084447860718, acc: 0.5] [gan loss: 3.223354, acc: 0.000000]\n",
            "587: [discriminator loss: 0.21466848254203796, acc: 0.5] [gan loss: 1.973473, acc: 0.000000]\n",
            "588: [discriminator loss: 0.3037406802177429, acc: 0.5] [gan loss: 6.126791, acc: 0.000000]\n",
            "589: [discriminator loss: 0.2569183111190796, acc: 0.5] [gan loss: 1.452576, acc: 0.000000]\n",
            "590: [discriminator loss: 0.47920429706573486, acc: 0.453125] [gan loss: 7.020140, acc: 0.000000]\n",
            "591: [discriminator loss: 0.2583475410938263, acc: 0.5] [gan loss: 2.106065, acc: 0.000000]\n",
            "592: [discriminator loss: 0.3725411295890808, acc: 0.5] [gan loss: 5.606965, acc: 0.000000]\n",
            "593: [discriminator loss: 0.19329731166362762, acc: 0.5] [gan loss: 2.657363, acc: 0.000000]\n",
            "594: [discriminator loss: 0.27159687876701355, acc: 0.5] [gan loss: 3.533021, acc: 0.000000]\n",
            "595: [discriminator loss: 0.21984857320785522, acc: 0.5] [gan loss: 2.606322, acc: 0.000000]\n",
            "596: [discriminator loss: 0.2687547206878662, acc: 0.5] [gan loss: 4.151429, acc: 0.000000]\n",
            "597: [discriminator loss: 0.20898696780204773, acc: 0.5] [gan loss: 2.271721, acc: 0.000000]\n",
            "598: [discriminator loss: 0.3855798840522766, acc: 0.484375] [gan loss: 8.393742, acc: 0.000000]\n",
            "599: [discriminator loss: 0.2547489106655121, acc: 0.5] [gan loss: 2.497872, acc: 0.000000]\n",
            "600: [discriminator loss: 0.31222185492515564, acc: 0.5] [gan loss: 3.649582, acc: 0.000000]\n",
            "601: [discriminator loss: 0.22264927625656128, acc: 0.5] [gan loss: 2.448199, acc: 0.000000]\n",
            "602: [discriminator loss: 0.2743397057056427, acc: 0.5] [gan loss: 3.743432, acc: 0.000000]\n",
            "603: [discriminator loss: 0.22294257581233978, acc: 0.5] [gan loss: 1.536781, acc: 0.000000]\n",
            "604: [discriminator loss: 0.35775765776634216, acc: 0.46875] [gan loss: 6.210592, acc: 0.000000]\n",
            "605: [discriminator loss: 0.21299727261066437, acc: 0.5] [gan loss: 1.364352, acc: 0.000000]\n",
            "606: [discriminator loss: 0.3071964979171753, acc: 0.5] [gan loss: 2.820179, acc: 0.000000]\n",
            "607: [discriminator loss: 0.22587169706821442, acc: 0.5] [gan loss: 0.628551, acc: 0.750000]\n",
            "608: [discriminator loss: 0.25652483105659485, acc: 0.5] [gan loss: 1.475793, acc: 0.000000]\n",
            "609: [discriminator loss: 0.22705478966236115, acc: 0.5] [gan loss: 0.905873, acc: 0.078125]\n",
            "610: [discriminator loss: 0.2513076663017273, acc: 0.5] [gan loss: 0.556193, acc: 0.921875]\n",
            "611: [discriminator loss: 0.2499806433916092, acc: 0.5] [gan loss: 0.827958, acc: 0.140625]\n",
            "612: [discriminator loss: 0.2416944056749344, acc: 0.5] [gan loss: 1.813653, acc: 0.000000]\n",
            "613: [discriminator loss: 0.20833241939544678, acc: 0.5] [gan loss: 0.509094, acc: 1.000000]\n",
            "614: [discriminator loss: 0.21138213574886322, acc: 0.5] [gan loss: 0.134598, acc: 1.000000]\n",
            "615: [discriminator loss: 0.21519897878170013, acc: 0.5] [gan loss: 0.395116, acc: 1.000000]\n",
            "616: [discriminator loss: 0.21449071168899536, acc: 0.5] [gan loss: 0.287886, acc: 1.000000]\n",
            "617: [discriminator loss: 0.21027904748916626, acc: 0.5] [gan loss: 1.007117, acc: 0.000000]\n",
            "618: [discriminator loss: 0.21342137455940247, acc: 0.5] [gan loss: 0.097762, acc: 1.000000]\n",
            "619: [discriminator loss: 0.19944337010383606, acc: 0.5] [gan loss: 0.323959, acc: 1.000000]\n",
            "620: [discriminator loss: 0.1994350254535675, acc: 0.5] [gan loss: 0.104145, acc: 1.000000]\n",
            "621: [discriminator loss: 0.21134541928768158, acc: 0.5] [gan loss: 0.113382, acc: 1.000000]\n",
            "622: [discriminator loss: 0.19470061361789703, acc: 0.5] [gan loss: 0.485420, acc: 0.984375]\n",
            "623: [discriminator loss: 0.20194865763187408, acc: 0.5] [gan loss: 0.089767, acc: 1.000000]\n",
            "624: [discriminator loss: 0.1984643191099167, acc: 0.5] [gan loss: 0.296629, acc: 1.000000]\n",
            "625: [discriminator loss: 0.2061167061328888, acc: 0.5] [gan loss: 0.443082, acc: 1.000000]\n",
            "626: [discriminator loss: 0.20792677998542786, acc: 0.5] [gan loss: 0.196816, acc: 1.000000]\n",
            "627: [discriminator loss: 0.1955956369638443, acc: 0.5] [gan loss: 0.197876, acc: 1.000000]\n",
            "628: [discriminator loss: 0.18584342300891876, acc: 0.5] [gan loss: 0.546751, acc: 0.921875]\n",
            "629: [discriminator loss: 0.21066588163375854, acc: 0.5] [gan loss: 0.028713, acc: 1.000000]\n",
            "630: [discriminator loss: 0.20000296831130981, acc: 0.5] [gan loss: 0.418875, acc: 1.000000]\n",
            "631: [discriminator loss: 0.1899757981300354, acc: 0.5] [gan loss: 0.110142, acc: 1.000000]\n",
            "632: [discriminator loss: 0.18405909836292267, acc: 0.5] [gan loss: 0.227645, acc: 1.000000]\n",
            "633: [discriminator loss: 0.18626557290554047, acc: 0.5] [gan loss: 0.193797, acc: 1.000000]\n",
            "634: [discriminator loss: 0.18374119699001312, acc: 0.5] [gan loss: 0.345809, acc: 1.000000]\n",
            "635: [discriminator loss: 0.1926659196615219, acc: 0.5] [gan loss: 1.030376, acc: 0.000000]\n",
            "636: [discriminator loss: 0.20647358894348145, acc: 0.5] [gan loss: 0.872789, acc: 0.125000]\n",
            "637: [discriminator loss: 0.28234514594078064, acc: 0.5] [gan loss: 10.590000, acc: 0.000000]\n",
            "638: [discriminator loss: 0.32519668340682983, acc: 0.5] [gan loss: 0.000975, acc: 1.000000]\n",
            "639: [discriminator loss: 0.800734281539917, acc: 0.1484375] [gan loss: 3.836978, acc: 0.000000]\n",
            "640: [discriminator loss: 0.21545232832431793, acc: 0.5] [gan loss: 0.531253, acc: 0.906250]\n",
            "641: [discriminator loss: 0.2854560911655426, acc: 0.4921875] [gan loss: 2.305067, acc: 0.000000]\n",
            "642: [discriminator loss: 0.36414915323257446, acc: 0.46875] [gan loss: 4.619024, acc: 0.000000]\n",
            "643: [discriminator loss: 0.2489185482263565, acc: 0.5] [gan loss: 0.758405, acc: 0.375000]\n",
            "644: [discriminator loss: 0.37060660123825073, acc: 0.46875] [gan loss: 8.711632, acc: 0.000000]\n",
            "645: [discriminator loss: 0.3450334370136261, acc: 0.5] [gan loss: 0.176613, acc: 1.000000]\n",
            "646: [discriminator loss: 0.44554731249809265, acc: 0.4140625] [gan loss: 5.877172, acc: 0.000000]\n",
            "647: [discriminator loss: 0.20604461431503296, acc: 0.5] [gan loss: 1.628564, acc: 0.000000]\n",
            "648: [discriminator loss: 0.40776920318603516, acc: 0.4453125] [gan loss: 7.549773, acc: 0.000000]\n",
            "649: [discriminator loss: 0.2670316696166992, acc: 0.5] [gan loss: 1.296865, acc: 0.000000]\n",
            "650: [discriminator loss: 0.47236010432243347, acc: 0.3671875] [gan loss: 5.957568, acc: 0.000000]\n",
            "651: [discriminator loss: 0.24700285494327545, acc: 0.5] [gan loss: 1.613245, acc: 0.000000]\n",
            "652: [discriminator loss: 0.3913607597351074, acc: 0.484375] [gan loss: 4.601323, acc: 0.000000]\n",
            "653: [discriminator loss: 0.2266538143157959, acc: 0.5] [gan loss: 2.006260, acc: 0.000000]\n",
            "654: [discriminator loss: 0.3147345781326294, acc: 0.5] [gan loss: 3.200011, acc: 0.000000]\n",
            "655: [discriminator loss: 0.2645151615142822, acc: 0.5] [gan loss: 1.739817, acc: 0.000000]\n",
            "656: [discriminator loss: 0.3598865568637848, acc: 0.5] [gan loss: 4.568932, acc: 0.000000]\n",
            "657: [discriminator loss: 0.22625920176506042, acc: 0.5] [gan loss: 1.955709, acc: 0.000000]\n",
            "658: [discriminator loss: 0.3922725319862366, acc: 0.5] [gan loss: 4.597350, acc: 0.000000]\n",
            "659: [discriminator loss: 0.2427043467760086, acc: 0.5] [gan loss: 1.535944, acc: 0.000000]\n",
            "660: [discriminator loss: 0.4239504635334015, acc: 0.484375] [gan loss: 5.445041, acc: 0.000000]\n",
            "661: [discriminator loss: 0.30701228976249695, acc: 0.5] [gan loss: 1.045226, acc: 0.015625]\n",
            "662: [discriminator loss: 0.4744204580783844, acc: 0.40625] [gan loss: 4.554387, acc: 0.000000]\n",
            "663: [discriminator loss: 0.23526304960250854, acc: 0.5] [gan loss: 2.143418, acc: 0.000000]\n",
            "664: [discriminator loss: 0.35226622223854065, acc: 0.5] [gan loss: 3.831483, acc: 0.000000]\n",
            "665: [discriminator loss: 0.24408678710460663, acc: 0.5] [gan loss: 2.063097, acc: 0.000000]\n",
            "666: [discriminator loss: 0.4053391218185425, acc: 0.46875] [gan loss: 5.791116, acc: 0.000000]\n",
            "667: [discriminator loss: 0.2819805145263672, acc: 0.5] [gan loss: 1.436384, acc: 0.000000]\n",
            "668: [discriminator loss: 0.40894946455955505, acc: 0.453125] [gan loss: 4.149136, acc: 0.000000]\n",
            "669: [discriminator loss: 0.24191144108772278, acc: 0.5] [gan loss: 1.949590, acc: 0.000000]\n",
            "670: [discriminator loss: 0.33350834250450134, acc: 0.5] [gan loss: 3.426059, acc: 0.000000]\n",
            "671: [discriminator loss: 0.28787755966186523, acc: 0.5] [gan loss: 2.217361, acc: 0.000000]\n",
            "672: [discriminator loss: 0.3051452934741974, acc: 0.5] [gan loss: 3.606535, acc: 0.000000]\n",
            "673: [discriminator loss: 0.24984833598136902, acc: 0.5] [gan loss: 2.069059, acc: 0.000000]\n",
            "674: [discriminator loss: 0.3424798250198364, acc: 0.4921875] [gan loss: 4.128103, acc: 0.000000]\n",
            "675: [discriminator loss: 0.2347841113805771, acc: 0.5] [gan loss: 1.772381, acc: 0.000000]\n",
            "676: [discriminator loss: 0.34992438554763794, acc: 0.4921875] [gan loss: 4.212315, acc: 0.000000]\n",
            "677: [discriminator loss: 0.2213638722896576, acc: 0.5] [gan loss: 1.768333, acc: 0.000000]\n",
            "678: [discriminator loss: 0.3284895718097687, acc: 0.4921875] [gan loss: 4.037949, acc: 0.000000]\n",
            "679: [discriminator loss: 0.25465458631515503, acc: 0.5] [gan loss: 1.283834, acc: 0.000000]\n",
            "680: [discriminator loss: 0.43796080350875854, acc: 0.3828125] [gan loss: 7.033566, acc: 0.000000]\n",
            "681: [discriminator loss: 0.3443336486816406, acc: 0.5] [gan loss: 1.271515, acc: 0.000000]\n",
            "682: [discriminator loss: 0.44205451011657715, acc: 0.4453125] [gan loss: 4.162999, acc: 0.000000]\n",
            "683: [discriminator loss: 0.2437264770269394, acc: 0.5] [gan loss: 1.911924, acc: 0.000000]\n",
            "684: [discriminator loss: 0.3658427596092224, acc: 0.5] [gan loss: 4.292186, acc: 0.000000]\n",
            "685: [discriminator loss: 0.2655138075351715, acc: 0.5] [gan loss: 1.631402, acc: 0.000000]\n",
            "686: [discriminator loss: 0.3611840009689331, acc: 0.5] [gan loss: 3.863514, acc: 0.000000]\n",
            "687: [discriminator loss: 0.24233148992061615, acc: 0.5] [gan loss: 1.890282, acc: 0.000000]\n",
            "688: [discriminator loss: 0.31757301092147827, acc: 0.5] [gan loss: 2.863511, acc: 0.000000]\n",
            "689: [discriminator loss: 0.2883976399898529, acc: 0.5] [gan loss: 1.158418, acc: 0.000000]\n",
            "690: [discriminator loss: 0.3343619108200073, acc: 0.4921875] [gan loss: 2.579258, acc: 0.000000]\n",
            "691: [discriminator loss: 0.26922905445098877, acc: 0.5] [gan loss: 0.893649, acc: 0.093750]\n",
            "692: [discriminator loss: 0.3029484748840332, acc: 0.5] [gan loss: 1.875483, acc: 0.000000]\n",
            "693: [discriminator loss: 0.24629826843738556, acc: 0.5] [gan loss: 0.665576, acc: 0.703125]\n",
            "694: [discriminator loss: 0.27090975642204285, acc: 0.5] [gan loss: 1.131535, acc: 0.000000]\n",
            "695: [discriminator loss: 0.2496856451034546, acc: 0.4921875] [gan loss: 0.578351, acc: 0.828125]\n",
            "696: [discriminator loss: 0.2565879225730896, acc: 0.5] [gan loss: 1.470038, acc: 0.000000]\n",
            "697: [discriminator loss: 0.2679719030857086, acc: 0.5] [gan loss: 0.359946, acc: 1.000000]\n",
            "698: [discriminator loss: 0.2607869803905487, acc: 0.5] [gan loss: 1.078466, acc: 0.015625]\n",
            "699: [discriminator loss: 0.2243347018957138, acc: 0.5] [gan loss: 0.766115, acc: 0.375000]\n",
            "700: [discriminator loss: 0.23042666912078857, acc: 0.5] [gan loss: 0.449486, acc: 0.906250]\n",
            "701: [discriminator loss: 0.2365356683731079, acc: 0.5] [gan loss: 1.418890, acc: 0.000000]\n",
            "702: [discriminator loss: 0.2506598234176636, acc: 0.5] [gan loss: 2.141517, acc: 0.000000]\n",
            "703: [discriminator loss: 0.24215056002140045, acc: 0.5] [gan loss: 1.732834, acc: 0.000000]\n",
            "704: [discriminator loss: 0.24637100100517273, acc: 0.5] [gan loss: 2.378182, acc: 0.000000]\n",
            "705: [discriminator loss: 0.278844952583313, acc: 0.5] [gan loss: 1.985298, acc: 0.000000]\n",
            "706: [discriminator loss: 0.25328847765922546, acc: 0.5] [gan loss: 4.784478, acc: 0.000000]\n",
            "707: [discriminator loss: 0.23415948450565338, acc: 0.5] [gan loss: 0.297841, acc: 1.000000]\n",
            "708: [discriminator loss: 0.2802613377571106, acc: 0.5] [gan loss: 5.709972, acc: 0.000000]\n",
            "709: [discriminator loss: 0.22641313076019287, acc: 0.5] [gan loss: 0.188927, acc: 1.000000]\n",
            "710: [discriminator loss: 0.27737441658973694, acc: 0.4921875] [gan loss: 5.255974, acc: 0.000000]\n",
            "711: [discriminator loss: 0.19667668640613556, acc: 0.5] [gan loss: 1.115971, acc: 0.031250]\n",
            "712: [discriminator loss: 0.3240949511528015, acc: 0.4921875] [gan loss: 11.159481, acc: 0.000000]\n",
            "713: [discriminator loss: 0.29247093200683594, acc: 0.5] [gan loss: 0.820939, acc: 0.265625]\n",
            "714: [discriminator loss: 0.4976816773414612, acc: 0.375] [gan loss: 9.482183, acc: 0.000000]\n",
            "715: [discriminator loss: 0.265713632106781, acc: 0.5] [gan loss: 3.265956, acc: 0.000000]\n",
            "716: [discriminator loss: 0.24182313680648804, acc: 0.5] [gan loss: 2.198296, acc: 0.000000]\n",
            "717: [discriminator loss: 0.2701336145401001, acc: 0.5] [gan loss: 3.150030, acc: 0.000000]\n",
            "718: [discriminator loss: 0.2511693239212036, acc: 0.5] [gan loss: 2.559552, acc: 0.000000]\n",
            "719: [discriminator loss: 0.26447564363479614, acc: 0.5] [gan loss: 3.572113, acc: 0.000000]\n",
            "720: [discriminator loss: 0.25952285528182983, acc: 0.5] [gan loss: 2.808002, acc: 0.000000]\n",
            "721: [discriminator loss: 0.26254165172576904, acc: 0.5] [gan loss: 3.447174, acc: 0.000000]\n",
            "722: [discriminator loss: 0.2460615485906601, acc: 0.5] [gan loss: 3.386592, acc: 0.000000]\n",
            "723: [discriminator loss: 0.24022580683231354, acc: 0.5] [gan loss: 3.575777, acc: 0.000000]\n",
            "724: [discriminator loss: 0.252407044172287, acc: 0.5] [gan loss: 3.352463, acc: 0.000000]\n",
            "725: [discriminator loss: 0.25241386890411377, acc: 0.5] [gan loss: 4.544622, acc: 0.000000]\n",
            "726: [discriminator loss: 0.23284612596035004, acc: 0.5] [gan loss: 1.944425, acc: 0.000000]\n",
            "727: [discriminator loss: 0.38108187913894653, acc: 0.5] [gan loss: 8.525608, acc: 0.000000]\n",
            "728: [discriminator loss: 0.40421953797340393, acc: 0.5] [gan loss: 1.649838, acc: 0.000000]\n",
            "729: [discriminator loss: 0.448168009519577, acc: 0.46875] [gan loss: 5.154418, acc: 0.000000]\n",
            "730: [discriminator loss: 0.21582677960395813, acc: 0.5] [gan loss: 2.627605, acc: 0.000000]\n",
            "731: [discriminator loss: 0.3019639551639557, acc: 0.5] [gan loss: 3.138278, acc: 0.000000]\n",
            "732: [discriminator loss: 0.22860068082809448, acc: 0.5] [gan loss: 1.927720, acc: 0.000000]\n",
            "733: [discriminator loss: 0.3263612687587738, acc: 0.4921875] [gan loss: 3.542451, acc: 0.000000]\n",
            "734: [discriminator loss: 0.2129518687725067, acc: 0.5] [gan loss: 1.125519, acc: 0.000000]\n",
            "735: [discriminator loss: 0.28418439626693726, acc: 0.5] [gan loss: 1.967787, acc: 0.000000]\n",
            "736: [discriminator loss: 0.23461972177028656, acc: 0.5] [gan loss: 0.933371, acc: 0.140625]\n",
            "737: [discriminator loss: 0.24495013058185577, acc: 0.5] [gan loss: 1.248600, acc: 0.000000]\n",
            "738: [discriminator loss: 0.24869248270988464, acc: 0.5] [gan loss: 0.954671, acc: 0.078125]\n",
            "739: [discriminator loss: 0.2732255160808563, acc: 0.4921875] [gan loss: 1.106747, acc: 0.015625]\n",
            "740: [discriminator loss: 0.2411787211894989, acc: 0.5] [gan loss: 0.947274, acc: 0.109375]\n",
            "741: [discriminator loss: 0.25218790769577026, acc: 0.5] [gan loss: 0.785446, acc: 0.375000]\n",
            "742: [discriminator loss: 0.2268981784582138, acc: 0.5] [gan loss: 0.587475, acc: 0.812500]\n",
            "743: [discriminator loss: 0.21900469064712524, acc: 0.5] [gan loss: 0.635401, acc: 0.671875]\n",
            "744: [discriminator loss: 0.22865813970565796, acc: 0.5] [gan loss: 0.263295, acc: 1.000000]\n",
            "745: [discriminator loss: 0.22528760135173798, acc: 0.5] [gan loss: 1.291839, acc: 0.015625]\n",
            "746: [discriminator loss: 0.20244449377059937, acc: 0.5] [gan loss: 0.236368, acc: 1.000000]\n",
            "747: [discriminator loss: 0.22191214561462402, acc: 0.5] [gan loss: 0.338048, acc: 0.984375]\n",
            "748: [discriminator loss: 0.21114835143089294, acc: 0.5] [gan loss: 0.174805, acc: 1.000000]\n",
            "749: [discriminator loss: 0.23023946583271027, acc: 0.5] [gan loss: 0.427338, acc: 0.984375]\n",
            "750: [discriminator loss: 0.2132808119058609, acc: 0.5] [gan loss: 0.551062, acc: 0.859375]\n",
            "751: [discriminator loss: 0.20024912059307098, acc: 0.5] [gan loss: 0.243947, acc: 1.000000]\n",
            "752: [discriminator loss: 0.2136070430278778, acc: 0.5] [gan loss: 0.095554, acc: 1.000000]\n",
            "753: [discriminator loss: 0.2122265100479126, acc: 0.5] [gan loss: 1.320199, acc: 0.015625]\n",
            "754: [discriminator loss: 0.22152277827262878, acc: 0.5] [gan loss: 0.298339, acc: 1.000000]\n",
            "755: [discriminator loss: 0.23063164949417114, acc: 0.5] [gan loss: 0.632706, acc: 0.671875]\n",
            "756: [discriminator loss: 0.22762107849121094, acc: 0.5] [gan loss: 0.992263, acc: 0.093750]\n",
            "757: [discriminator loss: 0.3028099238872528, acc: 0.4921875] [gan loss: 8.144669, acc: 0.000000]\n",
            "758: [discriminator loss: 0.35347071290016174, acc: 0.5] [gan loss: 0.041885, acc: 1.000000]\n",
            "759: [discriminator loss: 0.3078659176826477, acc: 0.5] [gan loss: 0.127216, acc: 1.000000]\n",
            "760: [discriminator loss: 0.22860391438007355, acc: 0.5] [gan loss: 0.207664, acc: 1.000000]\n",
            "761: [discriminator loss: 0.20614221692085266, acc: 0.5] [gan loss: 0.219099, acc: 1.000000]\n",
            "762: [discriminator loss: 0.2022680938243866, acc: 0.5] [gan loss: 0.339999, acc: 1.000000]\n",
            "763: [discriminator loss: 0.24094516038894653, acc: 0.5] [gan loss: 2.886433, acc: 0.000000]\n",
            "764: [discriminator loss: 0.28385239839553833, acc: 0.4921875] [gan loss: 5.960057, acc: 0.000000]\n",
            "765: [discriminator loss: 0.20145808160305023, acc: 0.5] [gan loss: 0.308218, acc: 0.984375]\n",
            "766: [discriminator loss: 0.2706366181373596, acc: 0.5] [gan loss: 3.598604, acc: 0.000000]\n",
            "767: [discriminator loss: 0.2722737491130829, acc: 0.5] [gan loss: 2.667584, acc: 0.000000]\n",
            "768: [discriminator loss: 0.28008174896240234, acc: 0.5] [gan loss: 5.205419, acc: 0.000000]\n",
            "769: [discriminator loss: 0.22258247435092926, acc: 0.5] [gan loss: 0.391711, acc: 1.000000]\n",
            "770: [discriminator loss: 0.4401279091835022, acc: 0.421875] [gan loss: 9.780365, acc: 0.000000]\n",
            "771: [discriminator loss: 0.43067580461502075, acc: 0.5] [gan loss: 0.665622, acc: 0.578125]\n",
            "772: [discriminator loss: 0.5123438239097595, acc: 0.328125] [gan loss: 5.176359, acc: 0.000000]\n",
            "773: [discriminator loss: 0.2113492637872696, acc: 0.5] [gan loss: 1.238386, acc: 0.000000]\n",
            "774: [discriminator loss: 0.35993099212646484, acc: 0.484375] [gan loss: 4.234835, acc: 0.000000]\n",
            "775: [discriminator loss: 0.22158688306808472, acc: 0.5] [gan loss: 0.735860, acc: 0.453125]\n",
            "776: [discriminator loss: 0.3501051962375641, acc: 0.4921875] [gan loss: 4.139863, acc: 0.000000]\n",
            "777: [discriminator loss: 0.22428834438323975, acc: 0.5] [gan loss: 0.537456, acc: 0.937500]\n",
            "778: [discriminator loss: 0.2878798246383667, acc: 0.5] [gan loss: 2.297461, acc: 0.000000]\n",
            "779: [discriminator loss: 0.2198590636253357, acc: 0.5] [gan loss: 0.675453, acc: 0.640625]\n",
            "780: [discriminator loss: 0.27481305599212646, acc: 0.5] [gan loss: 2.339315, acc: 0.000000]\n",
            "781: [discriminator loss: 0.23085694015026093, acc: 0.5] [gan loss: 1.020370, acc: 0.046875]\n",
            "782: [discriminator loss: 0.32412058115005493, acc: 0.4921875] [gan loss: 5.771301, acc: 0.000000]\n",
            "783: [discriminator loss: 0.2552100718021393, acc: 0.5] [gan loss: 0.605560, acc: 0.781250]\n",
            "784: [discriminator loss: 0.3377586603164673, acc: 0.4921875] [gan loss: 4.733146, acc: 0.000000]\n",
            "785: [discriminator loss: 0.2153087556362152, acc: 0.5] [gan loss: 0.739537, acc: 0.421875]\n",
            "786: [discriminator loss: 0.2809138894081116, acc: 0.5] [gan loss: 2.144995, acc: 0.000000]\n",
            "787: [discriminator loss: 0.2624914050102234, acc: 0.5] [gan loss: 2.728300, acc: 0.000000]\n",
            "788: [discriminator loss: 0.2276378720998764, acc: 0.5] [gan loss: 0.255634, acc: 1.000000]\n",
            "789: [discriminator loss: 0.24352173507213593, acc: 0.5] [gan loss: 1.322747, acc: 0.000000]\n",
            "790: [discriminator loss: 0.27040761709213257, acc: 0.5] [gan loss: 1.703072, acc: 0.000000]\n",
            "791: [discriminator loss: 0.21660174429416656, acc: 0.5] [gan loss: 0.651887, acc: 0.703125]\n",
            "792: [discriminator loss: 0.25206661224365234, acc: 0.5] [gan loss: 1.797040, acc: 0.000000]\n",
            "793: [discriminator loss: 0.22555431723594666, acc: 0.5] [gan loss: 0.227754, acc: 1.000000]\n",
            "794: [discriminator loss: 0.2316868156194687, acc: 0.5] [gan loss: 0.157612, acc: 1.000000]\n",
            "795: [discriminator loss: 0.2244759052991867, acc: 0.5] [gan loss: 0.675888, acc: 0.625000]\n",
            "796: [discriminator loss: 0.24180178344249725, acc: 0.5] [gan loss: 2.873883, acc: 0.000000]\n",
            "797: [discriminator loss: 0.2070743590593338, acc: 0.5] [gan loss: 0.031853, acc: 1.000000]\n",
            "798: [discriminator loss: 0.20332901179790497, acc: 0.5] [gan loss: 0.127385, acc: 1.000000]\n",
            "799: [discriminator loss: 0.22234340012073517, acc: 0.5] [gan loss: 0.060213, acc: 1.000000]\n",
            "800: [discriminator loss: 0.22596119344234467, acc: 0.5] [gan loss: 0.633593, acc: 0.656250]\n",
            "801: [discriminator loss: 0.25318947434425354, acc: 0.5] [gan loss: 2.592349, acc: 0.000000]\n",
            "802: [discriminator loss: 0.19776751101016998, acc: 0.5] [gan loss: 0.075853, acc: 1.000000]\n",
            "803: [discriminator loss: 0.19349926710128784, acc: 0.5] [gan loss: 0.089592, acc: 1.000000]\n",
            "804: [discriminator loss: 0.2139700949192047, acc: 0.5] [gan loss: 0.263623, acc: 1.000000]\n",
            "805: [discriminator loss: 0.23636306822299957, acc: 0.5] [gan loss: 1.781787, acc: 0.000000]\n",
            "806: [discriminator loss: 0.2282402217388153, acc: 0.5] [gan loss: 0.130119, acc: 1.000000]\n",
            "807: [discriminator loss: 0.22402538359165192, acc: 0.5] [gan loss: 1.825525, acc: 0.000000]\n",
            "808: [discriminator loss: 0.22759145498275757, acc: 0.5] [gan loss: 0.439000, acc: 0.968750]\n",
            "809: [discriminator loss: 0.25361305475234985, acc: 0.5] [gan loss: 7.474053, acc: 0.000000]\n",
            "810: [discriminator loss: 0.285885751247406, acc: 0.5] [gan loss: 0.013339, acc: 1.000000]\n",
            "811: [discriminator loss: 0.29699772596359253, acc: 0.5] [gan loss: 0.170421, acc: 1.000000]\n",
            "812: [discriminator loss: 0.20287640392780304, acc: 0.5] [gan loss: 0.179134, acc: 1.000000]\n",
            "813: [discriminator loss: 0.2075149416923523, acc: 0.5] [gan loss: 0.138156, acc: 1.000000]\n",
            "814: [discriminator loss: 0.21221128106117249, acc: 0.5] [gan loss: 0.507840, acc: 0.937500]\n",
            "815: [discriminator loss: 0.29699331521987915, acc: 0.5] [gan loss: 6.774966, acc: 0.000000]\n",
            "816: [discriminator loss: 0.19013041257858276, acc: 0.5] [gan loss: 0.718415, acc: 0.453125]\n",
            "817: [discriminator loss: 0.3234368562698364, acc: 0.5] [gan loss: 10.023489, acc: 0.000000]\n",
            "818: [discriminator loss: 0.4512479901313782, acc: 0.5] [gan loss: 0.041729, acc: 1.000000]\n",
            "819: [discriminator loss: 0.2827274203300476, acc: 0.5] [gan loss: 0.376034, acc: 1.000000]\n",
            "820: [discriminator loss: 0.2568972706794739, acc: 0.5] [gan loss: 1.096845, acc: 0.015625]\n",
            "821: [discriminator loss: 0.5783488750457764, acc: 0.1875] [gan loss: 11.163571, acc: 0.000000]\n",
            "822: [discriminator loss: 0.5285896062850952, acc: 0.5] [gan loss: 1.796779, acc: 0.000000]\n",
            "823: [discriminator loss: 0.3682069778442383, acc: 0.5] [gan loss: 2.371560, acc: 0.000000]\n",
            "824: [discriminator loss: 0.2453206479549408, acc: 0.5] [gan loss: 1.165001, acc: 0.000000]\n",
            "825: [discriminator loss: 0.41248878836631775, acc: 0.40625] [gan loss: 5.150672, acc: 0.000000]\n",
            "826: [discriminator loss: 0.24157492816448212, acc: 0.5] [gan loss: 0.848314, acc: 0.234375]\n",
            "827: [discriminator loss: 0.3996391296386719, acc: 0.453125] [gan loss: 4.608135, acc: 0.000000]\n",
            "828: [discriminator loss: 0.24676096439361572, acc: 0.5] [gan loss: 1.065461, acc: 0.000000]\n",
            "829: [discriminator loss: 0.3772512674331665, acc: 0.4921875] [gan loss: 3.989804, acc: 0.000000]\n",
            "830: [discriminator loss: 0.2166822850704193, acc: 0.5] [gan loss: 1.330396, acc: 0.000000]\n",
            "831: [discriminator loss: 0.39722272753715515, acc: 0.46875] [gan loss: 5.537836, acc: 0.000000]\n",
            "832: [discriminator loss: 0.2549656629562378, acc: 0.5] [gan loss: 1.491923, acc: 0.000000]\n",
            "833: [discriminator loss: 0.3503977656364441, acc: 0.4921875] [gan loss: 3.578181, acc: 0.000000]\n",
            "834: [discriminator loss: 0.2523980736732483, acc: 0.5] [gan loss: 1.977818, acc: 0.000000]\n",
            "835: [discriminator loss: 0.3207382559776306, acc: 0.5] [gan loss: 4.088169, acc: 0.000000]\n",
            "836: [discriminator loss: 0.2330331802368164, acc: 0.5] [gan loss: 1.776960, acc: 0.000000]\n",
            "837: [discriminator loss: 0.3322294056415558, acc: 0.5] [gan loss: 4.316251, acc: 0.000000]\n",
            "838: [discriminator loss: 0.22608298063278198, acc: 0.5] [gan loss: 1.396801, acc: 0.000000]\n",
            "839: [discriminator loss: 0.31549355387687683, acc: 0.5] [gan loss: 2.956731, acc: 0.000000]\n",
            "840: [discriminator loss: 0.2153560072183609, acc: 0.5] [gan loss: 1.267188, acc: 0.000000]\n",
            "841: [discriminator loss: 0.29769501090049744, acc: 0.5] [gan loss: 3.107168, acc: 0.000000]\n",
            "842: [discriminator loss: 0.22306403517723083, acc: 0.5] [gan loss: 0.957159, acc: 0.046875]\n",
            "843: [discriminator loss: 0.29318374395370483, acc: 0.5] [gan loss: 2.986291, acc: 0.000000]\n",
            "844: [discriminator loss: 0.22332844138145447, acc: 0.5] [gan loss: 2.427118, acc: 0.000000]\n",
            "845: [discriminator loss: 0.206250861287117, acc: 0.5] [gan loss: 0.791852, acc: 0.312500]\n",
            "846: [discriminator loss: 0.2749992609024048, acc: 0.5] [gan loss: 1.889311, acc: 0.000000]\n",
            "847: [discriminator loss: 0.2394275814294815, acc: 0.5] [gan loss: 0.340348, acc: 0.984375]\n",
            "848: [discriminator loss: 0.22469665110111237, acc: 0.5] [gan loss: 0.582725, acc: 0.828125]\n",
            "849: [discriminator loss: 0.22731256484985352, acc: 0.5] [gan loss: 0.898624, acc: 0.125000]\n",
            "850: [discriminator loss: 0.22562658786773682, acc: 0.5] [gan loss: 0.901720, acc: 0.109375]\n",
            "851: [discriminator loss: 0.23274850845336914, acc: 0.5] [gan loss: 0.352438, acc: 1.000000]\n",
            "852: [discriminator loss: 0.220382958650589, acc: 0.5] [gan loss: 0.557836, acc: 0.906250]\n",
            "853: [discriminator loss: 0.22827796638011932, acc: 0.5] [gan loss: 0.795683, acc: 0.250000]\n",
            "854: [discriminator loss: 0.22630995512008667, acc: 0.5] [gan loss: 1.447468, acc: 0.000000]\n",
            "855: [discriminator loss: 0.23376233875751495, acc: 0.5] [gan loss: 1.772524, acc: 0.000000]\n",
            "856: [discriminator loss: 0.2030913084745407, acc: 0.5] [gan loss: 0.230748, acc: 1.000000]\n",
            "857: [discriminator loss: 0.23824992775917053, acc: 0.5] [gan loss: 0.785927, acc: 0.343750]\n",
            "858: [discriminator loss: 0.26618707180023193, acc: 0.5] [gan loss: 8.250416, acc: 0.000000]\n",
            "859: [discriminator loss: 0.22617988288402557, acc: 0.5] [gan loss: 0.137915, acc: 1.000000]\n",
            "860: [discriminator loss: 0.21917302906513214, acc: 0.5] [gan loss: 0.416638, acc: 1.000000]\n",
            "861: [discriminator loss: 0.224539652466774, acc: 0.5] [gan loss: 1.237147, acc: 0.000000]\n",
            "862: [discriminator loss: 0.252086877822876, acc: 0.5] [gan loss: 4.417714, acc: 0.000000]\n",
            "863: [discriminator loss: 0.2716831862926483, acc: 0.5] [gan loss: 5.334531, acc: 0.000000]\n",
            "864: [discriminator loss: 0.242508664727211, acc: 0.5] [gan loss: 0.032350, acc: 1.000000]\n",
            "865: [discriminator loss: 0.2711276412010193, acc: 0.5] [gan loss: 2.631094, acc: 0.000000]\n",
            "866: [discriminator loss: 0.3568672239780426, acc: 0.46875] [gan loss: 12.077293, acc: 0.000000]\n",
            "867: [discriminator loss: 0.4693075120449066, acc: 0.5] [gan loss: 0.216132, acc: 1.000000]\n",
            "868: [discriminator loss: 0.3755542039871216, acc: 0.4921875] [gan loss: 4.628916, acc: 0.000000]\n",
            "869: [discriminator loss: 0.21329374611377716, acc: 0.5] [gan loss: 1.067099, acc: 0.000000]\n",
            "870: [discriminator loss: 0.29002073407173157, acc: 0.5] [gan loss: 3.688190, acc: 0.000000]\n",
            "871: [discriminator loss: 0.2058204561471939, acc: 0.5] [gan loss: 1.934117, acc: 0.000000]\n",
            "872: [discriminator loss: 0.30258259177207947, acc: 0.5] [gan loss: 6.632104, acc: 0.000000]\n",
            "873: [discriminator loss: 0.21028907597064972, acc: 0.5] [gan loss: 1.980766, acc: 0.000000]\n",
            "874: [discriminator loss: 0.4277383089065552, acc: 0.4296875] [gan loss: 9.898173, acc: 0.000000]\n",
            "875: [discriminator loss: 0.3474879860877991, acc: 0.5] [gan loss: 2.141593, acc: 0.000000]\n",
            "876: [discriminator loss: 0.3342311978340149, acc: 0.4921875] [gan loss: 3.347897, acc: 0.000000]\n",
            "877: [discriminator loss: 0.22244380414485931, acc: 0.5] [gan loss: 1.428262, acc: 0.000000]\n",
            "878: [discriminator loss: 0.3523058295249939, acc: 0.484375] [gan loss: 4.835033, acc: 0.000000]\n",
            "879: [discriminator loss: 0.2434898316860199, acc: 0.5] [gan loss: 1.562699, acc: 0.000000]\n",
            "880: [discriminator loss: 0.5993706583976746, acc: 0.171875] [gan loss: 10.141157, acc: 0.000000]\n",
            "881: [discriminator loss: 0.5005090236663818, acc: 0.5] [gan loss: 2.053769, acc: 0.000000]\n",
            "882: [discriminator loss: 0.41210079193115234, acc: 0.4453125] [gan loss: 3.192657, acc: 0.000000]\n",
            "883: [discriminator loss: 0.2761661112308502, acc: 0.5] [gan loss: 1.529574, acc: 0.000000]\n",
            "884: [discriminator loss: 0.3595978617668152, acc: 0.4921875] [gan loss: 3.578485, acc: 0.000000]\n",
            "885: [discriminator loss: 0.2776341736316681, acc: 0.5] [gan loss: 1.956750, acc: 0.000000]\n",
            "886: [discriminator loss: 0.41413602232933044, acc: 0.4453125] [gan loss: 5.012196, acc: 0.000000]\n",
            "887: [discriminator loss: 0.2758599817752838, acc: 0.5] [gan loss: 1.783078, acc: 0.000000]\n",
            "888: [discriminator loss: 0.5427917242050171, acc: 0.2421875] [gan loss: 7.292279, acc: 0.000000]\n",
            "889: [discriminator loss: 0.3675151765346527, acc: 0.5] [gan loss: 1.893383, acc: 0.000000]\n",
            "890: [discriminator loss: 0.40827372670173645, acc: 0.4765625] [gan loss: 3.713878, acc: 0.000000]\n",
            "891: [discriminator loss: 0.2694737911224365, acc: 0.5] [gan loss: 2.372561, acc: 0.000000]\n",
            "892: [discriminator loss: 0.38394954800605774, acc: 0.4921875] [gan loss: 3.781727, acc: 0.000000]\n",
            "893: [discriminator loss: 0.27319642901420593, acc: 0.5] [gan loss: 2.722088, acc: 0.000000]\n",
            "894: [discriminator loss: 0.35369640588760376, acc: 0.5] [gan loss: 3.986132, acc: 0.000000]\n",
            "895: [discriminator loss: 0.28788331151008606, acc: 0.5] [gan loss: 1.823370, acc: 0.000000]\n",
            "896: [discriminator loss: 0.4389672875404358, acc: 0.453125] [gan loss: 6.145118, acc: 0.000000]\n",
            "897: [discriminator loss: 0.3195854723453522, acc: 0.5] [gan loss: 1.954808, acc: 0.000000]\n",
            "898: [discriminator loss: 0.853218674659729, acc: 0.0078125] [gan loss: 7.204534, acc: 0.000000]\n",
            "899: [discriminator loss: 0.3531542420387268, acc: 0.5] [gan loss: 2.727560, acc: 0.000000]\n",
            "900: [discriminator loss: 0.3533257246017456, acc: 0.5] [gan loss: 2.428701, acc: 0.000000]\n",
            "901: [discriminator loss: 0.3840775191783905, acc: 0.5] [gan loss: 3.201020, acc: 0.000000]\n",
            "902: [discriminator loss: 0.28690770268440247, acc: 0.5] [gan loss: 3.028848, acc: 0.000000]\n",
            "903: [discriminator loss: 0.3616041839122772, acc: 0.5] [gan loss: 3.645090, acc: 0.000000]\n",
            "904: [discriminator loss: 0.3555099368095398, acc: 0.5] [gan loss: 4.076310, acc: 0.000000]\n",
            "905: [discriminator loss: 0.2715572416782379, acc: 0.5] [gan loss: 2.803129, acc: 0.000000]\n",
            "906: [discriminator loss: 0.36729395389556885, acc: 0.5] [gan loss: 5.315911, acc: 0.000000]\n",
            "907: [discriminator loss: 0.2337053269147873, acc: 0.5] [gan loss: 2.822461, acc: 0.000000]\n",
            "908: [discriminator loss: 0.3377721309661865, acc: 0.5] [gan loss: 4.287598, acc: 0.000000]\n",
            "909: [discriminator loss: 0.24583005905151367, acc: 0.5] [gan loss: 2.756096, acc: 0.000000]\n",
            "910: [discriminator loss: 0.3368666470050812, acc: 0.5] [gan loss: 4.295690, acc: 0.000000]\n",
            "911: [discriminator loss: 0.23181404173374176, acc: 0.5] [gan loss: 3.163538, acc: 0.000000]\n",
            "912: [discriminator loss: 0.3318534195423126, acc: 0.5] [gan loss: 4.575446, acc: 0.000000]\n",
            "913: [discriminator loss: 0.2658650279045105, acc: 0.5] [gan loss: 2.455718, acc: 0.000000]\n",
            "914: [discriminator loss: 0.39507871866226196, acc: 0.484375] [gan loss: 5.209870, acc: 0.000000]\n",
            "915: [discriminator loss: 0.24208593368530273, acc: 0.5] [gan loss: 2.857909, acc: 0.000000]\n",
            "916: [discriminator loss: 0.33817538619041443, acc: 0.5] [gan loss: 3.207288, acc: 0.000000]\n",
            "917: [discriminator loss: 0.2977970242500305, acc: 0.5] [gan loss: 2.606968, acc: 0.000000]\n",
            "918: [discriminator loss: 0.3506771922111511, acc: 0.5] [gan loss: 3.609763, acc: 0.000000]\n",
            "919: [discriminator loss: 0.25832125544548035, acc: 0.5] [gan loss: 3.031825, acc: 0.000000]\n",
            "920: [discriminator loss: 0.2941163182258606, acc: 0.5] [gan loss: 3.856290, acc: 0.000000]\n",
            "921: [discriminator loss: 0.23268629610538483, acc: 0.5] [gan loss: 2.710943, acc: 0.000000]\n",
            "922: [discriminator loss: 0.3438098132610321, acc: 0.5] [gan loss: 5.351811, acc: 0.000000]\n",
            "923: [discriminator loss: 0.23242536187171936, acc: 0.5] [gan loss: 2.763153, acc: 0.000000]\n",
            "924: [discriminator loss: 0.33673781156539917, acc: 0.5] [gan loss: 4.417216, acc: 0.000000]\n",
            "925: [discriminator loss: 0.25548669695854187, acc: 0.5] [gan loss: 2.250779, acc: 0.000000]\n",
            "926: [discriminator loss: 0.472079336643219, acc: 0.4140625] [gan loss: 6.290426, acc: 0.000000]\n",
            "927: [discriminator loss: 0.28422850370407104, acc: 0.5] [gan loss: 2.500499, acc: 0.000000]\n",
            "928: [discriminator loss: 0.3959263265132904, acc: 0.4921875] [gan loss: 3.805464, acc: 0.000000]\n",
            "929: [discriminator loss: 0.25273245573043823, acc: 0.5] [gan loss: 2.570898, acc: 0.000000]\n",
            "930: [discriminator loss: 0.3231102228164673, acc: 0.5] [gan loss: 3.457441, acc: 0.000000]\n",
            "931: [discriminator loss: 0.275962233543396, acc: 0.5] [gan loss: 1.676455, acc: 0.000000]\n",
            "932: [discriminator loss: 0.47827255725860596, acc: 0.390625] [gan loss: 4.863170, acc: 0.000000]\n",
            "933: [discriminator loss: 0.25224024057388306, acc: 0.5] [gan loss: 1.678447, acc: 0.000000]\n",
            "934: [discriminator loss: 0.5784341096878052, acc: 0.21875] [gan loss: 4.759873, acc: 0.000000]\n",
            "935: [discriminator loss: 0.3799496591091156, acc: 0.5] [gan loss: 0.990186, acc: 0.078125]\n",
            "936: [discriminator loss: 0.6052601933479309, acc: 0.15625] [gan loss: 3.481484, acc: 0.000000]\n",
            "937: [discriminator loss: 0.3676086366176605, acc: 0.5] [gan loss: 1.108318, acc: 0.015625]\n",
            "938: [discriminator loss: 0.6151793599128723, acc: 0.171875] [gan loss: 3.273441, acc: 0.000000]\n",
            "939: [discriminator loss: 0.40417104959487915, acc: 0.5] [gan loss: 0.767765, acc: 0.343750]\n",
            "940: [discriminator loss: 0.5592131614685059, acc: 0.2578125] [gan loss: 2.936984, acc: 0.000000]\n",
            "941: [discriminator loss: 0.42320260405540466, acc: 0.5] [gan loss: 0.825601, acc: 0.218750]\n",
            "942: [discriminator loss: 0.5477042198181152, acc: 0.2421875] [gan loss: 3.374881, acc: 0.000000]\n",
            "943: [discriminator loss: 0.35866403579711914, acc: 0.5] [gan loss: 1.443726, acc: 0.000000]\n",
            "944: [discriminator loss: 0.5076975226402283, acc: 0.3828125] [gan loss: 3.779265, acc: 0.000000]\n",
            "945: [discriminator loss: 0.3537721633911133, acc: 0.5] [gan loss: 1.298864, acc: 0.000000]\n",
            "946: [discriminator loss: 0.4214429259300232, acc: 0.4921875] [gan loss: 2.708354, acc: 0.000000]\n",
            "947: [discriminator loss: 0.3012983202934265, acc: 0.5] [gan loss: 1.672364, acc: 0.000000]\n",
            "948: [discriminator loss: 0.37634384632110596, acc: 0.5] [gan loss: 2.498312, acc: 0.000000]\n",
            "949: [discriminator loss: 0.2892676591873169, acc: 0.5] [gan loss: 2.219825, acc: 0.000000]\n",
            "950: [discriminator loss: 0.3287707567214966, acc: 0.5] [gan loss: 2.398703, acc: 0.000000]\n",
            "951: [discriminator loss: 0.30429568886756897, acc: 0.5] [gan loss: 2.188141, acc: 0.000000]\n",
            "952: [discriminator loss: 0.352669894695282, acc: 0.5] [gan loss: 2.719875, acc: 0.000000]\n",
            "953: [discriminator loss: 0.3303348422050476, acc: 0.5] [gan loss: 2.878252, acc: 0.000000]\n",
            "954: [discriminator loss: 0.3330800533294678, acc: 0.5] [gan loss: 2.111609, acc: 0.000000]\n",
            "955: [discriminator loss: 0.4031907320022583, acc: 0.5] [gan loss: 4.008432, acc: 0.000000]\n",
            "956: [discriminator loss: 0.29910582304000854, acc: 0.5] [gan loss: 1.391441, acc: 0.000000]\n",
            "957: [discriminator loss: 0.5821508169174194, acc: 0.1015625] [gan loss: 4.564790, acc: 0.000000]\n",
            "958: [discriminator loss: 0.3838120400905609, acc: 0.5] [gan loss: 0.908157, acc: 0.109375]\n",
            "959: [discriminator loss: 0.5605912804603577, acc: 0.1484375] [gan loss: 3.763238, acc: 0.000000]\n",
            "960: [discriminator loss: 0.2766527235507965, acc: 0.5] [gan loss: 2.302921, acc: 0.000000]\n",
            "961: [discriminator loss: 0.3777271509170532, acc: 0.5] [gan loss: 3.871249, acc: 0.000000]\n",
            "962: [discriminator loss: 0.253593772649765, acc: 0.5] [gan loss: 2.088391, acc: 0.000000]\n",
            "963: [discriminator loss: 0.3487458825111389, acc: 0.5] [gan loss: 3.394597, acc: 0.000000]\n",
            "964: [discriminator loss: 0.2757769525051117, acc: 0.5] [gan loss: 2.567885, acc: 0.000000]\n",
            "965: [discriminator loss: 0.3185988664627075, acc: 0.5] [gan loss: 2.911842, acc: 0.000000]\n",
            "966: [discriminator loss: 0.30199670791625977, acc: 0.5] [gan loss: 3.673461, acc: 0.000000]\n",
            "967: [discriminator loss: 0.2456127554178238, acc: 0.5] [gan loss: 1.742031, acc: 0.000000]\n",
            "968: [discriminator loss: 0.5145206451416016, acc: 0.2890625] [gan loss: 6.243572, acc: 0.000000]\n",
            "969: [discriminator loss: 0.34784746170043945, acc: 0.5] [gan loss: 1.924609, acc: 0.000000]\n",
            "970: [discriminator loss: 0.3270752429962158, acc: 0.5] [gan loss: 2.540813, acc: 0.000000]\n",
            "971: [discriminator loss: 0.23437830805778503, acc: 0.5] [gan loss: 1.843309, acc: 0.000000]\n",
            "972: [discriminator loss: 0.3074192404747009, acc: 0.5] [gan loss: 2.159145, acc: 0.000000]\n",
            "973: [discriminator loss: 0.24970652163028717, acc: 0.5] [gan loss: 1.738292, acc: 0.000000]\n",
            "974: [discriminator loss: 0.27943864464759827, acc: 0.5] [gan loss: 2.323475, acc: 0.000000]\n",
            "975: [discriminator loss: 0.2573634386062622, acc: 0.5] [gan loss: 1.880662, acc: 0.000000]\n",
            "976: [discriminator loss: 0.2717202305793762, acc: 0.5] [gan loss: 2.269799, acc: 0.000000]\n",
            "977: [discriminator loss: 0.2569063603878021, acc: 0.5] [gan loss: 2.018216, acc: 0.000000]\n",
            "978: [discriminator loss: 0.26581689715385437, acc: 0.5] [gan loss: 2.336072, acc: 0.000000]\n",
            "979: [discriminator loss: 0.27090728282928467, acc: 0.5] [gan loss: 2.672341, acc: 0.000000]\n",
            "980: [discriminator loss: 0.23049908876419067, acc: 0.5] [gan loss: 1.425140, acc: 0.000000]\n",
            "981: [discriminator loss: 0.3969859480857849, acc: 0.4921875] [gan loss: 5.194636, acc: 0.000000]\n",
            "982: [discriminator loss: 0.3312932848930359, acc: 0.5] [gan loss: 0.767950, acc: 0.281250]\n",
            "983: [discriminator loss: 0.7046606540679932, acc: 0.0625] [gan loss: 5.215822, acc: 0.000000]\n",
            "984: [discriminator loss: 0.3972838819026947, acc: 0.5] [gan loss: 0.751395, acc: 0.375000]\n",
            "985: [discriminator loss: 0.5142154693603516, acc: 0.3125] [gan loss: 3.015698, acc: 0.000000]\n",
            "986: [discriminator loss: 0.3525548279285431, acc: 0.5] [gan loss: 1.075321, acc: 0.000000]\n",
            "987: [discriminator loss: 0.5974416136741638, acc: 0.1640625] [gan loss: 3.512753, acc: 0.000000]\n",
            "988: [discriminator loss: 0.38662421703338623, acc: 0.5] [gan loss: 0.539856, acc: 0.890625]\n",
            "989: [discriminator loss: 0.6860659718513489, acc: 0.0234375] [gan loss: 3.042111, acc: 0.000000]\n",
            "990: [discriminator loss: 0.3696640431880951, acc: 0.5] [gan loss: 0.747352, acc: 0.375000]\n",
            "991: [discriminator loss: 0.6091042757034302, acc: 0.125] [gan loss: 2.601943, acc: 0.000000]\n",
            "992: [discriminator loss: 0.3702782690525055, acc: 0.5] [gan loss: 0.889937, acc: 0.062500]\n",
            "993: [discriminator loss: 0.6804453134536743, acc: 0.0546875] [gan loss: 3.166878, acc: 0.000000]\n",
            "994: [discriminator loss: 0.38079822063446045, acc: 0.5] [gan loss: 1.006065, acc: 0.000000]\n",
            "995: [discriminator loss: 0.6296358108520508, acc: 0.09375] [gan loss: 3.829812, acc: 0.000000]\n",
            "996: [discriminator loss: 0.3956029713153839, acc: 0.5] [gan loss: 1.099431, acc: 0.000000]\n",
            "997: [discriminator loss: 0.7584786415100098, acc: 0.0078125] [gan loss: 4.434047, acc: 0.000000]\n",
            "998: [discriminator loss: 0.4685463011264801, acc: 0.5] [gan loss: 1.108550, acc: 0.000000]\n",
            "999: [discriminator loss: 0.605158269405365, acc: 0.1015625] [gan loss: 2.653131, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAIuCAYAAABdOBlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzdZ5yUxdb27RpQUXIGyQKCBAkiIAoCKoKC20xSFMWEimKOmBXMWTe6zbcBjJhBBEVBQYkqoiCSUUCCBBPh/fLs38s6q+irYc/0TNf8j2/n2F3dTFdXl32tWZWzbds2BwAAEJsi+f0EAAAA8gKbHAAAECU2OQAAIEpscgAAQJTY5AAAgCixyQEAAFHaLdV/LFq0qPn7cv1zc/78PHsUKWL3s1u2bMnJ1GPrPNq6dWumHhq5LL/mEWtRPPJzLSpSpEjKeYTske484pscAAAQJTY5AAAgSjmpvq7Lycnhu7xIbdu2LWNfETOP4pWpecQcihdrEXLDjuYR3+QAAIAosckBAABRYpMDAACilPJPyHNy7CUu/twOAABkC77JAQAAUWKTAwAAosQmBwAARIlNDgAAiFLKwmMKjQEAQLbimxwAABAlNjkAACBKbHIAAECUUtbkFHbaDFHz1q1bE8fYfffdTd68ebPJhaHuiaaS1q78PooUsf8/ks7cQzxyYy1iDiE35lG24ZscAAAQJTY5AAAgSmxyAABAlLKyJme33ezT1joX55zbY489TC5RooTJkyZNMrl58+beGJMnTzb55JNPNvmrr74yuV69et4YY8aMMbl169YmN2jQwOQff/zRG6NYsWIm//XXXyanU+NRtGhR72eZos+noNTo6O9ky5Yt3m3Kly9vcqlSpUx+8803TW7Tpo03xtSpU1Pe5vvvvzd533339cY45JBDTJ4wYYLJxYsXN3nTpk3eGLkhP+dRQZTOXN5rr71M1rXp3XffNblTp07eGM8++6zJAwYMMHnKlCkmt2rVyhvjjjvuMPmqq64yuUyZMiavW7fOGyM33rs6BnZtDdfPhVdffdXkY4891hvjuuuuM3nYsGEmv/XWWyYfffTR3hiNGzc2+bvvvjO5ZMmSJm/YsMEbIzdojdkOb5cnjw4AAJDP2OQAAIAosckBAABRSlmTU1BqJ9SyZctMnjZtmneb+vXrm1ynTh2T9frmI4884o2hdTpz585N+bz02rtzzm3cuNHkv//+O+XzSKfmQa+Jjh071uRq1ap599F+PZmUTu1LJuh8Xr16tckvvviid59TTz3VZK3t0uvCAwcO9MZo1KiRyX/88UfK56XX2p1zrlKlSiavWrXK5NKlS5usdR+hx6lcubLJL730ksmHH364N0Zo3EwoKD1e9He4Zs0ak1977TXvPlqDpfNB3x/nnHOON8Zxxx1n8imnnJLyeVaoUCHlf3fOufXr15us8y70Wutz1Tl05513mtyvX7/EMTKpoM6jL7/80uTbbrvNu88ZZ5xhstbc6L9Na66cc27IkCEm33jjjSmfZ9myZb2f9ezZ0+RLL73UZP2s2XPPPb0x9LlqrarWqen655xfH7kjfJMDAACixCYHAABEiU0OAACIEpscAAAQpZxUxcQ5OTkFo9JYaDO8UCFbfhS3hYrYkhoW6X3efvtt7zbakGn69OkmL1261OQ+ffp4Y2hR9Nq1azPWkaugzCMtrNTiXW2o51z6Daf+K/R+2tnmZ1qc7pxfBKoNMLXAXQtAnfOLosePH2/yokWLTNbmcaHnkal5VFDmkBZV/v777yan0wxQ3/M6x0KF+UnrmY4ZapCa9McHWhB/+eWXe7cZOnSoydo8TptQXnTRRd4Y+j7csGFDoV+LtAg8NAdCBbyppLMWJf0xUTqfaXqbf/75x+TTTz/dG+Opp54y+ZtvvjG5Zs2aJjdp0sQbQ5sOLl26NDiP+CYHAABEiU0OAACIEpscAAAQpaw8oHPOnDkma+M/5/z6ivnz55tct25dk7UZk3P+Nb+TTjrJZK1pCF1/fuWVV0w+4ogjTD7++ONN1sZKzjl3/fXXm3zrrbearI2UQjUd8H8v99xzj8n6ew6ZPXu2yU2bNjV59OjR3n20eaM2mdTDFfv27euNobVaBx54oMndu3c3+emnn/bG0DqN++67z+T27dubrDUazuVf87SCQusNtIFijRo1vPvogZu6Tjz22GMmP/HEE94YBxxwgMnakO29994zOdSETw8lPuyww0zW1z/0PKpUqWLyLbfcYnKtWrVMDtUG7WxtSYy0rlRr6EJ1LNWrVzd55cqVJutrowdnOufX+uhr/v7775usNVjOOTdy5MiUY5x44okma92Wc/7c0/eEHoatNUvOpV/ryDc5AAAgSmxyAABAlNjkAACAKGVlnxytC3jhhRe82/Tu3dvkr7/+2mTtK3LUUUd5YyQdJKnXBEM9VXb2MMrQGNojQ+sCVOiAR61H2bp1a6HrTaGvl9YLfPbZZ9599HrzRx99ZPLChQtNPv/8870xdraOJTQHksbQ+4TqHnQeaU8Tnatak+acc3/++afJ//zzT6Hqk6O/Z30vhmqh9CBFvY0egnjBBRd4Y+jrr+u2vrahOaTzXcfQPjrprF1J8zI0D7UepTCuRUp/JzfddJN3myuvvNLktWvXmqx9v3Ttcs7/HNA5sNtuyWW6O/uah+aRzjX9PNbnETqMU//9W7ZsoU8OAAAoPNjkAACAKLHJAQAAUcqKmhztedOiRQuT33jjDe8+eqbMiBEjTB4yZEguPbv/Xzq1FJUrVzZ5xYoVJof+9j/pfJGdPY/k/92m0F0Hr1Onjsna0yNU26X9lbRHxOOPP77TzyPp9QqdU6TXtW+//XaTb7jhBpP1jCnn/L43Ol+1livUJ0efe6bqKQrKHNI6Ja23adWqlXcfPZdHey1NnjzZ5HTWEX19tdYi9Prrbbp27Wqy1puF6PPQ+aC1FEn1g84VzrVI32v777+/yaHPJz0Dbdq0aSZfc801Joc+S5LOTdP/HjrvTF/T4cOHm6w9b0I1OVofps9V32ehPjlqR/OIb3IAAECU2OQAAIAosckBAABRYpMDAACilBWFx2rdunUmh4qjtPAunSZH/6tdKRrOL4Wx2E8999xzJushiM45V7FiRZP33nvvPH1OzoXnkRYIJhWehuZdXhyumal5VFDnUJ8+fUweNGiQdxstomzWrFmePifn0j+8cHv5tVaxFjl3yimnmKyHMDvnXLly5VLmvBD63NQ/jKhZs6bJS5cuNVkbiDqXN3ONwmMAAFCosMkBAABRYpMDAACilPeFKrlAm7hpbUGoedrFF19ssjYU1MMJtdGSc34zNL1u+tJLL5msjeNCz1VrJxo1auTdB3lDG3Dpax46kLJt27Ymly5d2mRtOqnzzDl/Hl133XUmDxs2zOTffvvNG0Ofu9ZcVK1a1eQ1a9Z4Y+B/p4dpaj2CZuecO+mkk0zW9UwPeT3yyCO9MbRh4P3332/ywIEDTZ40aZI3hs5NnWd6oGNBrSeMgR5iqQ1uq1Wr5t2nU6dOKW/zyy+/mNyyZUtvDG1M+cADD5g8ePBgk3XeOedckyZNTNZ5ov993rx53hiZxDc5AAAgSmxyAABAlNjkAACAKBXImpwyZcqkzHpdfPHixd4YDz/8sMkLFiwwWa9ph3qkzJgxI+VttN5GH8M5/xBI7elz9913m3zFFVd4Y2DXaN2K5h49eph80003eWN89913Jrdr187k//u//zO5c+fO3hjjx483uWPHjibrgXcbNmzwxtC+OFqHduqpp5ocOmwUO09/75rvuOMOk4cOHeqNoa+/zpnevXub3KZNG28MraWoW7euyVrr98MPP3hj6Dqq9YAPPfSQyaGeP9g1+n7VAyovueQSk3Vtcs65zz//3GRdR0aNGmVyqLZr7ty5JterV89kXYtC9TRJPbtefvllk3v16uWNkUl8kwMAAKLEJgcAAESJTQ4AAIhSxs+u0rqI0OPvt99+Js+cOdNkvQa4YsUKbww910PP4NB6mgoVKnhjaK1PqVKlvNtsr3nz5t7PJkyYYPLKlStN1l4VBx10UMrHCEk6x8i54DXhrD4vJp1/s/aW0RorPYdq1apV3hhJ58Uk9Yhwzrlp06aZHOrJtL3nn3/e+5nWbei/5d133zU5dPZNXsjms6vSOd9JX2/thaW9lUJrUZUqVUzWmgadQw0aNPDG0Nownf+qfv363s9eeeUVkytXrmyy1olovUZeyfazq9L5TNO16K233jK5devWJi9atMgbo2nTpikfV+tpOnTo4I3x4osvmtywYUOT9bkffPDB3hiPP/64ydqvR+vBDj30UG+MJPq+CtUp6r9/69atnF0FAAAKDzY5AAAgSmxyAABAlNjkAACAKGW88DjwGN7PtBhXm1hp4V46tNBYc+j3oI379Db63LVwzzm/4Pnnn382WYv7MnUoXrYX+wUew/vZuHHjTNbCcC0qTUfS65NOAXQSnZvO+f++008/3eT8av6XzYXH6fj6669N1uaelSpVMjk0PwIFkiZv2bLF5I0bN3pj6Bqoa43+YUFoHmqx8pNPPmny+eefn/J55pXY1qKQ7t27m9y/f3+Tjz/+eJNDnyW6jujBv3/++afJoT9w0INBk9YzPYA4NO7TTz9t8gUXXGByXs0jCo8BAEChxiYHAABEiU0OAACIUr4f0HnMMcd4P9Oam7/++stkvSa4fPlybwy9z8iRI00+7bTTTNZros45N3z48JT3ueGGG0y+6667vDHGjBljclLzJeyaZs2aeT/T5o1JNVZ6TTv0s9tvv93kgQMHmnzvvfd6YwwbNixlvvjii03WAxydc+7CCy80+aWXXvJug/+Nvjed89eRUNPQ7YWaAeo809qXq666yuTzzjvPG0PXmgceeMDkli1bJj4PnZs67zJVgxO7UA2e1v81btzY5E2bNpm8fv16bwxtiqu30cZ9zz33nDfGiSeeaPLYsWNN1rpFrf1zzrlrr73W5EzV4OwqvskBAABRYpMDAACixCYHAABEKeN9cpIOp3POucGDB5v88ccfm6zXHkP1CdonQnNuXDfUv9MP9e/RHhgFRbb3ptB6m9ABboMGDTL5vffeM1nrtEIH2pUoUcJkPcQzncP5kuquypcvb3Lo3xLqe1IQZHOfHK2dCL1OWh+lfUNatWplco8ePbwx1qxZY7LWeenj7kqdnvbACa076fQGyw/ZvhalQw+d1vd4v379TNYeRs4516VLF5PffPNNk7UHTqjHTdLnXjoHHxdUO5pHfJMDAACixCYHAABEiU0OAACIUp7X5CTVrYTOHNJri61btzb5k08+SRxDrz3q2S55UStTs2ZN72eLFy82+f333zf56KOPzvXnkY5suw6uPW4qV65ssp535pzfT+nYY481ecSIESaH3gtaP5Eb8yjpunfoTC2t6zjqqKNM/uCDD3b6eeSGbKrJ0Tmk58qFzvrROoi+ffuafNlll5kc6tmlZ1EVL17cZO2Rko6kedikSRPvPt99953Jb731lsnHHXfcTj+P3JBta5FK5yxFnVtXX321yffcc4/JobVI51GVKlVMXrZsWeLzUElzUR/DOed+/fVXk4cOHWryNddcY3Lo8zmpHkx/p+mc58fZVQAAoFBhkwMAAKLEJgcAAESJTQ4AAIhSxpsBpqN+/fomz5s3z+R0ipIyUWisQgVo1apVM1mbQGnR6R9//OGNkRdNu7K92C/wGN7PunXrZrIW56Yzj9K5zf9Kn7sWxDrnXJkyZUxevXq1ybnRUG5XZFPhcRItTHbOuddff91kPch3V+ZHXqxF6cyhpIaBoQNqMyG2tSikY8eOJn/22Wcm69z7559/vDH09cuL10vnkRYmO+dcu3btTJ49e7bJ2qh14cKF3hj6+axF1buCZoAAAKBQYZMDAACixCYHAABEKd9rckJ1LHpdu27duiYvWrTI5FA9hl7TTLp2nk7Dossvv9zke++91+QJEyZ4Y+hhovo89N9Su3Ztb4y8ENt18FA9hTaV1GvaeoBdOoca5oZbbrklZX7jjTe8+xx++OEpn9ett95q8l133fW/PMW0xVSTE2rCqGuPNmXUBmyh9fSvv/4yOTfqvLQJ4YMPPmjyFVdc4d3nxhtvNHnlypUmd+7c2WSthcwrsa1Foc8SXXv22Wcfk/VzQOeMc/76tCufaWr+/Pkm64GzM2fO9O5TvXr1lI+jdadao+Nc3tQ6UpMDAAAKFTY5AAAgSmxyAABAlDJek1OyZEmTQ4fTNWjQwOQffvjB5E6dOpk8ceJEbwy9lj5nzhyT9e/0Q9cvr7rqKpP1ILLNmzebvGDBAm+MEiVKmKzXM/XapPbNySvZfh1cX79QbVe9evVM1jmg8yxUg6CPo7VeWgsUqg1q3LixyVOnTjVZ+1088sgj3hg6P7UmI1N9cVQ21+SUL1/e5FDfEa2d0DmkB3bqoa/O+fUGum6Eetoo7R2ma6Kuo99//703hs7Nli1bmqw1H7H1WnIuM2tRqL5E6wO15kb7qa1YscIbo1ixYiavX78+5X8P0dd88uTJJuscCPWv0V5u+pmmQj1/8gI1OQAAoFBhkwMAAKLEJgcAAEQp5cXg3Phb9v3228/k22+/3eSTTz7Zu8/SpUtN/umnn0yuUaOGyWeeeaY3xrhx40zWa436t/xt27b1xmjatGnKMdSqVau8n02bNs1k7auhfXQy1Zsik3LjHLHu3bubrHNz9OjR3n20B4Rm7d+gr7dz/vVkff1+/fVXkx944AFvDK39mDJlislao/H11197Y3z11Vcmjx8/3uQuXbqYnKnr4JmSG2vRc889Z7LWJxx44IHefZYsWWKy1ijo8wrV9bz77rspx9A6n379+nljHHbYYSbreUFaP6NnIznn3Lfffmvy9OnTTe7atavJOtdjkBvzSM+h0vOd9Iw85/w6Fu3Rpdq0aeP9TNcvXTd0zA4dOnhj6OfcF198kfJ5hOpMH3vsMZM/+ugjk4855hiT83st4pscAAAQJTY5AAAgSmxyAABAlNjkAACAKOX7AZ0hepiZFv+VLl06cQwt7tPD97S5WjqNr5IOPAuNoc9dm4d9/vnniY+bF7K9AVc6tOBZXwstQgw1FNQDGbUpm86JUCGjzgt9HB1j3bp13hh6QOeMGTNM3pVi7tyQzc0AA4/h/WyvvfYyefXq1SYnvbbO+YXF+++/f8oxQuuINgnV5nL63ENNRfVA4TvuuMNk/WOMTCmMa5H+kYo2hAwd0Kl/5NCoUaOUjxmaR7pOhJqXprq9c8699NJLJg8aNMjk0PqVCTQDBAAAhQqbHAAAECU2OQAAIEoFsiZHr1927tzZZL1O3rt3b2+MZs2amawHkR1//PEma9M+55xr3769ydroSw8bffvtt70xatWqZXLr1q292+SHTF4HL1KkiJlHeXHwX1K9lHN+wzxtMtmnTx/vPtogTQ/B00NAFy1a5I2hDTG1iZfOzdABnXr9fdiwYSbHfkBnJuZQqJ5G59XAgQNN/u2330wOrUUtWrQwWdev5cuXmzxz5kxvjJ49e5qsB3JqE9HQwYpaT9SrVy+TdV5mSmw1OaG1SGtutNmf1pkOGDDAG0Mbxyqtr9EDPJ3zG+lqk0Kt9dKmus45V7t2bZO1xqyg1QfyTQ4AAIgSmxwAABAlNjkAACBKBbImB3kvtuvgyIxAX6Bo+uRks13p+5Vf8msO/b/HLri/mAIgm+aRoiYHAAAUKmxyAABAlNjkAACAKO2W6j9qv5r8+vt3/O/S6SOTV7RHRH7148D/LtRLJhOYQ1Y2107k1xxyjs+0JNk0j/S13BG+yQEAAFFikwMAAKLEJgcAAESJTQ4AAIhSysLjrVu3Zup5IGIU92WnULF6fhUm6lqUzYW3uSFb/r35WWQcwmdadgrNo3TfAwVrBgIAAOQSNjkAACBKbHIAAECUUh7QCQAAkK34JgcAAESJTQ4AAIgSmxwAABAlNjkAACBKbHIAAECU2OQAAIAosckBAABRYpMDAACixCYHAABEiU0OAACIEpscAAAQJTY5AAAgSmxyAABAlNjkAACAKLHJAQAAUWKTAwAAosQmBwAARIlNDgAAiBKbHAAAECU2OQAAIEpscgAAQJTY5AAAgCixyQEAAFHaLdV/LFKkyLbt87Zt23Z0UxRwOTk5Jm/dujVnBzfNdUWLFk05j5hX2aNIEfv/RVu2bMnIPGItikd+rkXMo3ikuxbxTQ4AAIgSmxwAABCllJer+CovHvn5Wm7dujXfHhu5K79eS9aieOTna8k8ike6axHf5AAAgCixyQEAAFFikwMAAKLEJgcAAESJTQ4AAIgSmxwAABAlNjkAACBKbHIAAECU2OQAAIAosckBAABRYpMDAACixCYHAABEKeUBnYVdTk6OyUWLFjV58+bNmXw6iITOq3QODdyV+yBeRYrY/z/Vwwr1vzvn3J577mnyH3/8YTJzKi66ZjjnzwvN//zzz06PsWXLll19ihnBNzkAACBKbHIAAECU2OQAAIAoZWVNTjr1CSVLljRZ62m+/PJLk9u3b++NMXXqVJNbt25t8rvvvmvyIYcc4o3RoEEDk2fPnm1yuXLlTF6zZo03RtL193SErtEjmf7edt99d5OnTJli8gEHHOCNceaZZ5r84osvmvzoo4+aPGDAAG+MLl26mDxmzBiTq1atavLy5cu9MXJjHun7qLBL53eqtTDqvvvuM/niiy/2bvPkk0+afOmll5qs61mLFi28MZ5++mmT+/TpY3LTpk1N/vbbb70x9thjD5P//vtv7zbbC9V07LZbVn7s5Dv93ZcvX95kXYsaNmzojTF58mST9TNr3LhxJh922GHeGEcffbTJo0aNMrlMmTImr1u3zhsjN6T7mcYnHwAAiBKbHAAAECU2OQAAIEpZeXH0l19+MXn8+PHebfRaY/Xq1U3Wa8VaJ+Gcc7Vq1TL5119/TTmG1ms451zHjh1N/uqrr0zW6/Whmgf9WY0aNUyeOHGiyTVr1vTGKF26tPezwkZfr5UrV5p82223effRugWtudH6gltvvdUbY/DgwSY//vjjJuu15SuvvNIb4+qrrzZ5xIgRJhcvXtxkvX7vnP/v1znx6quvmty1a1dvjKT6ksJmxYoVJk+YMMG7Tdu2bU2uWLGiyfpaffTRR94YvXr1Mvm0004zWV9brddwzrmWLVuavGnTJpN1/QrVzmj9o9b+vPXWWybXqVPHG4M55NO1SD8nnHNu3333Nblu3bom6zpyxRVXeGM0adLE5LVr15qs8yg0B8qWLWuyfh5rnWmxYsW8MfRxSpUqZfLIkSNN7tatmzdGuvWBfJMDAACixCYHAABEiU0OAACIEpscAAAQpZxUh7Ll5OQUiBPbtMBID5bTQ8Wc8wsxk4R+D6FGVqmEDuzUwi1tFqbNtG655RZvjGuvvdZkbQS3cOFCky+77DJvDG2O+Pvvv+/cP+5/UFDnkRZehg6a22uvvXbqMULN4LQgUOeaPm5oPmuxpt5Hi88vuOACb4wHHnjA5Jtuusnkvffe2+S3337bG0Pn88aNGzMyjwrKHNI1QV+r0Osf+oOEVELzUOeuPo4+r9BapM9Db6Pvh1BTSm1cOGfOHJN/+OEHk0ONDfU9tWHDhkK3Fikt3i1RooR3G13Dk6SzFulcS+fQVn0ef/75Z8oxunfv7o3x/PPPm6zz6tBDDzX51FNP9cbQAudVq1YF5xHf5AAAgCixyQEAAFFikwMAAKKUFc0A9bqg1gro9Tvn/BoGrWPRRmfaxMo5/zq41sZo46+TTjrJG0Mft0OHDiZro7i7777bG0P/LTfeeKPJ++23n8mh66gciudfj542bZrJ2lzLOb9Rm9Y/abMzbajnnP96PPjggybffPPNJj/11FPeGHpAozaY08fQ2gjnnLvjjjtM1kMdtclX6PDFwt7ITeeQrgHabM055ypVqmSyNh7t3bu3ydqA0jnn9t9/f5OHDh1q8qxZs0zWQ2Gd89dNbWypzQL14EXn/HXz3HPPNblx48Ymh+pCdrbWsTDo37+/yffff793G20GqIdpdu7c2eTrrrvOG0NrVYcNG2ay1r58+OGH3hi6bjZr1sxkPZRaDwV1zrnnnnvOZF3f5s2bZ3JoHiUdDvtffJMDAACixCYHAABEiU0OAACIUoHsk5PUV0Svz/3444/eGPXq1TP5kUceMbl27domn3jiid4YSb0otGYndGCYXjfUMfTfGqJ1IUm9DEL1N9oTY9u2bdH3ptDXQ38v+nucO3euN4bOEz1Ib/HixSbrdXHn/F4q+nppnUuoT06o70kq6dQ9pHrvOxc+WE+f25YtW6Luk5P0fv3rr79MTuewYD188NNPPzX5hRde8MZIev1DB7ImjaHrm/bRCfXr0YM/V69enXLM0JoY6BMV/VqUROdRqC5Pa7d++uknk7XG7qKLLvLGSHrP54Z01h69jT4v/e/prEX//PMPfXIAAEDhwSYHAABEiU0OAACIUoGsydHrcdrP4d577zVZ+8Q459yjjz5qsvarCf3tfhK9Hq/Xn0PXxbUm57333jNZe+vsSj1G0vXNkMJQk6O/F/1dH3TQQSafddZZ3hgLFiwweenSpSb36NEj8XnoPNHaIH19Q9e09TWtWbOmyUuWLEl5+3TodW+tE3DOr7HYvHlz1DU5qk2bNiY3atTI5H79+nn30b44Wsc1duzYnX4eSe/5UD8jPWPomWeeMVnnf2geJq1FSWtkaNytW7dGvxapqlWrmqxnzR133HHefbQn07Jly0zWPlghOk/Seb2S6Oev9ugK1Z1qvZfeRvv5bNiwwRsjcA4XNTkAAKDwYJMDAACixCYHAABEiU0OAACIUoEsPN5rr71M1oK50aNHm1y/fn1vDC3s0kKmvFCyZEnvZ1rMqQ23tLlcqAFXXigMhcf6emzatMnkFStWmBx6L5QpU8ZkLejclUK9JKGCz6RC00w0+QrJ1DzKrzlUoUIFk7X5nR7I+u2333pjnHfeeSbr2pQXQn8EoT+rVq2aydpcLjS382KeFYa1KOmPDV566SWT9Q8LnPMPadW5manPjqQGmZl6HmpH84hvcgAAQJTY5AAAgCixyQEAAM4SV3UAACAASURBVFHyT3IsALQJmV5L1muRWufinHPVq1dPeRttuhe6BqrNlp544gmTL7/8cpMXLlzojaG1QNoc8OSTTzZZmwVi12kNTsWKFU3WeRWqhRkyZIjJpUqVMnn9+vUmN2vWzBtj9uzZJuthsK+//rrJr732mjdGnTp1TJ4xY4bJ/fv3NzkvaoWcS+/wvZj8/vvvJjdp0sRkXSP0vzvn3L777mtyUn2GzjHn/MNktVncO++8Y3KoNqhu3boma03aMcccY/LUqVO9MbBr9DXee++9Tdb3VWge3XnnnSnH0Ealbdu29cbQdaNnz54m6+GwI0aM8MbQJqraILRWrVomZ6q2a0f4JgcAAESJTQ4AAIgSmxwAABClAlmTo7SeRusennzySe8+ixYtMvmUU04xWWthzj33XG+MVatWmVylShWTtXZI6zOc83sI6PX2Tz75xOQPPvjAGyOv6itiV7p0aZO1Pkr76Hz66afeGLfddpvJU6ZMMfnxxx83WQ8Bdc65zz//3OSOHTuarPUWet3cOb+nSfPmzU3WWq7u3bt7Y6hduS6eX/148sKuHGw7b948k7UWSuutnPP7zyQdOKxjOufcyJEjTdZ5pr1J9DGd8/+9Woc4ceJEk7X2IjRGTPMhL2m/La2P0s+jjz76yBvj9ttvN1lr/b755huT69Wr543x888/m1yjRg2TdR79+OOP3hi6jlaqVMnkWbNmmRyqU8wkvskBAABRYpMDAACixCYHAABEKeXZVUWKFDH/MTeuv+rf1IfOudA+Enp9sk2bNibreTLO+bUvWp+hfQuaNm3qjTFz5kyT9dwipX1YnHPu66+/NrlEiRImL1682ORWrVqlfIyQXblOnsnzYvJiHukcCc0jvSY9efJkk/U6udZxOefXLWh9lD5u7dq1vTHGjh1rsp5Bo0JzUWu1dK5pPxftoZEO7cWzYMEC7zY617Zu3Zq1Z1clnUvlnH/O1FtvvWXyfvvtZ/K4ceO8MVq2bGmy9vDS94P21XHOr9PS9Uxr/7ROwjnn5s+fb7LO3bVr15q8zz77eGMk0eeRTj1htp1dpe8BXYu0B5tzzjVo0MBk7UGkdS6//vqrN0bSmWf6mRbq/aafRzoX9d+m89s55yZNmpTyPrpuaA1aOvRzcuPGjYn34ewqAABQqLDJAQAAUWKTAwAAosQmBwAARCll4XFeFPul45JLLjG5Q4cOJuvhdFpw5ZzfQFCL7LRYKlS4qmMkFcxu2LDB+9lee+1l8ksvvWTy6aefnnLMvJJtxX674q677jK5UaNGJvfo0cPkUJGkFlLqa6z3Cc3FcuXKmayFiVq4GCqy08aFWiSqzQL//PNPb4xdKQpNkql5lIk5pL8f5/xDd/UQRP0jiDVr1nhjlC9f3mRt/qhrT+h10UJjvY8+919++cUbQwtX58yZY7IeCpmpRn+FYS1avny5yfq5oH8Ekc5apLfRw5/1MZ3z/7hAx9A/DAqtZ7peaSGyNjsNjZEXKDwGAACFCpscAAAQJTY5AAAgSvl+QGeowd6mTZtM7tatm8la06C3d86vndBGb3po2B133OGNcd5555mshzXqIWtjxozxxhg1apTJAwYM8G6D/53WLDjn1xRoUyq9Hh2qY9DalqefftpkPVhv8ODB3hjnn3++yS+//LLJenjsU0895Y2h96lVq5bJelhsCAe9pqYN25xzbvTo0Sb37NnTZK2f0kMTnfPXibvvvtvkq6++2uQhQ4Z4Y+htLr30UpOvv/56k2+99VZvjBdffNHk1q1bm8xhm7lDa5ucc+6HH34wuWHDhiZrTc6yZcu8MfSz8rrrrjNZ6zvPOeccbww9uFcf56qrrjI5NBf1sNhDDz3U5IK2zvBNDgAAiBKbHAAAECU2OQAAIEoZ75Ojf+sfevx//etfJmsPkHPPPdfko446yhtDrwtqjY72FAhdRwz1ztneHnvskThGpnoE7Kxs702hPYxCh+KdccYZJk+cONFk7VnUuXNnb4yyZcuarL0n9DXflevRof4sqqBd5/6vbO6TozUOobqmI444wuSVK1eafO2115rcr18/b4xixYqZrGtROgc86hzRtUnHCM0XXWsLSg1Otq9F+jkQev30M0xfP619ueaaa7wxtL+Sfrbkxuup/eM0O5d9axHf5AAAgCixyQEAAFFikwMAAKKU5zU5er2yRIkSJutZGc45V6FCBZPvv/9+k7WvSKhPjl4Xbdq0qcmzZs3awTPeMe3F8vvvv5us9RvO+WcMTZ482eS2bdvu9PPIDdl2HVxrcPRasdYkOOdczZo1Tdazq95//32TQzVYev1Z+1no65sOrQXRXjx6ro1z/vX4Pn36mKx9dDIlm2py9PdevHhxk9evX+/dR9/zWnPz2GOPmRyaQ/oznUPr1q3bwTPesaQ5pGcUOefcggULTL7llltMvuGGG3b6eah0zkcL1Bdl1Vqkn1lacxU6N07vc/DBB5s8YcIEk3WOOOevNbrm7Ur9pz4vnasVK1b07rNq1SqTH3/8cZMHDhy4088jN1CTAwAAChU2OQAAIEpscgAAQJTY5AAAgChlvBlg4DG8n7366qsm9+7d22Qt8A018SpZsqTJ2sQtL2hxrHPO7b333iZrkaEe8Jep5oHZVnicxmN4P7voootMfvDBB01OpzFlOo3aclvo35Ikvxq7ZVPh8a446KCDTNY/HEhnDult8uI9ro8RKsTXA2rnzZtn8urVq03OVNO32NaikOrVq5u8dOnS/HgaOy20FmnRuzbWTWqim1coPAYAAIUKmxwAABAlNjkAACBK+V6TE2p81rx5c5MPP/xwk5988kmTQ820kq5763XD0PPQa41z5841eb/99jM5VPdTrly5lM9j5MiRJmv9UV6J7Tp46Nqx1iVoLZfWIITG0Hmkt9H3T2gMvc2RRx5p8pgxY0x+9NFHvTG0+Z/OV60dmTlzpjdGXoipJkdrDZzza7DatWtnstbohOpY9Gf6ONroURsQOufX7mk9zYEHHmjyAw884I1x4oknmqyN4O68806Tc6M5YDpiW4tCB+7quqCvsR7ams6B0UmNF9NZi7SR34UXXmjyvffe643Rt29fk7X+VdevK664whsjL1CTAwAAChU2OQAAIEpscgAAQJQyXpOjB4+FDjM74YQTTNa6lf79+5usBy0651+PXLZsmclVq1Y1OXQ4X8+ePU1+9tlnTdZroCtWrPDG0Lqe2rVrpxwjU7L9OrgeihfqX6M1OFq7pQd4LlmyJPFx9DBYPYA29HpqryR9HH0PfP/9994Yes2+ffv2Jut8j63HSV7MIa1rCvX30Dkyf/58k/UAwzVr1nhjaF3eypUrTdYeKlor5py/JuqBrDr/x44d642htV+heov8kO1rkR46HfpMCxxKanK1atVMDtV36uPoAdH62Ro6uPryyy83ediwYSbr59W3337rjaH1YL169TJZ9xSZ6uFFTQ4AAChU2OQAAIAosckBAABR8g842U7S3+GnQ/s3PPbYYyYffPDB3n3eeecdk7XWRa+l//TTT94Y2tNG+51oDc4xxxzjjXH++eeb/PTTT5us1xpHjRrljTFu3DiTtQ/BI488YnJ+1ejkpaTeMuk46aSTTG7UqJHJofoC7S2yatWqlI9x9tlnez/7/PPPTda6Hj037cYbb/TGOP30001u2rRpyucROotN63gWLlxoctu2bU3OxFltmZQba9HJJ5+c8r+//vrr3s/09z5p0iSTa9SoYbL2M3LOf620HkNrcELzUNcvzfqeWrt2rTeG1hRqX5yrr7465ZgxyI15dOyxx6Yc86233vLuo48zZcoUk7VW84ILLvDG+Pnnn1OOoXV7Rx99tDdGw4YNUz4vfc3feOMNbwyt7dL3RNeuXU0O9bHLJL7JAQAAUWKTAwAAosQmBwAARIlNDgAAiFK+H9AZogfYaRGlNr7afffdvTG0oEobcuVGMayOEWomp4fcPfPMMzv9OHkh2xtwqdCheDqPFi9ebLIWsGtzLef8AvW6deumfNzQPNKfhZ7r9kLFkFoArcWpP/74Y8ox80o2NwNMhx7yOmHCBJO1AZs2B3TOfz0rVapkcjprka55ocNEtxeay02aNDFZG6TSmDTv6Htef/fauFTnnXN+sbk2Kk2HFr3rIa0q1FBw8ODBJr/wwgsmh5ohZgLNAAEAQKHCJgcAAESJTQ4AAIhSymaAmaDXo53zrz/37dvX5OnTp5scauSnh3hqc6x27dqZrPUazvkN5/Q2ejjjOeec440RalSInZdUtxCqc9Hrz+eee67Jeh081IRvyJAhJmtjN6310mvtzjnXoEEDk/Watf7bRo8e7Y0xc+ZMk/WgSOQNrYPQgzLr169v8i233OKNobVfOofq1atncuhQxFatWpmsc0gbwT3//PPeGHoIZKiGEHlD1yudRx06dDBZG4g659y+++5rstZd6TwLfaZp00FtMKj1rXfccYc3RuXKlU3W90hBwzc5AAAgSmxyAABAlNjkAACAKBXIPjnIfVr3sXXr1uh7U2RCXvRbKsgHI2rt05YtW6Luk4Pcx1rkK2zrSG5Idy3imxwAABAlNjkAACBKbHIAAECUUvbJ0b+Z1/41yB5J5yXlJT2HpaD3VdgZhe3aeaivVSbENIcKe/1F6KzBTNGzmrSXVn4pbHMgN6S7FvFNDgAAiBKbHAAAECU2OQAAIEpscgAAQJRSFh4XlKIs7JxQkXF+FqVt3brV5MJeMJct/97QPNLXMlNiWoty4/XP5jmUn0Xj+TV/80K2zIHcoAXjzqX/7+ebHAAAECU2OQAAIEpscgAAQJRSHtAJAACQrfgmBwAARIlNDgAAiBKbHAAAECU2OQAAIEpscgAAQJTY5AAAgCixyQEAAFFikwMAAKLEJgcAAESJTQ4AAIgSmxwAABAlNjkAACBKbHIAAECU2OQAAIAosckBAABRYpMDAACixCYHAABEiU0OAACIEpscAAAQJTY5AAAgSmxyAABAlNjkAACAKLHJAQAAUdot1X8sUqTItu3ztm3bdnRTFHA5OTkmb926NWcHN811Oo8U8yp7FC1a1OTNmzdnZB6xFsUjP9eiokWLppxHzKvsUaSI/Y5my5YtwXnENzkAACBKbHIAAECUclJ9PZeTk8N3d5Hatm1bxr4iZh7FK1PziDkUL9Yi5IYdzSO+yQEAAFFikwMAAKLEJgcAAESJTQ4AAIgSmxwAABAlNjkAACBKbHIAAECU2OQAAIAosckBAABRYpMDAACixCYHAABEiU0OAACI0m75/QQKspwce97X7rvvbvLff/+d8vah+/zzzz8mpzogFfgvnVvMm8KlSJHU/z+6devWxDGKFi1q8pYtW/6n54SCLTRndtvNfuSXKFHC5LVr1yaOq2Ns3rzZ5IK2NvFNDgAAiBKbHAAAECU2OQAAIEqFpiZnr732Mvmyyy4z+c477/TuM2nSJJO7detm8nfffWdy48aNvTEuueQSk4cMGZJyzA8//NAbI6muJ516jaRr+oWR/k5CdQ1ax1C2bFmTzzrrLJPvvfdeb4wbb7zR5FtvvdXka665xuTbb7/dG6NOnTomz5s3z+SKFSuavGrVKm+M3KjrCdWdFWbp/E51DpUqVcrkt99+2+Tu3bt7Y+icuPzyy02ePn26ya1atfLG0LVo6NChJusa+ccff3hjpPOeSaK/D/jzKPQ+23PPPU3W2pivv/7a5AMPPNAb46effjK5UaNGJi9fvtzkevXqeWM88sgjJp955pkm77333inHzDQ++QAAQJTY5AAAgCixyQEAAFFKWZNTUHtzLF261OSLLrrIu43WSnTp0sXkpOvkzjnXvHlzk1euXJnyeYV+P+3atTP5r7/+Mlmvq6ZzvfqAAw4wedSoUSbXrFnTu09+XgcvKPNI6wm05mDy5MneferXr29y1apVTdZ/W61atbwxBgwYYPLVV1+d8nk99NBD3hjVqlUz+eOPPza5UqVKJhcvXtwbQ59rkyZNTH722WdNDl3Tz695VFDmkFqwYIHJgwYN8m5z9913m7zvvvuarP82rRd0zrlzzjnH5AsvvDDl8ypXrpz3sxo1api8Zs0ak7UmR2tAQipXrmzyu+++a3KbNm28+7AW+c9j7ty5Js+ePdu7T6dOnUzW97iuI1dccYU3Rvny5U1esWJFyucVmkdaY7Nx40aT99hjD5P1My5E63i0jvG8887z7pPO/HSOb3IAAECk2OQAAIAosckBAABRYpMDAACilJOq8ConJ6dAVPdpMZQ2wwvJi+I2bXylOfS71KIrvY0WIg8ePNgb48EHHzR5/vz5Juuhalqg5pxzxYoVM3n9+vUZ6+pWUOaRzon169ebrE0XnUuvaG57oTmg81cPRtT7hA5OTGrCtmTJEpPPPvtsb4zHH3/c5PHjx5usvw8tkHbO/x39+eefGZlHBWUO6WupRZjaLNI5/72XJNRgT1//pMM102lKmDSHtPjTOecee+wxk7XQWJtSHnXUUd4YWjC6bt26QrcWaZG3Fu/qoZfOhden3KZzIvQ8tLBY76Nz89RTT/XGGDZsmMlz5swxWT/jTzjhBG8MnUcbNmwIziO+yQEAAFFikwMAAKLEJgcAAEQpKw/oHDNmjMmhhlMVKlQw+ZNPPjFZ61ZeeOEFbwxtdKVN3V555RWThw8f7o2h17D1cfX65pQpU7wx9N+njZFatGhhcqhmaWfrAmKk1451Hh166KHefcqUKWOyHsq6//77m6w1Cs75Tdg6d+5s8jvvvGNy//79vTHGjRtncuvWrU3Wxn06353zDw995plnTN5vv/1MDtWGZKIuoCDTuhatLdA1wjn/4N5ly5aZXL16dZNDjf769u1r8vHHH2/y888/b/INN9zgjfHqq6+a3K9fP5OrVKli8ptvvumN0bZt25TPVRuV/v33394Y6TZxi9mff/5p8pdffmlyw4YNvftoYz6tzdTDND/77DNvDG16e8opp5isTUb79OnjjfHyyy+brI12Nb/xxhveGNpo9/rrrze5ZcuWJodq0LSuaUf4JgcAAESJTQ4AAIgSmxwAABClAtknR3tRJPUZCR1mpvUFWkuxadMmkzt27OiNEbqevD2t19DrrM75fXCU9r9I58A4vY3mUG8X7Xewbdu2QtebImkeffvtt959dB7poYY6r4488khvjFCvie2lMwd29iDB0BxIOpxQfx+h2gmdz1u2bCnUfXL0d/bBBx9499FDetetW2fyiBEjTB4yZIg3RlJvsKQ107lwjVUqOi9DksZMZw5t3bqVtUjmUaimTufRvHnzTNZeO6HPtKTPI31eoTmgz1Xvk06POl2f9LNW16aSJUt6Y+i/d0drEd/kAACAKLHJAQAAUWKTAwAAolQga3KUXo/T/g5nnnmmd59Vq1aZrH/br30l0qG9d1avXm1y6NwareHQs6kefvjhxMdNOqdGr4Em3d65wlmTo71DtD9J6HwUrbPSa+Wnn366yTtb9xASqqfQ96n2Rpo1a1bK24d+po9TvHhxk/Wad+g+maqnKChzSPtcab+iHj16ePfRM6G0l5KuTelIOsssnTmkNRsTJkxIfNyk2jDWovRoXyudR0OHDvXuc/PNN5tcv359ky+44AKT05kDSXV6ofoafU2vueYak++5556UYzqXXKeo/eNC9bHprkV8kwMAAKLEJgcAAESJTQ4AAIgSmxwAABClAll4rAdv/fHHHyZrgVyJEiW8MbSJW+g220unSEtpQyM9iNE5/7lrE8INGzbs1GPmlsJQ7Bd6TbenReGh373OGz3oNFOvlxYAauFpUrO4vJKpeZRfc0gLILXocvr06Slv75xze++9t8n6Bwp5MYdCcz+pWDlTc1kVhrVI54W+X1euXGly6EBlbayYicNyQ/NIP/f00M+1a9eanBt/jJGOHc0jvskBAABRYpMDAACixCYHAABEyT/FrwDQBmx6za9SpUoma1M355zr0qWLyVpbobUxoSZeU6ZMMXnGjBkmH3/88SaHDlXTWor169ebXKdOHZN///13bwzkjpNOOsnkdA6SO/HEE03Wa+U6V0MHEmojq8MOO8zk8ePHmzxx4kRvDK0x03nUtGlTk0PzKL9qLrKZzpHGjRubrIdtNmvWzBujV69eJtetW9fk+fPnm6w1PM4599tvv5ncsmVLkydPnmzys88+643Rtm1bk7V2Qg+AzKv5klQrVxiUK1fOZP18CjW/02amSZ9p2twzdBttCPnpp5+avGzZMm8MbYqr80RrzrQuNdP4JgcAAESJTQ4AAIgSmxwAABClAlmTowdy6jVArU945plnvDHGjRtncp8+fUx++umnTdYD0pxzbuHChSbrtfSlS5earH1XnHOuTJkyJuu/5aGHHjJ50KBB3hjYNRUrVjRZ62d0noXm0YgRI0zWgzG1vkbnVWiMvn37mqw9M6ZNm+aNob2j9Jq+3qdVq1beGIoanWT6/tU6B61j+b//+z9vjPfee89kPQj2rbfeMvnwww/3xtA6LV2vtH/PG2+84Y2htYy6jt53330mX3rppd4YuSG2eZd0yKVzzpUvX95krafRWr6ZM2d6Y+gBnYsXLzb5uOOOM/nUU0/1xtD6P12/tKeN1oI55/fJ0bq1Rx55xGQ9ODTT+CYHAABEiU0OAACIEpscAAAQpYzX5Og1bf27feeca968ucmvvfaayXrdsGvXromP+/rrr6cc46+//kocY8mSJSn/+8EHH5z4uFq3M2rUqMTHTaLXRPX6fIz0HJ6QTp06mfzCCy+YvHnzZpO1ZsE5v17mu+++M1l/13qumnPOHXLIISZrDwy9hn/FFVd4Y2itj/bB0TF35byYdN6bMdHagtD7RmsptG5L76O9aJzzX4s333zTZH399Tw75/zeOfq4WhcSqu3TOsW5c+eaHOrNsrOSzh3Mb+nUz+ws7Z2l64pz/hqtNVb6XtN6Quf897yuZ1pzGOqTc8ABB5ic9Fmhn8XOOTdnzhyTdc1Lp/9YkqRz1pxLv98S3+QAAIAosckBAABRYpMDAACixCYHAABEKSdV4VVOTk6ed20KFZBqkz0t/qxZs6bJoX+DFiXpGBs3bjQ5VCymxV9JxZy//vqr9zMtZtVmTN27d0/5PPPKtm3bMnZKXn7NIy3q1maOjRo1Mjn0u99jjz1M1gJ1LdbU2zvnFybqfNW5Gipw18LTBx54wGQtVs5Uw7VMzaNMzKGQH3/80WQ9hPeMM84wObRG6JzQtUYP+dS1yTnnatWqlXIMLfYMjaEN6H7++WeTmzRpYrIWsjqXN4W7sa1F+n53zi80btiwocn62oR+91rUreuVvjahPxwoXbq0yTpfdR3Vg4Cd8w/M1gaYp512msn5vRbxTQ4AAIgSmxwAABAlNjkAACBK+X5A5xFHHOH9bPny5SZrLYXSa8vO+dcjb7rpJpPPOecck4cMGeKNcffdd5usDbb0gLsvv/zSG0OvxR5zzDEmh2qBsPO0vsY5v3lW9erVTdZrxYsWLfLG0GvUV155pck6rz7++GNvjN69e5v8xBNPmNy+fXuTn3zySW8MPZzvqquuMjlT173TbcCVjfTAVuecu+WWW0y+5557TNa6iFBdntZXjBkzxuQuXbqYfOaZZ3pj6JzQ53XkkUea/J///McbQ5sQaqO3dBqixna4Zm7Q90S3bt2822h9pzai1HmkdVrOOffLL7+YrAe7duzY0eTQPLrzzjtNHjt2rMla+6UHSDvn3KxZs0w+66yzTC5oc4RvcgAAQJTY5AAAgCixyQEAAFHKeJ8cvRYZqkm57LLLTJ48ebLJWo9wwgkneGPsvvvuJicdFJcb1xFD9QoF7frkf2V7b4o999zT5FA9weWXX26y1ssMHz7cZK1rcM65evXqmTx79myTtT9JaJ5pLwrNSf0vnCu4tVvZ3CcnnQM6L7zwQpN/+OGHlP/9lFNO8caoUqWKyQsWLDA5aX6kQ+dhaAzWoryZR+n87nVt0dqtxx9/3OTDDz/cG0M/07TmUD9/dmUepVNzl23ziG9yAABAlNjkAACAKLHJAQAAUcrzmhy9XqlndISuAZYpU8bkrl27mvzss8+aHKpX0H9XOrVASfTfotfwy5cv791n9erVJn/00Ucma4+MTMm26+B6Hoy+nqF5pL0pevToYbL2mQj1ONF5Uq5cOZP19U2HnmWk51/ptXfn/DodrUvT/heZkk01OVr7lE79Qf369U2+5pprTB44cKDJoZosfX21H0/ofKAkuo7qWVXprEUvvviiyaF6okzItrVI6VoUOrtKaR3pyJEjTdY541zefKbpc9XaRj0zzzm/j92nn35qcqdOnUzm7CoAAIA8wCYHAABEiU0OAACIEpscAAAQpYw3Aww8hvez1q1bm/zVV1+ZnFQAHJKJ4ic9zNE558qWLWuyFghqQemuNHDaFdle7JeOtm3bmjxlyhST9fUKzZHixYubvGHDhlx6djsWmkdaaKrzRgsG87vYL7fl1xw6/vjjTR41apTJSQctOucXd+qBnblB11FdI51zrlq1aiZroX06B3TmhcKwFh166KEmT5gwwWR9/ULvX10XMvFZEZpHWsDfuHFjk6dOnWpyOp/PuYHCYwAAUKiwyQEAAFFikwMAAKKU7zU5etCic/517dq1a5usB9yFahi0MVLS9cx0Dte85ZZbTL711ltNfv31170xtJGhXsO/7bbbTL7xxhu9MfJCbNfBQw309GcVKlQwec2aNSaHGrnp9eTcmEcXX3yxyQ8//LDJl156qTfGddddl/J5HHXUUSZPmjTJGyMvxFSTE3rttCahVq1aJi9evNjkUJ2EziFd87RGJ9RMTtfE8ePHm6wHQE6cONEbY//990/5vHSMzz//3BsjL8S2FoU+j/Rn2mRP66NCdSw7uxaF1kT9XNSGkKeddprJb7/9tjdGt27dTNb17YknnjBZmh1cuwAAG+5JREFUG2bmFWpyAABAocImBwAARIlNDgAAiFLGa3L0b+xDPSO0J4j2b9Dr4kuXLvXG0Ovea9euNVlrY0LX0qtXr27ykiVLUj6vadOmeWPoQWsdO3Y0WesA6JOTHj0oM3TIoc417XFTqlQpkzdt2pT4uEm1XiE6j7SOQ2uBzjzzTG+M5s2bm6wHRWaqL47K5pocrVkI/Q71sODff//dZJ1joTmkvZZ0DF3vQvTQw/fff99kXWdCB8cuWrTIZO0jlam1R2X7WqTrSOigzHr16pk8e/Zskw8++GCTZ86c6Y2hc23lypUma51PqK6nX79+Jt93330p77Ns2TJvDFWjRo2U/z2/e3bxTQ4AAIgSmxwAABAlNjkAACBKu6X6j+mcp5GkS5cuJh9wwAEm33333d59tEbh66+/NrlmzZomn3XWWd4YM2bMMFmvLWqNznHHHeeNoefWXHXVVSbr76Ny5creGA8++KDJ8+bNM7lhw4Ym59d18byUG2eunHHGGSaXLl3a5Iceesi7j9bgfPHFFyZrrcwRRxzhjTF//nyTtV/Jt99+m/J5OufcgAEDTL7gggtM1uvgoffZf/7zH5P17But9YptHuXGHDrvvPNM1r5J99xzj3cfrZ/R+j/to9O3b19vjLFjx5q8bt06k7WOp1evXt4YWoeo/36dM9pHxznnHnvsMZP1TMAOHTqkfF4xyI15pP3RWrRoYXLos0TXff080rot7VfjnHOzZs0y+ZdffjFZa4H088o5f+0Jff5uT+eqc/75bR9//LHJuo7mV73gf/FNDgAAiBKbHAAAECU2OQAAIEpscgAAQJTy/YDOEC3mGz16tMn77LOPyRs3bvTG0MZJesinPkaogZMWpenBeek08nv22WdNvvDCC00OHQqZCdnegCsd+hrrAatKDyh0zi/gvOiii0zWppLpHKynjSp1HmmTSef8g1y1YDB0n0zI5maAKtTYUeeQFnumswYsX77c5MaNG6d83NAY+jOddyo0D19++WWTr776apNDTVUzIba1KJ15pAW9SYdvOufPgZIlS+7qU0xbaH/w/fffm6yNDEPFyplAM0AAAFCosMkBAABRYpMDAACiVCBrclTVqlVN1uuVPXv29O6jBxrWqVPH5KTDN51z7qCDDkr5vPS6+AcffODdpn79+iY3bdrU5NgPVnQuM/NIayNCqlSpYrI2C9TGlc7515v33Xdfk7Wh3Ny5c70xTj75ZJO1hkznszYtdM4/TPL00083WQ/ry5TYa3L0Z3pQpjaHbNeunTdGt27dTN5jjz1M1uamoXoaXa/0cfWg49Acatmypcl6sCIHdOaOUL2U1uTo+1ebTvbp08cbQ+deUp3WggULvDGaNGlisq5Fus48/fTT3hjauLB///7ebfIDNTkAAKBQYZMDAACixCYHAABEKStqcgoqvQYa6rVTUAT6eUR1HRyZkV/ziDkUD9Yi5AVqcgAAQKHCJgcAAESJTQ4AAIhSygNQ9G/7Q/0bsoVeB96V/jTaM6Mg1+CoUA+QTIlpHuWG3JiL+SXpzKS8ks75ToVJNs8h1iLkhnTXIr7JAQAAUWKTAwAAosQmBwAARIlNDgAAiFLKyp2YivtyozAvW34focK+/CxMzJbfW6ZkS5GoFmk6l68HyubL4xZU2fL7YC1CbggdwJzua8k3OQAAIEpscgAAQJTY5AAAgCilPKATAAAgW/FNDgAAiBKbHAAAECU2OQAAIEpscgAAQJTY5AAAgCixyQEAAFFikwMAAKLEJgcAAESJTQ4AAIgSmxwAABAlNjkAACBKbHIAAECU2OQAAIAosckBAABRYpMDAACixCYHAABEiU0OAACIEpscAAAQJTY5AAAgSmxyAABAlNjkAACAKLHJAQAAUWKTAwAAorRbqv9YtGjRbdvnbdtM9DIKriJF7H52y5YtORl87JQThXmUPfJrHukcYs5kL9Yi5IZ05xHf5AAAgCixyQEAAFHKSfX1XE5ODt/dRWrbtm0Z+4qYeRSvTM0j5lC8WIuQG3Y0j/gmBwAARIlNDgAAiBKbHAAAECU2OQAAIEpscgAAQJTY5AAAgCixyQEAAFFikwMAAKLEJgcAAESJTQ4AAIgSmxwAABAlNjkAACBKu+X3E8gmRYsWNXnLli2J98nJsWeGpToQFXEqUqRIyrx58+aU/90554oVK2byH3/8kUvPDtlAX3+dM6xF6SnsvwP996t0fh/Z9jvkmxwAABAlNjkAACBKbHIAAECUoq3J0euGu+++u8lDhgwx+YYbbvDGmDFjhslHHHGEydOmTTO5bt263hgHH3ywyZ9++qnJJUuWNHnDhg3eGEnSuUYaqvNAsgYNGpisv8fx48ebXK9ePW+MESNGmHzqqaeaPH36dJMbNmzojXHppZeafPvtt5tcp04dkxcsWOCNkRuYR9Zuu9klNPTe0znx999/m/zZZ5+ZHHr9v/zyS5N1Xfnqq69Mbt68uTfGwIEDTX7wwQdNrlChgsm//fabN0bSWlPQ16KCWj+yK783nXsDBgww+YknnvDGeOSRR0wePHiwyc8++6zJp5xyijfGYYcdZvLYsWNNLlWqlMnr16/3xtB/y9atW73bJNEa2R1hxQIAAFFikwMAAKLEJgcAAEQpZU1OQf17eL3Gd99993m3ufjii00uU6ZMyjFbtWrl/Wz//fc3ecWKFSnHKFGihPcz7V+xatUqk7X/hV7PdM6/fql1Inq9vmzZst4YWvtTGOl8Xrp0qcmhGoRq1aqZXK5cuZRjhmq7unbtavKaNWtSjrHXXnt5Y7Ro0cLk33//3WS9Pl+8eHFvDH0cnScTJ040uXHjxt4YWtuWKQVlLdLnsW7dOpP1veicX6tXs2ZNk7W2YNCgQd4Y+lro4+rz0vngnHP169c3WddRnXehMXQt0jVP68tCNWrUdfmv+eeff27yrFmzvPu0bt3aZP180jlQo0YNb4z+/fubfM4556QcIzQXdS164403TN5zzz1TZuf8f3/79u1NfvLJJ00O1amF1skQZhsAAIgSmxwAABAlNjkAACBKbHIAAECUclIV8OXk5BSISmMthtJmWqF/Q34USIYaGulz19usXLnS5GOPPdYb49577zV5+fLlJpcuXdrkY445xhtDi0xXrFiR+qS2XFRQ5pEWSWrxZuiQwz322GOnHiM0F5MOxdM5EZpHWqint/n+++9NDs2BF1980eSpU6earEXWffr08cbQYr9169ZlZB4VlDmk68pff/1lcjqvXZJ05pDOVc2h56EFoEnzThtOOufcVVddZfILL7xgcps2bUw+4IADvDF0vVqzZk2hW4u0qFv/kCC0FmXij0d0DoSehz73pMNiH330UW+MM8880+SHHnrIZC2aPv/88xOfx8aNG4PziG9yAABAlNjkAACAKLHJAQAAUcqKAzr1GvWbb75pcrt27bz7VK9e3WRtwlepUiWTZ8+e7Y2hB9b16NHDZG2e1r17d28MrYPQBk7dunUzWQ/ac85vMPfJJ5+YrPUX//zzjzeG1jEVRps2bTJ55syZJushl875tUxaH3X55ZebPGzYMG+Mjh07mqx1V3poa69evbwxPvjgA5MPOuggk7VWZuHChd4Yffv2NXnx4sUma5PJ0DwKNRksTLRm4fXXXze5S5cu3n20BuW7774zuUmTJibrHHPOn0O6bowaNcrkf//7394YzzzzjMk6hw499FCTtU7COb9G7frrrzdZGx2GaoP0fQj/MGidE875n4M///yzydp08osvvvDG0OaMWkOlB3jeeeed3hjvvPOOyWeffbbJukboZ6Bz/hy4+eabTdbP59DnV7oNQfkmBwAARIlNDgAAiBKbHAAAEKWs7JOj13k/++wz7z5ap6P9aLRHih465pzfAyPpeYX6oYSuSW9P+26EXg/tb7Fx40aT0+nDob0Mtm3bFn1vCu2joL9bvc47adIkbwytoRo+fLjJy5YtM1mvaTsX7jWxvXQOLEyaRyo0F5MOudQcqr/R98TmzZsLVZ8cpa/tU0895d1G+w1p3Z2+n08++WRvDP2962uVzjqiz1Vvk846ou+ppJ5loX5luhZt3bo1+rVI6Zr+xx9/mKy90Jzz61S0znTChAkm9+vXzxtD6+z09cqNtSidz8WktUeFnpfeZ0fziG9yAABAlNjkAACAKLHJAQAAUcqKmhztzXDuueeafMIJJ3j30euAK1asMLlnz54pb+9c8jVrvcYduqattxk6dKjJN910U8rHdC65x432SNHruyGFoSZHX9MqVaqY3KhRI5O7du3qjaE9IebPn29y6Np5Er2+rNe405mLhx9+uMkff/zx//w8ks5HCt0mU/UUBWUt0vPPBg8ebLKeyeOcc7/99pvJ+lpde+21JofqWHQN0Hop7TsSOnNNxzjqqKNMHjNmjMmhOohQ76TtJa2RIYVxLWrVqpXJp512msnHH3+8N4bWcj355JMmf/jhhyan00cmN9ai+vXrm/zTTz/t9PNQSfWDITuaR3yTAwAAosQmBwAARIlNDgAAiBKbHAAAEKWsKDxWekBn5cqVvdsceOCBJmtx7s42V0tHqFBPC/HKlStnshYM/vnnn94YSU28dkVhKPbT4kstvGzfvr3JnTt39sbQuTVo0KBcenY7lk4jv6SmbJmSqXlUUNciPeRVi9ud89ee8uXLm6zv73SKPZXeJ/RHELrmJRUJhx4zN9aewJjRr0U6L3799VeTx44da3LogE5dz/QA6cKOwmMAAFCosMkBAABRYpMDAACitFvyTfKfXjt+//33Tb7kkku8+zRv3tzk0qVLm/z777+bvO+++3pjzJs3z+QrrrjC5AceeMBkbdbknHP77befyXpNu1mzZibPmTPHGwO7Rg8C1NoIbajWqVMnb4wePXqkHENrqPQQPeecW716tclnn322yf/5z39MDs2jOnXqmLxmzRqTGzZsaHJ+1ejErlq1aiZrbYU2C3TOuYsvvtjkbt26mTx69GiTBwwY4I3xyiuvmKx1ido87vvvv/fG0HpAPfSzatWqKf87dp0eEF2yZEmTtWZHD/51zrnu3bubrJ+LWnNVtmxZbwz93DvuuONMHjVqlMn6WeuccxUrVjRZ19m2bduanBd1XM6Fa9dC+CYHAABEiU0OAACIEpscAAAQpQJZk5PU8+HRRx81OVSTo3UNJ554osl33XWXyR06dPDGmDFjhsktWrQwWa9FTps2zRtDFStWzOSRI0eafPLJJyeOgfQkHVz6+uuvm9yuXTtvjMWLF5t8+umnm/ziiy+arPUWzjk3d+5ck6tXr26y1s98++233hha66HX8GfNmmWy1oI5l3wYH3y69mzYsMFkrePSQxKd8+fIc889Z/Krr75qsq5Vzjk3ZcoUk/fff3+TtX5m1apV3hhaP6b9eq688kqTdY3ErtPftb7n9bDg+++/3xtDDwc+4ogjTB4+fLjJxxxzjDeGri21a9c2WQ9gXbBggTeG1rdqjc5JJ51k8muvveaNoe8r/SzNTXyTAwAAosQmBwAARIlNDgAAiFLGz65Kpy5A+5doH4m6deuavHHjRm+MWrVqmaznfui1x9D5V9onR88K0dqhpk2bemPoNXq9Vq7X0g866CBvjCRJZzSFZPt5MUn9apzzrxU/9thjJv/rX/8yWesenPNrH8qUKWOyvn969erljaE9TnTeaNaeOM45N3XqVJOTrp2H6ouS7ErNTuxnV+25554mf/LJJyZrLUXod6Y1DEm/55YtW3pjfPnllybr/Fehnk/aj0nrurT+LHR+Ul7I9rVIP6+09s855w499FCTtR5Q58g333zjjaGfe7oW6Tw67LDDvDE++ugjk/WzQ+nnqHPOvffeeyZrjzntxRM6zy1JvXr1TP7pp5+82wTqeji7CgAAFB5scgAAQJTY5AAAgCixyQEAAFHKeOFxOk477TST9fC5Y4891uQtW7Z4Y+y2m+1zqEVZmrVYyjm/gZPeRwsIf/vtt8QxrrnmGpMffvhhkzdt2uSNkReyvdgvHZ999pnJkyZNMlkPXA0dSKiFp/p+0RwqPNW5qPfRwuNQEbU+jyVLlpisB73qAZ55JabC49CBf9OnTzdZi/pbt25tcmg91XGT5lBoDdADHXXN0yLM0BharPzBBx+YrIfR5tXBiiq2tUg/F5zzC4n1c0GLc0O/+9C429OGeqH5rPMkSWhN1Ia2+gc6nTt3Nnnp0qXeGEl/LLMrc29H84hvcgAAQJTY5AAAgCixyQEAAFHK9wM6a9as6f2satWqJh944IEma91D6DA6vX45bNgwk/XwMm2S5JxzF110kcn33XefyXqYZuhQNW3ApWNoUzfsGp0zzjk3efJkk3v27GlyOocaapOup556yuTevXubfPPNN3tjDB482GStMXvooYdMPu+887wxtFHhPvvsY3Lo+jt2TqgJn86ho48+2mRdi2bOnOmNUapUKZP79Olj8tChQ03u16+fN8YzzzyT8j66Vmmtn3POvfvuuybrQYqZqsGJXaiB3o8//mhymzZtTNbf/dq1a70xtI7l448/NvmQQw4xOTSPdN4MHDjQ5CeeeMLk6667zhtDD5XWxqNa3xqaV6Fan7zCNzkAACBKbHIAAECU2OQAAIAoZbxPzu67726y/m2/c37PD71+d/XVV5t8wQUXJD6OHuKp/+5Qrx2tc9Dr73pA2vr1670xknpi5Jds702hv/tQn6PjjjvOZD3odcCAASYPHz7cG0PngNZQ5cbrq/VjoTEKyrxR2dwnR3uGhNYArZ/RvjmvvvqqyVo/6Jzf42b16tUm58Zrq+ud9mZyLnxwZEGQ7WtROvNID9fUvjA33XSTyRdeeKE3hvbWWbFihcnpfKYlSTo82Ln0Du7ND/TJAQAAhQqbHAAAECU2OQAAIEp5XpOj1ys1h2py9GwMrdHR6+Ihes1T+52EajiSNGjQwGTtfdCiRQvvPjNmzDBZ60K6detmMufFhJUoUcLk4sWLm7xhw4bEMVq1amXyxIkTTU7n+nPSmSvpSDrLKNQ7avHixSa/9957Jnfv3n2nn0duyKaaHP29a61M6LXUeXbxxRebfPvtt5usa5dz/tzUMXflvDo9h0rrbTp16uTd55NPPjFZ16KuXbvu9PPIDdm2Fmn9k9bUhWpW9DU/+OCDTR43bpzJoc9FrbHR8+xCZ94lSVqLtJbIOefmz59vsvbWOeecc3b6eeQGanIAAEChwiYHAABEiU0OAACIEpscAAAQpYw3A0yHNj3S5lnpHEaY1MgvN+hjhBpwaaGq5nXr1pmcqUZL2Vbstyvq169v8rx58/LjaSTSeaSFjc4516hRI5MXLFhgcjqH4uWFbCo8TuMxvJ9ddtllJushvPp+Dh24m9RQMjdowXNoLdJDPD/44AOT9Y8kMqUwrEUHHXSQyXrwazoNQXUe7Uqzv52lz8s5v4han6s23s0UCo8BAEChwiYHAABEiU0OAACIUr7X5ISug+tz0hqdtWvXmhy6bqjNlJIaNoWuYes1T23+17x5c5NHjBjhjdGxY0eTtYnXsGHDTB4yZIg3Rl6I7Tp4aA6oChUqmLxq1arE++hcTJpH6cznkSNHmnz22WebPGfOHG+MSpUqpXycRx55xGRtWpdXYqrJSaf+QBvmffjhh4ljaDPApIao6czld9991+TevXub/Morr3j30cajWht05513mnzDDTckPo/cENtaFKqp08+XpBqdUL2NHlSddDBoOmvR2LFjTdY58vbbb3tjHHbYYSnH7Ny5s8lffvmlN0ZeoCYHAAAUKmxyAABAlNjkAACAKGW8JkevV4YOItPeE3otUg9rDNHrldpHRB8jRA+5++ijj0zWa6DadyL0s0cffdTkTPUzUdl+HVxrJULzqGzZsiZrDU61atVMXrp0qTdG0lwsU6aMyaFDHtu2bWvyp59+arLW9fzyyy/eGHqI47777ptyjEzJ5pocPeAw9NqVK1fOZH0dBg0aZPK///1vbwxdr5YvX26yzuXQQYvt27c3ecKECSbrWjR79mxvDK1l1PUtE31XQrJ9LdLDn0P1naVKlTJZ15pTTz3V5JdfftkbQz/TdC7q4+rtnfPXos8//9xkXUdD9YE6X+vVq2dyQZtHfJMDAACixCYHAABEiU0OAACIkn/xcDv6d/a7Uj/Sv39/kytWrGjyvffe691H6x70uqD2DDnggAO8MdavX2+y9oTQuocTTzzRG0P/3n/06NEpx5w/f743hvbR0DGOPvpok0O1JdkuqbdMOrQ/SevWrU0eOnSodx8980z7lei14w4dOnhjLFy40GSdV1o/EZrPTZs2NXn8+PHebbYXOmNr+vTpJn/zzTcma8+m2OZRUk+QdOjZTVob8+qrr3r30bPl1qxZY7LWbIXO7fnkk09M1n5b+nrruuOcP5eT3kMff/yx97PHH3/c5CVLlpi8zz77mByqDcp2ubEW3XHHHSbrunHCCSd499HXT3+3+n7VWhnn/M8sXYv0MUKfaY0bNzZZ6wPV1KlTvZ+NGTPGZK0vqlWrlsmhWrdM4pscAAAQJTY5AAAgSmxyAABAlNjkAACAKBXIAzq1cdKiRYtM1uK+UGM/LQquUqWKyVqAFvo9aFGa3kefuzZncs65Qw45xORZs2alfIxMyfYGXIHH8H6mjSdnzpxpsh78qk2unPNf06SDMnNDaC4+99xzJl966aUma0FspmRzM8B0FCtWzGRde9L54wwtcNYmlEoLSp3z1zg96FeF1pVJkyaZnM7BsJlQGNYi/UzTP2jQtUeb1zrn3Ny5c01u06aNyel8pqmkw2DTmc96yPTDDz+c+Lh5gWaAAACgUGGTAwAAosQmBwAARKlA1uTodfCBAwea/MUXX5isB4Q559yQIUNM1pqFJk2amDxx4kRvDG3KpbVBP//8s8m33nqrN4Y+zvDhw73b5Idsuw6eVPsQmkd6G23Kp/OmatWq3hg33HCDyVoLodfa9RBE55yrUKGCyVovpo3t9PBF55yrX7++yQ0aNDA59tquIkWKmBczLw62DdUnaC2MNgzU+oTatWt7YzRr1szk1157LeXz0HoN55y77bbbTE5qhhhqKKlN2vbee2+T86v5XybXokzMo/+vvTvIQRAGogDKwstwGq7OiYhrx6aYWG07fW9nDIhJU3/g25bmoriYZfxtiYsBxi7ntm3bcRwvr2OnMG4WXLqOeExceDcec57n2zn2fa+es9dCpDo5AMBShBwAICUhBwBIqXsnh/+Iz1qv65qqkzOqFpvYtjjHv/QaR5nHUAstNp7sZbZ+4KhWm4sinRwAYClCDgCQkpADAKT0qL75eH271//f+d7dHiW/FNeIuFvjo2SUZ8WrP/fuNY7MRXUzdXB6ajEXjWL1uejTfQPdyQEAUhJyAICUhBwAICUhBwBIqVo8nrmUtbJSObRnMbHFZ49SiGtxHaN8lzulcdTr2s1FeXxaGP2FTAXtleaib8aMOzkAQEpCDgCQkpADAKRU3aATAGBW7uQAACkJOQBASkIOAJCSkAMApCTkAAApCTkAQEpPZOhRI1+83skAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1000: [discriminator loss: 0.38471829891204834, acc: 0.5] [gan loss: 1.700691, acc: 0.000000]\n",
            "1001: [discriminator loss: 0.5015870332717896, acc: 0.4296875] [gan loss: 2.809357, acc: 0.000000]\n",
            "1002: [discriminator loss: 0.38074636459350586, acc: 0.5] [gan loss: 1.701766, acc: 0.000000]\n",
            "1003: [discriminator loss: 0.5297238826751709, acc: 0.34375] [gan loss: 3.364573, acc: 0.000000]\n",
            "1004: [discriminator loss: 0.356853723526001, acc: 0.5] [gan loss: 1.643357, acc: 0.000000]\n",
            "1005: [discriminator loss: 0.49353164434432983, acc: 0.4296875] [gan loss: 2.782710, acc: 0.000000]\n",
            "1006: [discriminator loss: 0.38112345337867737, acc: 0.5] [gan loss: 1.968254, acc: 0.000000]\n",
            "1007: [discriminator loss: 0.5246958136558533, acc: 0.4140625] [gan loss: 2.547112, acc: 0.000000]\n",
            "1008: [discriminator loss: 0.48254483938217163, acc: 0.4921875] [gan loss: 1.998748, acc: 0.000000]\n",
            "1009: [discriminator loss: 0.47612494230270386, acc: 0.453125] [gan loss: 2.753142, acc: 0.000000]\n",
            "1010: [discriminator loss: 0.46323785185813904, acc: 0.5] [gan loss: 1.416452, acc: 0.000000]\n",
            "1011: [discriminator loss: 0.5458499789237976, acc: 0.3203125] [gan loss: 3.212037, acc: 0.000000]\n",
            "1012: [discriminator loss: 0.43171918392181396, acc: 0.5] [gan loss: 1.446872, acc: 0.000000]\n",
            "1013: [discriminator loss: 0.5319588780403137, acc: 0.3828125] [gan loss: 3.086885, acc: 0.000000]\n",
            "1014: [discriminator loss: 0.3691595494747162, acc: 0.5] [gan loss: 2.336442, acc: 0.000000]\n",
            "1015: [discriminator loss: 0.5025491118431091, acc: 0.4921875] [gan loss: 2.640035, acc: 0.000000]\n",
            "1016: [discriminator loss: 0.44622713327407837, acc: 0.5] [gan loss: 2.987253, acc: 0.000000]\n",
            "1017: [discriminator loss: 0.3745240271091461, acc: 0.5] [gan loss: 2.471514, acc: 0.000000]\n",
            "1018: [discriminator loss: 0.3970457911491394, acc: 0.5] [gan loss: 3.414026, acc: 0.000000]\n",
            "1019: [discriminator loss: 0.33773577213287354, acc: 0.5] [gan loss: 2.357424, acc: 0.000000]\n",
            "1020: [discriminator loss: 0.40269115567207336, acc: 0.5] [gan loss: 3.034308, acc: 0.000000]\n",
            "1021: [discriminator loss: 0.33842039108276367, acc: 0.5] [gan loss: 2.529156, acc: 0.000000]\n",
            "1022: [discriminator loss: 0.3397679030895233, acc: 0.5] [gan loss: 2.992078, acc: 0.000000]\n",
            "1023: [discriminator loss: 0.31592363119125366, acc: 0.5] [gan loss: 2.809677, acc: 0.000000]\n",
            "1024: [discriminator loss: 0.33656010031700134, acc: 0.5] [gan loss: 2.920506, acc: 0.000000]\n",
            "1025: [discriminator loss: 0.3001861572265625, acc: 0.5] [gan loss: 3.160857, acc: 0.000000]\n",
            "1026: [discriminator loss: 0.2931591272354126, acc: 0.5] [gan loss: 2.979630, acc: 0.000000]\n",
            "1027: [discriminator loss: 0.2892579138278961, acc: 0.5] [gan loss: 3.447237, acc: 0.000000]\n",
            "1028: [discriminator loss: 0.3315110504627228, acc: 0.5] [gan loss: 2.318380, acc: 0.000000]\n",
            "1029: [discriminator loss: 0.39189964532852173, acc: 0.5] [gan loss: 4.953782, acc: 0.000000]\n",
            "1030: [discriminator loss: 0.27673521637916565, acc: 0.5] [gan loss: 2.872408, acc: 0.000000]\n",
            "1031: [discriminator loss: 0.38433200120925903, acc: 0.5] [gan loss: 3.379015, acc: 0.000000]\n",
            "1032: [discriminator loss: 0.3447064161300659, acc: 0.5] [gan loss: 2.325484, acc: 0.000000]\n",
            "1033: [discriminator loss: 0.5143057703971863, acc: 0.3828125] [gan loss: 4.061860, acc: 0.000000]\n",
            "1034: [discriminator loss: 0.3721142113208771, acc: 0.5] [gan loss: 1.712009, acc: 0.000000]\n",
            "1035: [discriminator loss: 0.9352144002914429, acc: 0.0] [gan loss: 5.364157, acc: 0.000000]\n",
            "1036: [discriminator loss: 0.40748417377471924, acc: 0.5] [gan loss: 1.896017, acc: 0.000000]\n",
            "1037: [discriminator loss: 0.8139567375183105, acc: 0.0] [gan loss: 4.500559, acc: 0.000000]\n",
            "1038: [discriminator loss: 0.422959566116333, acc: 0.5] [gan loss: 2.069065, acc: 0.000000]\n",
            "1039: [discriminator loss: 0.6897374987602234, acc: 0.0625] [gan loss: 3.727930, acc: 0.000000]\n",
            "1040: [discriminator loss: 0.4291522800922394, acc: 0.5] [gan loss: 2.036970, acc: 0.000000]\n",
            "1041: [discriminator loss: 0.5999122262001038, acc: 0.203125] [gan loss: 3.541142, acc: 0.000000]\n",
            "1042: [discriminator loss: 0.441133588552475, acc: 0.5] [gan loss: 1.795136, acc: 0.000000]\n",
            "1043: [discriminator loss: 0.557878315448761, acc: 0.2578125] [gan loss: 3.187967, acc: 0.000000]\n",
            "1044: [discriminator loss: 0.4024132192134857, acc: 0.5] [gan loss: 2.002314, acc: 0.000000]\n",
            "1045: [discriminator loss: 0.4977169930934906, acc: 0.4609375] [gan loss: 2.881066, acc: 0.000000]\n",
            "1046: [discriminator loss: 0.394773006439209, acc: 0.5] [gan loss: 2.085898, acc: 0.000000]\n",
            "1047: [discriminator loss: 0.4828758239746094, acc: 0.4921875] [gan loss: 3.099836, acc: 0.000000]\n",
            "1048: [discriminator loss: 0.37184232473373413, acc: 0.5] [gan loss: 2.209362, acc: 0.000000]\n",
            "1049: [discriminator loss: 0.4426850378513336, acc: 0.484375] [gan loss: 3.120206, acc: 0.000000]\n",
            "1050: [discriminator loss: 0.3074105978012085, acc: 0.5] [gan loss: 2.329221, acc: 0.000000]\n",
            "1051: [discriminator loss: 0.4163360595703125, acc: 0.5] [gan loss: 2.890314, acc: 0.000000]\n",
            "1052: [discriminator loss: 0.3045632839202881, acc: 0.5] [gan loss: 2.440621, acc: 0.000000]\n",
            "1053: [discriminator loss: 0.35685545206069946, acc: 0.5] [gan loss: 2.463672, acc: 0.000000]\n",
            "1054: [discriminator loss: 0.3471865653991699, acc: 0.5] [gan loss: 2.766801, acc: 0.000000]\n",
            "1055: [discriminator loss: 0.3251767158508301, acc: 0.5] [gan loss: 2.210673, acc: 0.000000]\n",
            "1056: [discriminator loss: 0.3902771472930908, acc: 0.5] [gan loss: 3.895077, acc: 0.000000]\n",
            "1057: [discriminator loss: 0.293290376663208, acc: 0.5] [gan loss: 2.215509, acc: 0.000000]\n",
            "1058: [discriminator loss: 0.38093864917755127, acc: 0.5] [gan loss: 3.459990, acc: 0.000000]\n",
            "1059: [discriminator loss: 0.25780901312828064, acc: 0.5] [gan loss: 2.625850, acc: 0.000000]\n",
            "1060: [discriminator loss: 0.3128703832626343, acc: 0.5] [gan loss: 3.222509, acc: 0.000000]\n",
            "1061: [discriminator loss: 0.2888173460960388, acc: 0.5] [gan loss: 2.654665, acc: 0.000000]\n",
            "1062: [discriminator loss: 0.34845787286758423, acc: 0.5] [gan loss: 3.341217, acc: 0.000000]\n",
            "1063: [discriminator loss: 0.2520754635334015, acc: 0.5] [gan loss: 2.720468, acc: 0.000000]\n",
            "1064: [discriminator loss: 0.3326360285282135, acc: 0.5] [gan loss: 3.208770, acc: 0.000000]\n",
            "1065: [discriminator loss: 0.26178988814353943, acc: 0.5] [gan loss: 2.040705, acc: 0.000000]\n",
            "1066: [discriminator loss: 0.44988369941711426, acc: 0.4453125] [gan loss: 3.757459, acc: 0.000000]\n",
            "1067: [discriminator loss: 0.30553609132766724, acc: 0.5] [gan loss: 1.830882, acc: 0.000000]\n",
            "1068: [discriminator loss: 0.4328078031539917, acc: 0.5] [gan loss: 2.838396, acc: 0.000000]\n",
            "1069: [discriminator loss: 0.28989410400390625, acc: 0.5] [gan loss: 2.039009, acc: 0.000000]\n",
            "1070: [discriminator loss: 0.37401098012924194, acc: 0.5] [gan loss: 2.541460, acc: 0.000000]\n",
            "1071: [discriminator loss: 0.31932586431503296, acc: 0.5] [gan loss: 1.667792, acc: 0.000000]\n",
            "1072: [discriminator loss: 0.4232299327850342, acc: 0.5] [gan loss: 2.879026, acc: 0.000000]\n",
            "1073: [discriminator loss: 0.32657957077026367, acc: 0.5] [gan loss: 1.609619, acc: 0.000000]\n",
            "1074: [discriminator loss: 0.5794694423675537, acc: 0.171875] [gan loss: 4.001318, acc: 0.000000]\n",
            "1075: [discriminator loss: 0.36951935291290283, acc: 0.5] [gan loss: 1.254730, acc: 0.000000]\n",
            "1076: [discriminator loss: 0.6110571622848511, acc: 0.1328125] [gan loss: 2.592034, acc: 0.000000]\n",
            "1077: [discriminator loss: 0.4164161682128906, acc: 0.5] [gan loss: 1.189142, acc: 0.000000]\n",
            "1078: [discriminator loss: 0.5166479349136353, acc: 0.3515625] [gan loss: 2.337635, acc: 0.000000]\n",
            "1079: [discriminator loss: 0.42232459783554077, acc: 0.5] [gan loss: 1.535755, acc: 0.000000]\n",
            "1080: [discriminator loss: 0.48625966906547546, acc: 0.4296875] [gan loss: 2.529932, acc: 0.000000]\n",
            "1081: [discriminator loss: 0.39354684948921204, acc: 0.5] [gan loss: 1.513666, acc: 0.000000]\n",
            "1082: [discriminator loss: 0.5017651319503784, acc: 0.375] [gan loss: 2.771077, acc: 0.000000]\n",
            "1083: [discriminator loss: 0.3678082823753357, acc: 0.5] [gan loss: 1.674468, acc: 0.000000]\n",
            "1084: [discriminator loss: 0.4223487079143524, acc: 0.4765625] [gan loss: 2.485207, acc: 0.000000]\n",
            "1085: [discriminator loss: 0.35750240087509155, acc: 0.5] [gan loss: 1.729771, acc: 0.000000]\n",
            "1086: [discriminator loss: 0.3990122675895691, acc: 0.5] [gan loss: 2.484013, acc: 0.000000]\n",
            "1087: [discriminator loss: 0.3318640887737274, acc: 0.5] [gan loss: 1.609373, acc: 0.000000]\n",
            "1088: [discriminator loss: 0.3836634159088135, acc: 0.5] [gan loss: 2.309034, acc: 0.000000]\n",
            "1089: [discriminator loss: 0.3016800284385681, acc: 0.5] [gan loss: 1.682668, acc: 0.000000]\n",
            "1090: [discriminator loss: 0.371121346950531, acc: 0.5] [gan loss: 2.257389, acc: 0.000000]\n",
            "1091: [discriminator loss: 0.3146779239177704, acc: 0.5] [gan loss: 1.578053, acc: 0.000000]\n",
            "1092: [discriminator loss: 0.3434804677963257, acc: 0.5] [gan loss: 2.218012, acc: 0.000000]\n",
            "1093: [discriminator loss: 0.3387126922607422, acc: 0.5] [gan loss: 2.172705, acc: 0.000000]\n",
            "1094: [discriminator loss: 0.3310508728027344, acc: 0.5] [gan loss: 1.910968, acc: 0.000000]\n",
            "1095: [discriminator loss: 0.40404146909713745, acc: 0.4921875] [gan loss: 3.653945, acc: 0.000000]\n",
            "1096: [discriminator loss: 0.32765457034111023, acc: 0.5] [gan loss: 1.472251, acc: 0.000000]\n",
            "1097: [discriminator loss: 0.6338646411895752, acc: 0.0390625] [gan loss: 5.466563, acc: 0.000000]\n",
            "1098: [discriminator loss: 0.38705718517303467, acc: 0.5] [gan loss: 2.156579, acc: 0.000000]\n",
            "1099: [discriminator loss: 0.4315747618675232, acc: 0.5] [gan loss: 3.575008, acc: 0.000000]\n",
            "1100: [discriminator loss: 0.2723926305770874, acc: 0.5] [gan loss: 2.590038, acc: 0.000000]\n",
            "1101: [discriminator loss: 0.3748624920845032, acc: 0.4921875] [gan loss: 3.150761, acc: 0.000000]\n",
            "1102: [discriminator loss: 0.3659153878688812, acc: 0.5] [gan loss: 3.048028, acc: 0.000000]\n",
            "1103: [discriminator loss: 0.5069494247436523, acc: 0.4375] [gan loss: 4.139658, acc: 0.000000]\n",
            "1104: [discriminator loss: 0.42006435990333557, acc: 0.5] [gan loss: 2.791058, acc: 0.000000]\n",
            "1105: [discriminator loss: 0.5637727379798889, acc: 0.265625] [gan loss: 4.750680, acc: 0.000000]\n",
            "1106: [discriminator loss: 0.3362782299518585, acc: 0.5] [gan loss: 2.372099, acc: 0.000000]\n",
            "1107: [discriminator loss: 0.6503346562385559, acc: 0.0390625] [gan loss: 5.021382, acc: 0.000000]\n",
            "1108: [discriminator loss: 0.4211409389972687, acc: 0.5] [gan loss: 1.993286, acc: 0.000000]\n",
            "1109: [discriminator loss: 0.6014636158943176, acc: 0.078125] [gan loss: 4.250072, acc: 0.000000]\n",
            "1110: [discriminator loss: 0.3477160334587097, acc: 0.5] [gan loss: 2.587426, acc: 0.000000]\n",
            "1111: [discriminator loss: 0.5627377033233643, acc: 0.2421875] [gan loss: 3.643082, acc: 0.000000]\n",
            "1112: [discriminator loss: 0.40130558609962463, acc: 0.5] [gan loss: 2.028897, acc: 0.000000]\n",
            "1113: [discriminator loss: 0.5082807540893555, acc: 0.3984375] [gan loss: 3.158389, acc: 0.000000]\n",
            "1114: [discriminator loss: 0.33585941791534424, acc: 0.5] [gan loss: 2.040477, acc: 0.000000]\n",
            "1115: [discriminator loss: 0.5585710406303406, acc: 0.2890625] [gan loss: 3.232251, acc: 0.000000]\n",
            "1116: [discriminator loss: 0.4659035801887512, acc: 0.5] [gan loss: 2.626916, acc: 0.000000]\n",
            "1117: [discriminator loss: 0.4923214614391327, acc: 0.4921875] [gan loss: 2.675174, acc: 0.000000]\n",
            "1118: [discriminator loss: 0.4917494058609009, acc: 0.4765625] [gan loss: 2.490039, acc: 0.000000]\n",
            "1119: [discriminator loss: 0.5363401174545288, acc: 0.46875] [gan loss: 2.528523, acc: 0.000000]\n",
            "1120: [discriminator loss: 0.478446900844574, acc: 0.5] [gan loss: 2.496974, acc: 0.000000]\n",
            "1121: [discriminator loss: 0.4789268374443054, acc: 0.5] [gan loss: 2.701588, acc: 0.000000]\n",
            "1122: [discriminator loss: 0.3820551037788391, acc: 0.5] [gan loss: 2.350082, acc: 0.000000]\n",
            "1123: [discriminator loss: 0.4333875775337219, acc: 0.5] [gan loss: 2.636256, acc: 0.000000]\n",
            "1124: [discriminator loss: 0.3885020613670349, acc: 0.5] [gan loss: 1.959925, acc: 0.000000]\n",
            "1125: [discriminator loss: 0.434944748878479, acc: 0.5] [gan loss: 2.901453, acc: 0.000000]\n",
            "1126: [discriminator loss: 0.3380371630191803, acc: 0.5] [gan loss: 2.015615, acc: 0.000000]\n",
            "1127: [discriminator loss: 0.4230092763900757, acc: 0.5] [gan loss: 3.487447, acc: 0.000000]\n",
            "1128: [discriminator loss: 0.3006604313850403, acc: 0.5] [gan loss: 2.082485, acc: 0.000000]\n",
            "1129: [discriminator loss: 0.4890444576740265, acc: 0.484375] [gan loss: 3.664008, acc: 0.000000]\n",
            "1130: [discriminator loss: 0.30872225761413574, acc: 0.5] [gan loss: 2.199183, acc: 0.000000]\n",
            "1131: [discriminator loss: 0.48298829793930054, acc: 0.453125] [gan loss: 4.706799, acc: 0.000000]\n",
            "1132: [discriminator loss: 0.3232448697090149, acc: 0.5] [gan loss: 1.953905, acc: 0.000000]\n",
            "1133: [discriminator loss: 0.41595762968063354, acc: 0.5] [gan loss: 2.750402, acc: 0.000000]\n",
            "1134: [discriminator loss: 0.32036271691322327, acc: 0.5] [gan loss: 1.109173, acc: 0.000000]\n",
            "1135: [discriminator loss: 0.4351330101490021, acc: 0.4296875] [gan loss: 1.881411, acc: 0.000000]\n",
            "1136: [discriminator loss: 0.3093758225440979, acc: 0.5] [gan loss: 0.963908, acc: 0.109375]\n",
            "1137: [discriminator loss: 0.4887843132019043, acc: 0.421875] [gan loss: 2.660564, acc: 0.000000]\n",
            "1138: [discriminator loss: 0.4027803838253021, acc: 0.5] [gan loss: 0.829413, acc: 0.203125]\n",
            "1139: [discriminator loss: 0.657525897026062, acc: 0.1328125] [gan loss: 4.222663, acc: 0.000000]\n",
            "1140: [discriminator loss: 0.32101041078567505, acc: 0.5] [gan loss: 1.344838, acc: 0.000000]\n",
            "1141: [discriminator loss: 0.5247046947479248, acc: 0.359375] [gan loss: 2.598111, acc: 0.000000]\n",
            "1142: [discriminator loss: 0.32242000102996826, acc: 0.5] [gan loss: 1.517059, acc: 0.000000]\n",
            "1143: [discriminator loss: 0.4737551808357239, acc: 0.4375] [gan loss: 2.634974, acc: 0.000000]\n",
            "1144: [discriminator loss: 0.353207528591156, acc: 0.5] [gan loss: 2.234900, acc: 0.000000]\n",
            "1145: [discriminator loss: 0.40046828985214233, acc: 0.484375] [gan loss: 3.402216, acc: 0.000000]\n",
            "1146: [discriminator loss: 0.2893505394458771, acc: 0.5] [gan loss: 2.205416, acc: 0.000000]\n",
            "1147: [discriminator loss: 0.43790867924690247, acc: 0.46875] [gan loss: 4.423842, acc: 0.000000]\n",
            "1148: [discriminator loss: 0.2852243185043335, acc: 0.5] [gan loss: 2.171751, acc: 0.000000]\n",
            "1149: [discriminator loss: 0.4520964026451111, acc: 0.484375] [gan loss: 4.258969, acc: 0.000000]\n",
            "1150: [discriminator loss: 0.27515143156051636, acc: 0.5] [gan loss: 2.693129, acc: 0.000000]\n",
            "1151: [discriminator loss: 0.3917164206504822, acc: 0.484375] [gan loss: 3.869657, acc: 0.000000]\n",
            "1152: [discriminator loss: 0.2989247143268585, acc: 0.5] [gan loss: 1.961772, acc: 0.000000]\n",
            "1153: [discriminator loss: 0.376902312040329, acc: 0.4921875] [gan loss: 2.715549, acc: 0.000000]\n",
            "1154: [discriminator loss: 0.29340094327926636, acc: 0.5] [gan loss: 1.497654, acc: 0.000000]\n",
            "1155: [discriminator loss: 0.3612113296985626, acc: 0.5] [gan loss: 2.179377, acc: 0.000000]\n",
            "1156: [discriminator loss: 0.31947144865989685, acc: 0.5] [gan loss: 1.488448, acc: 0.000000]\n",
            "1157: [discriminator loss: 0.35547134280204773, acc: 0.5] [gan loss: 2.897869, acc: 0.000000]\n",
            "1158: [discriminator loss: 0.29215046763420105, acc: 0.5] [gan loss: 1.374475, acc: 0.000000]\n",
            "1159: [discriminator loss: 0.47176453471183777, acc: 0.453125] [gan loss: 4.288951, acc: 0.000000]\n",
            "1160: [discriminator loss: 0.286906898021698, acc: 0.5] [gan loss: 1.995132, acc: 0.000000]\n",
            "1161: [discriminator loss: 0.37275245785713196, acc: 0.5] [gan loss: 3.432883, acc: 0.000000]\n",
            "1162: [discriminator loss: 0.26717323064804077, acc: 0.5] [gan loss: 1.985882, acc: 0.000000]\n",
            "1163: [discriminator loss: 0.4231502413749695, acc: 0.46875] [gan loss: 4.345768, acc: 0.000000]\n",
            "1164: [discriminator loss: 0.3433355987071991, acc: 0.5] [gan loss: 2.170544, acc: 0.000000]\n",
            "1165: [discriminator loss: 0.4735337495803833, acc: 0.4609375] [gan loss: 4.026711, acc: 0.000000]\n",
            "1166: [discriminator loss: 0.2503926455974579, acc: 0.5] [gan loss: 2.211341, acc: 0.000000]\n",
            "1167: [discriminator loss: 0.3661800026893616, acc: 0.4765625] [gan loss: 2.493510, acc: 0.000000]\n",
            "1168: [discriminator loss: 0.3950526714324951, acc: 0.484375] [gan loss: 2.420302, acc: 0.000000]\n",
            "1169: [discriminator loss: 0.3708750903606415, acc: 0.5] [gan loss: 2.250742, acc: 0.000000]\n",
            "1170: [discriminator loss: 0.36965489387512207, acc: 0.5] [gan loss: 1.952980, acc: 0.000000]\n",
            "1171: [discriminator loss: 0.4240787625312805, acc: 0.4765625] [gan loss: 2.869080, acc: 0.000000]\n",
            "1172: [discriminator loss: 0.3272203803062439, acc: 0.5] [gan loss: 1.001376, acc: 0.062500]\n",
            "1173: [discriminator loss: 0.5308226346969604, acc: 0.3046875] [gan loss: 2.656056, acc: 0.000000]\n",
            "1174: [discriminator loss: 0.3243829309940338, acc: 0.5] [gan loss: 0.489192, acc: 0.921875]\n",
            "1175: [discriminator loss: 0.34893345832824707, acc: 0.4921875] [gan loss: 0.457380, acc: 0.953125]\n",
            "1176: [discriminator loss: 0.3575541079044342, acc: 0.5] [gan loss: 0.364087, acc: 0.984375]\n",
            "1177: [discriminator loss: 0.3188053071498871, acc: 0.5] [gan loss: 0.402545, acc: 0.984375]\n",
            "1178: [discriminator loss: 0.3394564092159271, acc: 0.4921875] [gan loss: 0.451502, acc: 0.921875]\n",
            "1179: [discriminator loss: 0.45601192116737366, acc: 0.4609375] [gan loss: 1.475201, acc: 0.000000]\n",
            "1180: [discriminator loss: 0.43941640853881836, acc: 0.46875] [gan loss: 2.103047, acc: 0.000000]\n",
            "1181: [discriminator loss: 0.40713202953338623, acc: 0.5] [gan loss: 1.486016, acc: 0.000000]\n",
            "1182: [discriminator loss: 0.5538413524627686, acc: 0.3203125] [gan loss: 4.610529, acc: 0.000000]\n",
            "1183: [discriminator loss: 0.4535728693008423, acc: 0.5] [gan loss: 0.753368, acc: 0.328125]\n",
            "1184: [discriminator loss: 0.6082730293273926, acc: 0.125] [gan loss: 3.507610, acc: 0.000000]\n",
            "1185: [discriminator loss: 0.32681146264076233, acc: 0.5] [gan loss: 1.783046, acc: 0.000000]\n",
            "1186: [discriminator loss: 0.39711275696754456, acc: 0.5] [gan loss: 2.309664, acc: 0.000000]\n",
            "1187: [discriminator loss: 0.3616582453250885, acc: 0.5] [gan loss: 2.331928, acc: 0.000000]\n",
            "1188: [discriminator loss: 0.3701671361923218, acc: 0.5] [gan loss: 2.470913, acc: 0.000000]\n",
            "1189: [discriminator loss: 0.34199124574661255, acc: 0.5] [gan loss: 2.284914, acc: 0.000000]\n",
            "1190: [discriminator loss: 0.34840914607048035, acc: 0.5] [gan loss: 2.344950, acc: 0.000000]\n",
            "1191: [discriminator loss: 0.3179841935634613, acc: 0.5] [gan loss: 1.755110, acc: 0.000000]\n",
            "1192: [discriminator loss: 0.387543261051178, acc: 0.4921875] [gan loss: 2.696725, acc: 0.000000]\n",
            "1193: [discriminator loss: 0.2920756936073303, acc: 0.5] [gan loss: 1.008148, acc: 0.000000]\n",
            "1194: [discriminator loss: 0.3879178762435913, acc: 0.4921875] [gan loss: 2.308176, acc: 0.000000]\n",
            "1195: [discriminator loss: 0.28019407391548157, acc: 0.5] [gan loss: 0.760043, acc: 0.343750]\n",
            "1196: [discriminator loss: 0.3435099124908447, acc: 0.5] [gan loss: 1.505258, acc: 0.000000]\n",
            "1197: [discriminator loss: 0.3046167194843292, acc: 0.5] [gan loss: 1.107678, acc: 0.000000]\n",
            "1198: [discriminator loss: 0.36673447489738464, acc: 0.5] [gan loss: 2.039136, acc: 0.000000]\n",
            "1199: [discriminator loss: 0.25211387872695923, acc: 0.5] [gan loss: 0.951881, acc: 0.000000]\n",
            "1200: [discriminator loss: 0.3555349111557007, acc: 0.5] [gan loss: 2.860924, acc: 0.000000]\n",
            "1201: [discriminator loss: 0.28443387150764465, acc: 0.5] [gan loss: 0.592213, acc: 0.812500]\n",
            "1202: [discriminator loss: 0.38312989473342896, acc: 0.484375] [gan loss: 2.431183, acc: 0.000000]\n",
            "1203: [discriminator loss: 0.2768743932247162, acc: 0.5] [gan loss: 0.795538, acc: 0.140625]\n",
            "1204: [discriminator loss: 0.335823118686676, acc: 0.5] [gan loss: 2.408228, acc: 0.000000]\n",
            "1205: [discriminator loss: 0.26555609703063965, acc: 0.5] [gan loss: 1.053650, acc: 0.000000]\n",
            "1206: [discriminator loss: 0.4340806007385254, acc: 0.4609375] [gan loss: 4.150042, acc: 0.000000]\n",
            "1207: [discriminator loss: 0.25583508610725403, acc: 0.5] [gan loss: 1.807860, acc: 0.000000]\n",
            "1208: [discriminator loss: 0.3474249243736267, acc: 0.5] [gan loss: 2.753896, acc: 0.000000]\n",
            "1209: [discriminator loss: 0.2844248116016388, acc: 0.5] [gan loss: 1.938670, acc: 0.000000]\n",
            "1210: [discriminator loss: 0.36713656783103943, acc: 0.5] [gan loss: 3.927527, acc: 0.000000]\n",
            "1211: [discriminator loss: 0.24793142080307007, acc: 0.5] [gan loss: 2.274369, acc: 0.000000]\n",
            "1212: [discriminator loss: 0.3270135223865509, acc: 0.5] [gan loss: 4.363185, acc: 0.000000]\n",
            "1213: [discriminator loss: 0.22691312432289124, acc: 0.5] [gan loss: 2.430882, acc: 0.000000]\n",
            "1214: [discriminator loss: 0.3409062325954437, acc: 0.5] [gan loss: 4.334924, acc: 0.000000]\n",
            "1215: [discriminator loss: 0.23972906172275543, acc: 0.5] [gan loss: 2.571987, acc: 0.000000]\n",
            "1216: [discriminator loss: 0.3843841552734375, acc: 0.5] [gan loss: 4.567118, acc: 0.000000]\n",
            "1217: [discriminator loss: 0.2541266083717346, acc: 0.5] [gan loss: 2.398923, acc: 0.000000]\n",
            "1218: [discriminator loss: 0.42857280373573303, acc: 0.4921875] [gan loss: 3.926786, acc: 0.000000]\n",
            "1219: [discriminator loss: 0.2778809070587158, acc: 0.5] [gan loss: 1.713529, acc: 0.000000]\n",
            "1220: [discriminator loss: 0.3419637978076935, acc: 0.5] [gan loss: 2.570552, acc: 0.000000]\n",
            "1221: [discriminator loss: 0.30348753929138184, acc: 0.5] [gan loss: 2.160329, acc: 0.000000]\n",
            "1222: [discriminator loss: 0.3155744969844818, acc: 0.5] [gan loss: 2.391629, acc: 0.000000]\n",
            "1223: [discriminator loss: 0.3690459430217743, acc: 0.5] [gan loss: 4.016245, acc: 0.000000]\n",
            "1224: [discriminator loss: 0.29103678464889526, acc: 0.5] [gan loss: 2.038535, acc: 0.000000]\n",
            "1225: [discriminator loss: 0.4031641483306885, acc: 0.5] [gan loss: 4.623508, acc: 0.000000]\n",
            "1226: [discriminator loss: 0.23443853855133057, acc: 0.5] [gan loss: 3.014841, acc: 0.000000]\n",
            "1227: [discriminator loss: 0.30699121952056885, acc: 0.5] [gan loss: 3.131124, acc: 0.000000]\n",
            "1228: [discriminator loss: 0.27353352308273315, acc: 0.5] [gan loss: 3.445035, acc: 0.000000]\n",
            "1229: [discriminator loss: 0.25870123505592346, acc: 0.5] [gan loss: 3.220691, acc: 0.000000]\n",
            "1230: [discriminator loss: 0.2760166525840759, acc: 0.5] [gan loss: 3.108021, acc: 0.000000]\n",
            "1231: [discriminator loss: 0.2715360224246979, acc: 0.5] [gan loss: 3.311415, acc: 0.000000]\n",
            "1232: [discriminator loss: 0.2556864321231842, acc: 0.5] [gan loss: 3.239889, acc: 0.000000]\n",
            "1233: [discriminator loss: 0.23986107110977173, acc: 0.5] [gan loss: 3.024247, acc: 0.000000]\n",
            "1234: [discriminator loss: 0.27570098638534546, acc: 0.5] [gan loss: 3.636802, acc: 0.000000]\n",
            "1235: [discriminator loss: 0.29117316007614136, acc: 0.5] [gan loss: 1.990058, acc: 0.000000]\n",
            "1236: [discriminator loss: 0.4870110750198364, acc: 0.40625] [gan loss: 5.554219, acc: 0.000000]\n",
            "1237: [discriminator loss: 0.27504828572273254, acc: 0.5] [gan loss: 3.213156, acc: 0.000000]\n",
            "1238: [discriminator loss: 0.2619428038597107, acc: 0.5] [gan loss: 2.816215, acc: 0.000000]\n",
            "1239: [discriminator loss: 0.28897714614868164, acc: 0.5] [gan loss: 3.169925, acc: 0.000000]\n",
            "1240: [discriminator loss: 0.26467618346214294, acc: 0.5] [gan loss: 2.790402, acc: 0.000000]\n",
            "1241: [discriminator loss: 0.3603765368461609, acc: 0.5] [gan loss: 3.430237, acc: 0.000000]\n",
            "1242: [discriminator loss: 0.25393882393836975, acc: 0.5] [gan loss: 2.978762, acc: 0.000000]\n",
            "1243: [discriminator loss: 0.3260393738746643, acc: 0.5] [gan loss: 3.086953, acc: 0.000000]\n",
            "1244: [discriminator loss: 0.29416123032569885, acc: 0.5] [gan loss: 2.768712, acc: 0.000000]\n",
            "1245: [discriminator loss: 0.37921273708343506, acc: 0.5] [gan loss: 3.405721, acc: 0.000000]\n",
            "1246: [discriminator loss: 0.31270554661750793, acc: 0.5] [gan loss: 2.370867, acc: 0.000000]\n",
            "1247: [discriminator loss: 0.4346689283847809, acc: 0.5] [gan loss: 3.292356, acc: 0.000000]\n",
            "1248: [discriminator loss: 0.3637194037437439, acc: 0.5] [gan loss: 2.471908, acc: 0.000000]\n",
            "1249: [discriminator loss: 0.41364046931266785, acc: 0.5] [gan loss: 2.701412, acc: 0.000000]\n",
            "1250: [discriminator loss: 0.3453156650066376, acc: 0.5] [gan loss: 1.800627, acc: 0.000000]\n",
            "1251: [discriminator loss: 0.4078630805015564, acc: 0.484375] [gan loss: 2.912709, acc: 0.000000]\n",
            "1252: [discriminator loss: 0.32533758878707886, acc: 0.5] [gan loss: 2.163060, acc: 0.000000]\n",
            "1253: [discriminator loss: 0.36399710178375244, acc: 0.5] [gan loss: 2.598040, acc: 0.000000]\n",
            "1254: [discriminator loss: 0.32353460788726807, acc: 0.5] [gan loss: 2.825112, acc: 0.000000]\n",
            "1255: [discriminator loss: 0.29308322072029114, acc: 0.5] [gan loss: 2.475634, acc: 0.000000]\n",
            "1256: [discriminator loss: 0.33085137605667114, acc: 0.5] [gan loss: 3.912344, acc: 0.000000]\n",
            "1257: [discriminator loss: 0.3104392886161804, acc: 0.5] [gan loss: 2.108392, acc: 0.000000]\n",
            "1258: [discriminator loss: 0.46038493514060974, acc: 0.453125] [gan loss: 4.368524, acc: 0.000000]\n",
            "1259: [discriminator loss: 0.32340341806411743, acc: 0.5] [gan loss: 2.208086, acc: 0.000000]\n",
            "1260: [discriminator loss: 0.623242199420929, acc: 0.21875] [gan loss: 3.966579, acc: 0.000000]\n",
            "1261: [discriminator loss: 0.3264457881450653, acc: 0.5] [gan loss: 2.187428, acc: 0.000000]\n",
            "1262: [discriminator loss: 0.5102222561836243, acc: 0.4765625] [gan loss: 3.805155, acc: 0.000000]\n",
            "1263: [discriminator loss: 0.2817743420600891, acc: 0.5] [gan loss: 2.518669, acc: 0.000000]\n",
            "1264: [discriminator loss: 0.36993035674095154, acc: 0.5] [gan loss: 3.420515, acc: 0.000000]\n",
            "1265: [discriminator loss: 0.3355875015258789, acc: 0.5] [gan loss: 2.281147, acc: 0.000000]\n",
            "1266: [discriminator loss: 0.42181119322776794, acc: 0.5] [gan loss: 3.847975, acc: 0.000000]\n",
            "1267: [discriminator loss: 0.30695366859436035, acc: 0.5] [gan loss: 2.425441, acc: 0.000000]\n",
            "1268: [discriminator loss: 0.46937131881713867, acc: 0.4765625] [gan loss: 3.693347, acc: 0.000000]\n",
            "1269: [discriminator loss: 0.2950027585029602, acc: 0.5] [gan loss: 2.715094, acc: 0.000000]\n",
            "1270: [discriminator loss: 0.4329620897769928, acc: 0.5] [gan loss: 2.966672, acc: 0.000000]\n",
            "1271: [discriminator loss: 0.3553122580051422, acc: 0.5] [gan loss: 2.828629, acc: 0.000000]\n",
            "1272: [discriminator loss: 0.37031087279319763, acc: 0.5] [gan loss: 2.593235, acc: 0.000000]\n",
            "1273: [discriminator loss: 0.39860767126083374, acc: 0.5] [gan loss: 2.494121, acc: 0.000000]\n",
            "1274: [discriminator loss: 0.40382120013237, acc: 0.5] [gan loss: 2.534000, acc: 0.000000]\n",
            "1275: [discriminator loss: 0.366808146238327, acc: 0.5] [gan loss: 2.761445, acc: 0.000000]\n",
            "1276: [discriminator loss: 0.38398563861846924, acc: 0.5] [gan loss: 2.227117, acc: 0.000000]\n",
            "1277: [discriminator loss: 0.4471392333507538, acc: 0.5] [gan loss: 3.756640, acc: 0.000000]\n",
            "1278: [discriminator loss: 0.3493688404560089, acc: 0.5] [gan loss: 1.860418, acc: 0.000000]\n",
            "1279: [discriminator loss: 0.4626900553703308, acc: 0.484375] [gan loss: 3.731273, acc: 0.000000]\n",
            "1280: [discriminator loss: 0.2907811403274536, acc: 0.5] [gan loss: 2.241533, acc: 0.000000]\n",
            "1281: [discriminator loss: 0.3930988013744354, acc: 0.5] [gan loss: 3.192618, acc: 0.000000]\n",
            "1282: [discriminator loss: 0.2999342978000641, acc: 0.5] [gan loss: 2.630633, acc: 0.000000]\n",
            "1283: [discriminator loss: 0.3735487163066864, acc: 0.5] [gan loss: 3.290268, acc: 0.000000]\n",
            "1284: [discriminator loss: 0.34272050857543945, acc: 0.5] [gan loss: 2.658131, acc: 0.000000]\n",
            "1285: [discriminator loss: 0.41930854320526123, acc: 0.5] [gan loss: 4.036290, acc: 0.000000]\n",
            "1286: [discriminator loss: 0.3715062737464905, acc: 0.5] [gan loss: 1.722436, acc: 0.000000]\n",
            "1287: [discriminator loss: 0.601235568523407, acc: 0.0703125] [gan loss: 4.433695, acc: 0.000000]\n",
            "1288: [discriminator loss: 0.35224270820617676, acc: 0.5] [gan loss: 2.272447, acc: 0.000000]\n",
            "1289: [discriminator loss: 0.4038981795310974, acc: 0.5] [gan loss: 2.870010, acc: 0.000000]\n",
            "1290: [discriminator loss: 0.3307841420173645, acc: 0.5] [gan loss: 2.611436, acc: 0.000000]\n",
            "1291: [discriminator loss: 0.36188140511512756, acc: 0.5] [gan loss: 2.596410, acc: 0.000000]\n",
            "1292: [discriminator loss: 0.37306493520736694, acc: 0.5] [gan loss: 2.907491, acc: 0.000000]\n",
            "1293: [discriminator loss: 0.325492262840271, acc: 0.5] [gan loss: 2.634673, acc: 0.000000]\n",
            "1294: [discriminator loss: 0.3390918970108032, acc: 0.5] [gan loss: 3.011455, acc: 0.000000]\n",
            "1295: [discriminator loss: 0.32904839515686035, acc: 0.5] [gan loss: 2.822172, acc: 0.000000]\n",
            "1296: [discriminator loss: 0.39233869314193726, acc: 0.5] [gan loss: 3.216921, acc: 0.000000]\n",
            "1297: [discriminator loss: 0.3733386695384979, acc: 0.5] [gan loss: 1.933245, acc: 0.000000]\n",
            "1298: [discriminator loss: 0.46114489436149597, acc: 0.4921875] [gan loss: 3.099024, acc: 0.000000]\n",
            "1299: [discriminator loss: 0.3942868113517761, acc: 0.5] [gan loss: 1.718740, acc: 0.000000]\n",
            "1300: [discriminator loss: 0.604118287563324, acc: 0.0703125] [gan loss: 3.006307, acc: 0.000000]\n",
            "1301: [discriminator loss: 0.4691639244556427, acc: 0.5] [gan loss: 2.197356, acc: 0.000000]\n",
            "1302: [discriminator loss: 0.5156483054161072, acc: 0.375] [gan loss: 3.027153, acc: 0.000000]\n",
            "1303: [discriminator loss: 0.4296568036079407, acc: 0.5] [gan loss: 2.166684, acc: 0.000000]\n",
            "1304: [discriminator loss: 0.47453564405441284, acc: 0.4765625] [gan loss: 2.681771, acc: 0.000000]\n",
            "1305: [discriminator loss: 0.37395811080932617, acc: 0.5] [gan loss: 2.484820, acc: 0.000000]\n",
            "1306: [discriminator loss: 0.40008801221847534, acc: 0.5] [gan loss: 2.527962, acc: 0.000000]\n",
            "1307: [discriminator loss: 0.4281768798828125, acc: 0.5] [gan loss: 2.614108, acc: 0.000000]\n",
            "1308: [discriminator loss: 0.37062686681747437, acc: 0.5] [gan loss: 3.039061, acc: 0.000000]\n",
            "1309: [discriminator loss: 0.36926817893981934, acc: 0.5] [gan loss: 2.608111, acc: 0.000000]\n",
            "1310: [discriminator loss: 0.3362031877040863, acc: 0.5] [gan loss: 3.053706, acc: 0.000000]\n",
            "1311: [discriminator loss: 0.3294887840747833, acc: 0.5] [gan loss: 2.622510, acc: 0.000000]\n",
            "1312: [discriminator loss: 0.32620006799697876, acc: 0.5] [gan loss: 3.286250, acc: 0.000000]\n",
            "1313: [discriminator loss: 0.3174660801887512, acc: 0.5] [gan loss: 2.657346, acc: 0.000000]\n",
            "1314: [discriminator loss: 0.3412325382232666, acc: 0.5] [gan loss: 3.829530, acc: 0.000000]\n",
            "1315: [discriminator loss: 0.30489906668663025, acc: 0.5] [gan loss: 2.281674, acc: 0.000000]\n",
            "1316: [discriminator loss: 0.38582032918930054, acc: 0.5] [gan loss: 3.899181, acc: 0.000000]\n",
            "1317: [discriminator loss: 0.3214658200740814, acc: 0.5] [gan loss: 2.198817, acc: 0.000000]\n",
            "1318: [discriminator loss: 0.47611862421035767, acc: 0.453125] [gan loss: 3.951080, acc: 0.000000]\n",
            "1319: [discriminator loss: 0.29306188225746155, acc: 0.5] [gan loss: 2.348730, acc: 0.000000]\n",
            "1320: [discriminator loss: 0.36571815609931946, acc: 0.5] [gan loss: 2.716566, acc: 0.000000]\n",
            "1321: [discriminator loss: 0.3088114857673645, acc: 0.5] [gan loss: 2.582687, acc: 0.000000]\n",
            "1322: [discriminator loss: 0.340258926153183, acc: 0.5] [gan loss: 2.489092, acc: 0.000000]\n",
            "1323: [discriminator loss: 0.41798216104507446, acc: 0.5] [gan loss: 2.904435, acc: 0.000000]\n",
            "1324: [discriminator loss: 0.413211852312088, acc: 0.5] [gan loss: 2.421828, acc: 0.000000]\n",
            "1325: [discriminator loss: 0.5718191862106323, acc: 0.25] [gan loss: 3.280710, acc: 0.000000]\n",
            "1326: [discriminator loss: 0.46629488468170166, acc: 0.5] [gan loss: 1.295040, acc: 0.000000]\n",
            "1327: [discriminator loss: 0.9285602569580078, acc: 0.0] [gan loss: 4.017300, acc: 0.000000]\n",
            "1328: [discriminator loss: 0.5591573715209961, acc: 0.5] [gan loss: 1.255297, acc: 0.000000]\n",
            "1329: [discriminator loss: 0.7626174688339233, acc: 0.0] [gan loss: 3.506289, acc: 0.000000]\n",
            "1330: [discriminator loss: 0.38844379782676697, acc: 0.5] [gan loss: 1.849468, acc: 0.000000]\n",
            "1331: [discriminator loss: 0.5880948901176453, acc: 0.2578125] [gan loss: 3.172945, acc: 0.000000]\n",
            "1332: [discriminator loss: 0.3908935785293579, acc: 0.5] [gan loss: 2.081851, acc: 0.000000]\n",
            "1333: [discriminator loss: 0.49243977665901184, acc: 0.4140625] [gan loss: 2.875138, acc: 0.000000]\n",
            "1334: [discriminator loss: 0.32820236682891846, acc: 0.5] [gan loss: 1.927888, acc: 0.000000]\n",
            "1335: [discriminator loss: 0.532313346862793, acc: 0.3828125] [gan loss: 3.341908, acc: 0.000000]\n",
            "1336: [discriminator loss: 0.33600038290023804, acc: 0.5] [gan loss: 1.750395, acc: 0.000000]\n",
            "1337: [discriminator loss: 0.47031089663505554, acc: 0.46875] [gan loss: 3.207898, acc: 0.000000]\n",
            "1338: [discriminator loss: 0.303281307220459, acc: 0.5] [gan loss: 1.805506, acc: 0.000000]\n",
            "1339: [discriminator loss: 0.4722375273704529, acc: 0.5] [gan loss: 2.708147, acc: 0.000000]\n",
            "1340: [discriminator loss: 0.3456857204437256, acc: 0.5] [gan loss: 1.978526, acc: 0.000000]\n",
            "1341: [discriminator loss: 0.4903590679168701, acc: 0.453125] [gan loss: 3.511938, acc: 0.000000]\n",
            "1342: [discriminator loss: 0.31728991866111755, acc: 0.5] [gan loss: 2.034977, acc: 0.000000]\n",
            "1343: [discriminator loss: 0.5340660810470581, acc: 0.375] [gan loss: 3.337221, acc: 0.000000]\n",
            "1344: [discriminator loss: 0.3411630392074585, acc: 0.5] [gan loss: 2.353219, acc: 0.000000]\n",
            "1345: [discriminator loss: 0.4010116457939148, acc: 0.5] [gan loss: 2.904222, acc: 0.000000]\n",
            "1346: [discriminator loss: 0.37621527910232544, acc: 0.5] [gan loss: 2.184577, acc: 0.000000]\n",
            "1347: [discriminator loss: 0.4268856644630432, acc: 0.5] [gan loss: 2.871009, acc: 0.000000]\n",
            "1348: [discriminator loss: 0.34897753596305847, acc: 0.5] [gan loss: 2.097450, acc: 0.000000]\n",
            "1349: [discriminator loss: 0.38389790058135986, acc: 0.5] [gan loss: 2.581294, acc: 0.000000]\n",
            "1350: [discriminator loss: 0.3628448247909546, acc: 0.5] [gan loss: 2.037240, acc: 0.000000]\n",
            "1351: [discriminator loss: 0.40780651569366455, acc: 0.5] [gan loss: 2.081969, acc: 0.000000]\n",
            "1352: [discriminator loss: 0.3555282950401306, acc: 0.5] [gan loss: 2.184998, acc: 0.000000]\n",
            "1353: [discriminator loss: 0.31981992721557617, acc: 0.5] [gan loss: 2.203644, acc: 0.000000]\n",
            "1354: [discriminator loss: 0.3821910619735718, acc: 0.5] [gan loss: 2.045510, acc: 0.000000]\n",
            "1355: [discriminator loss: 0.38731515407562256, acc: 0.5] [gan loss: 2.718036, acc: 0.000000]\n",
            "1356: [discriminator loss: 0.3290814757347107, acc: 0.5] [gan loss: 2.137393, acc: 0.000000]\n",
            "1357: [discriminator loss: 0.3488613963127136, acc: 0.5] [gan loss: 2.616549, acc: 0.000000]\n",
            "1358: [discriminator loss: 0.31390905380249023, acc: 0.5] [gan loss: 2.279824, acc: 0.000000]\n",
            "1359: [discriminator loss: 0.33077293634414673, acc: 0.5] [gan loss: 2.413379, acc: 0.000000]\n",
            "1360: [discriminator loss: 0.34597691893577576, acc: 0.5] [gan loss: 2.440835, acc: 0.000000]\n",
            "1361: [discriminator loss: 0.3627743422985077, acc: 0.5] [gan loss: 2.532591, acc: 0.000000]\n",
            "1362: [discriminator loss: 0.3255583345890045, acc: 0.5] [gan loss: 2.741146, acc: 0.000000]\n",
            "1363: [discriminator loss: 0.3388015627861023, acc: 0.5] [gan loss: 2.742260, acc: 0.000000]\n",
            "1364: [discriminator loss: 0.3471597135066986, acc: 0.5] [gan loss: 2.635865, acc: 0.000000]\n",
            "1365: [discriminator loss: 0.374254435300827, acc: 0.5] [gan loss: 2.946813, acc: 0.000000]\n",
            "1366: [discriminator loss: 0.3120585083961487, acc: 0.5] [gan loss: 2.823472, acc: 0.000000]\n",
            "1367: [discriminator loss: 0.3368474841117859, acc: 0.5] [gan loss: 3.352295, acc: 0.000000]\n",
            "1368: [discriminator loss: 0.2943311333656311, acc: 0.5] [gan loss: 2.453215, acc: 0.000000]\n",
            "1369: [discriminator loss: 0.38324856758117676, acc: 0.5] [gan loss: 3.858386, acc: 0.000000]\n",
            "1370: [discriminator loss: 0.31632736325263977, acc: 0.5] [gan loss: 2.236782, acc: 0.000000]\n",
            "1371: [discriminator loss: 0.498910129070282, acc: 0.4296875] [gan loss: 3.932045, acc: 0.000000]\n",
            "1372: [discriminator loss: 0.3800583779811859, acc: 0.5] [gan loss: 2.887513, acc: 0.000000]\n",
            "1373: [discriminator loss: 0.6751206517219543, acc: 0.03125] [gan loss: 3.299425, acc: 0.000000]\n",
            "1374: [discriminator loss: 0.5112491846084595, acc: 0.5] [gan loss: 2.778334, acc: 0.000000]\n",
            "1375: [discriminator loss: 0.46648937463760376, acc: 0.4921875] [gan loss: 3.122165, acc: 0.000000]\n",
            "1376: [discriminator loss: 0.4354383051395416, acc: 0.5] [gan loss: 2.260428, acc: 0.000000]\n",
            "1377: [discriminator loss: 0.5461133122444153, acc: 0.3515625] [gan loss: 3.603773, acc: 0.000000]\n",
            "1378: [discriminator loss: 0.4019874334335327, acc: 0.5] [gan loss: 2.267114, acc: 0.000000]\n",
            "1379: [discriminator loss: 0.5515940189361572, acc: 0.3203125] [gan loss: 3.938420, acc: 0.000000]\n",
            "1380: [discriminator loss: 0.4043990969657898, acc: 0.5] [gan loss: 1.826916, acc: 0.000000]\n",
            "1381: [discriminator loss: 0.7167977094650269, acc: 0.0] [gan loss: 4.379677, acc: 0.000000]\n",
            "1382: [discriminator loss: 0.47696202993392944, acc: 0.5] [gan loss: 1.623399, acc: 0.000000]\n",
            "1383: [discriminator loss: 0.598924994468689, acc: 0.125] [gan loss: 3.019545, acc: 0.000000]\n",
            "1384: [discriminator loss: 0.37009885907173157, acc: 0.5] [gan loss: 1.599083, acc: 0.000000]\n",
            "1385: [discriminator loss: 0.599127471446991, acc: 0.1171875] [gan loss: 3.477160, acc: 0.000000]\n",
            "1386: [discriminator loss: 0.3888240456581116, acc: 0.5] [gan loss: 1.511317, acc: 0.000000]\n",
            "1387: [discriminator loss: 0.6888584494590759, acc: 0.015625] [gan loss: 3.677616, acc: 0.000000]\n",
            "1388: [discriminator loss: 0.3927196264266968, acc: 0.5] [gan loss: 1.825173, acc: 0.000000]\n",
            "1389: [discriminator loss: 0.5420600175857544, acc: 0.3203125] [gan loss: 3.142344, acc: 0.000000]\n",
            "1390: [discriminator loss: 0.38674265146255493, acc: 0.5] [gan loss: 1.831609, acc: 0.000000]\n",
            "1391: [discriminator loss: 0.6288066506385803, acc: 0.0625] [gan loss: 3.624873, acc: 0.000000]\n",
            "1392: [discriminator loss: 0.43281927704811096, acc: 0.5] [gan loss: 1.638849, acc: 0.000000]\n",
            "1393: [discriminator loss: 0.6308822631835938, acc: 0.109375] [gan loss: 2.869367, acc: 0.000000]\n",
            "1394: [discriminator loss: 0.4236544668674469, acc: 0.5] [gan loss: 1.762391, acc: 0.000000]\n",
            "1395: [discriminator loss: 0.7849292159080505, acc: 0.0] [gan loss: 2.327763, acc: 0.000000]\n",
            "1396: [discriminator loss: 0.5208609700202942, acc: 0.5] [gan loss: 1.323615, acc: 0.000000]\n",
            "1397: [discriminator loss: 0.7558952569961548, acc: 0.0234375] [gan loss: 2.447385, acc: 0.000000]\n",
            "1398: [discriminator loss: 0.49073976278305054, acc: 0.5] [gan loss: 1.274364, acc: 0.000000]\n",
            "1399: [discriminator loss: 0.6673978567123413, acc: 0.0625] [gan loss: 2.105735, acc: 0.000000]\n",
            "1400: [discriminator loss: 0.5107016563415527, acc: 0.5] [gan loss: 1.545673, acc: 0.000000]\n",
            "1401: [discriminator loss: 0.5281581282615662, acc: 0.4453125] [gan loss: 1.923995, acc: 0.000000]\n",
            "1402: [discriminator loss: 0.4776018261909485, acc: 0.5] [gan loss: 1.714066, acc: 0.000000]\n",
            "1403: [discriminator loss: 0.5246639251708984, acc: 0.5] [gan loss: 1.831939, acc: 0.000000]\n",
            "1404: [discriminator loss: 0.4787364602088928, acc: 0.5] [gan loss: 1.833043, acc: 0.000000]\n",
            "1405: [discriminator loss: 0.43754398822784424, acc: 0.5] [gan loss: 1.955486, acc: 0.000000]\n",
            "1406: [discriminator loss: 0.4387456476688385, acc: 0.5] [gan loss: 1.948136, acc: 0.000000]\n",
            "1407: [discriminator loss: 0.4202687740325928, acc: 0.5] [gan loss: 2.160369, acc: 0.000000]\n",
            "1408: [discriminator loss: 0.4148993194103241, acc: 0.5] [gan loss: 1.965364, acc: 0.000000]\n",
            "1409: [discriminator loss: 0.4360387921333313, acc: 0.5] [gan loss: 2.070781, acc: 0.000000]\n",
            "1410: [discriminator loss: 0.4028136730194092, acc: 0.5] [gan loss: 2.035540, acc: 0.000000]\n",
            "1411: [discriminator loss: 0.4142150282859802, acc: 0.5] [gan loss: 2.266366, acc: 0.000000]\n",
            "1412: [discriminator loss: 0.3487982451915741, acc: 0.5] [gan loss: 1.660007, acc: 0.000000]\n",
            "1413: [discriminator loss: 0.4692525863647461, acc: 0.4453125] [gan loss: 2.480882, acc: 0.000000]\n",
            "1414: [discriminator loss: 0.3811633288860321, acc: 0.5] [gan loss: 1.994909, acc: 0.000000]\n",
            "1415: [discriminator loss: 0.4038613736629486, acc: 0.5] [gan loss: 2.361137, acc: 0.000000]\n",
            "1416: [discriminator loss: 0.35152962803840637, acc: 0.5] [gan loss: 2.007891, acc: 0.000000]\n",
            "1417: [discriminator loss: 0.3858751654624939, acc: 0.5] [gan loss: 2.562844, acc: 0.000000]\n",
            "1418: [discriminator loss: 0.3116324841976166, acc: 0.5] [gan loss: 2.158765, acc: 0.000000]\n",
            "1419: [discriminator loss: 0.3614758253097534, acc: 0.5] [gan loss: 2.524502, acc: 0.000000]\n",
            "1420: [discriminator loss: 0.34300974011421204, acc: 0.5] [gan loss: 2.241479, acc: 0.000000]\n",
            "1421: [discriminator loss: 0.3525376617908478, acc: 0.5] [gan loss: 2.720309, acc: 0.000000]\n",
            "1422: [discriminator loss: 0.3236284554004669, acc: 0.5] [gan loss: 2.165440, acc: 0.000000]\n",
            "1423: [discriminator loss: 0.38999173045158386, acc: 0.5] [gan loss: 2.789930, acc: 0.000000]\n",
            "1424: [discriminator loss: 0.32929664850234985, acc: 0.5] [gan loss: 1.749274, acc: 0.000000]\n",
            "1425: [discriminator loss: 0.49607977271080017, acc: 0.390625] [gan loss: 3.146343, acc: 0.000000]\n",
            "1426: [discriminator loss: 0.434146523475647, acc: 0.5] [gan loss: 1.583303, acc: 0.000000]\n",
            "1427: [discriminator loss: 0.576677143573761, acc: 0.1796875] [gan loss: 3.410084, acc: 0.000000]\n",
            "1428: [discriminator loss: 0.41470059752464294, acc: 0.5] [gan loss: 1.898069, acc: 0.000000]\n",
            "1429: [discriminator loss: 0.4442957639694214, acc: 0.4921875] [gan loss: 2.746726, acc: 0.000000]\n",
            "1430: [discriminator loss: 0.33151647448539734, acc: 0.5] [gan loss: 2.320262, acc: 0.000000]\n",
            "1431: [discriminator loss: 0.3887694776058197, acc: 0.5] [gan loss: 2.603071, acc: 0.000000]\n",
            "1432: [discriminator loss: 0.38343650102615356, acc: 0.5] [gan loss: 2.414447, acc: 0.000000]\n",
            "1433: [discriminator loss: 0.4636097252368927, acc: 0.5] [gan loss: 2.707597, acc: 0.000000]\n",
            "1434: [discriminator loss: 0.36376577615737915, acc: 0.5] [gan loss: 2.590960, acc: 0.000000]\n",
            "1435: [discriminator loss: 0.40413782000541687, acc: 0.5] [gan loss: 2.490417, acc: 0.000000]\n",
            "1436: [discriminator loss: 0.4036218523979187, acc: 0.5] [gan loss: 2.793593, acc: 0.000000]\n",
            "1437: [discriminator loss: 0.33142533898353577, acc: 0.5] [gan loss: 2.559783, acc: 0.000000]\n",
            "1438: [discriminator loss: 0.331963449716568, acc: 0.5] [gan loss: 3.021447, acc: 0.000000]\n",
            "1439: [discriminator loss: 0.33938068151474, acc: 0.5] [gan loss: 2.207658, acc: 0.000000]\n",
            "1440: [discriminator loss: 0.41116124391555786, acc: 0.5] [gan loss: 3.387090, acc: 0.000000]\n",
            "1441: [discriminator loss: 0.3203094005584717, acc: 0.5] [gan loss: 2.141726, acc: 0.000000]\n",
            "1442: [discriminator loss: 0.38922441005706787, acc: 0.5] [gan loss: 3.299320, acc: 0.000000]\n",
            "1443: [discriminator loss: 0.27454009652137756, acc: 0.5] [gan loss: 2.285879, acc: 0.000000]\n",
            "1444: [discriminator loss: 0.3796898126602173, acc: 0.5] [gan loss: 2.595318, acc: 0.000000]\n",
            "1445: [discriminator loss: 0.30408695340156555, acc: 0.5] [gan loss: 2.038989, acc: 0.000000]\n",
            "1446: [discriminator loss: 0.41185760498046875, acc: 0.5] [gan loss: 2.571980, acc: 0.000000]\n",
            "1447: [discriminator loss: 0.3310355842113495, acc: 0.5] [gan loss: 2.260790, acc: 0.000000]\n",
            "1448: [discriminator loss: 0.40434718132019043, acc: 0.4921875] [gan loss: 2.518327, acc: 0.000000]\n",
            "1449: [discriminator loss: 0.3615673780441284, acc: 0.5] [gan loss: 1.101847, acc: 0.046875]\n",
            "1450: [discriminator loss: 0.44303780794143677, acc: 0.4375] [gan loss: 0.964058, acc: 0.031250]\n",
            "1451: [discriminator loss: 0.38649770617485046, acc: 0.5] [gan loss: 0.309494, acc: 1.000000]\n",
            "1452: [discriminator loss: 0.3136368989944458, acc: 0.4921875] [gan loss: 0.424818, acc: 1.000000]\n",
            "1453: [discriminator loss: 0.29631492495536804, acc: 0.5] [gan loss: 0.318077, acc: 1.000000]\n",
            "1454: [discriminator loss: 0.3321540355682373, acc: 0.5] [gan loss: 0.432023, acc: 0.984375]\n",
            "1455: [discriminator loss: 0.33714666962623596, acc: 0.5] [gan loss: 0.465000, acc: 1.000000]\n",
            "1456: [discriminator loss: 0.3306817412376404, acc: 0.4921875] [gan loss: 0.937397, acc: 0.000000]\n",
            "1457: [discriminator loss: 0.6056575179100037, acc: 0.25] [gan loss: 2.992743, acc: 0.000000]\n",
            "1458: [discriminator loss: 0.5027531981468201, acc: 0.5] [gan loss: 0.929856, acc: 0.046875]\n",
            "1459: [discriminator loss: 0.9621638059616089, acc: 0.0] [gan loss: 4.793554, acc: 0.000000]\n",
            "1460: [discriminator loss: 0.5167545676231384, acc: 0.5] [gan loss: 1.986487, acc: 0.000000]\n",
            "1461: [discriminator loss: 0.5694236159324646, acc: 0.3046875] [gan loss: 3.164428, acc: 0.000000]\n",
            "1462: [discriminator loss: 0.40607908368110657, acc: 0.5] [gan loss: 2.264272, acc: 0.000000]\n",
            "1463: [discriminator loss: 0.4658689796924591, acc: 0.4921875] [gan loss: 2.793326, acc: 0.000000]\n",
            "1464: [discriminator loss: 0.42460665106773376, acc: 0.5] [gan loss: 2.280587, acc: 0.000000]\n",
            "1465: [discriminator loss: 0.44155263900756836, acc: 0.5] [gan loss: 2.404466, acc: 0.000000]\n",
            "1466: [discriminator loss: 0.4456399381160736, acc: 0.5] [gan loss: 2.359720, acc: 0.000000]\n",
            "1467: [discriminator loss: 0.3796877861022949, acc: 0.5] [gan loss: 2.087959, acc: 0.000000]\n",
            "1468: [discriminator loss: 0.388016939163208, acc: 0.5] [gan loss: 2.075546, acc: 0.000000]\n",
            "1469: [discriminator loss: 0.41080811619758606, acc: 0.5] [gan loss: 1.535459, acc: 0.000000]\n",
            "1470: [discriminator loss: 0.5049091577529907, acc: 0.46875] [gan loss: 2.703886, acc: 0.000000]\n",
            "1471: [discriminator loss: 0.3935069143772125, acc: 0.5] [gan loss: 2.283003, acc: 0.000000]\n",
            "1472: [discriminator loss: 0.4544937014579773, acc: 0.5] [gan loss: 2.825705, acc: 0.000000]\n",
            "1473: [discriminator loss: 0.34918659925460815, acc: 0.5] [gan loss: 2.697801, acc: 0.000000]\n",
            "1474: [discriminator loss: 0.3967227637767792, acc: 0.5] [gan loss: 2.948912, acc: 0.000000]\n",
            "1475: [discriminator loss: 0.33158180117607117, acc: 0.5] [gan loss: 2.660096, acc: 0.000000]\n",
            "1476: [discriminator loss: 0.38031449913978577, acc: 0.5] [gan loss: 3.309350, acc: 0.000000]\n",
            "1477: [discriminator loss: 0.3513200283050537, acc: 0.5] [gan loss: 2.716431, acc: 0.000000]\n",
            "1478: [discriminator loss: 0.48575812578201294, acc: 0.4765625] [gan loss: 3.692669, acc: 0.000000]\n",
            "1479: [discriminator loss: 0.42463353276252747, acc: 0.5] [gan loss: 1.782694, acc: 0.000000]\n",
            "1480: [discriminator loss: 0.5672259330749512, acc: 0.296875] [gan loss: 2.856542, acc: 0.000000]\n",
            "1481: [discriminator loss: 0.4586333632469177, acc: 0.5] [gan loss: 1.393874, acc: 0.000000]\n",
            "1482: [discriminator loss: 0.5271388292312622, acc: 0.390625] [gan loss: 2.809999, acc: 0.000000]\n",
            "1483: [discriminator loss: 0.3784192204475403, acc: 0.5] [gan loss: 1.363738, acc: 0.000000]\n",
            "1484: [discriminator loss: 0.48788967728614807, acc: 0.4375] [gan loss: 3.175922, acc: 0.000000]\n",
            "1485: [discriminator loss: 0.32102298736572266, acc: 0.5] [gan loss: 1.486482, acc: 0.000000]\n",
            "1486: [discriminator loss: 0.4451276957988739, acc: 0.453125] [gan loss: 2.682716, acc: 0.000000]\n",
            "1487: [discriminator loss: 0.3032580614089966, acc: 0.5] [gan loss: 1.784259, acc: 0.000000]\n",
            "1488: [discriminator loss: 0.41633203625679016, acc: 0.5] [gan loss: 3.019170, acc: 0.000000]\n",
            "1489: [discriminator loss: 0.26843908429145813, acc: 0.5] [gan loss: 1.735549, acc: 0.000000]\n",
            "1490: [discriminator loss: 0.36673206090927124, acc: 0.5] [gan loss: 2.162986, acc: 0.000000]\n",
            "1491: [discriminator loss: 0.30965766310691833, acc: 0.5] [gan loss: 1.421883, acc: 0.000000]\n",
            "1492: [discriminator loss: 0.3366096615791321, acc: 0.5] [gan loss: 1.533850, acc: 0.000000]\n",
            "1493: [discriminator loss: 0.27371788024902344, acc: 0.5] [gan loss: 1.293917, acc: 0.000000]\n",
            "1494: [discriminator loss: 0.4146263897418976, acc: 0.5] [gan loss: 2.521502, acc: 0.000000]\n",
            "1495: [discriminator loss: 0.31052061915397644, acc: 0.5] [gan loss: 1.589770, acc: 0.000000]\n",
            "1496: [discriminator loss: 0.32022523880004883, acc: 0.5] [gan loss: 2.090928, acc: 0.000000]\n",
            "1497: [discriminator loss: 0.3374308943748474, acc: 0.5] [gan loss: 2.918868, acc: 0.000000]\n",
            "1498: [discriminator loss: 0.2623043656349182, acc: 0.5] [gan loss: 1.773220, acc: 0.000000]\n",
            "1499: [discriminator loss: 0.37911084294319153, acc: 0.5] [gan loss: 3.833360, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1500: [discriminator loss: 0.2607521414756775, acc: 0.5] [gan loss: 2.047242, acc: 0.000000]\n",
            "1501: [discriminator loss: 0.3087010979652405, acc: 0.5] [gan loss: 2.321105, acc: 0.000000]\n",
            "1502: [discriminator loss: 0.24663907289505005, acc: 0.5] [gan loss: 1.653837, acc: 0.000000]\n",
            "1503: [discriminator loss: 0.3257228136062622, acc: 0.5] [gan loss: 2.710145, acc: 0.000000]\n",
            "1504: [discriminator loss: 0.27956274151802063, acc: 0.5] [gan loss: 1.899410, acc: 0.000000]\n",
            "1505: [discriminator loss: 0.3562100827693939, acc: 0.5] [gan loss: 3.254152, acc: 0.000000]\n",
            "1506: [discriminator loss: 0.2907017767429352, acc: 0.5] [gan loss: 2.490891, acc: 0.000000]\n",
            "1507: [discriminator loss: 0.38935935497283936, acc: 0.5] [gan loss: 3.062200, acc: 0.000000]\n",
            "1508: [discriminator loss: 0.35113102197647095, acc: 0.5] [gan loss: 2.776018, acc: 0.000000]\n",
            "1509: [discriminator loss: 0.3496338129043579, acc: 0.5] [gan loss: 2.201623, acc: 0.000000]\n",
            "1510: [discriminator loss: 0.3789750039577484, acc: 0.5] [gan loss: 3.053345, acc: 0.000000]\n",
            "1511: [discriminator loss: 0.3132520020008087, acc: 0.5] [gan loss: 2.141868, acc: 0.000000]\n",
            "1512: [discriminator loss: 0.5699759721755981, acc: 0.203125] [gan loss: 4.013568, acc: 0.000000]\n",
            "1513: [discriminator loss: 0.2913573682308197, acc: 0.5] [gan loss: 1.579872, acc: 0.000000]\n",
            "1514: [discriminator loss: 0.5033114552497864, acc: 0.3359375] [gan loss: 4.117705, acc: 0.000000]\n",
            "1515: [discriminator loss: 0.3215176463127136, acc: 0.5] [gan loss: 1.986007, acc: 0.000000]\n",
            "1516: [discriminator loss: 0.40892839431762695, acc: 0.5] [gan loss: 2.974494, acc: 0.000000]\n",
            "1517: [discriminator loss: 0.32095611095428467, acc: 0.5] [gan loss: 2.010552, acc: 0.000000]\n",
            "1518: [discriminator loss: 0.4441063404083252, acc: 0.5] [gan loss: 3.100377, acc: 0.000000]\n",
            "1519: [discriminator loss: 0.3632892370223999, acc: 0.5] [gan loss: 1.841966, acc: 0.000000]\n",
            "1520: [discriminator loss: 0.5915690660476685, acc: 0.2890625] [gan loss: 2.979052, acc: 0.000000]\n",
            "1521: [discriminator loss: 0.3536853492259979, acc: 0.5] [gan loss: 1.351066, acc: 0.000000]\n",
            "1522: [discriminator loss: 0.4648502767086029, acc: 0.4921875] [gan loss: 1.640853, acc: 0.000000]\n",
            "1523: [discriminator loss: 0.40355533361434937, acc: 0.5] [gan loss: 1.440948, acc: 0.000000]\n",
            "1524: [discriminator loss: 0.4898703694343567, acc: 0.4375] [gan loss: 2.760751, acc: 0.000000]\n",
            "1525: [discriminator loss: 0.40893301367759705, acc: 0.5] [gan loss: 1.698558, acc: 0.000000]\n",
            "1526: [discriminator loss: 0.5649438500404358, acc: 0.3203125] [gan loss: 3.192615, acc: 0.000000]\n",
            "1527: [discriminator loss: 0.4108487665653229, acc: 0.5] [gan loss: 1.924828, acc: 0.000000]\n",
            "1528: [discriminator loss: 0.5138841271400452, acc: 0.375] [gan loss: 2.897391, acc: 0.000000]\n",
            "1529: [discriminator loss: 0.3776669502258301, acc: 0.5] [gan loss: 1.808520, acc: 0.000000]\n",
            "1530: [discriminator loss: 0.46357738971710205, acc: 0.4921875] [gan loss: 2.470684, acc: 0.000000]\n",
            "1531: [discriminator loss: 0.3788478374481201, acc: 0.5] [gan loss: 1.763037, acc: 0.000000]\n",
            "1532: [discriminator loss: 0.4694015383720398, acc: 0.5] [gan loss: 2.774674, acc: 0.000000]\n",
            "1533: [discriminator loss: 0.35950493812561035, acc: 0.5] [gan loss: 1.777394, acc: 0.000000]\n",
            "1534: [discriminator loss: 0.41731828451156616, acc: 0.5] [gan loss: 2.937880, acc: 0.000000]\n",
            "1535: [discriminator loss: 0.3249956965446472, acc: 0.5] [gan loss: 2.132564, acc: 0.000000]\n",
            "1536: [discriminator loss: 0.3414178490638733, acc: 0.5] [gan loss: 2.684298, acc: 0.000000]\n",
            "1537: [discriminator loss: 0.2916741967201233, acc: 0.5] [gan loss: 2.483216, acc: 0.000000]\n",
            "1538: [discriminator loss: 0.31254369020462036, acc: 0.5] [gan loss: 2.650037, acc: 0.000000]\n",
            "1539: [discriminator loss: 0.33354854583740234, acc: 0.5] [gan loss: 2.452975, acc: 0.000000]\n",
            "1540: [discriminator loss: 0.3414969742298126, acc: 0.5] [gan loss: 3.005495, acc: 0.000000]\n",
            "1541: [discriminator loss: 0.29190582036972046, acc: 0.5] [gan loss: 2.447092, acc: 0.000000]\n",
            "1542: [discriminator loss: 0.396091103553772, acc: 0.4921875] [gan loss: 3.293265, acc: 0.000000]\n",
            "1543: [discriminator loss: 0.3933025002479553, acc: 0.5] [gan loss: 1.882838, acc: 0.000000]\n",
            "1544: [discriminator loss: 0.6297310590744019, acc: 0.1328125] [gan loss: 3.362760, acc: 0.000000]\n",
            "1545: [discriminator loss: 0.46614640951156616, acc: 0.4921875] [gan loss: 1.798187, acc: 0.000000]\n",
            "1546: [discriminator loss: 0.5300717949867249, acc: 0.3515625] [gan loss: 2.092651, acc: 0.000000]\n",
            "1547: [discriminator loss: 0.4377615451812744, acc: 0.5] [gan loss: 1.417386, acc: 0.000000]\n",
            "1548: [discriminator loss: 0.47424978017807007, acc: 0.421875] [gan loss: 2.276077, acc: 0.000000]\n",
            "1549: [discriminator loss: 0.38279664516448975, acc: 0.5] [gan loss: 1.715333, acc: 0.000000]\n",
            "1550: [discriminator loss: 0.4122907519340515, acc: 0.484375] [gan loss: 1.717787, acc: 0.000000]\n",
            "1551: [discriminator loss: 0.42984890937805176, acc: 0.453125] [gan loss: 2.366957, acc: 0.000000]\n",
            "1552: [discriminator loss: 0.38034942746162415, acc: 0.5] [gan loss: 0.951045, acc: 0.000000]\n",
            "1553: [discriminator loss: 0.5269730687141418, acc: 0.2734375] [gan loss: 2.668801, acc: 0.000000]\n",
            "1554: [discriminator loss: 0.40265953540802, acc: 0.5] [gan loss: 1.417361, acc: 0.000000]\n",
            "1555: [discriminator loss: 0.4754737317562103, acc: 0.40625] [gan loss: 2.777628, acc: 0.000000]\n",
            "1556: [discriminator loss: 0.3859565854072571, acc: 0.5] [gan loss: 1.572665, acc: 0.000000]\n",
            "1557: [discriminator loss: 0.6096072196960449, acc: 0.171875] [gan loss: 3.216279, acc: 0.000000]\n",
            "1558: [discriminator loss: 0.33808717131614685, acc: 0.5] [gan loss: 1.693795, acc: 0.000000]\n",
            "1559: [discriminator loss: 0.5079873204231262, acc: 0.40625] [gan loss: 2.804556, acc: 0.000000]\n",
            "1560: [discriminator loss: 0.34768351912498474, acc: 0.5] [gan loss: 1.583708, acc: 0.000000]\n",
            "1561: [discriminator loss: 0.4999198019504547, acc: 0.3359375] [gan loss: 2.857399, acc: 0.000000]\n",
            "1562: [discriminator loss: 0.3247416019439697, acc: 0.5] [gan loss: 1.565606, acc: 0.000000]\n",
            "1563: [discriminator loss: 0.3663066327571869, acc: 0.4921875] [gan loss: 0.997985, acc: 0.078125]\n",
            "1564: [discriminator loss: 0.3160226345062256, acc: 0.5] [gan loss: 0.450947, acc: 0.968750]\n",
            "1565: [discriminator loss: 0.3064107894897461, acc: 0.5] [gan loss: 0.474962, acc: 0.937500]\n",
            "1566: [discriminator loss: 0.32802239060401917, acc: 0.5] [gan loss: 0.556352, acc: 0.828125]\n",
            "1567: [discriminator loss: 0.3590126633644104, acc: 0.5] [gan loss: 1.040273, acc: 0.000000]\n",
            "1568: [discriminator loss: 0.4147477149963379, acc: 0.484375] [gan loss: 1.876338, acc: 0.000000]\n",
            "1569: [discriminator loss: 0.4098324179649353, acc: 0.5] [gan loss: 1.964118, acc: 0.000000]\n",
            "1570: [discriminator loss: 0.556159257888794, acc: 0.34375] [gan loss: 4.398818, acc: 0.000000]\n",
            "1571: [discriminator loss: 0.3523980975151062, acc: 0.5] [gan loss: 1.389321, acc: 0.000000]\n",
            "1572: [discriminator loss: 0.6575536131858826, acc: 0.015625] [gan loss: 4.187842, acc: 0.000000]\n",
            "1573: [discriminator loss: 0.27083486318588257, acc: 0.5] [gan loss: 2.153724, acc: 0.000000]\n",
            "1574: [discriminator loss: 0.47982853651046753, acc: 0.390625] [gan loss: 2.982640, acc: 0.000000]\n",
            "1575: [discriminator loss: 0.3695110082626343, acc: 0.5] [gan loss: 1.733663, acc: 0.000000]\n",
            "1576: [discriminator loss: 0.651402473449707, acc: 0.1015625] [gan loss: 3.507345, acc: 0.000000]\n",
            "1577: [discriminator loss: 0.3833934962749481, acc: 0.5] [gan loss: 1.956836, acc: 0.000000]\n",
            "1578: [discriminator loss: 0.6342718601226807, acc: 0.1328125] [gan loss: 3.482450, acc: 0.000000]\n",
            "1579: [discriminator loss: 0.45384055376052856, acc: 0.5] [gan loss: 2.180346, acc: 0.000000]\n",
            "1580: [discriminator loss: 0.7679677605628967, acc: 0.0078125] [gan loss: 3.981061, acc: 0.000000]\n",
            "1581: [discriminator loss: 0.36522194743156433, acc: 0.5] [gan loss: 1.448413, acc: 0.000000]\n",
            "1582: [discriminator loss: 0.6703379154205322, acc: 0.0859375] [gan loss: 3.524067, acc: 0.000000]\n",
            "1583: [discriminator loss: 0.4890282452106476, acc: 0.5] [gan loss: 1.454674, acc: 0.000000]\n",
            "1584: [discriminator loss: 0.7591692805290222, acc: 0.0] [gan loss: 3.665988, acc: 0.000000]\n",
            "1585: [discriminator loss: 0.4566390812397003, acc: 0.5] [gan loss: 1.569959, acc: 0.000000]\n",
            "1586: [discriminator loss: 0.6029309630393982, acc: 0.1875] [gan loss: 2.592553, acc: 0.000000]\n",
            "1587: [discriminator loss: 0.5278968811035156, acc: 0.4609375] [gan loss: 2.033145, acc: 0.000000]\n",
            "1588: [discriminator loss: 0.4995936155319214, acc: 0.484375] [gan loss: 1.893156, acc: 0.000000]\n",
            "1589: [discriminator loss: 0.4597773253917694, acc: 0.484375] [gan loss: 1.891753, acc: 0.000000]\n",
            "1590: [discriminator loss: 0.4723021388053894, acc: 0.4765625] [gan loss: 2.029647, acc: 0.000000]\n",
            "1591: [discriminator loss: 0.49845612049102783, acc: 0.484375] [gan loss: 2.170938, acc: 0.000000]\n",
            "1592: [discriminator loss: 0.5020593404769897, acc: 0.4453125] [gan loss: 2.327319, acc: 0.000000]\n",
            "1593: [discriminator loss: 0.6374424695968628, acc: 0.3671875] [gan loss: 2.211969, acc: 0.000000]\n",
            "1594: [discriminator loss: 0.6927984356880188, acc: 0.203125] [gan loss: 2.211946, acc: 0.000000]\n",
            "1595: [discriminator loss: 0.6418931484222412, acc: 0.25] [gan loss: 2.083974, acc: 0.000000]\n",
            "1596: [discriminator loss: 0.5973823070526123, acc: 0.453125] [gan loss: 2.036962, acc: 0.000000]\n",
            "1597: [discriminator loss: 0.5449745059013367, acc: 0.5] [gan loss: 2.127939, acc: 0.000000]\n",
            "1598: [discriminator loss: 0.525899350643158, acc: 0.5] [gan loss: 2.157474, acc: 0.000000]\n",
            "1599: [discriminator loss: 0.45590469241142273, acc: 0.5] [gan loss: 2.268227, acc: 0.000000]\n",
            "1600: [discriminator loss: 0.44622015953063965, acc: 0.5] [gan loss: 2.290582, acc: 0.000000]\n",
            "1601: [discriminator loss: 0.5262981653213501, acc: 0.453125] [gan loss: 2.472508, acc: 0.000000]\n",
            "1602: [discriminator loss: 0.5532922744750977, acc: 0.4609375] [gan loss: 2.076581, acc: 0.000000]\n",
            "1603: [discriminator loss: 0.4751076400279999, acc: 0.4609375] [gan loss: 2.065716, acc: 0.000000]\n",
            "1604: [discriminator loss: 0.4850638508796692, acc: 0.5] [gan loss: 2.189140, acc: 0.000000]\n",
            "1605: [discriminator loss: 0.44265347719192505, acc: 0.5] [gan loss: 1.906610, acc: 0.000000]\n",
            "1606: [discriminator loss: 0.4438607394695282, acc: 0.5] [gan loss: 2.418607, acc: 0.000000]\n",
            "1607: [discriminator loss: 0.399445116519928, acc: 0.5] [gan loss: 1.819262, acc: 0.000000]\n",
            "1608: [discriminator loss: 0.4714725613594055, acc: 0.4921875] [gan loss: 2.549543, acc: 0.000000]\n",
            "1609: [discriminator loss: 0.3808596730232239, acc: 0.5] [gan loss: 1.969310, acc: 0.000000]\n",
            "1610: [discriminator loss: 0.4463862478733063, acc: 0.5] [gan loss: 2.619513, acc: 0.000000]\n",
            "1611: [discriminator loss: 0.3874618709087372, acc: 0.5] [gan loss: 1.966463, acc: 0.000000]\n",
            "1612: [discriminator loss: 0.43630751967430115, acc: 0.5] [gan loss: 2.446522, acc: 0.000000]\n",
            "1613: [discriminator loss: 0.38435256481170654, acc: 0.5] [gan loss: 2.026577, acc: 0.000000]\n",
            "1614: [discriminator loss: 0.43771860003471375, acc: 0.5] [gan loss: 2.447578, acc: 0.000000]\n",
            "1615: [discriminator loss: 0.3795487880706787, acc: 0.5] [gan loss: 2.160007, acc: 0.000000]\n",
            "1616: [discriminator loss: 0.39404550194740295, acc: 0.5] [gan loss: 2.922588, acc: 0.000000]\n",
            "1617: [discriminator loss: 0.33548980951309204, acc: 0.5] [gan loss: 2.103564, acc: 0.000000]\n",
            "1618: [discriminator loss: 0.3961777091026306, acc: 0.5] [gan loss: 2.704234, acc: 0.000000]\n",
            "1619: [discriminator loss: 0.32962387800216675, acc: 0.5] [gan loss: 2.364449, acc: 0.000000]\n",
            "1620: [discriminator loss: 0.33898594975471497, acc: 0.5] [gan loss: 2.583215, acc: 0.000000]\n",
            "1621: [discriminator loss: 0.36351528763771057, acc: 0.5] [gan loss: 2.478811, acc: 0.000000]\n",
            "1622: [discriminator loss: 0.35806185007095337, acc: 0.5] [gan loss: 2.513926, acc: 0.000000]\n",
            "1623: [discriminator loss: 0.4005690813064575, acc: 0.5] [gan loss: 2.804231, acc: 0.000000]\n",
            "1624: [discriminator loss: 0.4848408102989197, acc: 0.4921875] [gan loss: 1.487939, acc: 0.000000]\n",
            "1625: [discriminator loss: 0.5171837210655212, acc: 0.375] [gan loss: 2.391572, acc: 0.000000]\n",
            "1626: [discriminator loss: 0.35602685809135437, acc: 0.5] [gan loss: 1.383724, acc: 0.000000]\n",
            "1627: [discriminator loss: 0.5702075958251953, acc: 0.1796875] [gan loss: 3.249817, acc: 0.000000]\n",
            "1628: [discriminator loss: 0.378210186958313, acc: 0.5] [gan loss: 1.241967, acc: 0.000000]\n",
            "1629: [discriminator loss: 0.6086317300796509, acc: 0.0625] [gan loss: 3.082514, acc: 0.000000]\n",
            "1630: [discriminator loss: 0.3868659436702728, acc: 0.5] [gan loss: 1.264362, acc: 0.000000]\n",
            "1631: [discriminator loss: 0.48547792434692383, acc: 0.453125] [gan loss: 1.718018, acc: 0.000000]\n",
            "1632: [discriminator loss: 0.4040394723415375, acc: 0.5] [gan loss: 1.066606, acc: 0.000000]\n",
            "1633: [discriminator loss: 0.4713428020477295, acc: 0.46875] [gan loss: 1.931152, acc: 0.000000]\n",
            "1634: [discriminator loss: 0.43547242879867554, acc: 0.5] [gan loss: 1.280419, acc: 0.000000]\n",
            "1635: [discriminator loss: 0.6907199621200562, acc: 0.03125] [gan loss: 2.558950, acc: 0.000000]\n",
            "1636: [discriminator loss: 0.5243576169013977, acc: 0.5] [gan loss: 1.128237, acc: 0.000000]\n",
            "1637: [discriminator loss: 0.6043651103973389, acc: 0.2734375] [gan loss: 1.775749, acc: 0.000000]\n",
            "1638: [discriminator loss: 0.3985711932182312, acc: 0.5] [gan loss: 0.974325, acc: 0.046875]\n",
            "1639: [discriminator loss: 0.532197892665863, acc: 0.4296875] [gan loss: 1.180839, acc: 0.000000]\n",
            "1640: [discriminator loss: 0.426832914352417, acc: 0.5] [gan loss: 1.200164, acc: 0.000000]\n",
            "1641: [discriminator loss: 0.5552745461463928, acc: 0.4453125] [gan loss: 1.622871, acc: 0.000000]\n",
            "1642: [discriminator loss: 0.4844062030315399, acc: 0.5] [gan loss: 1.737208, acc: 0.000000]\n",
            "1643: [discriminator loss: 0.49155497550964355, acc: 0.4453125] [gan loss: 2.548695, acc: 0.000000]\n",
            "1644: [discriminator loss: 0.37111788988113403, acc: 0.5] [gan loss: 1.652833, acc: 0.000000]\n",
            "1645: [discriminator loss: 0.5654678344726562, acc: 0.2109375] [gan loss: 2.939944, acc: 0.000000]\n",
            "1646: [discriminator loss: 0.39445239305496216, acc: 0.5] [gan loss: 1.344188, acc: 0.000000]\n",
            "1647: [discriminator loss: 0.5740268230438232, acc: 0.1796875] [gan loss: 2.697284, acc: 0.000000]\n",
            "1648: [discriminator loss: 0.39091166853904724, acc: 0.5] [gan loss: 1.517148, acc: 0.000000]\n",
            "1649: [discriminator loss: 0.572187066078186, acc: 0.125] [gan loss: 3.054006, acc: 0.000000]\n",
            "1650: [discriminator loss: 0.38258180022239685, acc: 0.5] [gan loss: 1.770419, acc: 0.000000]\n",
            "1651: [discriminator loss: 0.4513016939163208, acc: 0.5] [gan loss: 2.684257, acc: 0.000000]\n",
            "1652: [discriminator loss: 0.3695860803127289, acc: 0.5] [gan loss: 1.894541, acc: 0.000000]\n",
            "1653: [discriminator loss: 0.38265979290008545, acc: 0.5] [gan loss: 2.522643, acc: 0.000000]\n",
            "1654: [discriminator loss: 0.3379497826099396, acc: 0.5] [gan loss: 2.298727, acc: 0.000000]\n",
            "1655: [discriminator loss: 0.4638637900352478, acc: 0.4296875] [gan loss: 2.757119, acc: 0.000000]\n",
            "1656: [discriminator loss: 0.4255869686603546, acc: 0.5] [gan loss: 2.014624, acc: 0.000000]\n",
            "1657: [discriminator loss: 0.42743781208992004, acc: 0.5] [gan loss: 2.440360, acc: 0.000000]\n",
            "1658: [discriminator loss: 0.3829338848590851, acc: 0.5] [gan loss: 1.768182, acc: 0.000000]\n",
            "1659: [discriminator loss: 0.410567045211792, acc: 0.5] [gan loss: 2.213613, acc: 0.000000]\n",
            "1660: [discriminator loss: 0.4084824025630951, acc: 0.5] [gan loss: 1.801144, acc: 0.000000]\n",
            "1661: [discriminator loss: 0.42769867181777954, acc: 0.5] [gan loss: 2.504959, acc: 0.000000]\n",
            "1662: [discriminator loss: 0.4460010528564453, acc: 0.5] [gan loss: 1.993092, acc: 0.000000]\n",
            "1663: [discriminator loss: 0.48815810680389404, acc: 0.4921875] [gan loss: 2.685164, acc: 0.000000]\n",
            "1664: [discriminator loss: 0.4111703038215637, acc: 0.5] [gan loss: 1.632976, acc: 0.000000]\n",
            "1665: [discriminator loss: 0.4881737232208252, acc: 0.4921875] [gan loss: 2.900210, acc: 0.000000]\n",
            "1666: [discriminator loss: 0.3675408959388733, acc: 0.5] [gan loss: 1.760916, acc: 0.000000]\n",
            "1667: [discriminator loss: 0.39570945501327515, acc: 0.5] [gan loss: 2.220687, acc: 0.000000]\n",
            "1668: [discriminator loss: 0.36940181255340576, acc: 0.5] [gan loss: 2.340778, acc: 0.000000]\n",
            "1669: [discriminator loss: 0.3032909035682678, acc: 0.5] [gan loss: 2.089885, acc: 0.000000]\n",
            "1670: [discriminator loss: 0.39146149158477783, acc: 0.5] [gan loss: 2.473895, acc: 0.000000]\n",
            "1671: [discriminator loss: 0.3414919078350067, acc: 0.5] [gan loss: 2.024813, acc: 0.000000]\n",
            "1672: [discriminator loss: 0.3755217492580414, acc: 0.5] [gan loss: 2.672293, acc: 0.000000]\n",
            "1673: [discriminator loss: 0.3280167281627655, acc: 0.5] [gan loss: 2.145089, acc: 0.000000]\n",
            "1674: [discriminator loss: 0.3752051293849945, acc: 0.5] [gan loss: 2.721452, acc: 0.000000]\n",
            "1675: [discriminator loss: 0.3641037046909332, acc: 0.5] [gan loss: 2.011399, acc: 0.000000]\n",
            "1676: [discriminator loss: 0.49625205993652344, acc: 0.4375] [gan loss: 2.520487, acc: 0.000000]\n",
            "1677: [discriminator loss: 0.39462414383888245, acc: 0.5] [gan loss: 1.947763, acc: 0.000000]\n",
            "1678: [discriminator loss: 0.31955552101135254, acc: 0.5] [gan loss: 1.905894, acc: 0.000000]\n",
            "1679: [discriminator loss: 0.4468303322792053, acc: 0.5] [gan loss: 2.573176, acc: 0.000000]\n",
            "1680: [discriminator loss: 0.30555078387260437, acc: 0.5] [gan loss: 2.237793, acc: 0.000000]\n",
            "1681: [discriminator loss: 0.3696715831756592, acc: 0.5] [gan loss: 2.150682, acc: 0.000000]\n",
            "1682: [discriminator loss: 0.43753838539123535, acc: 0.5] [gan loss: 2.507066, acc: 0.000000]\n",
            "1683: [discriminator loss: 0.29201966524124146, acc: 0.5] [gan loss: 1.321908, acc: 0.000000]\n",
            "1684: [discriminator loss: 0.7051048278808594, acc: 0.0078125] [gan loss: 3.121941, acc: 0.000000]\n",
            "1685: [discriminator loss: 0.34529945254325867, acc: 0.5] [gan loss: 1.618037, acc: 0.000000]\n",
            "1686: [discriminator loss: 0.5351117253303528, acc: 0.3671875] [gan loss: 2.488856, acc: 0.000000]\n",
            "1687: [discriminator loss: 0.3360757827758789, acc: 0.5] [gan loss: 1.703967, acc: 0.000000]\n",
            "1688: [discriminator loss: 0.5653428435325623, acc: 0.140625] [gan loss: 3.131977, acc: 0.000000]\n",
            "1689: [discriminator loss: 0.3905322551727295, acc: 0.5] [gan loss: 1.493779, acc: 0.000000]\n",
            "1690: [discriminator loss: 0.7253737449645996, acc: 0.0078125] [gan loss: 3.532653, acc: 0.000000]\n",
            "1691: [discriminator loss: 0.4350336790084839, acc: 0.5] [gan loss: 1.638107, acc: 0.000000]\n",
            "1692: [discriminator loss: 0.6233969926834106, acc: 0.0859375] [gan loss: 2.879088, acc: 0.000000]\n",
            "1693: [discriminator loss: 0.41000860929489136, acc: 0.5] [gan loss: 1.675459, acc: 0.000000]\n",
            "1694: [discriminator loss: 0.5623611807823181, acc: 0.296875] [gan loss: 2.990526, acc: 0.000000]\n",
            "1695: [discriminator loss: 0.41092655062675476, acc: 0.5] [gan loss: 1.791963, acc: 0.000000]\n",
            "1696: [discriminator loss: 0.4893006980419159, acc: 0.4921875] [gan loss: 2.753788, acc: 0.000000]\n",
            "1697: [discriminator loss: 0.43377751111984253, acc: 0.5] [gan loss: 1.790040, acc: 0.000000]\n",
            "1698: [discriminator loss: 0.548268735408783, acc: 0.3671875] [gan loss: 3.182598, acc: 0.000000]\n",
            "1699: [discriminator loss: 0.3425191640853882, acc: 0.5] [gan loss: 2.090807, acc: 0.000000]\n",
            "1700: [discriminator loss: 0.44453614950180054, acc: 0.5] [gan loss: 2.547946, acc: 0.000000]\n",
            "1701: [discriminator loss: 0.4186129868030548, acc: 0.5] [gan loss: 2.053645, acc: 0.000000]\n",
            "1702: [discriminator loss: 0.4685315787792206, acc: 0.5] [gan loss: 2.605736, acc: 0.000000]\n",
            "1703: [discriminator loss: 0.44686779379844666, acc: 0.5] [gan loss: 2.056113, acc: 0.000000]\n",
            "1704: [discriminator loss: 0.5318880081176758, acc: 0.453125] [gan loss: 2.710388, acc: 0.000000]\n",
            "1705: [discriminator loss: 0.4706282615661621, acc: 0.5] [gan loss: 1.786350, acc: 0.000000]\n",
            "1706: [discriminator loss: 0.7107632160186768, acc: 0.0078125] [gan loss: 3.222084, acc: 0.000000]\n",
            "1707: [discriminator loss: 0.4775094985961914, acc: 0.5] [gan loss: 1.577704, acc: 0.000000]\n",
            "1708: [discriminator loss: 0.8309383392333984, acc: 0.0] [gan loss: 2.914665, acc: 0.000000]\n",
            "1709: [discriminator loss: 0.5128962397575378, acc: 0.5] [gan loss: 1.511692, acc: 0.000000]\n",
            "1710: [discriminator loss: 0.641556441783905, acc: 0.03125] [gan loss: 2.562195, acc: 0.000000]\n",
            "1711: [discriminator loss: 0.48850512504577637, acc: 0.5] [gan loss: 1.713745, acc: 0.000000]\n",
            "1712: [discriminator loss: 0.5652967691421509, acc: 0.2734375] [gan loss: 2.316282, acc: 0.000000]\n",
            "1713: [discriminator loss: 0.45694419741630554, acc: 0.5] [gan loss: 1.902601, acc: 0.000000]\n",
            "1714: [discriminator loss: 0.4433732330799103, acc: 0.5] [gan loss: 2.253854, acc: 0.000000]\n",
            "1715: [discriminator loss: 0.40916693210601807, acc: 0.5] [gan loss: 2.180591, acc: 0.000000]\n",
            "1716: [discriminator loss: 0.38852253556251526, acc: 0.5] [gan loss: 2.635342, acc: 0.000000]\n",
            "1717: [discriminator loss: 0.33964717388153076, acc: 0.5] [gan loss: 2.205542, acc: 0.000000]\n",
            "1718: [discriminator loss: 0.3821921944618225, acc: 0.5] [gan loss: 2.786633, acc: 0.000000]\n",
            "1719: [discriminator loss: 0.299773633480072, acc: 0.5] [gan loss: 2.477419, acc: 0.000000]\n",
            "1720: [discriminator loss: 0.36217305064201355, acc: 0.5] [gan loss: 2.711994, acc: 0.000000]\n",
            "1721: [discriminator loss: 0.33310073614120483, acc: 0.5] [gan loss: 2.510251, acc: 0.000000]\n",
            "1722: [discriminator loss: 0.39244550466537476, acc: 0.5] [gan loss: 2.262963, acc: 0.000000]\n",
            "1723: [discriminator loss: 0.34212160110473633, acc: 0.5] [gan loss: 2.125552, acc: 0.000000]\n",
            "1724: [discriminator loss: 0.4545421898365021, acc: 0.5] [gan loss: 3.079246, acc: 0.000000]\n",
            "1725: [discriminator loss: 0.3644079566001892, acc: 0.5] [gan loss: 2.016021, acc: 0.000000]\n",
            "1726: [discriminator loss: 0.4319639801979065, acc: 0.5] [gan loss: 3.379307, acc: 0.000000]\n",
            "1727: [discriminator loss: 0.2974938452243805, acc: 0.5] [gan loss: 2.174307, acc: 0.000000]\n",
            "1728: [discriminator loss: 0.4339645504951477, acc: 0.5] [gan loss: 3.145081, acc: 0.000000]\n",
            "1729: [discriminator loss: 0.33311763405799866, acc: 0.5] [gan loss: 2.150635, acc: 0.000000]\n",
            "1730: [discriminator loss: 0.42984044551849365, acc: 0.5] [gan loss: 3.074986, acc: 0.000000]\n",
            "1731: [discriminator loss: 0.3895631730556488, acc: 0.5] [gan loss: 1.801858, acc: 0.000000]\n",
            "1732: [discriminator loss: 0.4753715395927429, acc: 0.4609375] [gan loss: 2.995111, acc: 0.000000]\n",
            "1733: [discriminator loss: 0.4322558641433716, acc: 0.5] [gan loss: 1.915285, acc: 0.000000]\n",
            "1734: [discriminator loss: 0.5924813747406006, acc: 0.2734375] [gan loss: 2.694095, acc: 0.000000]\n",
            "1735: [discriminator loss: 0.5162164568901062, acc: 0.5] [gan loss: 1.866566, acc: 0.000000]\n",
            "1736: [discriminator loss: 0.5269877314567566, acc: 0.484375] [gan loss: 2.321122, acc: 0.000000]\n",
            "1737: [discriminator loss: 0.4649285078048706, acc: 0.5] [gan loss: 1.682816, acc: 0.000000]\n",
            "1738: [discriminator loss: 0.5221751928329468, acc: 0.3984375] [gan loss: 2.586562, acc: 0.000000]\n",
            "1739: [discriminator loss: 0.374550998210907, acc: 0.5] [gan loss: 1.856619, acc: 0.000000]\n",
            "1740: [discriminator loss: 0.43721288442611694, acc: 0.5] [gan loss: 2.389023, acc: 0.000000]\n",
            "1741: [discriminator loss: 0.3788054287433624, acc: 0.5] [gan loss: 2.049997, acc: 0.000000]\n",
            "1742: [discriminator loss: 0.5007989406585693, acc: 0.453125] [gan loss: 2.708575, acc: 0.000000]\n",
            "1743: [discriminator loss: 0.43886592984199524, acc: 0.5] [gan loss: 1.590139, acc: 0.000000]\n",
            "1744: [discriminator loss: 0.48449647426605225, acc: 0.4140625] [gan loss: 2.863948, acc: 0.000000]\n",
            "1745: [discriminator loss: 0.4455258548259735, acc: 0.5] [gan loss: 1.849695, acc: 0.000000]\n",
            "1746: [discriminator loss: 0.47104963660240173, acc: 0.4765625] [gan loss: 2.715240, acc: 0.000000]\n",
            "1747: [discriminator loss: 0.4374021589756012, acc: 0.5] [gan loss: 2.067622, acc: 0.000000]\n",
            "1748: [discriminator loss: 0.5140795707702637, acc: 0.4375] [gan loss: 2.588899, acc: 0.000000]\n",
            "1749: [discriminator loss: 0.40405070781707764, acc: 0.5] [gan loss: 1.776125, acc: 0.000000]\n",
            "1750: [discriminator loss: 0.5193252563476562, acc: 0.359375] [gan loss: 2.943207, acc: 0.000000]\n",
            "1751: [discriminator loss: 0.4174962043762207, acc: 0.5] [gan loss: 1.464976, acc: 0.000000]\n",
            "1752: [discriminator loss: 0.5213654637336731, acc: 0.390625] [gan loss: 2.231354, acc: 0.000000]\n",
            "1753: [discriminator loss: 0.3843630254268646, acc: 0.5] [gan loss: 1.346576, acc: 0.000000]\n",
            "1754: [discriminator loss: 0.6586807370185852, acc: 0.1015625] [gan loss: 2.846613, acc: 0.000000]\n",
            "1755: [discriminator loss: 0.45233678817749023, acc: 0.5] [gan loss: 1.235695, acc: 0.000000]\n",
            "1756: [discriminator loss: 0.602718710899353, acc: 0.15625] [gan loss: 2.648833, acc: 0.000000]\n",
            "1757: [discriminator loss: 0.38843902945518494, acc: 0.5] [gan loss: 1.331393, acc: 0.000000]\n",
            "1758: [discriminator loss: 0.5802025198936462, acc: 0.21875] [gan loss: 2.696765, acc: 0.000000]\n",
            "1759: [discriminator loss: 0.39274847507476807, acc: 0.5] [gan loss: 1.578956, acc: 0.000000]\n",
            "1760: [discriminator loss: 0.580312192440033, acc: 0.1875] [gan loss: 2.917298, acc: 0.000000]\n",
            "1761: [discriminator loss: 0.40852850675582886, acc: 0.5] [gan loss: 1.622760, acc: 0.000000]\n",
            "1762: [discriminator loss: 0.5830912590026855, acc: 0.1953125] [gan loss: 2.774609, acc: 0.000000]\n",
            "1763: [discriminator loss: 0.36984145641326904, acc: 0.5] [gan loss: 1.582305, acc: 0.000000]\n",
            "1764: [discriminator loss: 0.5615463852882385, acc: 0.2890625] [gan loss: 2.394959, acc: 0.000000]\n",
            "1765: [discriminator loss: 0.4616372585296631, acc: 0.5] [gan loss: 1.236247, acc: 0.000000]\n",
            "1766: [discriminator loss: 0.5767595767974854, acc: 0.2421875] [gan loss: 2.039712, acc: 0.000000]\n",
            "1767: [discriminator loss: 0.5099941492080688, acc: 0.5] [gan loss: 1.097422, acc: 0.015625]\n",
            "1768: [discriminator loss: 0.5701882839202881, acc: 0.3046875] [gan loss: 2.027016, acc: 0.000000]\n",
            "1769: [discriminator loss: 0.4218754768371582, acc: 0.4921875] [gan loss: 1.196978, acc: 0.000000]\n",
            "1770: [discriminator loss: 0.532436728477478, acc: 0.34375] [gan loss: 2.150936, acc: 0.000000]\n",
            "1771: [discriminator loss: 0.3721078634262085, acc: 0.5] [gan loss: 1.556666, acc: 0.000000]\n",
            "1772: [discriminator loss: 0.43894684314727783, acc: 0.5] [gan loss: 1.878151, acc: 0.000000]\n",
            "1773: [discriminator loss: 0.37767326831817627, acc: 0.5] [gan loss: 1.855774, acc: 0.000000]\n",
            "1774: [discriminator loss: 0.40663379430770874, acc: 0.5] [gan loss: 2.386266, acc: 0.000000]\n",
            "1775: [discriminator loss: 0.35386982560157776, acc: 0.5] [gan loss: 1.948003, acc: 0.000000]\n",
            "1776: [discriminator loss: 0.41619887948036194, acc: 0.5] [gan loss: 2.726034, acc: 0.000000]\n",
            "1777: [discriminator loss: 0.3334069848060608, acc: 0.5] [gan loss: 2.129137, acc: 0.000000]\n",
            "1778: [discriminator loss: 0.4299573302268982, acc: 0.4921875] [gan loss: 3.013935, acc: 0.000000]\n",
            "1779: [discriminator loss: 0.296479195356369, acc: 0.5] [gan loss: 2.199047, acc: 0.000000]\n",
            "1780: [discriminator loss: 0.41446641087532043, acc: 0.4921875] [gan loss: 2.728891, acc: 0.000000]\n",
            "1781: [discriminator loss: 0.346355676651001, acc: 0.5] [gan loss: 2.283281, acc: 0.000000]\n",
            "1782: [discriminator loss: 0.4428325891494751, acc: 0.4921875] [gan loss: 3.030254, acc: 0.000000]\n",
            "1783: [discriminator loss: 0.34129396080970764, acc: 0.5] [gan loss: 1.788253, acc: 0.000000]\n",
            "1784: [discriminator loss: 0.479451060295105, acc: 0.4453125] [gan loss: 2.216165, acc: 0.000000]\n",
            "1785: [discriminator loss: 0.3342665135860443, acc: 0.5] [gan loss: 1.141009, acc: 0.000000]\n",
            "1786: [discriminator loss: 0.2853439152240753, acc: 0.5] [gan loss: 0.457920, acc: 0.968750]\n",
            "1787: [discriminator loss: 0.3557521104812622, acc: 0.5] [gan loss: 0.387805, acc: 1.000000]\n",
            "1788: [discriminator loss: 0.3408743143081665, acc: 0.484375] [gan loss: 0.342219, acc: 1.000000]\n",
            "1789: [discriminator loss: 0.33348172903060913, acc: 0.5] [gan loss: 0.178244, acc: 1.000000]\n",
            "1790: [discriminator loss: 0.3150087296962738, acc: 0.4921875] [gan loss: 0.306825, acc: 1.000000]\n",
            "1791: [discriminator loss: 0.4193614423274994, acc: 0.453125] [gan loss: 1.109856, acc: 0.000000]\n",
            "1792: [discriminator loss: 0.28935033082962036, acc: 0.5] [gan loss: 0.058740, acc: 1.000000]\n",
            "1793: [discriminator loss: 0.2930586636066437, acc: 0.5] [gan loss: 0.257110, acc: 1.000000]\n",
            "1794: [discriminator loss: 0.36242997646331787, acc: 0.4921875] [gan loss: 0.589067, acc: 0.812500]\n",
            "1795: [discriminator loss: 0.529159665107727, acc: 0.375] [gan loss: 3.187826, acc: 0.000000]\n",
            "1796: [discriminator loss: 0.5342254638671875, acc: 0.4609375] [gan loss: 2.904281, acc: 0.000000]\n",
            "1797: [discriminator loss: 0.6262930631637573, acc: 0.4296875] [gan loss: 4.272560, acc: 0.000000]\n",
            "1798: [discriminator loss: 0.44598498940467834, acc: 0.5] [gan loss: 1.134429, acc: 0.000000]\n",
            "1799: [discriminator loss: 0.9236430525779724, acc: 0.0] [gan loss: 4.696895, acc: 0.000000]\n",
            "1800: [discriminator loss: 0.4359600245952606, acc: 0.5] [gan loss: 2.239151, acc: 0.000000]\n",
            "1801: [discriminator loss: 0.5790724754333496, acc: 0.296875] [gan loss: 3.217831, acc: 0.000000]\n",
            "1802: [discriminator loss: 0.3844640851020813, acc: 0.5] [gan loss: 1.757586, acc: 0.000000]\n",
            "1803: [discriminator loss: 0.603643536567688, acc: 0.1328125] [gan loss: 2.831351, acc: 0.000000]\n",
            "1804: [discriminator loss: 0.3841377794742584, acc: 0.5] [gan loss: 2.109631, acc: 0.000000]\n",
            "1805: [discriminator loss: 0.5233392715454102, acc: 0.5] [gan loss: 2.299020, acc: 0.000000]\n",
            "1806: [discriminator loss: 0.4273744225502014, acc: 0.5] [gan loss: 2.294772, acc: 0.000000]\n",
            "1807: [discriminator loss: 0.5204171538352966, acc: 0.484375] [gan loss: 2.366372, acc: 0.000000]\n",
            "1808: [discriminator loss: 0.3680129647254944, acc: 0.5] [gan loss: 1.585322, acc: 0.000000]\n",
            "1809: [discriminator loss: 0.5592703819274902, acc: 0.34375] [gan loss: 2.603175, acc: 0.000000]\n",
            "1810: [discriminator loss: 0.3693707287311554, acc: 0.5] [gan loss: 1.699004, acc: 0.000000]\n",
            "1811: [discriminator loss: 0.4966243803501129, acc: 0.4609375] [gan loss: 2.789146, acc: 0.000000]\n",
            "1812: [discriminator loss: 0.40112781524658203, acc: 0.5] [gan loss: 2.113075, acc: 0.000000]\n",
            "1813: [discriminator loss: 0.4818355143070221, acc: 0.484375] [gan loss: 2.763952, acc: 0.000000]\n",
            "1814: [discriminator loss: 0.3784923553466797, acc: 0.5] [gan loss: 2.218834, acc: 0.000000]\n",
            "1815: [discriminator loss: 0.41205018758773804, acc: 0.5] [gan loss: 2.342821, acc: 0.000000]\n",
            "1816: [discriminator loss: 0.3796935975551605, acc: 0.5] [gan loss: 2.576163, acc: 0.000000]\n",
            "1817: [discriminator loss: 0.3508763015270233, acc: 0.5] [gan loss: 2.139122, acc: 0.000000]\n",
            "1818: [discriminator loss: 0.4223569333553314, acc: 0.5] [gan loss: 2.707027, acc: 0.000000]\n",
            "1819: [discriminator loss: 0.32280606031417847, acc: 0.5] [gan loss: 2.334741, acc: 0.000000]\n",
            "1820: [discriminator loss: 0.39405912160873413, acc: 0.5] [gan loss: 2.721615, acc: 0.000000]\n",
            "1821: [discriminator loss: 0.3487046957015991, acc: 0.5] [gan loss: 2.213487, acc: 0.000000]\n",
            "1822: [discriminator loss: 0.4184168577194214, acc: 0.5] [gan loss: 2.850307, acc: 0.000000]\n",
            "1823: [discriminator loss: 0.3112714886665344, acc: 0.5] [gan loss: 1.914739, acc: 0.000000]\n",
            "1824: [discriminator loss: 0.3625790476799011, acc: 0.5] [gan loss: 2.499279, acc: 0.000000]\n",
            "1825: [discriminator loss: 0.3579978346824646, acc: 0.5] [gan loss: 2.262815, acc: 0.000000]\n",
            "1826: [discriminator loss: 0.37188106775283813, acc: 0.5] [gan loss: 2.602814, acc: 0.000000]\n",
            "1827: [discriminator loss: 0.3222057819366455, acc: 0.5] [gan loss: 2.350713, acc: 0.000000]\n",
            "1828: [discriminator loss: 0.36442136764526367, acc: 0.5] [gan loss: 2.853279, acc: 0.000000]\n",
            "1829: [discriminator loss: 0.3322347402572632, acc: 0.5] [gan loss: 1.989662, acc: 0.000000]\n",
            "1830: [discriminator loss: 0.4772025942802429, acc: 0.4765625] [gan loss: 3.384398, acc: 0.000000]\n",
            "1831: [discriminator loss: 0.38532590866088867, acc: 0.5] [gan loss: 1.874779, acc: 0.000000]\n",
            "1832: [discriminator loss: 0.4557967782020569, acc: 0.4921875] [gan loss: 2.633073, acc: 0.000000]\n",
            "1833: [discriminator loss: 0.31154415011405945, acc: 0.5] [gan loss: 2.464480, acc: 0.000000]\n",
            "1834: [discriminator loss: 0.3195687532424927, acc: 0.5] [gan loss: 2.099921, acc: 0.000000]\n",
            "1835: [discriminator loss: 0.4398481547832489, acc: 0.4765625] [gan loss: 3.172705, acc: 0.000000]\n",
            "1836: [discriminator loss: 0.3365902602672577, acc: 0.5] [gan loss: 1.780247, acc: 0.000000]\n",
            "1837: [discriminator loss: 0.5350849628448486, acc: 0.28125] [gan loss: 2.889567, acc: 0.000000]\n",
            "1838: [discriminator loss: 0.3935398459434509, acc: 0.5] [gan loss: 1.503709, acc: 0.000000]\n",
            "1839: [discriminator loss: 0.6534062623977661, acc: 0.03125] [gan loss: 3.061709, acc: 0.000000]\n",
            "1840: [discriminator loss: 0.4003297984600067, acc: 0.5] [gan loss: 1.439976, acc: 0.000000]\n",
            "1841: [discriminator loss: 0.552618682384491, acc: 0.25] [gan loss: 2.746237, acc: 0.000000]\n",
            "1842: [discriminator loss: 0.3952850103378296, acc: 0.5] [gan loss: 1.559607, acc: 0.000000]\n",
            "1843: [discriminator loss: 0.5597865581512451, acc: 0.265625] [gan loss: 2.899293, acc: 0.000000]\n",
            "1844: [discriminator loss: 0.3476720452308655, acc: 0.5] [gan loss: 1.742157, acc: 0.000000]\n",
            "1845: [discriminator loss: 0.4615108370780945, acc: 0.4765625] [gan loss: 2.407660, acc: 0.000000]\n",
            "1846: [discriminator loss: 0.3673170804977417, acc: 0.5] [gan loss: 2.061125, acc: 0.000000]\n",
            "1847: [discriminator loss: 0.39301276206970215, acc: 0.5] [gan loss: 2.578992, acc: 0.000000]\n",
            "1848: [discriminator loss: 0.3742714524269104, acc: 0.5] [gan loss: 2.023091, acc: 0.000000]\n",
            "1849: [discriminator loss: 0.4250997304916382, acc: 0.5] [gan loss: 2.766758, acc: 0.000000]\n",
            "1850: [discriminator loss: 0.40668392181396484, acc: 0.5] [gan loss: 2.716465, acc: 0.000000]\n",
            "1851: [discriminator loss: 0.46609407663345337, acc: 0.4765625] [gan loss: 3.026433, acc: 0.000000]\n",
            "1852: [discriminator loss: 0.38822317123413086, acc: 0.5] [gan loss: 2.112359, acc: 0.000000]\n",
            "1853: [discriminator loss: 0.5237095355987549, acc: 0.375] [gan loss: 2.959992, acc: 0.000000]\n",
            "1854: [discriminator loss: 0.5186890363693237, acc: 0.5] [gan loss: 2.431859, acc: 0.000000]\n",
            "1855: [discriminator loss: 0.48211804032325745, acc: 0.5] [gan loss: 2.278747, acc: 0.000000]\n",
            "1856: [discriminator loss: 0.43640321493148804, acc: 0.5] [gan loss: 2.710602, acc: 0.000000]\n",
            "1857: [discriminator loss: 0.38399481773376465, acc: 0.5] [gan loss: 2.192231, acc: 0.000000]\n",
            "1858: [discriminator loss: 0.4016740024089813, acc: 0.5] [gan loss: 3.090806, acc: 0.000000]\n",
            "1859: [discriminator loss: 0.3393666744232178, acc: 0.5] [gan loss: 2.104677, acc: 0.000000]\n",
            "1860: [discriminator loss: 0.5301275253295898, acc: 0.3203125] [gan loss: 3.736441, acc: 0.000000]\n",
            "1861: [discriminator loss: 0.4113449454307556, acc: 0.5] [gan loss: 2.079468, acc: 0.000000]\n",
            "1862: [discriminator loss: 0.6019236445426941, acc: 0.1796875] [gan loss: 3.343839, acc: 0.000000]\n",
            "1863: [discriminator loss: 0.417458713054657, acc: 0.5] [gan loss: 1.946715, acc: 0.000000]\n",
            "1864: [discriminator loss: 0.5826494097709656, acc: 0.34375] [gan loss: 3.031117, acc: 0.000000]\n",
            "1865: [discriminator loss: 0.47049665451049805, acc: 0.5] [gan loss: 1.691779, acc: 0.000000]\n",
            "1866: [discriminator loss: 0.6224963068962097, acc: 0.171875] [gan loss: 2.975434, acc: 0.000000]\n",
            "1867: [discriminator loss: 0.47587376832962036, acc: 0.5] [gan loss: 1.504508, acc: 0.000000]\n",
            "1868: [discriminator loss: 0.570842981338501, acc: 0.34375] [gan loss: 2.710247, acc: 0.000000]\n",
            "1869: [discriminator loss: 0.4017031788825989, acc: 0.5] [gan loss: 1.867863, acc: 0.000000]\n",
            "1870: [discriminator loss: 0.4535207152366638, acc: 0.5] [gan loss: 2.773819, acc: 0.000000]\n",
            "1871: [discriminator loss: 0.3797420263290405, acc: 0.5] [gan loss: 1.892610, acc: 0.000000]\n",
            "1872: [discriminator loss: 0.4684191346168518, acc: 0.4765625] [gan loss: 2.410963, acc: 0.000000]\n",
            "1873: [discriminator loss: 0.4349425137042999, acc: 0.5] [gan loss: 1.814795, acc: 0.000000]\n",
            "1874: [discriminator loss: 0.4653113782405853, acc: 0.4921875] [gan loss: 2.176053, acc: 0.000000]\n",
            "1875: [discriminator loss: 0.36877596378326416, acc: 0.5] [gan loss: 2.013123, acc: 0.000000]\n",
            "1876: [discriminator loss: 0.36365002393722534, acc: 0.5] [gan loss: 2.191027, acc: 0.000000]\n",
            "1877: [discriminator loss: 0.35401731729507446, acc: 0.5] [gan loss: 2.318754, acc: 0.000000]\n",
            "1878: [discriminator loss: 0.3748493194580078, acc: 0.5] [gan loss: 2.183181, acc: 0.000000]\n",
            "1879: [discriminator loss: 0.4868776798248291, acc: 0.4296875] [gan loss: 2.538817, acc: 0.000000]\n",
            "1880: [discriminator loss: 0.4368685483932495, acc: 0.5] [gan loss: 1.981659, acc: 0.000000]\n",
            "1881: [discriminator loss: 0.44544804096221924, acc: 0.5] [gan loss: 2.317860, acc: 0.000000]\n",
            "1882: [discriminator loss: 0.3836538791656494, acc: 0.5] [gan loss: 1.928494, acc: 0.000000]\n",
            "1883: [discriminator loss: 0.36570411920547485, acc: 0.5] [gan loss: 2.453332, acc: 0.000000]\n",
            "1884: [discriminator loss: 0.3267563581466675, acc: 0.5] [gan loss: 2.220005, acc: 0.000000]\n",
            "1885: [discriminator loss: 0.3546864986419678, acc: 0.5] [gan loss: 2.300084, acc: 0.000000]\n",
            "1886: [discriminator loss: 0.3356142044067383, acc: 0.5] [gan loss: 2.365652, acc: 0.000000]\n",
            "1887: [discriminator loss: 0.33896201848983765, acc: 0.5] [gan loss: 2.695123, acc: 0.000000]\n",
            "1888: [discriminator loss: 0.30072370171546936, acc: 0.5] [gan loss: 2.509143, acc: 0.000000]\n",
            "1889: [discriminator loss: 0.34374454617500305, acc: 0.5] [gan loss: 2.588564, acc: 0.000000]\n",
            "1890: [discriminator loss: 0.3083238899707794, acc: 0.5] [gan loss: 2.249445, acc: 0.000000]\n",
            "1891: [discriminator loss: 0.3207523226737976, acc: 0.5] [gan loss: 2.689174, acc: 0.000000]\n",
            "1892: [discriminator loss: 0.28247272968292236, acc: 0.5] [gan loss: 2.076618, acc: 0.000000]\n",
            "1893: [discriminator loss: 0.3713527321815491, acc: 0.5] [gan loss: 2.888360, acc: 0.000000]\n",
            "1894: [discriminator loss: 0.2758972644805908, acc: 0.5] [gan loss: 2.059318, acc: 0.000000]\n",
            "1895: [discriminator loss: 0.38940882682800293, acc: 0.5] [gan loss: 3.700264, acc: 0.000000]\n",
            "1896: [discriminator loss: 0.32139474153518677, acc: 0.5] [gan loss: 1.604951, acc: 0.000000]\n",
            "1897: [discriminator loss: 0.5628538727760315, acc: 0.203125] [gan loss: 4.275995, acc: 0.000000]\n",
            "1898: [discriminator loss: 0.34185242652893066, acc: 0.5] [gan loss: 2.376680, acc: 0.000000]\n",
            "1899: [discriminator loss: 0.3963075280189514, acc: 0.484375] [gan loss: 2.872385, acc: 0.000000]\n",
            "1900: [discriminator loss: 0.35850754380226135, acc: 0.5] [gan loss: 2.050198, acc: 0.000000]\n",
            "1901: [discriminator loss: 0.41240495443344116, acc: 0.484375] [gan loss: 2.811524, acc: 0.000000]\n",
            "1902: [discriminator loss: 0.3634987771511078, acc: 0.5] [gan loss: 2.068783, acc: 0.000000]\n",
            "1903: [discriminator loss: 0.4947507679462433, acc: 0.4375] [gan loss: 3.223583, acc: 0.000000]\n",
            "1904: [discriminator loss: 0.3496033549308777, acc: 0.5] [gan loss: 2.175468, acc: 0.000000]\n",
            "1905: [discriminator loss: 0.5536786317825317, acc: 0.375] [gan loss: 3.118202, acc: 0.000000]\n",
            "1906: [discriminator loss: 0.4070156514644623, acc: 0.5] [gan loss: 2.110577, acc: 0.000000]\n",
            "1907: [discriminator loss: 0.4701192080974579, acc: 0.4921875] [gan loss: 3.053226, acc: 0.000000]\n",
            "1908: [discriminator loss: 0.35631877183914185, acc: 0.5] [gan loss: 2.342363, acc: 0.000000]\n",
            "1909: [discriminator loss: 0.5214989185333252, acc: 0.359375] [gan loss: 3.339664, acc: 0.000000]\n",
            "1910: [discriminator loss: 0.5034040212631226, acc: 0.5] [gan loss: 1.329987, acc: 0.000000]\n",
            "1911: [discriminator loss: 0.8469961881637573, acc: 0.0078125] [gan loss: 2.974482, acc: 0.000000]\n",
            "1912: [discriminator loss: 0.5242274403572083, acc: 0.5] [gan loss: 1.672827, acc: 0.000000]\n",
            "1913: [discriminator loss: 0.5307421088218689, acc: 0.4296875] [gan loss: 1.841624, acc: 0.000000]\n",
            "1914: [discriminator loss: 0.5467959046363831, acc: 0.484375] [gan loss: 1.884595, acc: 0.000000]\n",
            "1915: [discriminator loss: 0.4753984212875366, acc: 0.4921875] [gan loss: 2.351745, acc: 0.000000]\n",
            "1916: [discriminator loss: 0.42166000604629517, acc: 0.4921875] [gan loss: 1.922755, acc: 0.000000]\n",
            "1917: [discriminator loss: 0.4821245074272156, acc: 0.4609375] [gan loss: 2.566015, acc: 0.000000]\n",
            "1918: [discriminator loss: 0.40197819471359253, acc: 0.5] [gan loss: 1.785501, acc: 0.000000]\n",
            "1919: [discriminator loss: 0.5212945938110352, acc: 0.390625] [gan loss: 2.565591, acc: 0.000000]\n",
            "1920: [discriminator loss: 0.41127920150756836, acc: 0.5] [gan loss: 1.618410, acc: 0.000000]\n",
            "1921: [discriminator loss: 0.5722789764404297, acc: 0.2578125] [gan loss: 2.678200, acc: 0.000000]\n",
            "1922: [discriminator loss: 0.4888859689235687, acc: 0.4921875] [gan loss: 1.396407, acc: 0.000000]\n",
            "1923: [discriminator loss: 0.6300225853919983, acc: 0.140625] [gan loss: 2.211425, acc: 0.000000]\n",
            "1924: [discriminator loss: 0.45548635721206665, acc: 0.5] [gan loss: 1.391685, acc: 0.000000]\n",
            "1925: [discriminator loss: 0.6414095163345337, acc: 0.1484375] [gan loss: 2.497631, acc: 0.000000]\n",
            "1926: [discriminator loss: 0.45717713236808777, acc: 0.5] [gan loss: 1.640957, acc: 0.000000]\n",
            "1927: [discriminator loss: 0.5654308199882507, acc: 0.34375] [gan loss: 2.032282, acc: 0.000000]\n",
            "1928: [discriminator loss: 0.5189273357391357, acc: 0.4609375] [gan loss: 1.843591, acc: 0.000000]\n",
            "1929: [discriminator loss: 0.47029751539230347, acc: 0.5] [gan loss: 1.754190, acc: 0.000000]\n",
            "1930: [discriminator loss: 0.4910653531551361, acc: 0.484375] [gan loss: 2.055516, acc: 0.000000]\n",
            "1931: [discriminator loss: 0.4270360767841339, acc: 0.5] [gan loss: 1.687447, acc: 0.000000]\n",
            "1932: [discriminator loss: 0.5268092155456543, acc: 0.4921875] [gan loss: 2.120134, acc: 0.000000]\n",
            "1933: [discriminator loss: 0.4143742024898529, acc: 0.5] [gan loss: 2.111098, acc: 0.000000]\n",
            "1934: [discriminator loss: 0.42317742109298706, acc: 0.5] [gan loss: 2.032543, acc: 0.000000]\n",
            "1935: [discriminator loss: 0.37996381521224976, acc: 0.5] [gan loss: 1.910461, acc: 0.000000]\n",
            "1936: [discriminator loss: 0.45952901244163513, acc: 0.484375] [gan loss: 2.338895, acc: 0.000000]\n",
            "1937: [discriminator loss: 0.364909291267395, acc: 0.5] [gan loss: 2.021055, acc: 0.000000]\n",
            "1938: [discriminator loss: 0.4652291536331177, acc: 0.4375] [gan loss: 2.803397, acc: 0.000000]\n",
            "1939: [discriminator loss: 0.42006716132164, acc: 0.5] [gan loss: 1.772522, acc: 0.000000]\n",
            "1940: [discriminator loss: 0.6232609748840332, acc: 0.2109375] [gan loss: 2.318462, acc: 0.000000]\n",
            "1941: [discriminator loss: 0.5105354189872742, acc: 0.4375] [gan loss: 1.607589, acc: 0.000000]\n",
            "1942: [discriminator loss: 0.4889061450958252, acc: 0.4375] [gan loss: 1.504740, acc: 0.000000]\n",
            "1943: [discriminator loss: 0.6738599538803101, acc: 0.1875] [gan loss: 2.920290, acc: 0.000000]\n",
            "1944: [discriminator loss: 0.5115717649459839, acc: 0.5] [gan loss: 0.961062, acc: 0.140625]\n",
            "1945: [discriminator loss: 0.8970718383789062, acc: 0.0] [gan loss: 3.258847, acc: 0.000000]\n",
            "1946: [discriminator loss: 0.5122112035751343, acc: 0.5] [gan loss: 1.407238, acc: 0.000000]\n",
            "1947: [discriminator loss: 0.6686815023422241, acc: 0.1875] [gan loss: 2.439577, acc: 0.000000]\n",
            "1948: [discriminator loss: 0.48971307277679443, acc: 0.5] [gan loss: 1.805714, acc: 0.000000]\n",
            "1949: [discriminator loss: 0.6314085721969604, acc: 0.3125] [gan loss: 2.501858, acc: 0.000000]\n",
            "1950: [discriminator loss: 0.49890804290771484, acc: 0.4921875] [gan loss: 2.136681, acc: 0.000000]\n",
            "1951: [discriminator loss: 0.5230313539505005, acc: 0.4921875] [gan loss: 2.037221, acc: 0.000000]\n",
            "1952: [discriminator loss: 0.5329474210739136, acc: 0.4765625] [gan loss: 2.228891, acc: 0.000000]\n",
            "1953: [discriminator loss: 0.5498754382133484, acc: 0.484375] [gan loss: 1.957749, acc: 0.000000]\n",
            "1954: [discriminator loss: 0.49586206674575806, acc: 0.5] [gan loss: 2.239745, acc: 0.000000]\n",
            "1955: [discriminator loss: 0.46304231882095337, acc: 0.5] [gan loss: 2.056925, acc: 0.000000]\n",
            "1956: [discriminator loss: 0.4821370542049408, acc: 0.4921875] [gan loss: 2.021348, acc: 0.000000]\n",
            "1957: [discriminator loss: 0.47460120916366577, acc: 0.484375] [gan loss: 2.180183, acc: 0.000000]\n",
            "1958: [discriminator loss: 0.45688319206237793, acc: 0.5] [gan loss: 2.043547, acc: 0.000000]\n",
            "1959: [discriminator loss: 0.5065902471542358, acc: 0.4921875] [gan loss: 2.132176, acc: 0.000000]\n",
            "1960: [discriminator loss: 0.3991834223270416, acc: 0.5] [gan loss: 1.914007, acc: 0.000000]\n",
            "1961: [discriminator loss: 0.45871230959892273, acc: 0.484375] [gan loss: 2.220147, acc: 0.000000]\n",
            "1962: [discriminator loss: 0.42904043197631836, acc: 0.4921875] [gan loss: 2.218784, acc: 0.000000]\n",
            "1963: [discriminator loss: 0.41911908984184265, acc: 0.5] [gan loss: 2.047517, acc: 0.000000]\n",
            "1964: [discriminator loss: 0.4492844343185425, acc: 0.4921875] [gan loss: 2.013551, acc: 0.000000]\n",
            "1965: [discriminator loss: 0.48021864891052246, acc: 0.5] [gan loss: 2.095264, acc: 0.000000]\n",
            "1966: [discriminator loss: 0.42712104320526123, acc: 0.5] [gan loss: 2.538277, acc: 0.000000]\n",
            "1967: [discriminator loss: 0.397127628326416, acc: 0.5] [gan loss: 1.987968, acc: 0.000000]\n",
            "1968: [discriminator loss: 0.4582539498806, acc: 0.4453125] [gan loss: 2.881530, acc: 0.000000]\n",
            "1969: [discriminator loss: 0.43327948451042175, acc: 0.5] [gan loss: 1.442427, acc: 0.000000]\n",
            "1970: [discriminator loss: 0.5948443412780762, acc: 0.1484375] [gan loss: 3.275957, acc: 0.000000]\n",
            "1971: [discriminator loss: 0.35580241680145264, acc: 0.5] [gan loss: 1.772219, acc: 0.000000]\n",
            "1972: [discriminator loss: 0.5488461256027222, acc: 0.3046875] [gan loss: 2.685835, acc: 0.000000]\n",
            "1973: [discriminator loss: 0.3821420669555664, acc: 0.5] [gan loss: 1.796012, acc: 0.000000]\n",
            "1974: [discriminator loss: 0.5738793015480042, acc: 0.28125] [gan loss: 2.794768, acc: 0.000000]\n",
            "1975: [discriminator loss: 0.43953993916511536, acc: 0.5] [gan loss: 1.760863, acc: 0.000000]\n",
            "1976: [discriminator loss: 0.5479047894477844, acc: 0.421875] [gan loss: 2.397180, acc: 0.000000]\n",
            "1977: [discriminator loss: 0.4521145820617676, acc: 0.5] [gan loss: 2.152884, acc: 0.000000]\n",
            "1978: [discriminator loss: 0.44760262966156006, acc: 0.484375] [gan loss: 2.454644, acc: 0.000000]\n",
            "1979: [discriminator loss: 0.4577975869178772, acc: 0.484375] [gan loss: 2.447269, acc: 0.000000]\n",
            "1980: [discriminator loss: 0.41646790504455566, acc: 0.5] [gan loss: 2.272760, acc: 0.000000]\n",
            "1981: [discriminator loss: 0.45136183500289917, acc: 0.484375] [gan loss: 2.771667, acc: 0.000000]\n",
            "1982: [discriminator loss: 0.37527602910995483, acc: 0.5] [gan loss: 2.003407, acc: 0.000000]\n",
            "1983: [discriminator loss: 0.44079601764678955, acc: 0.484375] [gan loss: 2.767846, acc: 0.000000]\n",
            "1984: [discriminator loss: 0.4363752007484436, acc: 0.5] [gan loss: 2.184629, acc: 0.000000]\n",
            "1985: [discriminator loss: 0.47420039772987366, acc: 0.484375] [gan loss: 2.919997, acc: 0.000000]\n",
            "1986: [discriminator loss: 0.3929818868637085, acc: 0.5] [gan loss: 2.237268, acc: 0.000000]\n",
            "1987: [discriminator loss: 0.6287657022476196, acc: 0.28125] [gan loss: 2.581979, acc: 0.000000]\n",
            "1988: [discriminator loss: 0.608594536781311, acc: 0.3984375] [gan loss: 1.667585, acc: 0.000000]\n",
            "1989: [discriminator loss: 0.56938636302948, acc: 0.4296875] [gan loss: 0.915667, acc: 0.187500]\n",
            "1990: [discriminator loss: 0.6301590204238892, acc: 0.2578125] [gan loss: 1.351249, acc: 0.000000]\n",
            "1991: [discriminator loss: 0.7528859972953796, acc: 0.1328125] [gan loss: 2.134913, acc: 0.000000]\n",
            "1992: [discriminator loss: 0.6019870042800903, acc: 0.5] [gan loss: 1.448342, acc: 0.000000]\n",
            "1993: [discriminator loss: 0.6878610849380493, acc: 0.15625] [gan loss: 2.637503, acc: 0.000000]\n",
            "1994: [discriminator loss: 0.5267925262451172, acc: 0.5] [gan loss: 1.399403, acc: 0.000000]\n",
            "1995: [discriminator loss: 0.6210096478462219, acc: 0.296875] [gan loss: 2.234743, acc: 0.000000]\n",
            "1996: [discriminator loss: 0.5138700008392334, acc: 0.4921875] [gan loss: 1.547998, acc: 0.000000]\n",
            "1997: [discriminator loss: 0.49451717734336853, acc: 0.453125] [gan loss: 1.786603, acc: 0.000000]\n",
            "1998: [discriminator loss: 0.521582305431366, acc: 0.453125] [gan loss: 1.692266, acc: 0.000000]\n",
            "1999: [discriminator loss: 0.5671834945678711, acc: 0.375] [gan loss: 1.596626, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2000: [discriminator loss: 0.5664644837379456, acc: 0.3828125] [gan loss: 1.704592, acc: 0.000000]\n",
            "2001: [discriminator loss: 0.6326727867126465, acc: 0.359375] [gan loss: 1.554629, acc: 0.000000]\n",
            "2002: [discriminator loss: 0.5992388725280762, acc: 0.3671875] [gan loss: 1.630115, acc: 0.000000]\n",
            "2003: [discriminator loss: 0.5141295194625854, acc: 0.453125] [gan loss: 1.933191, acc: 0.000000]\n",
            "2004: [discriminator loss: 0.49516481161117554, acc: 0.4609375] [gan loss: 1.448488, acc: 0.046875]\n",
            "2005: [discriminator loss: 0.6349406838417053, acc: 0.296875] [gan loss: 1.876415, acc: 0.000000]\n",
            "2006: [discriminator loss: 0.5422326326370239, acc: 0.453125] [gan loss: 1.312139, acc: 0.000000]\n",
            "2007: [discriminator loss: 0.5785199999809265, acc: 0.3203125] [gan loss: 2.210020, acc: 0.000000]\n",
            "2008: [discriminator loss: 0.5248277187347412, acc: 0.5] [gan loss: 1.068890, acc: 0.109375]\n",
            "2009: [discriminator loss: 0.6537964344024658, acc: 0.1171875] [gan loss: 2.350915, acc: 0.000000]\n",
            "2010: [discriminator loss: 0.438585489988327, acc: 0.5] [gan loss: 1.220959, acc: 0.015625]\n",
            "2011: [discriminator loss: 0.5501827001571655, acc: 0.3515625] [gan loss: 1.640474, acc: 0.000000]\n",
            "2012: [discriminator loss: 0.5370209217071533, acc: 0.4140625] [gan loss: 1.908147, acc: 0.000000]\n",
            "2013: [discriminator loss: 0.4724782109260559, acc: 0.5] [gan loss: 1.845123, acc: 0.000000]\n",
            "2014: [discriminator loss: 0.45912840962409973, acc: 0.4921875] [gan loss: 1.838235, acc: 0.000000]\n",
            "2015: [discriminator loss: 0.4347723722457886, acc: 0.484375] [gan loss: 2.019167, acc: 0.000000]\n",
            "2016: [discriminator loss: 0.44931653141975403, acc: 0.4921875] [gan loss: 1.697562, acc: 0.000000]\n",
            "2017: [discriminator loss: 0.4930765628814697, acc: 0.4375] [gan loss: 2.165691, acc: 0.000000]\n",
            "2018: [discriminator loss: 0.3906574547290802, acc: 0.5] [gan loss: 1.746334, acc: 0.000000]\n",
            "2019: [discriminator loss: 0.5089670419692993, acc: 0.3515625] [gan loss: 2.496323, acc: 0.000000]\n",
            "2020: [discriminator loss: 0.4383787214756012, acc: 0.4921875] [gan loss: 1.726794, acc: 0.000000]\n",
            "2021: [discriminator loss: 0.593589186668396, acc: 0.21875] [gan loss: 2.422561, acc: 0.000000]\n",
            "2022: [discriminator loss: 0.5052269697189331, acc: 0.4765625] [gan loss: 1.366257, acc: 0.015625]\n",
            "2023: [discriminator loss: 0.6617294549942017, acc: 0.15625] [gan loss: 2.439791, acc: 0.000000]\n",
            "2024: [discriminator loss: 0.4890938699245453, acc: 0.5] [gan loss: 1.173082, acc: 0.031250]\n",
            "2025: [discriminator loss: 0.6336672306060791, acc: 0.2578125] [gan loss: 2.171272, acc: 0.000000]\n",
            "2026: [discriminator loss: 0.539470911026001, acc: 0.4921875] [gan loss: 1.034235, acc: 0.046875]\n",
            "2027: [discriminator loss: 0.6862978935241699, acc: 0.09375] [gan loss: 1.932297, acc: 0.000000]\n",
            "2028: [discriminator loss: 0.5286480188369751, acc: 0.4765625] [gan loss: 1.384683, acc: 0.031250]\n",
            "2029: [discriminator loss: 0.6142816543579102, acc: 0.359375] [gan loss: 1.674619, acc: 0.000000]\n",
            "2030: [discriminator loss: 0.617965579032898, acc: 0.4375] [gan loss: 1.135325, acc: 0.109375]\n",
            "2031: [discriminator loss: 0.689685583114624, acc: 0.15625] [gan loss: 2.122536, acc: 0.000000]\n",
            "2032: [discriminator loss: 0.4922725558280945, acc: 0.484375] [gan loss: 1.106989, acc: 0.109375]\n",
            "2033: [discriminator loss: 0.6100059151649475, acc: 0.25] [gan loss: 1.801528, acc: 0.000000]\n",
            "2034: [discriminator loss: 0.5158436894416809, acc: 0.4765625] [gan loss: 1.214446, acc: 0.062500]\n",
            "2035: [discriminator loss: 0.6261374354362488, acc: 0.21875] [gan loss: 1.825260, acc: 0.000000]\n",
            "2036: [discriminator loss: 0.5486294031143188, acc: 0.484375] [gan loss: 1.069457, acc: 0.171875]\n",
            "2037: [discriminator loss: 0.7474995851516724, acc: 0.1171875] [gan loss: 2.059990, acc: 0.015625]\n",
            "2038: [discriminator loss: 0.6301078796386719, acc: 0.4609375] [gan loss: 0.935676, acc: 0.328125]\n",
            "2039: [discriminator loss: 0.8271759748458862, acc: 0.1328125] [gan loss: 1.847131, acc: 0.015625]\n",
            "2040: [discriminator loss: 0.6284910440444946, acc: 0.4609375] [gan loss: 1.064937, acc: 0.093750]\n",
            "2041: [discriminator loss: 0.7559798359870911, acc: 0.1953125] [gan loss: 1.510444, acc: 0.046875]\n",
            "2042: [discriminator loss: 0.6193681359291077, acc: 0.375] [gan loss: 1.276880, acc: 0.078125]\n",
            "2043: [discriminator loss: 0.6782711744308472, acc: 0.2734375] [gan loss: 1.128378, acc: 0.125000]\n",
            "2044: [discriminator loss: 0.747384786605835, acc: 0.3359375] [gan loss: 1.242310, acc: 0.062500]\n",
            "2045: [discriminator loss: 0.7191603183746338, acc: 0.2890625] [gan loss: 1.202036, acc: 0.062500]\n",
            "2046: [discriminator loss: 0.7077252864837646, acc: 0.28125] [gan loss: 1.299257, acc: 0.000000]\n",
            "2047: [discriminator loss: 0.6765694618225098, acc: 0.3359375] [gan loss: 1.214779, acc: 0.031250]\n",
            "2048: [discriminator loss: 0.603387176990509, acc: 0.40625] [gan loss: 0.905750, acc: 0.234375]\n",
            "2049: [discriminator loss: 0.7056272029876709, acc: 0.203125] [gan loss: 1.502803, acc: 0.031250]\n",
            "2050: [discriminator loss: 0.6032797694206238, acc: 0.40625] [gan loss: 0.885328, acc: 0.250000]\n",
            "2051: [discriminator loss: 0.6719143390655518, acc: 0.2421875] [gan loss: 1.194399, acc: 0.093750]\n",
            "2052: [discriminator loss: 0.5861910581588745, acc: 0.3984375] [gan loss: 0.951129, acc: 0.312500]\n",
            "2053: [discriminator loss: 0.6686468124389648, acc: 0.2265625] [gan loss: 1.287078, acc: 0.031250]\n",
            "2054: [discriminator loss: 0.6621627807617188, acc: 0.34375] [gan loss: 1.359431, acc: 0.015625]\n",
            "2055: [discriminator loss: 0.6369146108627319, acc: 0.328125] [gan loss: 1.493722, acc: 0.015625]\n",
            "2056: [discriminator loss: 0.654996395111084, acc: 0.375] [gan loss: 1.351192, acc: 0.015625]\n",
            "2057: [discriminator loss: 0.6588245630264282, acc: 0.2734375] [gan loss: 1.381324, acc: 0.015625]\n",
            "2058: [discriminator loss: 0.6410873532295227, acc: 0.40625] [gan loss: 1.167529, acc: 0.031250]\n",
            "2059: [discriminator loss: 0.6832873821258545, acc: 0.265625] [gan loss: 1.328603, acc: 0.015625]\n",
            "2060: [discriminator loss: 0.5999295115470886, acc: 0.453125] [gan loss: 1.101703, acc: 0.078125]\n",
            "2061: [discriminator loss: 0.6494905948638916, acc: 0.28125] [gan loss: 1.272104, acc: 0.031250]\n",
            "2062: [discriminator loss: 0.58507239818573, acc: 0.40625] [gan loss: 1.206837, acc: 0.046875]\n",
            "2063: [discriminator loss: 0.6036403179168701, acc: 0.359375] [gan loss: 1.343658, acc: 0.015625]\n",
            "2064: [discriminator loss: 0.610369086265564, acc: 0.390625] [gan loss: 1.294303, acc: 0.031250]\n",
            "2065: [discriminator loss: 0.5540214776992798, acc: 0.3828125] [gan loss: 1.436584, acc: 0.000000]\n",
            "2066: [discriminator loss: 0.5595267415046692, acc: 0.4375] [gan loss: 1.220433, acc: 0.015625]\n",
            "2067: [discriminator loss: 0.5766428709030151, acc: 0.328125] [gan loss: 1.436467, acc: 0.000000]\n",
            "2068: [discriminator loss: 0.6158773899078369, acc: 0.359375] [gan loss: 0.967380, acc: 0.171875]\n",
            "2069: [discriminator loss: 0.6984252333641052, acc: 0.1484375] [gan loss: 1.680346, acc: 0.015625]\n",
            "2070: [discriminator loss: 0.5861823558807373, acc: 0.4453125] [gan loss: 1.192418, acc: 0.078125]\n",
            "2071: [discriminator loss: 0.603556752204895, acc: 0.359375] [gan loss: 1.573512, acc: 0.000000]\n",
            "2072: [discriminator loss: 0.5315995216369629, acc: 0.453125] [gan loss: 1.214106, acc: 0.078125]\n",
            "2073: [discriminator loss: 0.6046777367591858, acc: 0.3125] [gan loss: 1.421068, acc: 0.015625]\n",
            "2074: [discriminator loss: 0.5382190942764282, acc: 0.4453125] [gan loss: 1.130734, acc: 0.093750]\n",
            "2075: [discriminator loss: 0.6288823485374451, acc: 0.28125] [gan loss: 1.446900, acc: 0.015625]\n",
            "2076: [discriminator loss: 0.5959765315055847, acc: 0.3671875] [gan loss: 1.486883, acc: 0.046875]\n",
            "2077: [discriminator loss: 0.6137291193008423, acc: 0.3515625] [gan loss: 1.580897, acc: 0.015625]\n",
            "2078: [discriminator loss: 0.6731750965118408, acc: 0.359375] [gan loss: 1.321197, acc: 0.015625]\n",
            "2079: [discriminator loss: 0.6942251920700073, acc: 0.2734375] [gan loss: 1.275960, acc: 0.031250]\n",
            "2080: [discriminator loss: 0.6514664888381958, acc: 0.34375] [gan loss: 1.273585, acc: 0.031250]\n",
            "2081: [discriminator loss: 0.6163802742958069, acc: 0.34375] [gan loss: 1.179575, acc: 0.062500]\n",
            "2082: [discriminator loss: 0.6556839942932129, acc: 0.203125] [gan loss: 1.525729, acc: 0.000000]\n",
            "2083: [discriminator loss: 0.5500262975692749, acc: 0.4765625] [gan loss: 0.810038, acc: 0.421875]\n",
            "2084: [discriminator loss: 0.71650230884552, acc: 0.078125] [gan loss: 1.812046, acc: 0.000000]\n",
            "2085: [discriminator loss: 0.5967675447463989, acc: 0.4921875] [gan loss: 1.083809, acc: 0.093750]\n",
            "2086: [discriminator loss: 0.5920388102531433, acc: 0.265625] [gan loss: 1.529408, acc: 0.000000]\n",
            "2087: [discriminator loss: 0.5465776920318604, acc: 0.4375] [gan loss: 1.057243, acc: 0.125000]\n",
            "2088: [discriminator loss: 0.5866479873657227, acc: 0.2890625] [gan loss: 0.913953, acc: 0.218750]\n",
            "2089: [discriminator loss: 0.5553280115127563, acc: 0.4140625] [gan loss: 0.587367, acc: 0.750000]\n",
            "2090: [discriminator loss: 0.6809016466140747, acc: 0.15625] [gan loss: 1.446188, acc: 0.046875]\n",
            "2091: [discriminator loss: 0.47313743829727173, acc: 0.46875] [gan loss: 0.452506, acc: 0.890625]\n",
            "2092: [discriminator loss: 0.7351030111312866, acc: 0.1328125] [gan loss: 1.419728, acc: 0.062500]\n",
            "2093: [discriminator loss: 0.569550096988678, acc: 0.421875] [gan loss: 1.000737, acc: 0.171875]\n",
            "2094: [discriminator loss: 0.6988828182220459, acc: 0.1796875] [gan loss: 1.761372, acc: 0.000000]\n",
            "2095: [discriminator loss: 0.5821791291236877, acc: 0.4375] [gan loss: 1.148269, acc: 0.078125]\n",
            "2096: [discriminator loss: 0.5775864124298096, acc: 0.34375] [gan loss: 1.220519, acc: 0.031250]\n",
            "2097: [discriminator loss: 0.6003779172897339, acc: 0.34375] [gan loss: 1.428138, acc: 0.031250]\n",
            "2098: [discriminator loss: 0.6171813011169434, acc: 0.3125] [gan loss: 0.970495, acc: 0.093750]\n",
            "2099: [discriminator loss: 0.5500317215919495, acc: 0.390625] [gan loss: 0.860512, acc: 0.343750]\n",
            "2100: [discriminator loss: 0.6590806245803833, acc: 0.25] [gan loss: 1.207061, acc: 0.031250]\n",
            "2101: [discriminator loss: 0.5662466287612915, acc: 0.4296875] [gan loss: 1.100080, acc: 0.046875]\n",
            "2102: [discriminator loss: 0.5985973477363586, acc: 0.2421875] [gan loss: 1.431413, acc: 0.000000]\n",
            "2103: [discriminator loss: 0.5075262188911438, acc: 0.4765625] [gan loss: 1.101012, acc: 0.046875]\n",
            "2104: [discriminator loss: 0.5309323072433472, acc: 0.3671875] [gan loss: 1.813793, acc: 0.000000]\n",
            "2105: [discriminator loss: 0.4823867082595825, acc: 0.3984375] [gan loss: 1.126286, acc: 0.031250]\n",
            "2106: [discriminator loss: 0.5564408898353577, acc: 0.375] [gan loss: 1.488649, acc: 0.000000]\n",
            "2107: [discriminator loss: 0.4675467908382416, acc: 0.453125] [gan loss: 1.147610, acc: 0.031250]\n",
            "2108: [discriminator loss: 0.5045866370201111, acc: 0.390625] [gan loss: 1.378435, acc: 0.015625]\n",
            "2109: [discriminator loss: 0.4810584783554077, acc: 0.4609375] [gan loss: 1.182766, acc: 0.062500]\n",
            "2110: [discriminator loss: 0.5271897315979004, acc: 0.40625] [gan loss: 1.824811, acc: 0.000000]\n",
            "2111: [discriminator loss: 0.3383149206638336, acc: 0.5] [gan loss: 1.194132, acc: 0.031250]\n",
            "2112: [discriminator loss: 0.5262394547462463, acc: 0.3046875] [gan loss: 2.226595, acc: 0.000000]\n",
            "2113: [discriminator loss: 0.466049462556839, acc: 0.484375] [gan loss: 1.234225, acc: 0.000000]\n",
            "2114: [discriminator loss: 0.5616759061813354, acc: 0.3046875] [gan loss: 1.713329, acc: 0.000000]\n",
            "2115: [discriminator loss: 0.5117398500442505, acc: 0.5] [gan loss: 1.406736, acc: 0.000000]\n",
            "2116: [discriminator loss: 0.5171909928321838, acc: 0.3046875] [gan loss: 1.983244, acc: 0.000000]\n",
            "2117: [discriminator loss: 0.4355248212814331, acc: 0.46875] [gan loss: 1.228264, acc: 0.031250]\n",
            "2118: [discriminator loss: 0.5500755310058594, acc: 0.375] [gan loss: 1.533232, acc: 0.000000]\n",
            "2119: [discriminator loss: 0.5177797675132751, acc: 0.4296875] [gan loss: 1.034112, acc: 0.046875]\n",
            "2120: [discriminator loss: 0.567190945148468, acc: 0.34375] [gan loss: 1.344445, acc: 0.015625]\n",
            "2121: [discriminator loss: 0.5466284155845642, acc: 0.4140625] [gan loss: 0.946759, acc: 0.140625]\n",
            "2122: [discriminator loss: 0.6239111423492432, acc: 0.25] [gan loss: 1.484586, acc: 0.000000]\n",
            "2123: [discriminator loss: 0.5087312459945679, acc: 0.4609375] [gan loss: 0.755713, acc: 0.406250]\n",
            "2124: [discriminator loss: 0.655937135219574, acc: 0.171875] [gan loss: 1.365636, acc: 0.015625]\n",
            "2125: [discriminator loss: 0.6013034582138062, acc: 0.453125] [gan loss: 0.824625, acc: 0.375000]\n",
            "2126: [discriminator loss: 0.6069227457046509, acc: 0.2421875] [gan loss: 1.207139, acc: 0.046875]\n",
            "2127: [discriminator loss: 0.5334973931312561, acc: 0.40625] [gan loss: 0.887170, acc: 0.265625]\n",
            "2128: [discriminator loss: 0.612383246421814, acc: 0.2734375] [gan loss: 1.051926, acc: 0.078125]\n",
            "2129: [discriminator loss: 0.5774201154708862, acc: 0.4453125] [gan loss: 0.715774, acc: 0.531250]\n",
            "2130: [discriminator loss: 0.6771753430366516, acc: 0.171875] [gan loss: 1.329288, acc: 0.015625]\n",
            "2131: [discriminator loss: 0.5688199996948242, acc: 0.421875] [gan loss: 0.902652, acc: 0.156250]\n",
            "2132: [discriminator loss: 0.6173326969146729, acc: 0.2890625] [gan loss: 1.090270, acc: 0.046875]\n",
            "2133: [discriminator loss: 0.5511772632598877, acc: 0.484375] [gan loss: 0.719553, acc: 0.453125]\n",
            "2134: [discriminator loss: 0.6436942219734192, acc: 0.1875] [gan loss: 1.166212, acc: 0.015625]\n",
            "2135: [discriminator loss: 0.5092165470123291, acc: 0.4765625] [gan loss: 0.746487, acc: 0.437500]\n",
            "2136: [discriminator loss: 0.6277647018432617, acc: 0.2421875] [gan loss: 1.267717, acc: 0.000000]\n",
            "2137: [discriminator loss: 0.5710721611976624, acc: 0.4765625] [gan loss: 1.053349, acc: 0.015625]\n",
            "2138: [discriminator loss: 0.5345674157142639, acc: 0.3515625] [gan loss: 1.394523, acc: 0.000000]\n",
            "2139: [discriminator loss: 0.5264629125595093, acc: 0.453125] [gan loss: 1.052339, acc: 0.046875]\n",
            "2140: [discriminator loss: 0.5449637174606323, acc: 0.3984375] [gan loss: 1.282180, acc: 0.031250]\n",
            "2141: [discriminator loss: 0.5058141350746155, acc: 0.4453125] [gan loss: 1.091754, acc: 0.093750]\n",
            "2142: [discriminator loss: 0.5289347171783447, acc: 0.4375] [gan loss: 0.928173, acc: 0.156250]\n",
            "2143: [discriminator loss: 0.5405742526054382, acc: 0.3359375] [gan loss: 1.201346, acc: 0.015625]\n",
            "2144: [discriminator loss: 0.4698576331138611, acc: 0.4921875] [gan loss: 0.457145, acc: 0.890625]\n",
            "2145: [discriminator loss: 0.5800849199295044, acc: 0.25] [gan loss: 0.918497, acc: 0.171875]\n",
            "2146: [discriminator loss: 0.45435822010040283, acc: 0.484375] [gan loss: 0.234476, acc: 1.000000]\n",
            "2147: [discriminator loss: 0.5749002695083618, acc: 0.2734375] [gan loss: 0.723116, acc: 0.437500]\n",
            "2148: [discriminator loss: 0.4174098074436188, acc: 0.5] [gan loss: 0.327575, acc: 1.000000]\n",
            "2149: [discriminator loss: 0.48195987939834595, acc: 0.359375] [gan loss: 0.814358, acc: 0.312500]\n",
            "2150: [discriminator loss: 0.4814310073852539, acc: 0.453125] [gan loss: 0.532931, acc: 0.812500]\n",
            "2151: [discriminator loss: 0.7484577894210815, acc: 0.0234375] [gan loss: 2.345955, acc: 0.000000]\n",
            "2152: [discriminator loss: 0.48126110434532166, acc: 0.5] [gan loss: 1.029413, acc: 0.125000]\n",
            "2153: [discriminator loss: 0.6373753547668457, acc: 0.15625] [gan loss: 1.748353, acc: 0.000000]\n",
            "2154: [discriminator loss: 0.47617796063423157, acc: 0.4609375] [gan loss: 1.234486, acc: 0.015625]\n",
            "2155: [discriminator loss: 0.5467560887336731, acc: 0.3828125] [gan loss: 1.636572, acc: 0.000000]\n",
            "2156: [discriminator loss: 0.4266256093978882, acc: 0.4765625] [gan loss: 1.764106, acc: 0.000000]\n",
            "2157: [discriminator loss: 0.4400583803653717, acc: 0.5] [gan loss: 1.609941, acc: 0.000000]\n",
            "2158: [discriminator loss: 0.462526798248291, acc: 0.421875] [gan loss: 2.044857, acc: 0.000000]\n",
            "2159: [discriminator loss: 0.3876992166042328, acc: 0.5] [gan loss: 1.739324, acc: 0.000000]\n",
            "2160: [discriminator loss: 0.44265586137771606, acc: 0.4609375] [gan loss: 1.998145, acc: 0.000000]\n",
            "2161: [discriminator loss: 0.41879135370254517, acc: 0.46875] [gan loss: 1.767235, acc: 0.000000]\n",
            "2162: [discriminator loss: 0.45450353622436523, acc: 0.4609375] [gan loss: 1.795163, acc: 0.000000]\n",
            "2163: [discriminator loss: 0.44819772243499756, acc: 0.453125] [gan loss: 1.668661, acc: 0.000000]\n",
            "2164: [discriminator loss: 0.4777798652648926, acc: 0.453125] [gan loss: 1.307143, acc: 0.031250]\n",
            "2165: [discriminator loss: 0.49811503291130066, acc: 0.3828125] [gan loss: 1.774101, acc: 0.000000]\n",
            "2166: [discriminator loss: 0.46053555607795715, acc: 0.46875] [gan loss: 1.413378, acc: 0.015625]\n",
            "2167: [discriminator loss: 0.4532487392425537, acc: 0.46875] [gan loss: 1.645943, acc: 0.000000]\n",
            "2168: [discriminator loss: 0.4543556869029999, acc: 0.4765625] [gan loss: 1.068516, acc: 0.140625]\n",
            "2169: [discriminator loss: 0.5436491966247559, acc: 0.3359375] [gan loss: 2.181983, acc: 0.000000]\n",
            "2170: [discriminator loss: 0.4442959427833557, acc: 0.4140625] [gan loss: 0.989219, acc: 0.093750]\n",
            "2171: [discriminator loss: 0.5352455377578735, acc: 0.296875] [gan loss: 2.034185, acc: 0.000000]\n",
            "2172: [discriminator loss: 0.4153120815753937, acc: 0.5] [gan loss: 0.692218, acc: 0.531250]\n",
            "2173: [discriminator loss: 0.6443851590156555, acc: 0.171875] [gan loss: 1.844267, acc: 0.000000]\n",
            "2174: [discriminator loss: 0.42949509620666504, acc: 0.5] [gan loss: 1.110080, acc: 0.140625]\n",
            "2175: [discriminator loss: 0.5573678612709045, acc: 0.265625] [gan loss: 1.676155, acc: 0.000000]\n",
            "2176: [discriminator loss: 0.4085679054260254, acc: 0.4921875] [gan loss: 0.882904, acc: 0.203125]\n",
            "2177: [discriminator loss: 0.5495334267616272, acc: 0.3046875] [gan loss: 1.551762, acc: 0.015625]\n",
            "2178: [discriminator loss: 0.4559963345527649, acc: 0.484375] [gan loss: 1.242039, acc: 0.031250]\n",
            "2179: [discriminator loss: 0.5319960117340088, acc: 0.3046875] [gan loss: 1.936292, acc: 0.000000]\n",
            "2180: [discriminator loss: 0.4504649043083191, acc: 0.4296875] [gan loss: 1.127683, acc: 0.109375]\n",
            "2181: [discriminator loss: 0.5940558910369873, acc: 0.21875] [gan loss: 1.966034, acc: 0.000000]\n",
            "2182: [discriminator loss: 0.4487467408180237, acc: 0.453125] [gan loss: 1.008940, acc: 0.078125]\n",
            "2183: [discriminator loss: 0.6340808272361755, acc: 0.203125] [gan loss: 1.758002, acc: 0.000000]\n",
            "2184: [discriminator loss: 0.5184369087219238, acc: 0.4921875] [gan loss: 0.900273, acc: 0.281250]\n",
            "2185: [discriminator loss: 0.6846604347229004, acc: 0.09375] [gan loss: 2.048488, acc: 0.000000]\n",
            "2186: [discriminator loss: 0.4834820628166199, acc: 0.5] [gan loss: 0.969165, acc: 0.125000]\n",
            "2187: [discriminator loss: 0.6857786178588867, acc: 0.125] [gan loss: 1.838500, acc: 0.000000]\n",
            "2188: [discriminator loss: 0.48716944456100464, acc: 0.4765625] [gan loss: 1.037087, acc: 0.109375]\n",
            "2189: [discriminator loss: 0.7097065448760986, acc: 0.09375] [gan loss: 1.903907, acc: 0.000000]\n",
            "2190: [discriminator loss: 0.5412708520889282, acc: 0.4765625] [gan loss: 1.158151, acc: 0.015625]\n",
            "2191: [discriminator loss: 0.6638984084129333, acc: 0.1171875] [gan loss: 1.790951, acc: 0.000000]\n",
            "2192: [discriminator loss: 0.5752195715904236, acc: 0.3671875] [gan loss: 1.647435, acc: 0.000000]\n",
            "2193: [discriminator loss: 0.5189628601074219, acc: 0.4609375] [gan loss: 1.649190, acc: 0.000000]\n",
            "2194: [discriminator loss: 0.57257080078125, acc: 0.3046875] [gan loss: 1.674437, acc: 0.000000]\n",
            "2195: [discriminator loss: 0.40909940004348755, acc: 0.5] [gan loss: 0.958758, acc: 0.125000]\n",
            "2196: [discriminator loss: 0.727422833442688, acc: 0.0546875] [gan loss: 2.018878, acc: 0.000000]\n",
            "2197: [discriminator loss: 0.6450389623641968, acc: 0.3828125] [gan loss: 1.434301, acc: 0.031250]\n",
            "2198: [discriminator loss: 0.7190172076225281, acc: 0.2421875] [gan loss: 1.740073, acc: 0.000000]\n",
            "2199: [discriminator loss: 0.649594247341156, acc: 0.3984375] [gan loss: 1.349605, acc: 0.031250]\n",
            "2200: [discriminator loss: 0.6718390583992004, acc: 0.2578125] [gan loss: 1.598432, acc: 0.000000]\n",
            "2201: [discriminator loss: 0.6062876582145691, acc: 0.3984375] [gan loss: 1.536258, acc: 0.000000]\n",
            "2202: [discriminator loss: 0.616931676864624, acc: 0.3359375] [gan loss: 1.631230, acc: 0.000000]\n",
            "2203: [discriminator loss: 0.5797311067581177, acc: 0.4453125] [gan loss: 1.550675, acc: 0.000000]\n",
            "2204: [discriminator loss: 0.5752869844436646, acc: 0.40625] [gan loss: 1.674606, acc: 0.000000]\n",
            "2205: [discriminator loss: 0.5366983413696289, acc: 0.4140625] [gan loss: 1.494640, acc: 0.000000]\n",
            "2206: [discriminator loss: 0.6074938178062439, acc: 0.3828125] [gan loss: 1.435650, acc: 0.000000]\n",
            "2207: [discriminator loss: 0.6004746556282043, acc: 0.3984375] [gan loss: 1.374017, acc: 0.015625]\n",
            "2208: [discriminator loss: 0.627710223197937, acc: 0.3125] [gan loss: 1.541384, acc: 0.000000]\n",
            "2209: [discriminator loss: 0.6288901567459106, acc: 0.328125] [gan loss: 1.519668, acc: 0.015625]\n",
            "2210: [discriminator loss: 0.6597020626068115, acc: 0.3359375] [gan loss: 1.586795, acc: 0.000000]\n",
            "2211: [discriminator loss: 0.5954763889312744, acc: 0.390625] [gan loss: 1.541322, acc: 0.000000]\n",
            "2212: [discriminator loss: 0.569895327091217, acc: 0.390625] [gan loss: 1.508854, acc: 0.000000]\n",
            "2213: [discriminator loss: 0.5631377696990967, acc: 0.4609375] [gan loss: 1.275288, acc: 0.000000]\n",
            "2214: [discriminator loss: 0.6021301746368408, acc: 0.234375] [gan loss: 2.075439, acc: 0.000000]\n",
            "2215: [discriminator loss: 0.5357842445373535, acc: 0.5] [gan loss: 1.118505, acc: 0.156250]\n",
            "2216: [discriminator loss: 0.6789567470550537, acc: 0.1875] [gan loss: 1.955323, acc: 0.000000]\n",
            "2217: [discriminator loss: 0.5539531707763672, acc: 0.4765625] [gan loss: 1.256791, acc: 0.062500]\n",
            "2218: [discriminator loss: 0.6322166919708252, acc: 0.2578125] [gan loss: 1.693474, acc: 0.000000]\n",
            "2219: [discriminator loss: 0.5614728331565857, acc: 0.4453125] [gan loss: 1.325269, acc: 0.046875]\n",
            "2220: [discriminator loss: 0.6074852347373962, acc: 0.3125] [gan loss: 1.641381, acc: 0.000000]\n",
            "2221: [discriminator loss: 0.5459334850311279, acc: 0.4609375] [gan loss: 1.413739, acc: 0.015625]\n",
            "2222: [discriminator loss: 0.565358579158783, acc: 0.359375] [gan loss: 1.339757, acc: 0.015625]\n",
            "2223: [discriminator loss: 0.5903242826461792, acc: 0.375] [gan loss: 1.406133, acc: 0.015625]\n",
            "2224: [discriminator loss: 0.5703431367874146, acc: 0.3203125] [gan loss: 1.358739, acc: 0.031250]\n",
            "2225: [discriminator loss: 0.527389645576477, acc: 0.390625] [gan loss: 1.180129, acc: 0.046875]\n",
            "2226: [discriminator loss: 0.49696192145347595, acc: 0.4375] [gan loss: 0.890404, acc: 0.265625]\n",
            "2227: [discriminator loss: 0.6468285322189331, acc: 0.2265625] [gan loss: 1.261972, acc: 0.031250]\n",
            "2228: [discriminator loss: 0.6441034078598022, acc: 0.359375] [gan loss: 1.022947, acc: 0.171875]\n",
            "2229: [discriminator loss: 0.7007060050964355, acc: 0.1953125] [gan loss: 1.778542, acc: 0.000000]\n",
            "2230: [discriminator loss: 0.675611138343811, acc: 0.4375] [gan loss: 0.771350, acc: 0.359375]\n",
            "2231: [discriminator loss: 0.7184252738952637, acc: 0.125] [gan loss: 1.563374, acc: 0.000000]\n",
            "2232: [discriminator loss: 0.6151182055473328, acc: 0.4453125] [gan loss: 0.983298, acc: 0.171875]\n",
            "2233: [discriminator loss: 0.7083309292793274, acc: 0.1171875] [gan loss: 1.776066, acc: 0.000000]\n",
            "2234: [discriminator loss: 0.6321526169776917, acc: 0.4921875] [gan loss: 0.946281, acc: 0.187500]\n",
            "2235: [discriminator loss: 0.7160367965698242, acc: 0.1015625] [gan loss: 1.398915, acc: 0.000000]\n",
            "2236: [discriminator loss: 0.6351649761199951, acc: 0.3984375] [gan loss: 0.888686, acc: 0.187500]\n",
            "2237: [discriminator loss: 0.6464794874191284, acc: 0.25] [gan loss: 1.176437, acc: 0.015625]\n",
            "2238: [discriminator loss: 0.6621819138526917, acc: 0.3125] [gan loss: 1.127478, acc: 0.031250]\n",
            "2239: [discriminator loss: 0.6351650953292847, acc: 0.328125] [gan loss: 1.269592, acc: 0.000000]\n",
            "2240: [discriminator loss: 0.6086058616638184, acc: 0.375] [gan loss: 1.172715, acc: 0.062500]\n",
            "2241: [discriminator loss: 0.6634575128555298, acc: 0.2578125] [gan loss: 1.327193, acc: 0.015625]\n",
            "2242: [discriminator loss: 0.612153947353363, acc: 0.3984375] [gan loss: 1.146008, acc: 0.078125]\n",
            "2243: [discriminator loss: 0.5930150747299194, acc: 0.2890625] [gan loss: 1.458719, acc: 0.000000]\n",
            "2244: [discriminator loss: 0.5630634427070618, acc: 0.453125] [gan loss: 1.119964, acc: 0.078125]\n",
            "2245: [discriminator loss: 0.6669125556945801, acc: 0.1640625] [gan loss: 1.699678, acc: 0.000000]\n",
            "2246: [discriminator loss: 0.5789653062820435, acc: 0.453125] [gan loss: 1.097533, acc: 0.046875]\n",
            "2247: [discriminator loss: 0.6137277483940125, acc: 0.21875] [gan loss: 1.536926, acc: 0.000000]\n",
            "2248: [discriminator loss: 0.5629414916038513, acc: 0.453125] [gan loss: 1.133860, acc: 0.031250]\n",
            "2249: [discriminator loss: 0.5932266712188721, acc: 0.28125] [gan loss: 1.653083, acc: 0.000000]\n",
            "2250: [discriminator loss: 0.5241624712944031, acc: 0.46875] [gan loss: 1.234421, acc: 0.031250]\n",
            "2251: [discriminator loss: 0.6503043174743652, acc: 0.21875] [gan loss: 1.351890, acc: 0.015625]\n",
            "2252: [discriminator loss: 0.6180576086044312, acc: 0.328125] [gan loss: 1.544960, acc: 0.000000]\n",
            "2253: [discriminator loss: 0.5194875001907349, acc: 0.4765625] [gan loss: 1.244871, acc: 0.062500]\n",
            "2254: [discriminator loss: 0.5713920593261719, acc: 0.375] [gan loss: 1.510352, acc: 0.015625]\n",
            "2255: [discriminator loss: 0.5876281261444092, acc: 0.40625] [gan loss: 1.284149, acc: 0.000000]\n",
            "2256: [discriminator loss: 0.5303347110748291, acc: 0.3984375] [gan loss: 1.186833, acc: 0.015625]\n",
            "2257: [discriminator loss: 0.6143614053726196, acc: 0.3359375] [gan loss: 1.353216, acc: 0.000000]\n",
            "2258: [discriminator loss: 0.5769293904304504, acc: 0.3984375] [gan loss: 1.170030, acc: 0.031250]\n",
            "2259: [discriminator loss: 0.5815597772598267, acc: 0.2890625] [gan loss: 1.538578, acc: 0.000000]\n",
            "2260: [discriminator loss: 0.564269483089447, acc: 0.40625] [gan loss: 1.227988, acc: 0.015625]\n",
            "2261: [discriminator loss: 0.6398214101791382, acc: 0.28125] [gan loss: 1.392171, acc: 0.000000]\n",
            "2262: [discriminator loss: 0.5544613599777222, acc: 0.4375] [gan loss: 1.128933, acc: 0.031250]\n",
            "2263: [discriminator loss: 0.6210640668869019, acc: 0.296875] [gan loss: 1.427459, acc: 0.000000]\n",
            "2264: [discriminator loss: 0.6018875241279602, acc: 0.3828125] [gan loss: 1.437184, acc: 0.000000]\n",
            "2265: [discriminator loss: 0.5940964221954346, acc: 0.4375] [gan loss: 1.148905, acc: 0.015625]\n",
            "2266: [discriminator loss: 0.6012402772903442, acc: 0.2890625] [gan loss: 1.519046, acc: 0.000000]\n",
            "2267: [discriminator loss: 0.624870777130127, acc: 0.4609375] [gan loss: 0.847218, acc: 0.265625]\n",
            "2268: [discriminator loss: 0.6675787568092346, acc: 0.1171875] [gan loss: 1.732862, acc: 0.000000]\n",
            "2269: [discriminator loss: 0.601134181022644, acc: 0.5] [gan loss: 0.587486, acc: 0.703125]\n",
            "2270: [discriminator loss: 0.7452002763748169, acc: 0.0625] [gan loss: 1.575712, acc: 0.000000]\n",
            "2271: [discriminator loss: 0.5970619916915894, acc: 0.4921875] [gan loss: 0.997350, acc: 0.062500]\n",
            "2272: [discriminator loss: 0.6104832291603088, acc: 0.2890625] [gan loss: 1.270932, acc: 0.000000]\n",
            "2273: [discriminator loss: 0.5941441655158997, acc: 0.3671875] [gan loss: 1.080360, acc: 0.109375]\n",
            "2274: [discriminator loss: 0.6180969476699829, acc: 0.3203125] [gan loss: 1.476808, acc: 0.000000]\n",
            "2275: [discriminator loss: 0.5859279036521912, acc: 0.4609375] [gan loss: 1.019443, acc: 0.125000]\n",
            "2276: [discriminator loss: 0.6630250215530396, acc: 0.2265625] [gan loss: 1.580221, acc: 0.000000]\n",
            "2277: [discriminator loss: 0.5722613334655762, acc: 0.4140625] [gan loss: 1.242360, acc: 0.015625]\n",
            "2278: [discriminator loss: 0.6393622159957886, acc: 0.265625] [gan loss: 1.570963, acc: 0.000000]\n",
            "2279: [discriminator loss: 0.6077674627304077, acc: 0.453125] [gan loss: 1.004878, acc: 0.093750]\n",
            "2280: [discriminator loss: 0.6476311683654785, acc: 0.2109375] [gan loss: 1.779480, acc: 0.000000]\n",
            "2281: [discriminator loss: 0.5443707704544067, acc: 0.4921875] [gan loss: 1.039686, acc: 0.062500]\n",
            "2282: [discriminator loss: 0.6083470582962036, acc: 0.1953125] [gan loss: 1.630822, acc: 0.000000]\n",
            "2283: [discriminator loss: 0.5221599340438843, acc: 0.4765625] [gan loss: 0.974492, acc: 0.109375]\n",
            "2284: [discriminator loss: 0.6345880031585693, acc: 0.1484375] [gan loss: 1.692289, acc: 0.000000]\n",
            "2285: [discriminator loss: 0.585364818572998, acc: 0.34375] [gan loss: 1.369402, acc: 0.000000]\n",
            "2286: [discriminator loss: 0.5752902030944824, acc: 0.4453125] [gan loss: 1.086446, acc: 0.031250]\n",
            "2287: [discriminator loss: 0.594889760017395, acc: 0.3359375] [gan loss: 1.197908, acc: 0.015625]\n",
            "2288: [discriminator loss: 0.5867975950241089, acc: 0.3828125] [gan loss: 1.319890, acc: 0.000000]\n",
            "2289: [discriminator loss: 0.55119788646698, acc: 0.3828125] [gan loss: 1.508611, acc: 0.000000]\n",
            "2290: [discriminator loss: 0.5481388568878174, acc: 0.421875] [gan loss: 1.365172, acc: 0.015625]\n",
            "2291: [discriminator loss: 0.5580257177352905, acc: 0.3828125] [gan loss: 1.333557, acc: 0.000000]\n",
            "2292: [discriminator loss: 0.5358185768127441, acc: 0.40625] [gan loss: 1.516480, acc: 0.000000]\n",
            "2293: [discriminator loss: 0.5627263784408569, acc: 0.4296875] [gan loss: 1.086820, acc: 0.078125]\n",
            "2294: [discriminator loss: 0.624012291431427, acc: 0.203125] [gan loss: 1.898760, acc: 0.000000]\n",
            "2295: [discriminator loss: 0.5642275214195251, acc: 0.5] [gan loss: 0.894803, acc: 0.281250]\n",
            "2296: [discriminator loss: 0.6494227647781372, acc: 0.1875] [gan loss: 1.641187, acc: 0.000000]\n",
            "2297: [discriminator loss: 0.619260311126709, acc: 0.484375] [gan loss: 0.865872, acc: 0.296875]\n",
            "2298: [discriminator loss: 0.6396710872650146, acc: 0.1484375] [gan loss: 1.825753, acc: 0.000000]\n",
            "2299: [discriminator loss: 0.5748288631439209, acc: 0.5] [gan loss: 1.008534, acc: 0.031250]\n",
            "2300: [discriminator loss: 0.6760834455490112, acc: 0.125] [gan loss: 1.706050, acc: 0.000000]\n",
            "2301: [discriminator loss: 0.6277921199798584, acc: 0.4765625] [gan loss: 1.118875, acc: 0.046875]\n",
            "2302: [discriminator loss: 0.6195371150970459, acc: 0.2265625] [gan loss: 1.322458, acc: 0.000000]\n",
            "2303: [discriminator loss: 0.5620042085647583, acc: 0.4140625] [gan loss: 1.199493, acc: 0.031250]\n",
            "2304: [discriminator loss: 0.5710805654525757, acc: 0.359375] [gan loss: 1.368055, acc: 0.000000]\n",
            "2305: [discriminator loss: 0.5743833184242249, acc: 0.4140625] [gan loss: 1.259744, acc: 0.000000]\n",
            "2306: [discriminator loss: 0.6117172241210938, acc: 0.2890625] [gan loss: 1.504491, acc: 0.000000]\n",
            "2307: [discriminator loss: 0.5578667521476746, acc: 0.390625] [gan loss: 1.329549, acc: 0.000000]\n",
            "2308: [discriminator loss: 0.5472861528396606, acc: 0.421875] [gan loss: 1.273804, acc: 0.031250]\n",
            "2309: [discriminator loss: 0.6023741960525513, acc: 0.296875] [gan loss: 1.475310, acc: 0.000000]\n",
            "2310: [discriminator loss: 0.5868164896965027, acc: 0.4140625] [gan loss: 1.004662, acc: 0.140625]\n",
            "2311: [discriminator loss: 0.6724578142166138, acc: 0.15625] [gan loss: 1.745025, acc: 0.000000]\n",
            "2312: [discriminator loss: 0.6148428320884705, acc: 0.4453125] [gan loss: 0.909825, acc: 0.203125]\n",
            "2313: [discriminator loss: 0.7110069990158081, acc: 0.109375] [gan loss: 1.702849, acc: 0.000000]\n",
            "2314: [discriminator loss: 0.5720827579498291, acc: 0.4765625] [gan loss: 0.887367, acc: 0.218750]\n",
            "2315: [discriminator loss: 0.6717003583908081, acc: 0.1796875] [gan loss: 1.300135, acc: 0.000000]\n",
            "2316: [discriminator loss: 0.5926233530044556, acc: 0.4296875] [gan loss: 1.076263, acc: 0.093750]\n",
            "2317: [discriminator loss: 0.6262335777282715, acc: 0.296875] [gan loss: 1.387779, acc: 0.000000]\n",
            "2318: [discriminator loss: 0.617957592010498, acc: 0.4140625] [gan loss: 1.041054, acc: 0.062500]\n",
            "2319: [discriminator loss: 0.6199730038642883, acc: 0.2734375] [gan loss: 1.279284, acc: 0.000000]\n",
            "2320: [discriminator loss: 0.6375260353088379, acc: 0.34375] [gan loss: 1.290738, acc: 0.000000]\n",
            "2321: [discriminator loss: 0.5802455544471741, acc: 0.3828125] [gan loss: 1.316188, acc: 0.000000]\n",
            "2322: [discriminator loss: 0.5921175479888916, acc: 0.375] [gan loss: 1.317077, acc: 0.000000]\n",
            "2323: [discriminator loss: 0.6220824718475342, acc: 0.2578125] [gan loss: 1.578142, acc: 0.000000]\n",
            "2324: [discriminator loss: 0.6153528094291687, acc: 0.46875] [gan loss: 0.991380, acc: 0.125000]\n",
            "2325: [discriminator loss: 0.668567419052124, acc: 0.1875] [gan loss: 1.435092, acc: 0.000000]\n",
            "2326: [discriminator loss: 0.6155184507369995, acc: 0.421875] [gan loss: 0.976428, acc: 0.093750]\n",
            "2327: [discriminator loss: 0.635279655456543, acc: 0.203125] [gan loss: 1.680237, acc: 0.000000]\n",
            "2328: [discriminator loss: 0.5914375185966492, acc: 0.4453125] [gan loss: 1.005717, acc: 0.062500]\n",
            "2329: [discriminator loss: 0.6544342041015625, acc: 0.21875] [gan loss: 1.413679, acc: 0.000000]\n",
            "2330: [discriminator loss: 0.6164318919181824, acc: 0.4609375] [gan loss: 1.000546, acc: 0.093750]\n",
            "2331: [discriminator loss: 0.6110641956329346, acc: 0.2578125] [gan loss: 1.415542, acc: 0.015625]\n",
            "2332: [discriminator loss: 0.586848258972168, acc: 0.4921875] [gan loss: 0.810504, acc: 0.312500]\n",
            "2333: [discriminator loss: 0.7076108455657959, acc: 0.109375] [gan loss: 1.580936, acc: 0.000000]\n",
            "2334: [discriminator loss: 0.631096363067627, acc: 0.4609375] [gan loss: 1.039119, acc: 0.109375]\n",
            "2335: [discriminator loss: 0.6061736345291138, acc: 0.2578125] [gan loss: 1.386748, acc: 0.000000]\n",
            "2336: [discriminator loss: 0.5870146155357361, acc: 0.4609375] [gan loss: 1.131300, acc: 0.031250]\n",
            "2337: [discriminator loss: 0.6031359434127808, acc: 0.2890625] [gan loss: 1.430048, acc: 0.000000]\n",
            "2338: [discriminator loss: 0.5627219080924988, acc: 0.46875] [gan loss: 1.157151, acc: 0.000000]\n",
            "2339: [discriminator loss: 0.5788655281066895, acc: 0.3046875] [gan loss: 1.371765, acc: 0.000000]\n",
            "2340: [discriminator loss: 0.5964242219924927, acc: 0.4453125] [gan loss: 1.083468, acc: 0.062500]\n",
            "2341: [discriminator loss: 0.6219086647033691, acc: 0.2421875] [gan loss: 1.774837, acc: 0.000000]\n",
            "2342: [discriminator loss: 0.5837123990058899, acc: 0.4609375] [gan loss: 1.112427, acc: 0.062500]\n",
            "2343: [discriminator loss: 0.613852322101593, acc: 0.28125] [gan loss: 1.526590, acc: 0.000000]\n",
            "2344: [discriminator loss: 0.5839357972145081, acc: 0.4453125] [gan loss: 1.025379, acc: 0.078125]\n",
            "2345: [discriminator loss: 0.6183903813362122, acc: 0.21875] [gan loss: 1.539970, acc: 0.000000]\n",
            "2346: [discriminator loss: 0.5818963050842285, acc: 0.453125] [gan loss: 0.912255, acc: 0.234375]\n",
            "2347: [discriminator loss: 0.6924567818641663, acc: 0.1484375] [gan loss: 1.752919, acc: 0.000000]\n",
            "2348: [discriminator loss: 0.6225391626358032, acc: 0.484375] [gan loss: 0.840887, acc: 0.328125]\n",
            "2349: [discriminator loss: 0.7126526236534119, acc: 0.078125] [gan loss: 1.624531, acc: 0.000000]\n",
            "2350: [discriminator loss: 0.5905378460884094, acc: 0.484375] [gan loss: 0.970259, acc: 0.187500]\n",
            "2351: [discriminator loss: 0.6539280414581299, acc: 0.171875] [gan loss: 1.361321, acc: 0.000000]\n",
            "2352: [discriminator loss: 0.5785265564918518, acc: 0.4453125] [gan loss: 1.127989, acc: 0.031250]\n",
            "2353: [discriminator loss: 0.6180314421653748, acc: 0.3125] [gan loss: 0.978759, acc: 0.187500]\n",
            "2354: [discriminator loss: 0.648730993270874, acc: 0.265625] [gan loss: 1.303658, acc: 0.015625]\n",
            "2355: [discriminator loss: 0.6089215278625488, acc: 0.3671875] [gan loss: 1.002439, acc: 0.140625]\n",
            "2356: [discriminator loss: 0.6202268004417419, acc: 0.265625] [gan loss: 1.258584, acc: 0.000000]\n",
            "2357: [discriminator loss: 0.5846933126449585, acc: 0.4375] [gan loss: 0.955562, acc: 0.203125]\n",
            "2358: [discriminator loss: 0.6799983978271484, acc: 0.21875] [gan loss: 1.344195, acc: 0.000000]\n",
            "2359: [discriminator loss: 0.6093586683273315, acc: 0.46875] [gan loss: 0.842314, acc: 0.250000]\n",
            "2360: [discriminator loss: 0.6924213171005249, acc: 0.078125] [gan loss: 1.480594, acc: 0.000000]\n",
            "2361: [discriminator loss: 0.6019883751869202, acc: 0.4765625] [gan loss: 0.889452, acc: 0.156250]\n",
            "2362: [discriminator loss: 0.6552973985671997, acc: 0.09375] [gan loss: 1.553555, acc: 0.000000]\n",
            "2363: [discriminator loss: 0.58956378698349, acc: 0.453125] [gan loss: 0.974881, acc: 0.078125]\n",
            "2364: [discriminator loss: 0.6466807723045349, acc: 0.21875] [gan loss: 1.328329, acc: 0.000000]\n",
            "2365: [discriminator loss: 0.5840947031974792, acc: 0.4609375] [gan loss: 0.955136, acc: 0.171875]\n",
            "2366: [discriminator loss: 0.6229986548423767, acc: 0.2890625] [gan loss: 1.238015, acc: 0.000000]\n",
            "2367: [discriminator loss: 0.5699948668479919, acc: 0.453125] [gan loss: 0.809257, acc: 0.265625]\n",
            "2368: [discriminator loss: 0.6044586896896362, acc: 0.265625] [gan loss: 1.148811, acc: 0.078125]\n",
            "2369: [discriminator loss: 0.668222188949585, acc: 0.328125] [gan loss: 1.183159, acc: 0.000000]\n",
            "2370: [discriminator loss: 0.5771401524543762, acc: 0.40625] [gan loss: 1.105184, acc: 0.078125]\n",
            "2371: [discriminator loss: 0.6157596111297607, acc: 0.34375] [gan loss: 1.291880, acc: 0.000000]\n",
            "2372: [discriminator loss: 0.5865532159805298, acc: 0.421875] [gan loss: 1.136172, acc: 0.015625]\n",
            "2373: [discriminator loss: 0.6184177398681641, acc: 0.3125] [gan loss: 1.215005, acc: 0.000000]\n",
            "2374: [discriminator loss: 0.5770545601844788, acc: 0.4296875] [gan loss: 1.059852, acc: 0.078125]\n",
            "2375: [discriminator loss: 0.6226995587348938, acc: 0.2578125] [gan loss: 1.484136, acc: 0.000000]\n",
            "2376: [discriminator loss: 0.5531501173973083, acc: 0.484375] [gan loss: 0.746572, acc: 0.406250]\n",
            "2377: [discriminator loss: 0.7502362728118896, acc: 0.09375] [gan loss: 1.666311, acc: 0.000000]\n",
            "2378: [discriminator loss: 0.6412633657455444, acc: 0.5] [gan loss: 0.649393, acc: 0.609375]\n",
            "2379: [discriminator loss: 0.7948751449584961, acc: 0.0234375] [gan loss: 1.458431, acc: 0.000000]\n",
            "2380: [discriminator loss: 0.6016156673431396, acc: 0.4765625] [gan loss: 0.914326, acc: 0.156250]\n",
            "2381: [discriminator loss: 0.6536113619804382, acc: 0.140625] [gan loss: 1.290789, acc: 0.000000]\n",
            "2382: [discriminator loss: 0.6009084582328796, acc: 0.40625] [gan loss: 1.099921, acc: 0.015625]\n",
            "2383: [discriminator loss: 0.6106311082839966, acc: 0.3359375] [gan loss: 1.053546, acc: 0.031250]\n",
            "2384: [discriminator loss: 0.6099051833152771, acc: 0.328125] [gan loss: 1.163118, acc: 0.000000]\n",
            "2385: [discriminator loss: 0.5965882539749146, acc: 0.3671875] [gan loss: 1.145827, acc: 0.015625]\n",
            "2386: [discriminator loss: 0.6082315444946289, acc: 0.421875] [gan loss: 0.996495, acc: 0.093750]\n",
            "2387: [discriminator loss: 0.6122621297836304, acc: 0.2578125] [gan loss: 1.335969, acc: 0.000000]\n",
            "2388: [discriminator loss: 0.5480543375015259, acc: 0.484375] [gan loss: 0.906286, acc: 0.171875]\n",
            "2389: [discriminator loss: 0.6779789924621582, acc: 0.109375] [gan loss: 1.657766, acc: 0.000000]\n",
            "2390: [discriminator loss: 0.5885895490646362, acc: 0.4921875] [gan loss: 0.737355, acc: 0.359375]\n",
            "2391: [discriminator loss: 0.6574320793151855, acc: 0.078125] [gan loss: 1.403348, acc: 0.000000]\n",
            "2392: [discriminator loss: 0.5874121189117432, acc: 0.4609375] [gan loss: 1.000153, acc: 0.062500]\n",
            "2393: [discriminator loss: 0.6118297576904297, acc: 0.2421875] [gan loss: 1.248109, acc: 0.015625]\n",
            "2394: [discriminator loss: 0.5503737330436707, acc: 0.4609375] [gan loss: 1.016847, acc: 0.015625]\n",
            "2395: [discriminator loss: 0.6115404367446899, acc: 0.234375] [gan loss: 1.355570, acc: 0.000000]\n",
            "2396: [discriminator loss: 0.5527976155281067, acc: 0.4375] [gan loss: 1.039724, acc: 0.062500]\n",
            "2397: [discriminator loss: 0.6149870157241821, acc: 0.2734375] [gan loss: 1.402146, acc: 0.000000]\n",
            "2398: [discriminator loss: 0.5470885634422302, acc: 0.4453125] [gan loss: 1.049049, acc: 0.078125]\n",
            "2399: [discriminator loss: 0.6011204719543457, acc: 0.3203125] [gan loss: 1.263026, acc: 0.000000]\n",
            "2400: [discriminator loss: 0.5560368895530701, acc: 0.4296875] [gan loss: 1.088697, acc: 0.000000]\n",
            "2401: [discriminator loss: 0.5658519268035889, acc: 0.359375] [gan loss: 1.226699, acc: 0.000000]\n",
            "2402: [discriminator loss: 0.5913102626800537, acc: 0.3671875] [gan loss: 1.430231, acc: 0.000000]\n",
            "2403: [discriminator loss: 0.6242939233779907, acc: 0.34375] [gan loss: 1.195213, acc: 0.000000]\n",
            "2404: [discriminator loss: 0.6039341688156128, acc: 0.40625] [gan loss: 1.266728, acc: 0.015625]\n",
            "2405: [discriminator loss: 0.5809081196784973, acc: 0.3828125] [gan loss: 1.239577, acc: 0.031250]\n",
            "2406: [discriminator loss: 0.5676650404930115, acc: 0.4453125] [gan loss: 1.005398, acc: 0.093750]\n",
            "2407: [discriminator loss: 0.63299161195755, acc: 0.3125] [gan loss: 1.244068, acc: 0.015625]\n",
            "2408: [discriminator loss: 0.5631693005561829, acc: 0.421875] [gan loss: 1.270296, acc: 0.000000]\n",
            "2409: [discriminator loss: 0.5765494108200073, acc: 0.4453125] [gan loss: 1.111073, acc: 0.062500]\n",
            "2410: [discriminator loss: 0.6083289384841919, acc: 0.2890625] [gan loss: 1.537750, acc: 0.000000]\n",
            "2411: [discriminator loss: 0.5728641152381897, acc: 0.4921875] [gan loss: 0.790874, acc: 0.296875]\n",
            "2412: [discriminator loss: 0.6409116983413696, acc: 0.140625] [gan loss: 1.708683, acc: 0.000000]\n",
            "2413: [discriminator loss: 0.5683467984199524, acc: 0.5] [gan loss: 0.891450, acc: 0.234375]\n",
            "2414: [discriminator loss: 0.6037843823432922, acc: 0.2265625] [gan loss: 1.432622, acc: 0.000000]\n",
            "2415: [discriminator loss: 0.5629560947418213, acc: 0.4765625] [gan loss: 0.959783, acc: 0.062500]\n",
            "2416: [discriminator loss: 0.5972464680671692, acc: 0.328125] [gan loss: 1.346586, acc: 0.000000]\n",
            "2417: [discriminator loss: 0.5663743019104004, acc: 0.4140625] [gan loss: 1.022304, acc: 0.187500]\n",
            "2418: [discriminator loss: 0.5865969061851501, acc: 0.28125] [gan loss: 1.388364, acc: 0.000000]\n",
            "2419: [discriminator loss: 0.6281559467315674, acc: 0.3828125] [gan loss: 1.048939, acc: 0.125000]\n",
            "2420: [discriminator loss: 0.5939663648605347, acc: 0.3671875] [gan loss: 1.199835, acc: 0.031250]\n",
            "2421: [discriminator loss: 0.5806875228881836, acc: 0.3828125] [gan loss: 1.002590, acc: 0.140625]\n",
            "2422: [discriminator loss: 0.6005733013153076, acc: 0.3359375] [gan loss: 1.360952, acc: 0.031250]\n",
            "2423: [discriminator loss: 0.5915103554725647, acc: 0.421875] [gan loss: 1.009595, acc: 0.125000]\n",
            "2424: [discriminator loss: 0.5678490996360779, acc: 0.3125] [gan loss: 1.425193, acc: 0.000000]\n",
            "2425: [discriminator loss: 0.5331523418426514, acc: 0.46875] [gan loss: 1.069285, acc: 0.125000]\n",
            "2426: [discriminator loss: 0.6041386723518372, acc: 0.28125] [gan loss: 1.444539, acc: 0.000000]\n",
            "2427: [discriminator loss: 0.6264115571975708, acc: 0.4765625] [gan loss: 0.677638, acc: 0.593750]\n",
            "2428: [discriminator loss: 0.7069123983383179, acc: 0.109375] [gan loss: 1.646523, acc: 0.000000]\n",
            "2429: [discriminator loss: 0.6202855706214905, acc: 0.4609375] [gan loss: 0.757917, acc: 0.437500]\n",
            "2430: [discriminator loss: 0.6508912444114685, acc: 0.15625] [gan loss: 1.405912, acc: 0.000000]\n",
            "2431: [discriminator loss: 0.6196666955947876, acc: 0.46875] [gan loss: 0.838156, acc: 0.375000]\n",
            "2432: [discriminator loss: 0.5939890146255493, acc: 0.28125] [gan loss: 1.110833, acc: 0.062500]\n",
            "2433: [discriminator loss: 0.6100184321403503, acc: 0.3984375] [gan loss: 1.062843, acc: 0.015625]\n",
            "2434: [discriminator loss: 0.5971885919570923, acc: 0.375] [gan loss: 1.058014, acc: 0.046875]\n",
            "2435: [discriminator loss: 0.6156721711158752, acc: 0.328125] [gan loss: 1.114583, acc: 0.031250]\n",
            "2436: [discriminator loss: 0.6230465173721313, acc: 0.40625] [gan loss: 0.905690, acc: 0.140625]\n",
            "2437: [discriminator loss: 0.577106237411499, acc: 0.3671875] [gan loss: 1.105883, acc: 0.078125]\n",
            "2438: [discriminator loss: 0.6181586384773254, acc: 0.3359375] [gan loss: 0.970064, acc: 0.093750]\n",
            "2439: [discriminator loss: 0.6387123465538025, acc: 0.2734375] [gan loss: 1.207740, acc: 0.000000]\n",
            "2440: [discriminator loss: 0.6160852909088135, acc: 0.375] [gan loss: 0.914577, acc: 0.218750]\n",
            "2441: [discriminator loss: 0.6121991872787476, acc: 0.265625] [gan loss: 1.369228, acc: 0.000000]\n",
            "2442: [discriminator loss: 0.6206939816474915, acc: 0.46875] [gan loss: 0.828257, acc: 0.281250]\n",
            "2443: [discriminator loss: 0.641923189163208, acc: 0.171875] [gan loss: 1.508149, acc: 0.000000]\n",
            "2444: [discriminator loss: 0.5721657276153564, acc: 0.5] [gan loss: 0.913208, acc: 0.171875]\n",
            "2445: [discriminator loss: 0.657183051109314, acc: 0.171875] [gan loss: 1.571722, acc: 0.000000]\n",
            "2446: [discriminator loss: 0.5988240838050842, acc: 0.46875] [gan loss: 0.765819, acc: 0.375000]\n",
            "2447: [discriminator loss: 0.6651360988616943, acc: 0.140625] [gan loss: 1.514244, acc: 0.000000]\n",
            "2448: [discriminator loss: 0.6050988435745239, acc: 0.4609375] [gan loss: 0.973325, acc: 0.046875]\n",
            "2449: [discriminator loss: 0.643570065498352, acc: 0.1796875] [gan loss: 1.480336, acc: 0.015625]\n",
            "2450: [discriminator loss: 0.6139331459999084, acc: 0.46875] [gan loss: 0.790830, acc: 0.281250]\n",
            "2451: [discriminator loss: 0.6873453855514526, acc: 0.109375] [gan loss: 1.535264, acc: 0.000000]\n",
            "2452: [discriminator loss: 0.5741187930107117, acc: 0.484375] [gan loss: 0.936256, acc: 0.156250]\n",
            "2453: [discriminator loss: 0.6358547210693359, acc: 0.2265625] [gan loss: 1.464585, acc: 0.000000]\n",
            "2454: [discriminator loss: 0.5381399989128113, acc: 0.484375] [gan loss: 0.996680, acc: 0.046875]\n",
            "2455: [discriminator loss: 0.6007336378097534, acc: 0.203125] [gan loss: 1.554789, acc: 0.000000]\n",
            "2456: [discriminator loss: 0.597422182559967, acc: 0.4921875] [gan loss: 0.908268, acc: 0.265625]\n",
            "2457: [discriminator loss: 0.6159149408340454, acc: 0.171875] [gan loss: 1.589635, acc: 0.000000]\n",
            "2458: [discriminator loss: 0.5686728358268738, acc: 0.4609375] [gan loss: 1.079977, acc: 0.062500]\n",
            "2459: [discriminator loss: 0.576723039150238, acc: 0.3359375] [gan loss: 1.486083, acc: 0.000000]\n",
            "2460: [discriminator loss: 0.5871950387954712, acc: 0.3515625] [gan loss: 1.341019, acc: 0.000000]\n",
            "2461: [discriminator loss: 0.5479597449302673, acc: 0.3828125] [gan loss: 1.344925, acc: 0.000000]\n",
            "2462: [discriminator loss: 0.5582612752914429, acc: 0.3828125] [gan loss: 1.248927, acc: 0.031250]\n",
            "2463: [discriminator loss: 0.550705075263977, acc: 0.3828125] [gan loss: 1.250582, acc: 0.015625]\n",
            "2464: [discriminator loss: 0.544244647026062, acc: 0.4296875] [gan loss: 1.193031, acc: 0.000000]\n",
            "2465: [discriminator loss: 0.5677853226661682, acc: 0.3125] [gan loss: 1.592278, acc: 0.000000]\n",
            "2466: [discriminator loss: 0.6104457378387451, acc: 0.4609375] [gan loss: 1.273326, acc: 0.015625]\n",
            "2467: [discriminator loss: 0.5719307065010071, acc: 0.3515625] [gan loss: 1.676904, acc: 0.000000]\n",
            "2468: [discriminator loss: 0.5655637383460999, acc: 0.421875] [gan loss: 1.323062, acc: 0.015625]\n",
            "2469: [discriminator loss: 0.5858181715011597, acc: 0.3203125] [gan loss: 1.537982, acc: 0.000000]\n",
            "2470: [discriminator loss: 0.559886634349823, acc: 0.46875] [gan loss: 1.135225, acc: 0.000000]\n",
            "2471: [discriminator loss: 0.617737352848053, acc: 0.21875] [gan loss: 1.796145, acc: 0.000000]\n",
            "2472: [discriminator loss: 0.5719563961029053, acc: 0.4765625] [gan loss: 0.956596, acc: 0.171875]\n",
            "2473: [discriminator loss: 0.6418391466140747, acc: 0.1640625] [gan loss: 1.581412, acc: 0.000000]\n",
            "2474: [discriminator loss: 0.5453948378562927, acc: 0.4921875] [gan loss: 1.024244, acc: 0.125000]\n",
            "2475: [discriminator loss: 0.6078465580940247, acc: 0.21875] [gan loss: 1.564462, acc: 0.000000]\n",
            "2476: [discriminator loss: 0.5481271743774414, acc: 0.4921875] [gan loss: 1.106704, acc: 0.046875]\n",
            "2477: [discriminator loss: 0.6636927723884583, acc: 0.125] [gan loss: 1.877399, acc: 0.000000]\n",
            "2478: [discriminator loss: 0.574474036693573, acc: 0.4921875] [gan loss: 0.916282, acc: 0.281250]\n",
            "2479: [discriminator loss: 0.7009516358375549, acc: 0.1171875] [gan loss: 1.741369, acc: 0.000000]\n",
            "2480: [discriminator loss: 0.5858137607574463, acc: 0.4453125] [gan loss: 1.058471, acc: 0.062500]\n",
            "2481: [discriminator loss: 0.6443684101104736, acc: 0.21875] [gan loss: 1.385813, acc: 0.000000]\n",
            "2482: [discriminator loss: 0.5630568265914917, acc: 0.46875] [gan loss: 1.124088, acc: 0.015625]\n",
            "2483: [discriminator loss: 0.6015704274177551, acc: 0.3046875] [gan loss: 1.469295, acc: 0.000000]\n",
            "2484: [discriminator loss: 0.5735648274421692, acc: 0.4296875] [gan loss: 1.118339, acc: 0.000000]\n",
            "2485: [discriminator loss: 0.6298047304153442, acc: 0.28125] [gan loss: 1.514013, acc: 0.000000]\n",
            "2486: [discriminator loss: 0.5717349052429199, acc: 0.4453125] [gan loss: 1.248436, acc: 0.015625]\n",
            "2487: [discriminator loss: 0.5818927884101868, acc: 0.40625] [gan loss: 1.216819, acc: 0.031250]\n",
            "2488: [discriminator loss: 0.6492782235145569, acc: 0.2421875] [gan loss: 1.633257, acc: 0.000000]\n",
            "2489: [discriminator loss: 0.5466349124908447, acc: 0.4453125] [gan loss: 1.218637, acc: 0.015625]\n",
            "2490: [discriminator loss: 0.5869585275650024, acc: 0.296875] [gan loss: 1.682632, acc: 0.000000]\n",
            "2491: [discriminator loss: 0.5607965588569641, acc: 0.4765625] [gan loss: 0.919004, acc: 0.234375]\n",
            "2492: [discriminator loss: 0.6276403665542603, acc: 0.1953125] [gan loss: 1.817063, acc: 0.000000]\n",
            "2493: [discriminator loss: 0.5832654237747192, acc: 0.4921875] [gan loss: 0.953089, acc: 0.156250]\n",
            "2494: [discriminator loss: 0.669813871383667, acc: 0.1875] [gan loss: 1.766702, acc: 0.000000]\n",
            "2495: [discriminator loss: 0.6008274555206299, acc: 0.4609375] [gan loss: 0.920648, acc: 0.312500]\n",
            "2496: [discriminator loss: 0.6710519790649414, acc: 0.1171875] [gan loss: 1.781961, acc: 0.000000]\n",
            "2497: [discriminator loss: 0.5685926079750061, acc: 0.484375] [gan loss: 1.020544, acc: 0.093750]\n",
            "2498: [discriminator loss: 0.6252697110176086, acc: 0.2109375] [gan loss: 1.560701, acc: 0.000000]\n",
            "2499: [discriminator loss: 0.5632807016372681, acc: 0.484375] [gan loss: 1.082654, acc: 0.062500]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2500: [discriminator loss: 0.5801247358322144, acc: 0.2421875] [gan loss: 1.661974, acc: 0.000000]\n",
            "2501: [discriminator loss: 0.5678980350494385, acc: 0.4609375] [gan loss: 0.969489, acc: 0.093750]\n",
            "2502: [discriminator loss: 0.6234380006790161, acc: 0.203125] [gan loss: 1.482856, acc: 0.000000]\n",
            "2503: [discriminator loss: 0.5670226216316223, acc: 0.46875] [gan loss: 1.299392, acc: 0.000000]\n",
            "2504: [discriminator loss: 0.5922294855117798, acc: 0.3671875] [gan loss: 1.386453, acc: 0.015625]\n",
            "2505: [discriminator loss: 0.583215594291687, acc: 0.359375] [gan loss: 1.471699, acc: 0.000000]\n",
            "2506: [discriminator loss: 0.590178906917572, acc: 0.4375] [gan loss: 1.205977, acc: 0.015625]\n",
            "2507: [discriminator loss: 0.5876519680023193, acc: 0.34375] [gan loss: 1.440279, acc: 0.000000]\n",
            "2508: [discriminator loss: 0.5829926133155823, acc: 0.4140625] [gan loss: 1.033936, acc: 0.125000]\n",
            "2509: [discriminator loss: 0.6246169805526733, acc: 0.234375] [gan loss: 1.704981, acc: 0.000000]\n",
            "2510: [discriminator loss: 0.5711677670478821, acc: 0.484375] [gan loss: 0.994130, acc: 0.125000]\n",
            "2511: [discriminator loss: 0.6106799840927124, acc: 0.2578125] [gan loss: 1.702825, acc: 0.000000]\n",
            "2512: [discriminator loss: 0.5741212368011475, acc: 0.453125] [gan loss: 1.192645, acc: 0.046875]\n",
            "2513: [discriminator loss: 0.5655495524406433, acc: 0.3125] [gan loss: 1.314658, acc: 0.015625]\n",
            "2514: [discriminator loss: 0.6278024911880493, acc: 0.2734375] [gan loss: 1.574618, acc: 0.000000]\n",
            "2515: [discriminator loss: 0.5658087730407715, acc: 0.4609375] [gan loss: 1.020756, acc: 0.109375]\n",
            "2516: [discriminator loss: 0.6361309289932251, acc: 0.2109375] [gan loss: 1.636709, acc: 0.000000]\n",
            "2517: [discriminator loss: 0.562120795249939, acc: 0.484375] [gan loss: 1.092496, acc: 0.125000]\n",
            "2518: [discriminator loss: 0.6365756988525391, acc: 0.171875] [gan loss: 1.900507, acc: 0.000000]\n",
            "2519: [discriminator loss: 0.5733094215393066, acc: 0.4609375] [gan loss: 1.032139, acc: 0.093750]\n",
            "2520: [discriminator loss: 0.5622876882553101, acc: 0.3046875] [gan loss: 1.339461, acc: 0.015625]\n",
            "2521: [discriminator loss: 0.5733127593994141, acc: 0.4296875] [gan loss: 1.325610, acc: 0.015625]\n",
            "2522: [discriminator loss: 0.5848705768585205, acc: 0.4296875] [gan loss: 1.227645, acc: 0.031250]\n",
            "2523: [discriminator loss: 0.6047630310058594, acc: 0.3203125] [gan loss: 1.336436, acc: 0.000000]\n",
            "2524: [discriminator loss: 0.5879576206207275, acc: 0.4296875] [gan loss: 1.408132, acc: 0.000000]\n",
            "2525: [discriminator loss: 0.5766685009002686, acc: 0.40625] [gan loss: 1.353788, acc: 0.000000]\n",
            "2526: [discriminator loss: 0.6310244202613831, acc: 0.34375] [gan loss: 1.445236, acc: 0.000000]\n",
            "2527: [discriminator loss: 0.5731658935546875, acc: 0.4140625] [gan loss: 1.384359, acc: 0.015625]\n",
            "2528: [discriminator loss: 0.5832556486129761, acc: 0.3671875] [gan loss: 1.582119, acc: 0.000000]\n",
            "2529: [discriminator loss: 0.561710000038147, acc: 0.4609375] [gan loss: 1.104304, acc: 0.125000]\n",
            "2530: [discriminator loss: 0.6602718830108643, acc: 0.2109375] [gan loss: 1.874063, acc: 0.000000]\n",
            "2531: [discriminator loss: 0.5801571011543274, acc: 0.4921875] [gan loss: 0.769664, acc: 0.421875]\n",
            "2532: [discriminator loss: 0.6180123090744019, acc: 0.1953125] [gan loss: 1.481725, acc: 0.000000]\n",
            "2533: [discriminator loss: 0.5725417733192444, acc: 0.4296875] [gan loss: 1.325063, acc: 0.031250]\n",
            "2534: [discriminator loss: 0.5939383506774902, acc: 0.3828125] [gan loss: 1.288355, acc: 0.031250]\n",
            "2535: [discriminator loss: 0.591404914855957, acc: 0.3671875] [gan loss: 1.818531, acc: 0.000000]\n",
            "2536: [discriminator loss: 0.5336964130401611, acc: 0.4765625] [gan loss: 1.112731, acc: 0.062500]\n",
            "2537: [discriminator loss: 0.62891685962677, acc: 0.2421875] [gan loss: 1.680628, acc: 0.015625]\n",
            "2538: [discriminator loss: 0.5695414543151855, acc: 0.453125] [gan loss: 1.034523, acc: 0.031250]\n",
            "2539: [discriminator loss: 0.6096798777580261, acc: 0.234375] [gan loss: 1.827744, acc: 0.000000]\n",
            "2540: [discriminator loss: 0.571285605430603, acc: 0.4921875] [gan loss: 0.994448, acc: 0.109375]\n",
            "2541: [discriminator loss: 0.6281052827835083, acc: 0.1484375] [gan loss: 1.776679, acc: 0.000000]\n",
            "2542: [discriminator loss: 0.5586361289024353, acc: 0.4921875] [gan loss: 0.982602, acc: 0.125000]\n",
            "2543: [discriminator loss: 0.6096604466438293, acc: 0.171875] [gan loss: 1.945628, acc: 0.000000]\n",
            "2544: [discriminator loss: 0.5502468347549438, acc: 0.4765625] [gan loss: 1.014587, acc: 0.093750]\n",
            "2545: [discriminator loss: 0.6047481298446655, acc: 0.2421875] [gan loss: 1.525827, acc: 0.000000]\n",
            "2546: [discriminator loss: 0.551586389541626, acc: 0.4453125] [gan loss: 1.293641, acc: 0.031250]\n",
            "2547: [discriminator loss: 0.5385451912879944, acc: 0.421875] [gan loss: 1.131969, acc: 0.109375]\n",
            "2548: [discriminator loss: 0.5741729736328125, acc: 0.3125] [gan loss: 1.634897, acc: 0.000000]\n",
            "2549: [discriminator loss: 0.5427266955375671, acc: 0.46875] [gan loss: 1.296423, acc: 0.015625]\n",
            "2550: [discriminator loss: 0.5418626070022583, acc: 0.3828125] [gan loss: 1.471054, acc: 0.015625]\n",
            "2551: [discriminator loss: 0.5757908225059509, acc: 0.4140625] [gan loss: 1.101480, acc: 0.109375]\n",
            "2552: [discriminator loss: 0.5941354036331177, acc: 0.25] [gan loss: 1.627056, acc: 0.015625]\n",
            "2553: [discriminator loss: 0.5823728442192078, acc: 0.484375] [gan loss: 0.844916, acc: 0.328125]\n",
            "2554: [discriminator loss: 0.709422767162323, acc: 0.1328125] [gan loss: 1.986958, acc: 0.000000]\n",
            "2555: [discriminator loss: 0.5897241830825806, acc: 0.4921875] [gan loss: 0.803341, acc: 0.359375]\n",
            "2556: [discriminator loss: 0.6959238052368164, acc: 0.0703125] [gan loss: 1.790401, acc: 0.000000]\n",
            "2557: [discriminator loss: 0.5557851791381836, acc: 0.4921875] [gan loss: 1.017883, acc: 0.078125]\n",
            "2558: [discriminator loss: 0.6246850490570068, acc: 0.28125] [gan loss: 1.461968, acc: 0.015625]\n",
            "2559: [discriminator loss: 0.565773606300354, acc: 0.4296875] [gan loss: 1.206097, acc: 0.000000]\n",
            "2560: [discriminator loss: 0.5780137777328491, acc: 0.3359375] [gan loss: 1.511089, acc: 0.000000]\n",
            "2561: [discriminator loss: 0.568751871585846, acc: 0.390625] [gan loss: 1.345173, acc: 0.015625]\n",
            "2562: [discriminator loss: 0.5460793972015381, acc: 0.390625] [gan loss: 1.332589, acc: 0.015625]\n",
            "2563: [discriminator loss: 0.5662091970443726, acc: 0.3828125] [gan loss: 1.484585, acc: 0.000000]\n",
            "2564: [discriminator loss: 0.578494131565094, acc: 0.4140625] [gan loss: 1.242736, acc: 0.031250]\n",
            "2565: [discriminator loss: 0.5947908759117126, acc: 0.3203125] [gan loss: 1.511879, acc: 0.000000]\n",
            "2566: [discriminator loss: 0.5576692819595337, acc: 0.421875] [gan loss: 1.365905, acc: 0.015625]\n",
            "2567: [discriminator loss: 0.5481563806533813, acc: 0.40625] [gan loss: 1.602549, acc: 0.000000]\n",
            "2568: [discriminator loss: 0.5891706943511963, acc: 0.40625] [gan loss: 1.121923, acc: 0.062500]\n",
            "2569: [discriminator loss: 0.6243308782577515, acc: 0.21875] [gan loss: 1.857520, acc: 0.000000]\n",
            "2570: [discriminator loss: 0.5946696400642395, acc: 0.5] [gan loss: 0.858720, acc: 0.281250]\n",
            "2571: [discriminator loss: 0.6867527961730957, acc: 0.0703125] [gan loss: 2.126950, acc: 0.000000]\n",
            "2572: [discriminator loss: 0.5837199687957764, acc: 0.484375] [gan loss: 0.883523, acc: 0.203125]\n",
            "2573: [discriminator loss: 0.6739377379417419, acc: 0.15625] [gan loss: 1.641197, acc: 0.000000]\n",
            "2574: [discriminator loss: 0.586257815361023, acc: 0.4921875] [gan loss: 1.039367, acc: 0.093750]\n",
            "2575: [discriminator loss: 0.5897294282913208, acc: 0.2578125] [gan loss: 1.551073, acc: 0.000000]\n",
            "2576: [discriminator loss: 0.5520925521850586, acc: 0.4765625] [gan loss: 1.162593, acc: 0.046875]\n",
            "2577: [discriminator loss: 0.5792971849441528, acc: 0.3203125] [gan loss: 1.470092, acc: 0.000000]\n",
            "2578: [discriminator loss: 0.5487679243087769, acc: 0.4375] [gan loss: 1.113574, acc: 0.062500]\n",
            "2579: [discriminator loss: 0.6229628324508667, acc: 0.25] [gan loss: 1.586275, acc: 0.015625]\n",
            "2580: [discriminator loss: 0.5802320837974548, acc: 0.4453125] [gan loss: 1.102372, acc: 0.078125]\n",
            "2581: [discriminator loss: 0.6453283429145813, acc: 0.1796875] [gan loss: 1.656452, acc: 0.000000]\n",
            "2582: [discriminator loss: 0.566320538520813, acc: 0.484375] [gan loss: 1.179193, acc: 0.031250]\n",
            "2583: [discriminator loss: 0.5710602402687073, acc: 0.3125] [gan loss: 1.643228, acc: 0.000000]\n",
            "2584: [discriminator loss: 0.5896719098091125, acc: 0.4375] [gan loss: 1.083990, acc: 0.093750]\n",
            "2585: [discriminator loss: 0.5905642509460449, acc: 0.2578125] [gan loss: 1.641335, acc: 0.015625]\n",
            "2586: [discriminator loss: 0.5499259233474731, acc: 0.4453125] [gan loss: 1.118978, acc: 0.109375]\n",
            "2587: [discriminator loss: 0.5660322904586792, acc: 0.3515625] [gan loss: 1.678744, acc: 0.000000]\n",
            "2588: [discriminator loss: 0.5563809871673584, acc: 0.421875] [gan loss: 1.072147, acc: 0.125000]\n",
            "2589: [discriminator loss: 0.5772271752357483, acc: 0.328125] [gan loss: 1.490102, acc: 0.000000]\n",
            "2590: [discriminator loss: 0.5352919101715088, acc: 0.4609375] [gan loss: 1.126455, acc: 0.109375]\n",
            "2591: [discriminator loss: 0.5879828929901123, acc: 0.28125] [gan loss: 1.731013, acc: 0.000000]\n",
            "2592: [discriminator loss: 0.5526740550994873, acc: 0.46875] [gan loss: 1.007334, acc: 0.093750]\n",
            "2593: [discriminator loss: 0.6094862818717957, acc: 0.25] [gan loss: 1.824785, acc: 0.000000]\n",
            "2594: [discriminator loss: 0.5699474215507507, acc: 0.4765625] [gan loss: 0.998861, acc: 0.156250]\n",
            "2595: [discriminator loss: 0.6365885138511658, acc: 0.1953125] [gan loss: 1.808238, acc: 0.000000]\n",
            "2596: [discriminator loss: 0.5677897930145264, acc: 0.4765625] [gan loss: 1.049447, acc: 0.109375]\n",
            "2597: [discriminator loss: 0.6179453730583191, acc: 0.2109375] [gan loss: 1.876226, acc: 0.000000]\n",
            "2598: [discriminator loss: 0.5738729238510132, acc: 0.4765625] [gan loss: 1.017905, acc: 0.140625]\n",
            "2599: [discriminator loss: 0.6337890625, acc: 0.203125] [gan loss: 1.633828, acc: 0.000000]\n",
            "2600: [discriminator loss: 0.5867623686790466, acc: 0.421875] [gan loss: 1.348525, acc: 0.031250]\n",
            "2601: [discriminator loss: 0.5841493606567383, acc: 0.359375] [gan loss: 1.543175, acc: 0.015625]\n",
            "2602: [discriminator loss: 0.5419173240661621, acc: 0.46875] [gan loss: 1.224180, acc: 0.093750]\n",
            "2603: [discriminator loss: 0.560469388961792, acc: 0.3046875] [gan loss: 1.663695, acc: 0.000000]\n",
            "2604: [discriminator loss: 0.5563744306564331, acc: 0.453125] [gan loss: 1.032230, acc: 0.078125]\n",
            "2605: [discriminator loss: 0.5948017239570618, acc: 0.2578125] [gan loss: 1.828296, acc: 0.000000]\n",
            "2606: [discriminator loss: 0.5947204828262329, acc: 0.4921875] [gan loss: 0.959611, acc: 0.203125]\n",
            "2607: [discriminator loss: 0.6501709222793579, acc: 0.1796875] [gan loss: 1.955470, acc: 0.000000]\n",
            "2608: [discriminator loss: 0.5627480745315552, acc: 0.484375] [gan loss: 0.934696, acc: 0.234375]\n",
            "2609: [discriminator loss: 0.6013344526290894, acc: 0.2578125] [gan loss: 1.639646, acc: 0.000000]\n",
            "2610: [discriminator loss: 0.5429909229278564, acc: 0.4453125] [gan loss: 1.279655, acc: 0.031250]\n",
            "2611: [discriminator loss: 0.5744780898094177, acc: 0.328125] [gan loss: 1.619549, acc: 0.000000]\n",
            "2612: [discriminator loss: 0.5512632131576538, acc: 0.453125] [gan loss: 0.997706, acc: 0.125000]\n",
            "2613: [discriminator loss: 0.6036324501037598, acc: 0.234375] [gan loss: 1.608808, acc: 0.000000]\n",
            "2614: [discriminator loss: 0.5883846879005432, acc: 0.453125] [gan loss: 1.117586, acc: 0.078125]\n",
            "2615: [discriminator loss: 0.598674476146698, acc: 0.265625] [gan loss: 1.605135, acc: 0.015625]\n",
            "2616: [discriminator loss: 0.5220308899879456, acc: 0.484375] [gan loss: 0.959224, acc: 0.234375]\n",
            "2617: [discriminator loss: 0.6316431760787964, acc: 0.1796875] [gan loss: 1.914344, acc: 0.000000]\n",
            "2618: [discriminator loss: 0.5734288692474365, acc: 0.453125] [gan loss: 1.119553, acc: 0.031250]\n",
            "2619: [discriminator loss: 0.599922776222229, acc: 0.265625] [gan loss: 1.629859, acc: 0.000000]\n",
            "2620: [discriminator loss: 0.5511729121208191, acc: 0.453125] [gan loss: 1.212830, acc: 0.046875]\n",
            "2621: [discriminator loss: 0.5953102111816406, acc: 0.2578125] [gan loss: 1.549678, acc: 0.000000]\n",
            "2622: [discriminator loss: 0.5151164531707764, acc: 0.4609375] [gan loss: 1.251026, acc: 0.062500]\n",
            "2623: [discriminator loss: 0.5800442695617676, acc: 0.3125] [gan loss: 1.668394, acc: 0.000000]\n",
            "2624: [discriminator loss: 0.5253514051437378, acc: 0.4921875] [gan loss: 1.147554, acc: 0.078125]\n",
            "2625: [discriminator loss: 0.5850727558135986, acc: 0.296875] [gan loss: 1.669810, acc: 0.000000]\n",
            "2626: [discriminator loss: 0.5591933131217957, acc: 0.46875] [gan loss: 0.995358, acc: 0.171875]\n",
            "2627: [discriminator loss: 0.6370387077331543, acc: 0.171875] [gan loss: 1.941344, acc: 0.000000]\n",
            "2628: [discriminator loss: 0.5286961793899536, acc: 0.484375] [gan loss: 0.999345, acc: 0.125000]\n",
            "2629: [discriminator loss: 0.6274397373199463, acc: 0.1875] [gan loss: 1.904588, acc: 0.000000]\n",
            "2630: [discriminator loss: 0.5308303236961365, acc: 0.4921875] [gan loss: 1.047080, acc: 0.156250]\n",
            "2631: [discriminator loss: 0.6727718114852905, acc: 0.1640625] [gan loss: 2.003297, acc: 0.000000]\n",
            "2632: [discriminator loss: 0.5722644329071045, acc: 0.4921875] [gan loss: 0.999847, acc: 0.156250]\n",
            "2633: [discriminator loss: 0.650310754776001, acc: 0.15625] [gan loss: 1.874329, acc: 0.015625]\n",
            "2634: [discriminator loss: 0.589779794216156, acc: 0.484375] [gan loss: 0.990718, acc: 0.093750]\n",
            "2635: [discriminator loss: 0.6320287585258484, acc: 0.2109375] [gan loss: 1.524607, acc: 0.015625]\n",
            "2636: [discriminator loss: 0.5941815376281738, acc: 0.4453125] [gan loss: 1.280156, acc: 0.031250]\n",
            "2637: [discriminator loss: 0.589065670967102, acc: 0.3125] [gan loss: 1.507708, acc: 0.031250]\n",
            "2638: [discriminator loss: 0.5948000550270081, acc: 0.40625] [gan loss: 1.296638, acc: 0.031250]\n",
            "2639: [discriminator loss: 0.5838111639022827, acc: 0.3359375] [gan loss: 1.411893, acc: 0.000000]\n",
            "2640: [discriminator loss: 0.586604118347168, acc: 0.4140625] [gan loss: 1.140453, acc: 0.046875]\n",
            "2641: [discriminator loss: 0.6239719986915588, acc: 0.2265625] [gan loss: 2.010392, acc: 0.000000]\n",
            "2642: [discriminator loss: 0.562073826789856, acc: 0.4765625] [gan loss: 1.025509, acc: 0.093750]\n",
            "2643: [discriminator loss: 0.6439525485038757, acc: 0.171875] [gan loss: 1.744717, acc: 0.000000]\n",
            "2644: [discriminator loss: 0.5235462784767151, acc: 0.46875] [gan loss: 1.138288, acc: 0.078125]\n",
            "2645: [discriminator loss: 0.6111684441566467, acc: 0.234375] [gan loss: 1.872386, acc: 0.000000]\n",
            "2646: [discriminator loss: 0.5652510523796082, acc: 0.484375] [gan loss: 0.982257, acc: 0.156250]\n",
            "2647: [discriminator loss: 0.6695045232772827, acc: 0.109375] [gan loss: 1.847112, acc: 0.000000]\n",
            "2648: [discriminator loss: 0.5887699723243713, acc: 0.484375] [gan loss: 0.954890, acc: 0.171875]\n",
            "2649: [discriminator loss: 0.625074028968811, acc: 0.1484375] [gan loss: 1.719526, acc: 0.000000]\n",
            "2650: [discriminator loss: 0.5709255933761597, acc: 0.46875] [gan loss: 1.057669, acc: 0.156250]\n",
            "2651: [discriminator loss: 0.6104171872138977, acc: 0.234375] [gan loss: 1.568426, acc: 0.000000]\n",
            "2652: [discriminator loss: 0.5713904500007629, acc: 0.40625] [gan loss: 1.363961, acc: 0.015625]\n",
            "2653: [discriminator loss: 0.559844970703125, acc: 0.34375] [gan loss: 1.451666, acc: 0.000000]\n",
            "2654: [discriminator loss: 0.5461286306381226, acc: 0.421875] [gan loss: 1.225781, acc: 0.046875]\n",
            "2655: [discriminator loss: 0.587531566619873, acc: 0.296875] [gan loss: 1.598296, acc: 0.015625]\n",
            "2656: [discriminator loss: 0.5271782875061035, acc: 0.4609375] [gan loss: 1.233722, acc: 0.031250]\n",
            "2657: [discriminator loss: 0.5760288238525391, acc: 0.328125] [gan loss: 1.571975, acc: 0.000000]\n",
            "2658: [discriminator loss: 0.545140266418457, acc: 0.4296875] [gan loss: 1.081155, acc: 0.125000]\n",
            "2659: [discriminator loss: 0.5762671232223511, acc: 0.328125] [gan loss: 1.467880, acc: 0.031250]\n",
            "2660: [discriminator loss: 0.5379329323768616, acc: 0.453125] [gan loss: 1.317918, acc: 0.015625]\n",
            "2661: [discriminator loss: 0.574762225151062, acc: 0.296875] [gan loss: 1.904883, acc: 0.000000]\n",
            "2662: [discriminator loss: 0.5698996782302856, acc: 0.4765625] [gan loss: 0.972448, acc: 0.171875]\n",
            "2663: [discriminator loss: 0.6255045533180237, acc: 0.1796875] [gan loss: 1.793858, acc: 0.000000]\n",
            "2664: [discriminator loss: 0.5718719363212585, acc: 0.4375] [gan loss: 0.995015, acc: 0.156250]\n",
            "2665: [discriminator loss: 0.6302717328071594, acc: 0.1796875] [gan loss: 1.836898, acc: 0.000000]\n",
            "2666: [discriminator loss: 0.5215210318565369, acc: 0.4921875] [gan loss: 0.939260, acc: 0.187500]\n",
            "2667: [discriminator loss: 0.6165085434913635, acc: 0.1875] [gan loss: 1.659453, acc: 0.000000]\n",
            "2668: [discriminator loss: 0.5591487884521484, acc: 0.484375] [gan loss: 0.930073, acc: 0.250000]\n",
            "2669: [discriminator loss: 0.6278423070907593, acc: 0.1328125] [gan loss: 1.665222, acc: 0.000000]\n",
            "2670: [discriminator loss: 0.5437438488006592, acc: 0.484375] [gan loss: 1.026468, acc: 0.140625]\n",
            "2671: [discriminator loss: 0.6028241515159607, acc: 0.2578125] [gan loss: 1.535644, acc: 0.000000]\n",
            "2672: [discriminator loss: 0.5467299222946167, acc: 0.4609375] [gan loss: 1.191445, acc: 0.031250]\n",
            "2673: [discriminator loss: 0.5977744460105896, acc: 0.2578125] [gan loss: 1.633933, acc: 0.000000]\n",
            "2674: [discriminator loss: 0.5239515900611877, acc: 0.46875] [gan loss: 1.251843, acc: 0.031250]\n",
            "2675: [discriminator loss: 0.5805842876434326, acc: 0.34375] [gan loss: 1.433206, acc: 0.015625]\n",
            "2676: [discriminator loss: 0.6116117238998413, acc: 0.328125] [gan loss: 1.383090, acc: 0.078125]\n",
            "2677: [discriminator loss: 0.5415661334991455, acc: 0.390625] [gan loss: 1.595140, acc: 0.015625]\n",
            "2678: [discriminator loss: 0.5502476096153259, acc: 0.46875] [gan loss: 1.061822, acc: 0.109375]\n",
            "2679: [discriminator loss: 0.6123719215393066, acc: 0.2734375] [gan loss: 1.639650, acc: 0.000000]\n",
            "2680: [discriminator loss: 0.5269216299057007, acc: 0.4375] [gan loss: 1.232856, acc: 0.031250]\n",
            "2681: [discriminator loss: 0.5777565836906433, acc: 0.3046875] [gan loss: 1.728575, acc: 0.000000]\n",
            "2682: [discriminator loss: 0.5391336679458618, acc: 0.484375] [gan loss: 1.109862, acc: 0.093750]\n",
            "2683: [discriminator loss: 0.6024336814880371, acc: 0.2578125] [gan loss: 1.804027, acc: 0.000000]\n",
            "2684: [discriminator loss: 0.5730218291282654, acc: 0.4765625] [gan loss: 1.058405, acc: 0.093750]\n",
            "2685: [discriminator loss: 0.6283941268920898, acc: 0.234375] [gan loss: 2.103842, acc: 0.000000]\n",
            "2686: [discriminator loss: 0.6183831691741943, acc: 0.5] [gan loss: 0.768884, acc: 0.421875]\n",
            "2687: [discriminator loss: 0.6841374635696411, acc: 0.1328125] [gan loss: 1.821330, acc: 0.000000]\n",
            "2688: [discriminator loss: 0.5707263946533203, acc: 0.46875] [gan loss: 1.087558, acc: 0.015625]\n",
            "2689: [discriminator loss: 0.6311711668968201, acc: 0.21875] [gan loss: 1.645495, acc: 0.015625]\n",
            "2690: [discriminator loss: 0.5740053653717041, acc: 0.4765625] [gan loss: 1.034958, acc: 0.156250]\n",
            "2691: [discriminator loss: 0.6132973432540894, acc: 0.2265625] [gan loss: 1.655301, acc: 0.015625]\n",
            "2692: [discriminator loss: 0.5352041125297546, acc: 0.453125] [gan loss: 1.057880, acc: 0.156250]\n",
            "2693: [discriminator loss: 0.634811520576477, acc: 0.234375] [gan loss: 1.557270, acc: 0.015625]\n",
            "2694: [discriminator loss: 0.557150661945343, acc: 0.3984375] [gan loss: 1.274727, acc: 0.046875]\n",
            "2695: [discriminator loss: 0.5804895162582397, acc: 0.3515625] [gan loss: 1.362105, acc: 0.046875]\n",
            "2696: [discriminator loss: 0.5868411064147949, acc: 0.375] [gan loss: 1.405676, acc: 0.015625]\n",
            "2697: [discriminator loss: 0.5469698905944824, acc: 0.4375] [gan loss: 1.324471, acc: 0.015625]\n",
            "2698: [discriminator loss: 0.5565615892410278, acc: 0.3359375] [gan loss: 1.683716, acc: 0.000000]\n",
            "2699: [discriminator loss: 0.5923075675964355, acc: 0.4296875] [gan loss: 1.220633, acc: 0.031250]\n",
            "2700: [discriminator loss: 0.6126211881637573, acc: 0.2265625] [gan loss: 1.857885, acc: 0.000000]\n",
            "2701: [discriminator loss: 0.545168399810791, acc: 0.484375] [gan loss: 0.842205, acc: 0.375000]\n",
            "2702: [discriminator loss: 0.6207141876220703, acc: 0.203125] [gan loss: 1.832505, acc: 0.000000]\n",
            "2703: [discriminator loss: 0.5511025190353394, acc: 0.46875] [gan loss: 1.021566, acc: 0.109375]\n",
            "2704: [discriminator loss: 0.6700031161308289, acc: 0.1640625] [gan loss: 1.823997, acc: 0.015625]\n",
            "2705: [discriminator loss: 0.5402436256408691, acc: 0.484375] [gan loss: 1.063726, acc: 0.062500]\n",
            "2706: [discriminator loss: 0.6798980236053467, acc: 0.15625] [gan loss: 1.818239, acc: 0.000000]\n",
            "2707: [discriminator loss: 0.5834475755691528, acc: 0.484375] [gan loss: 0.767289, acc: 0.453125]\n",
            "2708: [discriminator loss: 0.7054380178451538, acc: 0.078125] [gan loss: 1.802912, acc: 0.000000]\n",
            "2709: [discriminator loss: 0.5734341144561768, acc: 0.4609375] [gan loss: 1.070468, acc: 0.093750]\n",
            "2710: [discriminator loss: 0.6131885051727295, acc: 0.21875] [gan loss: 1.534564, acc: 0.015625]\n",
            "2711: [discriminator loss: 0.5597130060195923, acc: 0.40625] [gan loss: 1.272234, acc: 0.000000]\n",
            "2712: [discriminator loss: 0.5854665637016296, acc: 0.328125] [gan loss: 1.330033, acc: 0.062500]\n",
            "2713: [discriminator loss: 0.6027535796165466, acc: 0.34375] [gan loss: 1.305501, acc: 0.000000]\n",
            "2714: [discriminator loss: 0.6069836616516113, acc: 0.3515625] [gan loss: 1.626973, acc: 0.000000]\n",
            "2715: [discriminator loss: 0.5295164585113525, acc: 0.421875] [gan loss: 1.345308, acc: 0.046875]\n",
            "2716: [discriminator loss: 0.5827912092208862, acc: 0.3515625] [gan loss: 1.364136, acc: 0.000000]\n",
            "2717: [discriminator loss: 0.5781932473182678, acc: 0.3515625] [gan loss: 1.426590, acc: 0.046875]\n",
            "2718: [discriminator loss: 0.6452386975288391, acc: 0.3203125] [gan loss: 1.556245, acc: 0.000000]\n",
            "2719: [discriminator loss: 0.6093022227287292, acc: 0.34375] [gan loss: 1.448185, acc: 0.015625]\n",
            "2720: [discriminator loss: 0.5443021655082703, acc: 0.421875] [gan loss: 1.490936, acc: 0.015625]\n",
            "2721: [discriminator loss: 0.5829925537109375, acc: 0.3828125] [gan loss: 1.541444, acc: 0.000000]\n",
            "2722: [discriminator loss: 0.5571129322052002, acc: 0.46875] [gan loss: 1.258073, acc: 0.046875]\n",
            "2723: [discriminator loss: 0.6021112203598022, acc: 0.2890625] [gan loss: 1.786904, acc: 0.000000]\n",
            "2724: [discriminator loss: 0.5589315295219421, acc: 0.46875] [gan loss: 1.018368, acc: 0.171875]\n",
            "2725: [discriminator loss: 0.6723579168319702, acc: 0.1171875] [gan loss: 2.122398, acc: 0.015625]\n",
            "2726: [discriminator loss: 0.5728154182434082, acc: 0.5] [gan loss: 0.893318, acc: 0.234375]\n",
            "2727: [discriminator loss: 0.6785808205604553, acc: 0.140625] [gan loss: 1.871699, acc: 0.000000]\n",
            "2728: [discriminator loss: 0.5559237599372864, acc: 0.4921875] [gan loss: 0.932757, acc: 0.234375]\n",
            "2729: [discriminator loss: 0.6859822273254395, acc: 0.1171875] [gan loss: 1.781317, acc: 0.000000]\n",
            "2730: [discriminator loss: 0.5879205465316772, acc: 0.4765625] [gan loss: 1.040781, acc: 0.062500]\n",
            "2731: [discriminator loss: 0.6217891573905945, acc: 0.1953125] [gan loss: 1.639537, acc: 0.000000]\n",
            "2732: [discriminator loss: 0.5710055828094482, acc: 0.4609375] [gan loss: 1.205489, acc: 0.015625]\n",
            "2733: [discriminator loss: 0.6156554222106934, acc: 0.2890625] [gan loss: 1.551342, acc: 0.000000]\n",
            "2734: [discriminator loss: 0.5617395639419556, acc: 0.453125] [gan loss: 1.112222, acc: 0.046875]\n",
            "2735: [discriminator loss: 0.595053493976593, acc: 0.25] [gan loss: 1.610565, acc: 0.000000]\n",
            "2736: [discriminator loss: 0.5616048574447632, acc: 0.4453125] [gan loss: 1.063285, acc: 0.171875]\n",
            "2737: [discriminator loss: 0.5533690452575684, acc: 0.3515625] [gan loss: 1.520855, acc: 0.031250]\n",
            "2738: [discriminator loss: 0.5599352717399597, acc: 0.46875] [gan loss: 1.106052, acc: 0.046875]\n",
            "2739: [discriminator loss: 0.6313550472259521, acc: 0.2734375] [gan loss: 1.655963, acc: 0.000000]\n",
            "2740: [discriminator loss: 0.5317320823669434, acc: 0.4609375] [gan loss: 1.151834, acc: 0.031250]\n",
            "2741: [discriminator loss: 0.6362634897232056, acc: 0.2578125] [gan loss: 1.717914, acc: 0.000000]\n",
            "2742: [discriminator loss: 0.5861141085624695, acc: 0.4609375] [gan loss: 0.970390, acc: 0.234375]\n",
            "2743: [discriminator loss: 0.6744610667228699, acc: 0.1171875] [gan loss: 2.028371, acc: 0.000000]\n",
            "2744: [discriminator loss: 0.5664549469947815, acc: 0.4921875] [gan loss: 1.016507, acc: 0.078125]\n",
            "2745: [discriminator loss: 0.6993750333786011, acc: 0.109375] [gan loss: 1.738637, acc: 0.000000]\n",
            "2746: [discriminator loss: 0.5811184048652649, acc: 0.453125] [gan loss: 1.128844, acc: 0.046875]\n",
            "2747: [discriminator loss: 0.5998040437698364, acc: 0.2734375] [gan loss: 1.512688, acc: 0.000000]\n",
            "2748: [discriminator loss: 0.5575821995735168, acc: 0.4453125] [gan loss: 1.201007, acc: 0.015625]\n",
            "2749: [discriminator loss: 0.6218760013580322, acc: 0.265625] [gan loss: 1.621947, acc: 0.000000]\n",
            "2750: [discriminator loss: 0.5618304014205933, acc: 0.453125] [gan loss: 1.227004, acc: 0.046875]\n",
            "2751: [discriminator loss: 0.6188878417015076, acc: 0.328125] [gan loss: 1.417400, acc: 0.000000]\n",
            "2752: [discriminator loss: 0.5577772259712219, acc: 0.4140625] [gan loss: 1.344949, acc: 0.031250]\n",
            "2753: [discriminator loss: 0.5456998348236084, acc: 0.3828125] [gan loss: 1.618660, acc: 0.000000]\n",
            "2754: [discriminator loss: 0.5391063690185547, acc: 0.4453125] [gan loss: 1.376752, acc: 0.000000]\n",
            "2755: [discriminator loss: 0.5724658966064453, acc: 0.359375] [gan loss: 1.632939, acc: 0.000000]\n",
            "2756: [discriminator loss: 0.5372534394264221, acc: 0.4765625] [gan loss: 1.151768, acc: 0.046875]\n",
            "2757: [discriminator loss: 0.6241043210029602, acc: 0.2265625] [gan loss: 1.976955, acc: 0.000000]\n",
            "2758: [discriminator loss: 0.5482169985771179, acc: 0.4921875] [gan loss: 0.982404, acc: 0.140625]\n",
            "2759: [discriminator loss: 0.6502907276153564, acc: 0.1953125] [gan loss: 2.203601, acc: 0.000000]\n",
            "2760: [discriminator loss: 0.5932838916778564, acc: 0.484375] [gan loss: 0.871567, acc: 0.265625]\n",
            "2761: [discriminator loss: 0.6411855220794678, acc: 0.171875] [gan loss: 1.781068, acc: 0.031250]\n",
            "2762: [discriminator loss: 0.5952091217041016, acc: 0.4765625] [gan loss: 0.967566, acc: 0.125000]\n",
            "2763: [discriminator loss: 0.6265993118286133, acc: 0.21875] [gan loss: 1.791112, acc: 0.000000]\n",
            "2764: [discriminator loss: 0.5654323697090149, acc: 0.484375] [gan loss: 0.981546, acc: 0.218750]\n",
            "2765: [discriminator loss: 0.6263432502746582, acc: 0.21875] [gan loss: 1.676423, acc: 0.000000]\n",
            "2766: [discriminator loss: 0.6105362176895142, acc: 0.4453125] [gan loss: 1.001453, acc: 0.109375]\n",
            "2767: [discriminator loss: 0.602475643157959, acc: 0.28125] [gan loss: 1.497505, acc: 0.000000]\n",
            "2768: [discriminator loss: 0.5861428380012512, acc: 0.4140625] [gan loss: 1.145795, acc: 0.046875]\n",
            "2769: [discriminator loss: 0.6076989769935608, acc: 0.2734375] [gan loss: 1.555784, acc: 0.015625]\n",
            "2770: [discriminator loss: 0.571230947971344, acc: 0.40625] [gan loss: 1.122372, acc: 0.078125]\n",
            "2771: [discriminator loss: 0.5787352919578552, acc: 0.296875] [gan loss: 1.526978, acc: 0.031250]\n",
            "2772: [discriminator loss: 0.5581374764442444, acc: 0.4140625] [gan loss: 1.192777, acc: 0.031250]\n",
            "2773: [discriminator loss: 0.5652947425842285, acc: 0.296875] [gan loss: 1.511467, acc: 0.015625]\n",
            "2774: [discriminator loss: 0.5895776748657227, acc: 0.359375] [gan loss: 1.453877, acc: 0.015625]\n",
            "2775: [discriminator loss: 0.5679241418838501, acc: 0.3984375] [gan loss: 1.443093, acc: 0.000000]\n",
            "2776: [discriminator loss: 0.5856783986091614, acc: 0.4140625] [gan loss: 1.426115, acc: 0.000000]\n",
            "2777: [discriminator loss: 0.5661612749099731, acc: 0.40625] [gan loss: 1.364587, acc: 0.031250]\n",
            "2778: [discriminator loss: 0.5688705444335938, acc: 0.359375] [gan loss: 1.420325, acc: 0.015625]\n",
            "2779: [discriminator loss: 0.5723953247070312, acc: 0.3984375] [gan loss: 1.164083, acc: 0.031250]\n",
            "2780: [discriminator loss: 0.6172205209732056, acc: 0.25] [gan loss: 1.835809, acc: 0.000000]\n",
            "2781: [discriminator loss: 0.5747817158699036, acc: 0.4765625] [gan loss: 0.982324, acc: 0.062500]\n",
            "2782: [discriminator loss: 0.664462149143219, acc: 0.1484375] [gan loss: 2.129542, acc: 0.000000]\n",
            "2783: [discriminator loss: 0.6194071769714355, acc: 0.4921875] [gan loss: 0.739641, acc: 0.406250]\n",
            "2784: [discriminator loss: 0.7282121777534485, acc: 0.046875] [gan loss: 1.986435, acc: 0.000000]\n",
            "2785: [discriminator loss: 0.5653758645057678, acc: 0.4765625] [gan loss: 0.872168, acc: 0.296875]\n",
            "2786: [discriminator loss: 0.671209990978241, acc: 0.1953125] [gan loss: 1.587933, acc: 0.000000]\n",
            "2787: [discriminator loss: 0.574530303478241, acc: 0.4609375] [gan loss: 1.246295, acc: 0.031250]\n",
            "2788: [discriminator loss: 0.5900262594223022, acc: 0.3046875] [gan loss: 1.510006, acc: 0.000000]\n",
            "2789: [discriminator loss: 0.5494014620780945, acc: 0.4453125] [gan loss: 1.243108, acc: 0.015625]\n",
            "2790: [discriminator loss: 0.5792334079742432, acc: 0.3515625] [gan loss: 1.350123, acc: 0.015625]\n",
            "2791: [discriminator loss: 0.6003493070602417, acc: 0.3125] [gan loss: 1.597951, acc: 0.015625]\n",
            "2792: [discriminator loss: 0.5482203364372253, acc: 0.4453125] [gan loss: 1.298628, acc: 0.046875]\n",
            "2793: [discriminator loss: 0.587135374546051, acc: 0.3046875] [gan loss: 1.507775, acc: 0.000000]\n",
            "2794: [discriminator loss: 0.5506266951560974, acc: 0.4375] [gan loss: 1.151886, acc: 0.078125]\n",
            "2795: [discriminator loss: 0.5916718244552612, acc: 0.328125] [gan loss: 1.724947, acc: 0.000000]\n",
            "2796: [discriminator loss: 0.6357605457305908, acc: 0.421875] [gan loss: 1.070970, acc: 0.109375]\n",
            "2797: [discriminator loss: 0.6620502471923828, acc: 0.1796875] [gan loss: 1.845937, acc: 0.000000]\n",
            "2798: [discriminator loss: 0.5793175101280212, acc: 0.4765625] [gan loss: 0.823558, acc: 0.390625]\n",
            "2799: [discriminator loss: 0.7038235664367676, acc: 0.140625] [gan loss: 1.805132, acc: 0.000000]\n",
            "2800: [discriminator loss: 0.5844553709030151, acc: 0.4609375] [gan loss: 0.945239, acc: 0.187500]\n",
            "2801: [discriminator loss: 0.6323783993721008, acc: 0.1875] [gan loss: 1.683984, acc: 0.015625]\n",
            "2802: [discriminator loss: 0.5680451393127441, acc: 0.453125] [gan loss: 0.939895, acc: 0.203125]\n",
            "2803: [discriminator loss: 0.6780656576156616, acc: 0.1796875] [gan loss: 1.629751, acc: 0.000000]\n",
            "2804: [discriminator loss: 0.601203978061676, acc: 0.46875] [gan loss: 0.999803, acc: 0.109375]\n",
            "2805: [discriminator loss: 0.662036657333374, acc: 0.1953125] [gan loss: 1.654737, acc: 0.000000]\n",
            "2806: [discriminator loss: 0.5361533164978027, acc: 0.4765625] [gan loss: 1.151286, acc: 0.078125]\n",
            "2807: [discriminator loss: 0.6590561866760254, acc: 0.171875] [gan loss: 1.813689, acc: 0.000000]\n",
            "2808: [discriminator loss: 0.6178464293479919, acc: 0.4765625] [gan loss: 0.886035, acc: 0.187500]\n",
            "2809: [discriminator loss: 0.6708471775054932, acc: 0.171875] [gan loss: 1.698137, acc: 0.015625]\n",
            "2810: [discriminator loss: 0.5678965449333191, acc: 0.4296875] [gan loss: 1.197571, acc: 0.062500]\n",
            "2811: [discriminator loss: 0.6269868612289429, acc: 0.2734375] [gan loss: 1.467165, acc: 0.031250]\n",
            "2812: [discriminator loss: 0.5923590660095215, acc: 0.3984375] [gan loss: 1.405419, acc: 0.000000]\n",
            "2813: [discriminator loss: 0.5563907623291016, acc: 0.40625] [gan loss: 1.194222, acc: 0.046875]\n",
            "2814: [discriminator loss: 0.6011412739753723, acc: 0.3203125] [gan loss: 1.424378, acc: 0.031250]\n",
            "2815: [discriminator loss: 0.532118558883667, acc: 0.4453125] [gan loss: 1.292171, acc: 0.031250]\n",
            "2816: [discriminator loss: 0.58354651927948, acc: 0.3203125] [gan loss: 1.618874, acc: 0.015625]\n",
            "2817: [discriminator loss: 0.598511815071106, acc: 0.4375] [gan loss: 1.126074, acc: 0.062500]\n",
            "2818: [discriminator loss: 0.6577349901199341, acc: 0.203125] [gan loss: 1.823614, acc: 0.000000]\n",
            "2819: [discriminator loss: 0.571292519569397, acc: 0.4921875] [gan loss: 0.853725, acc: 0.234375]\n",
            "2820: [discriminator loss: 0.6791133880615234, acc: 0.1171875] [gan loss: 1.963093, acc: 0.000000]\n",
            "2821: [discriminator loss: 0.6055381298065186, acc: 0.5] [gan loss: 0.755568, acc: 0.453125]\n",
            "2822: [discriminator loss: 0.6535553932189941, acc: 0.140625] [gan loss: 1.652533, acc: 0.000000]\n",
            "2823: [discriminator loss: 0.597123384475708, acc: 0.4375] [gan loss: 0.989249, acc: 0.203125]\n",
            "2824: [discriminator loss: 0.6661032438278198, acc: 0.2109375] [gan loss: 1.572485, acc: 0.000000]\n",
            "2825: [discriminator loss: 0.5620510578155518, acc: 0.4453125] [gan loss: 1.166524, acc: 0.046875]\n",
            "2826: [discriminator loss: 0.6571815609931946, acc: 0.28125] [gan loss: 1.617797, acc: 0.031250]\n",
            "2827: [discriminator loss: 0.6104704141616821, acc: 0.453125] [gan loss: 0.963806, acc: 0.218750]\n",
            "2828: [discriminator loss: 0.6880171298980713, acc: 0.1875] [gan loss: 1.690400, acc: 0.000000]\n",
            "2829: [discriminator loss: 0.5954150557518005, acc: 0.4140625] [gan loss: 1.104473, acc: 0.078125]\n",
            "2830: [discriminator loss: 0.604328989982605, acc: 0.2890625] [gan loss: 1.535000, acc: 0.000000]\n",
            "2831: [discriminator loss: 0.5549561381340027, acc: 0.3984375] [gan loss: 1.190727, acc: 0.031250]\n",
            "2832: [discriminator loss: 0.5944803953170776, acc: 0.3046875] [gan loss: 1.555332, acc: 0.000000]\n",
            "2833: [discriminator loss: 0.5747472047805786, acc: 0.4140625] [gan loss: 1.200333, acc: 0.031250]\n",
            "2834: [discriminator loss: 0.6150103807449341, acc: 0.3515625] [gan loss: 1.477770, acc: 0.000000]\n",
            "2835: [discriminator loss: 0.5779334902763367, acc: 0.390625] [gan loss: 1.312642, acc: 0.078125]\n",
            "2836: [discriminator loss: 0.5882036089897156, acc: 0.390625] [gan loss: 1.364097, acc: 0.015625]\n",
            "2837: [discriminator loss: 0.5546996593475342, acc: 0.4453125] [gan loss: 1.305710, acc: 0.046875]\n",
            "2838: [discriminator loss: 0.5818116068840027, acc: 0.3515625] [gan loss: 1.443833, acc: 0.015625]\n",
            "2839: [discriminator loss: 0.5827375650405884, acc: 0.375] [gan loss: 1.456461, acc: 0.000000]\n",
            "2840: [discriminator loss: 0.6040731072425842, acc: 0.3984375] [gan loss: 1.358384, acc: 0.000000]\n",
            "2841: [discriminator loss: 0.6016063690185547, acc: 0.328125] [gan loss: 1.740127, acc: 0.015625]\n",
            "2842: [discriminator loss: 0.5663644075393677, acc: 0.4453125] [gan loss: 1.056449, acc: 0.125000]\n",
            "2843: [discriminator loss: 0.6457778811454773, acc: 0.2578125] [gan loss: 1.802853, acc: 0.015625]\n",
            "2844: [discriminator loss: 0.618388831615448, acc: 0.4921875] [gan loss: 0.761821, acc: 0.453125]\n",
            "2845: [discriminator loss: 0.7330487966537476, acc: 0.0546875] [gan loss: 2.129148, acc: 0.000000]\n",
            "2846: [discriminator loss: 0.6467479467391968, acc: 0.4921875] [gan loss: 0.720086, acc: 0.546875]\n",
            "2847: [discriminator loss: 0.7061523795127869, acc: 0.109375] [gan loss: 1.739580, acc: 0.000000]\n",
            "2848: [discriminator loss: 0.5777573585510254, acc: 0.46875] [gan loss: 0.987356, acc: 0.218750]\n",
            "2849: [discriminator loss: 0.6513793468475342, acc: 0.1640625] [gan loss: 1.496317, acc: 0.015625]\n",
            "2850: [discriminator loss: 0.5669817924499512, acc: 0.390625] [gan loss: 1.161669, acc: 0.062500]\n",
            "2851: [discriminator loss: 0.648507297039032, acc: 0.2421875] [gan loss: 1.508866, acc: 0.015625]\n",
            "2852: [discriminator loss: 0.6299747228622437, acc: 0.359375] [gan loss: 1.183877, acc: 0.031250]\n",
            "2853: [discriminator loss: 0.6301639080047607, acc: 0.265625] [gan loss: 1.454477, acc: 0.015625]\n",
            "2854: [discriminator loss: 0.5898592472076416, acc: 0.3828125] [gan loss: 1.271610, acc: 0.000000]\n",
            "2855: [discriminator loss: 0.5818927884101868, acc: 0.34375] [gan loss: 1.415445, acc: 0.015625]\n",
            "2856: [discriminator loss: 0.6603921055793762, acc: 0.3203125] [gan loss: 1.202761, acc: 0.093750]\n",
            "2857: [discriminator loss: 0.582939624786377, acc: 0.328125] [gan loss: 1.485350, acc: 0.000000]\n",
            "2858: [discriminator loss: 0.5597145557403564, acc: 0.4296875] [gan loss: 1.104393, acc: 0.125000]\n",
            "2859: [discriminator loss: 0.6464954614639282, acc: 0.171875] [gan loss: 1.715536, acc: 0.000000]\n",
            "2860: [discriminator loss: 0.655327558517456, acc: 0.453125] [gan loss: 0.820819, acc: 0.312500]\n",
            "2861: [discriminator loss: 0.6985080242156982, acc: 0.09375] [gan loss: 1.774869, acc: 0.000000]\n",
            "2862: [discriminator loss: 0.5610141754150391, acc: 0.484375] [gan loss: 0.915277, acc: 0.234375]\n",
            "2863: [discriminator loss: 0.6388559937477112, acc: 0.15625] [gan loss: 1.603448, acc: 0.000000]\n",
            "2864: [discriminator loss: 0.5811869502067566, acc: 0.453125] [gan loss: 1.072811, acc: 0.156250]\n",
            "2865: [discriminator loss: 0.6026238799095154, acc: 0.34375] [gan loss: 1.438062, acc: 0.015625]\n",
            "2866: [discriminator loss: 0.576046347618103, acc: 0.390625] [gan loss: 1.210840, acc: 0.046875]\n",
            "2867: [discriminator loss: 0.5977928638458252, acc: 0.34375] [gan loss: 1.444603, acc: 0.015625]\n",
            "2868: [discriminator loss: 0.5998884439468384, acc: 0.4140625] [gan loss: 1.151511, acc: 0.046875]\n",
            "2869: [discriminator loss: 0.6223217248916626, acc: 0.2578125] [gan loss: 1.550465, acc: 0.000000]\n",
            "2870: [discriminator loss: 0.5776113271713257, acc: 0.453125] [gan loss: 0.922438, acc: 0.234375]\n",
            "2871: [discriminator loss: 0.6338695883750916, acc: 0.1953125] [gan loss: 1.733381, acc: 0.000000]\n",
            "2872: [discriminator loss: 0.5599478483200073, acc: 0.4609375] [gan loss: 0.894855, acc: 0.250000]\n",
            "2873: [discriminator loss: 0.6961767673492432, acc: 0.1328125] [gan loss: 1.704423, acc: 0.000000]\n",
            "2874: [discriminator loss: 0.5822255611419678, acc: 0.484375] [gan loss: 0.831628, acc: 0.250000]\n",
            "2875: [discriminator loss: 0.6679977178573608, acc: 0.1640625] [gan loss: 1.813594, acc: 0.015625]\n",
            "2876: [discriminator loss: 0.628229022026062, acc: 0.46875] [gan loss: 0.892889, acc: 0.312500]\n",
            "2877: [discriminator loss: 0.6496903300285339, acc: 0.1796875] [gan loss: 1.709399, acc: 0.015625]\n",
            "2878: [discriminator loss: 0.6152300834655762, acc: 0.421875] [gan loss: 1.065069, acc: 0.046875]\n",
            "2879: [discriminator loss: 0.6318911910057068, acc: 0.2265625] [gan loss: 1.470137, acc: 0.015625]\n",
            "2880: [discriminator loss: 0.5575351715087891, acc: 0.4609375] [gan loss: 1.043194, acc: 0.171875]\n",
            "2881: [discriminator loss: 0.6091318130493164, acc: 0.28125] [gan loss: 1.532040, acc: 0.000000]\n",
            "2882: [discriminator loss: 0.6103533506393433, acc: 0.40625] [gan loss: 1.031348, acc: 0.125000]\n",
            "2883: [discriminator loss: 0.6077085733413696, acc: 0.2421875] [gan loss: 1.737709, acc: 0.000000]\n",
            "2884: [discriminator loss: 0.6500369310379028, acc: 0.453125] [gan loss: 0.925343, acc: 0.187500]\n",
            "2885: [discriminator loss: 0.6659225225448608, acc: 0.15625] [gan loss: 1.794191, acc: 0.000000]\n",
            "2886: [discriminator loss: 0.594201922416687, acc: 0.4921875] [gan loss: 0.955372, acc: 0.187500]\n",
            "2887: [discriminator loss: 0.6624338626861572, acc: 0.1640625] [gan loss: 1.763054, acc: 0.000000]\n",
            "2888: [discriminator loss: 0.6162641048431396, acc: 0.4765625] [gan loss: 0.901257, acc: 0.234375]\n",
            "2889: [discriminator loss: 0.6719311475753784, acc: 0.15625] [gan loss: 1.640612, acc: 0.015625]\n",
            "2890: [discriminator loss: 0.6101646423339844, acc: 0.4375] [gan loss: 0.976820, acc: 0.093750]\n",
            "2891: [discriminator loss: 0.6374915838241577, acc: 0.1796875] [gan loss: 1.528505, acc: 0.000000]\n",
            "2892: [discriminator loss: 0.5970656275749207, acc: 0.453125] [gan loss: 0.899622, acc: 0.281250]\n",
            "2893: [discriminator loss: 0.6663159728050232, acc: 0.203125] [gan loss: 1.652904, acc: 0.015625]\n",
            "2894: [discriminator loss: 0.588917076587677, acc: 0.4765625] [gan loss: 0.869351, acc: 0.187500]\n",
            "2895: [discriminator loss: 0.6781072616577148, acc: 0.1640625] [gan loss: 1.678569, acc: 0.015625]\n",
            "2896: [discriminator loss: 0.5862290859222412, acc: 0.46875] [gan loss: 0.762035, acc: 0.453125]\n",
            "2897: [discriminator loss: 0.683112382888794, acc: 0.1328125] [gan loss: 1.410820, acc: 0.000000]\n",
            "2898: [discriminator loss: 0.6126447319984436, acc: 0.4375] [gan loss: 1.138057, acc: 0.062500]\n",
            "2899: [discriminator loss: 0.6196038126945496, acc: 0.2734375] [gan loss: 1.254578, acc: 0.015625]\n",
            "2900: [discriminator loss: 0.5540944933891296, acc: 0.390625] [gan loss: 1.081731, acc: 0.109375]\n",
            "2901: [discriminator loss: 0.5747667551040649, acc: 0.296875] [gan loss: 1.468026, acc: 0.015625]\n",
            "2902: [discriminator loss: 0.6088515520095825, acc: 0.4296875] [gan loss: 0.998654, acc: 0.156250]\n",
            "2903: [discriminator loss: 0.6218997240066528, acc: 0.1796875] [gan loss: 1.896323, acc: 0.000000]\n",
            "2904: [discriminator loss: 0.6048177480697632, acc: 0.453125] [gan loss: 0.830883, acc: 0.375000]\n",
            "2905: [discriminator loss: 0.7362097501754761, acc: 0.109375] [gan loss: 1.712055, acc: 0.000000]\n",
            "2906: [discriminator loss: 0.6063569188117981, acc: 0.4375] [gan loss: 1.078620, acc: 0.109375]\n",
            "2907: [discriminator loss: 0.6338668465614319, acc: 0.265625] [gan loss: 1.595550, acc: 0.000000]\n",
            "2908: [discriminator loss: 0.5910883545875549, acc: 0.453125] [gan loss: 0.888155, acc: 0.281250]\n",
            "2909: [discriminator loss: 0.6693134903907776, acc: 0.109375] [gan loss: 1.604156, acc: 0.000000]\n",
            "2910: [discriminator loss: 0.6359513401985168, acc: 0.4765625] [gan loss: 0.792245, acc: 0.281250]\n",
            "2911: [discriminator loss: 0.7057137489318848, acc: 0.125] [gan loss: 1.692203, acc: 0.000000]\n",
            "2912: [discriminator loss: 0.6096810102462769, acc: 0.4765625] [gan loss: 0.865398, acc: 0.312500]\n",
            "2913: [discriminator loss: 0.6454157829284668, acc: 0.15625] [gan loss: 1.423117, acc: 0.015625]\n",
            "2914: [discriminator loss: 0.6347517967224121, acc: 0.3515625] [gan loss: 1.144066, acc: 0.046875]\n",
            "2915: [discriminator loss: 0.6704906225204468, acc: 0.171875] [gan loss: 1.460078, acc: 0.000000]\n",
            "2916: [discriminator loss: 0.6108771562576294, acc: 0.359375] [gan loss: 1.274640, acc: 0.031250]\n",
            "2917: [discriminator loss: 0.6526738405227661, acc: 0.3203125] [gan loss: 1.276456, acc: 0.046875]\n",
            "2918: [discriminator loss: 0.5869972109794617, acc: 0.296875] [gan loss: 1.426574, acc: 0.000000]\n",
            "2919: [discriminator loss: 0.6199384927749634, acc: 0.3828125] [gan loss: 1.244483, acc: 0.031250]\n",
            "2920: [discriminator loss: 0.6305525302886963, acc: 0.265625] [gan loss: 1.475295, acc: 0.015625]\n",
            "2921: [discriminator loss: 0.5722439885139465, acc: 0.4375] [gan loss: 1.150546, acc: 0.046875]\n",
            "2922: [discriminator loss: 0.6674827933311462, acc: 0.1875] [gan loss: 1.757368, acc: 0.000000]\n",
            "2923: [discriminator loss: 0.6379544734954834, acc: 0.4296875] [gan loss: 0.882233, acc: 0.250000]\n",
            "2924: [discriminator loss: 0.6859714388847351, acc: 0.125] [gan loss: 1.707991, acc: 0.000000]\n",
            "2925: [discriminator loss: 0.5993512868881226, acc: 0.484375] [gan loss: 0.804711, acc: 0.453125]\n",
            "2926: [discriminator loss: 0.7197215557098389, acc: 0.0859375] [gan loss: 1.655367, acc: 0.000000]\n",
            "2927: [discriminator loss: 0.6125643253326416, acc: 0.4609375] [gan loss: 0.856001, acc: 0.234375]\n",
            "2928: [discriminator loss: 0.6818010807037354, acc: 0.109375] [gan loss: 1.585165, acc: 0.000000]\n",
            "2929: [discriminator loss: 0.6039800047874451, acc: 0.4453125] [gan loss: 1.146518, acc: 0.062500]\n",
            "2930: [discriminator loss: 0.6252754926681519, acc: 0.2421875] [gan loss: 1.348812, acc: 0.015625]\n",
            "2931: [discriminator loss: 0.5970931053161621, acc: 0.3828125] [gan loss: 1.198292, acc: 0.078125]\n",
            "2932: [discriminator loss: 0.61372971534729, acc: 0.3359375] [gan loss: 1.087991, acc: 0.093750]\n",
            "2933: [discriminator loss: 0.6062093377113342, acc: 0.3046875] [gan loss: 1.460163, acc: 0.031250]\n",
            "2934: [discriminator loss: 0.5965292453765869, acc: 0.3984375] [gan loss: 1.137335, acc: 0.093750]\n",
            "2935: [discriminator loss: 0.5823938250541687, acc: 0.3671875] [gan loss: 1.301619, acc: 0.015625]\n",
            "2936: [discriminator loss: 0.638066828250885, acc: 0.296875] [gan loss: 1.343565, acc: 0.015625]\n",
            "2937: [discriminator loss: 0.5737901329994202, acc: 0.375] [gan loss: 1.187976, acc: 0.093750]\n",
            "2938: [discriminator loss: 0.6391793489456177, acc: 0.2890625] [gan loss: 1.385547, acc: 0.015625]\n",
            "2939: [discriminator loss: 0.6027765274047852, acc: 0.4296875] [gan loss: 1.108326, acc: 0.093750]\n",
            "2940: [discriminator loss: 0.6506774425506592, acc: 0.265625] [gan loss: 1.636722, acc: 0.000000]\n",
            "2941: [discriminator loss: 0.6117561459541321, acc: 0.4765625] [gan loss: 0.833394, acc: 0.281250]\n",
            "2942: [discriminator loss: 0.6609188318252563, acc: 0.1875] [gan loss: 1.709953, acc: 0.015625]\n",
            "2943: [discriminator loss: 0.5375650525093079, acc: 0.5] [gan loss: 0.909998, acc: 0.234375]\n",
            "2944: [discriminator loss: 0.7049643993377686, acc: 0.125] [gan loss: 2.043521, acc: 0.000000]\n",
            "2945: [discriminator loss: 0.6101894378662109, acc: 0.484375] [gan loss: 0.798272, acc: 0.281250]\n",
            "2946: [discriminator loss: 0.7099072933197021, acc: 0.1015625] [gan loss: 1.624760, acc: 0.000000]\n",
            "2947: [discriminator loss: 0.6120286583900452, acc: 0.4453125] [gan loss: 0.988727, acc: 0.171875]\n",
            "2948: [discriminator loss: 0.6189167499542236, acc: 0.2421875] [gan loss: 1.430478, acc: 0.015625]\n",
            "2949: [discriminator loss: 0.615912675857544, acc: 0.4453125] [gan loss: 0.861503, acc: 0.265625]\n",
            "2950: [discriminator loss: 0.6613081693649292, acc: 0.171875] [gan loss: 1.513940, acc: 0.000000]\n",
            "2951: [discriminator loss: 0.6145557165145874, acc: 0.4375] [gan loss: 0.830428, acc: 0.328125]\n",
            "2952: [discriminator loss: 0.6842615604400635, acc: 0.1484375] [gan loss: 1.593357, acc: 0.000000]\n",
            "2953: [discriminator loss: 0.6183230876922607, acc: 0.4296875] [gan loss: 0.898031, acc: 0.281250]\n",
            "2954: [discriminator loss: 0.6475856304168701, acc: 0.1875] [gan loss: 1.582812, acc: 0.000000]\n",
            "2955: [discriminator loss: 0.6153936386108398, acc: 0.421875] [gan loss: 0.972854, acc: 0.156250]\n",
            "2956: [discriminator loss: 0.6470137238502502, acc: 0.203125] [gan loss: 1.543356, acc: 0.000000]\n",
            "2957: [discriminator loss: 0.6211072206497192, acc: 0.4375] [gan loss: 0.937936, acc: 0.171875]\n",
            "2958: [discriminator loss: 0.7025023698806763, acc: 0.1953125] [gan loss: 1.589101, acc: 0.000000]\n",
            "2959: [discriminator loss: 0.6060200333595276, acc: 0.4140625] [gan loss: 1.127293, acc: 0.062500]\n",
            "2960: [discriminator loss: 0.6358872652053833, acc: 0.1953125] [gan loss: 1.466510, acc: 0.015625]\n",
            "2961: [discriminator loss: 0.5958737730979919, acc: 0.4296875] [gan loss: 1.003620, acc: 0.140625]\n",
            "2962: [discriminator loss: 0.6437194347381592, acc: 0.2265625] [gan loss: 1.544616, acc: 0.000000]\n",
            "2963: [discriminator loss: 0.6055546998977661, acc: 0.4375] [gan loss: 0.862702, acc: 0.312500]\n",
            "2964: [discriminator loss: 0.6759645938873291, acc: 0.1640625] [gan loss: 1.581739, acc: 0.015625]\n",
            "2965: [discriminator loss: 0.6362702250480652, acc: 0.421875] [gan loss: 1.034821, acc: 0.078125]\n",
            "2966: [discriminator loss: 0.6646766662597656, acc: 0.171875] [gan loss: 1.494823, acc: 0.015625]\n",
            "2967: [discriminator loss: 0.6605045795440674, acc: 0.3515625] [gan loss: 0.999154, acc: 0.187500]\n",
            "2968: [discriminator loss: 0.6550254821777344, acc: 0.265625] [gan loss: 1.463002, acc: 0.015625]\n",
            "2969: [discriminator loss: 0.6096189022064209, acc: 0.3984375] [gan loss: 1.101148, acc: 0.031250]\n",
            "2970: [discriminator loss: 0.6542395353317261, acc: 0.2109375] [gan loss: 1.505820, acc: 0.000000]\n",
            "2971: [discriminator loss: 0.5969821214675903, acc: 0.4375] [gan loss: 1.073149, acc: 0.062500]\n",
            "2972: [discriminator loss: 0.6231619715690613, acc: 0.25] [gan loss: 1.485992, acc: 0.000000]\n",
            "2973: [discriminator loss: 0.5885076522827148, acc: 0.4453125] [gan loss: 0.944127, acc: 0.265625]\n",
            "2974: [discriminator loss: 0.6846202611923218, acc: 0.140625] [gan loss: 1.851568, acc: 0.000000]\n",
            "2975: [discriminator loss: 0.6010546684265137, acc: 0.4609375] [gan loss: 0.833878, acc: 0.328125]\n",
            "2976: [discriminator loss: 0.6805198192596436, acc: 0.140625] [gan loss: 1.764095, acc: 0.000000]\n",
            "2977: [discriminator loss: 0.614650309085846, acc: 0.4765625] [gan loss: 0.954786, acc: 0.140625]\n",
            "2978: [discriminator loss: 0.7035132646560669, acc: 0.1640625] [gan loss: 1.551934, acc: 0.000000]\n",
            "2979: [discriminator loss: 0.6277057528495789, acc: 0.40625] [gan loss: 0.928612, acc: 0.187500]\n",
            "2980: [discriminator loss: 0.6520209312438965, acc: 0.1796875] [gan loss: 1.612033, acc: 0.000000]\n",
            "2981: [discriminator loss: 0.6164605021476746, acc: 0.453125] [gan loss: 0.938212, acc: 0.265625]\n",
            "2982: [discriminator loss: 0.6729120016098022, acc: 0.1640625] [gan loss: 1.631594, acc: 0.000000]\n",
            "2983: [discriminator loss: 0.5947329998016357, acc: 0.46875] [gan loss: 0.960204, acc: 0.187500]\n",
            "2984: [discriminator loss: 0.6452661156654358, acc: 0.1875] [gan loss: 1.438653, acc: 0.000000]\n",
            "2985: [discriminator loss: 0.6059163808822632, acc: 0.4296875] [gan loss: 0.940009, acc: 0.203125]\n",
            "2986: [discriminator loss: 0.647060751914978, acc: 0.25] [gan loss: 1.459309, acc: 0.000000]\n",
            "2987: [discriminator loss: 0.6023792624473572, acc: 0.453125] [gan loss: 0.936094, acc: 0.140625]\n",
            "2988: [discriminator loss: 0.6607576608657837, acc: 0.1796875] [gan loss: 1.535411, acc: 0.000000]\n",
            "2989: [discriminator loss: 0.6122406721115112, acc: 0.453125] [gan loss: 0.937704, acc: 0.156250]\n",
            "2990: [discriminator loss: 0.6783702969551086, acc: 0.140625] [gan loss: 1.702954, acc: 0.000000]\n",
            "2991: [discriminator loss: 0.6045622825622559, acc: 0.46875] [gan loss: 0.991152, acc: 0.265625]\n",
            "2992: [discriminator loss: 0.6642704606056213, acc: 0.203125] [gan loss: 1.486033, acc: 0.015625]\n",
            "2993: [discriminator loss: 0.575485110282898, acc: 0.453125] [gan loss: 1.065297, acc: 0.046875]\n",
            "2994: [discriminator loss: 0.6290403008460999, acc: 0.2734375] [gan loss: 1.527793, acc: 0.015625]\n",
            "2995: [discriminator loss: 0.633812427520752, acc: 0.375] [gan loss: 1.049220, acc: 0.062500]\n",
            "2996: [discriminator loss: 0.6510952711105347, acc: 0.25] [gan loss: 1.287147, acc: 0.078125]\n",
            "2997: [discriminator loss: 0.6496564149856567, acc: 0.2890625] [gan loss: 1.337444, acc: 0.000000]\n",
            "2998: [discriminator loss: 0.6143442988395691, acc: 0.34375] [gan loss: 1.206186, acc: 0.046875]\n",
            "2999: [discriminator loss: 0.5992560386657715, acc: 0.3203125] [gan loss: 1.334857, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "3000: [discriminator loss: 0.6430150270462036, acc: 0.3125] [gan loss: 1.365736, acc: 0.015625]\n",
            "3001: [discriminator loss: 0.6442784667015076, acc: 0.3203125] [gan loss: 1.379258, acc: 0.015625]\n",
            "3002: [discriminator loss: 0.625117838382721, acc: 0.3515625] [gan loss: 1.277740, acc: 0.015625]\n",
            "3003: [discriminator loss: 0.6093671917915344, acc: 0.375] [gan loss: 1.214195, acc: 0.031250]\n",
            "3004: [discriminator loss: 0.6276209354400635, acc: 0.34375] [gan loss: 1.414490, acc: 0.000000]\n",
            "3005: [discriminator loss: 0.605641782283783, acc: 0.390625] [gan loss: 1.236040, acc: 0.031250]\n",
            "3006: [discriminator loss: 0.6045078635215759, acc: 0.296875] [gan loss: 1.506987, acc: 0.015625]\n",
            "3007: [discriminator loss: 0.6277526617050171, acc: 0.4453125] [gan loss: 0.929052, acc: 0.171875]\n",
            "3008: [discriminator loss: 0.6897645592689514, acc: 0.1015625] [gan loss: 1.982646, acc: 0.000000]\n",
            "3009: [discriminator loss: 0.6711046099662781, acc: 0.484375] [gan loss: 0.606609, acc: 0.687500]\n",
            "3010: [discriminator loss: 0.7914779186248779, acc: 0.03125] [gan loss: 1.802833, acc: 0.000000]\n",
            "3011: [discriminator loss: 0.6618314385414124, acc: 0.4921875] [gan loss: 0.716379, acc: 0.500000]\n",
            "3012: [discriminator loss: 0.6862789988517761, acc: 0.125] [gan loss: 1.481101, acc: 0.015625]\n",
            "3013: [discriminator loss: 0.6647011041641235, acc: 0.4140625] [gan loss: 1.046533, acc: 0.062500]\n",
            "3014: [discriminator loss: 0.6671924591064453, acc: 0.1640625] [gan loss: 1.520269, acc: 0.031250]\n",
            "3015: [discriminator loss: 0.5757832527160645, acc: 0.4453125] [gan loss: 0.986593, acc: 0.109375]\n",
            "3016: [discriminator loss: 0.6602004170417786, acc: 0.140625] [gan loss: 1.519031, acc: 0.031250]\n",
            "3017: [discriminator loss: 0.6205045580863953, acc: 0.453125] [gan loss: 1.049015, acc: 0.062500]\n",
            "3018: [discriminator loss: 0.6546475291252136, acc: 0.1875] [gan loss: 1.477062, acc: 0.000000]\n",
            "3019: [discriminator loss: 0.5969071388244629, acc: 0.4765625] [gan loss: 0.927125, acc: 0.187500]\n",
            "3020: [discriminator loss: 0.6533181667327881, acc: 0.1328125] [gan loss: 1.505107, acc: 0.031250]\n",
            "3021: [discriminator loss: 0.6382263898849487, acc: 0.40625] [gan loss: 1.025850, acc: 0.078125]\n",
            "3022: [discriminator loss: 0.6002844572067261, acc: 0.265625] [gan loss: 1.478473, acc: 0.031250]\n",
            "3023: [discriminator loss: 0.586265504360199, acc: 0.421875] [gan loss: 0.933516, acc: 0.156250]\n",
            "3024: [discriminator loss: 0.6971731781959534, acc: 0.1640625] [gan loss: 1.625685, acc: 0.015625]\n",
            "3025: [discriminator loss: 0.5852407217025757, acc: 0.4765625] [gan loss: 0.825524, acc: 0.250000]\n",
            "3026: [discriminator loss: 0.6705852746963501, acc: 0.1328125] [gan loss: 1.687134, acc: 0.000000]\n",
            "3027: [discriminator loss: 0.6429519057273865, acc: 0.453125] [gan loss: 0.797280, acc: 0.328125]\n",
            "3028: [discriminator loss: 0.6835899353027344, acc: 0.125] [gan loss: 1.563588, acc: 0.000000]\n",
            "3029: [discriminator loss: 0.6082814931869507, acc: 0.4453125] [gan loss: 0.963385, acc: 0.156250]\n",
            "3030: [discriminator loss: 0.6251040697097778, acc: 0.2109375] [gan loss: 1.545063, acc: 0.000000]\n",
            "3031: [discriminator loss: 0.5969407558441162, acc: 0.421875] [gan loss: 0.905797, acc: 0.250000]\n",
            "3032: [discriminator loss: 0.6795390844345093, acc: 0.15625] [gan loss: 1.552910, acc: 0.000000]\n",
            "3033: [discriminator loss: 0.5950468182563782, acc: 0.4609375] [gan loss: 0.947618, acc: 0.218750]\n",
            "3034: [discriminator loss: 0.6596843004226685, acc: 0.1875] [gan loss: 1.432142, acc: 0.000000]\n",
            "3035: [discriminator loss: 0.5996845960617065, acc: 0.4140625] [gan loss: 1.153326, acc: 0.046875]\n",
            "3036: [discriminator loss: 0.6547093987464905, acc: 0.265625] [gan loss: 1.350688, acc: 0.015625]\n",
            "3037: [discriminator loss: 0.6145875453948975, acc: 0.40625] [gan loss: 0.930434, acc: 0.156250]\n",
            "3038: [discriminator loss: 0.6731747388839722, acc: 0.15625] [gan loss: 1.555334, acc: 0.000000]\n",
            "3039: [discriminator loss: 0.6310625076293945, acc: 0.4453125] [gan loss: 0.922628, acc: 0.218750]\n",
            "3040: [discriminator loss: 0.6914680004119873, acc: 0.1328125] [gan loss: 1.581614, acc: 0.000000]\n",
            "3041: [discriminator loss: 0.5883644223213196, acc: 0.4765625] [gan loss: 0.854244, acc: 0.203125]\n",
            "3042: [discriminator loss: 0.7097280025482178, acc: 0.109375] [gan loss: 1.749392, acc: 0.000000]\n",
            "3043: [discriminator loss: 0.6159363389015198, acc: 0.46875] [gan loss: 0.906365, acc: 0.187500]\n",
            "3044: [discriminator loss: 0.7053972482681274, acc: 0.109375] [gan loss: 1.529991, acc: 0.000000]\n",
            "3045: [discriminator loss: 0.6072882413864136, acc: 0.46875] [gan loss: 1.031279, acc: 0.062500]\n",
            "3046: [discriminator loss: 0.6215788722038269, acc: 0.25] [gan loss: 1.545874, acc: 0.046875]\n",
            "3047: [discriminator loss: 0.6107475757598877, acc: 0.453125] [gan loss: 0.898392, acc: 0.218750]\n",
            "3048: [discriminator loss: 0.6562630534172058, acc: 0.140625] [gan loss: 1.532129, acc: 0.000000]\n",
            "3049: [discriminator loss: 0.5603126883506775, acc: 0.453125] [gan loss: 0.948193, acc: 0.156250]\n",
            "3050: [discriminator loss: 0.6569910645484924, acc: 0.2578125] [gan loss: 1.315473, acc: 0.062500]\n",
            "3051: [discriminator loss: 0.6156278848648071, acc: 0.390625] [gan loss: 1.078930, acc: 0.171875]\n",
            "3052: [discriminator loss: 0.6507688760757446, acc: 0.25] [gan loss: 1.392659, acc: 0.000000]\n",
            "3053: [discriminator loss: 0.650015115737915, acc: 0.34375] [gan loss: 1.100461, acc: 0.109375]\n",
            "3054: [discriminator loss: 0.6387391090393066, acc: 0.28125] [gan loss: 1.507058, acc: 0.015625]\n",
            "3055: [discriminator loss: 0.6074913740158081, acc: 0.3984375] [gan loss: 1.119751, acc: 0.078125]\n",
            "3056: [discriminator loss: 0.6669720411300659, acc: 0.2734375] [gan loss: 1.751768, acc: 0.000000]\n",
            "3057: [discriminator loss: 0.6136612892150879, acc: 0.46875] [gan loss: 0.889996, acc: 0.234375]\n",
            "3058: [discriminator loss: 0.7449566721916199, acc: 0.09375] [gan loss: 1.897996, acc: 0.000000]\n",
            "3059: [discriminator loss: 0.6561381220817566, acc: 0.4921875] [gan loss: 0.591747, acc: 0.734375]\n",
            "3060: [discriminator loss: 0.8183074593544006, acc: 0.0078125] [gan loss: 1.877950, acc: 0.000000]\n",
            "3061: [discriminator loss: 0.6226072311401367, acc: 0.4765625] [gan loss: 0.822087, acc: 0.312500]\n",
            "3062: [discriminator loss: 0.6825453639030457, acc: 0.140625] [gan loss: 1.411773, acc: 0.000000]\n",
            "3063: [discriminator loss: 0.6029423475265503, acc: 0.40625] [gan loss: 0.988421, acc: 0.062500]\n",
            "3064: [discriminator loss: 0.6332359313964844, acc: 0.25] [gan loss: 1.323162, acc: 0.015625]\n",
            "3065: [discriminator loss: 0.6103078126907349, acc: 0.390625] [gan loss: 1.051436, acc: 0.078125]\n",
            "3066: [discriminator loss: 0.6396287679672241, acc: 0.1953125] [gan loss: 1.431331, acc: 0.015625]\n",
            "3067: [discriminator loss: 0.6104394197463989, acc: 0.4375] [gan loss: 0.904929, acc: 0.156250]\n",
            "3068: [discriminator loss: 0.661634087562561, acc: 0.1875] [gan loss: 1.593301, acc: 0.000000]\n",
            "3069: [discriminator loss: 0.6148605346679688, acc: 0.46875] [gan loss: 0.871023, acc: 0.203125]\n",
            "3070: [discriminator loss: 0.6202554702758789, acc: 0.2109375] [gan loss: 1.607633, acc: 0.000000]\n",
            "3071: [discriminator loss: 0.572155773639679, acc: 0.4609375] [gan loss: 0.991255, acc: 0.171875]\n",
            "3072: [discriminator loss: 0.6933633685112, acc: 0.171875] [gan loss: 1.597194, acc: 0.000000]\n",
            "3073: [discriminator loss: 0.6060776114463806, acc: 0.4921875] [gan loss: 0.792698, acc: 0.359375]\n",
            "3074: [discriminator loss: 0.7008053064346313, acc: 0.1328125] [gan loss: 1.618570, acc: 0.015625]\n",
            "3075: [discriminator loss: 0.6047464609146118, acc: 0.4765625] [gan loss: 0.826305, acc: 0.312500]\n",
            "3076: [discriminator loss: 0.7557003498077393, acc: 0.0390625] [gan loss: 1.718114, acc: 0.000000]\n",
            "3077: [discriminator loss: 0.616550087928772, acc: 0.46875] [gan loss: 0.915099, acc: 0.265625]\n",
            "3078: [discriminator loss: 0.6757378578186035, acc: 0.1953125] [gan loss: 1.384015, acc: 0.031250]\n",
            "3079: [discriminator loss: 0.6087461709976196, acc: 0.3828125] [gan loss: 1.170019, acc: 0.031250]\n",
            "3080: [discriminator loss: 0.6418693661689758, acc: 0.2421875] [gan loss: 1.392493, acc: 0.031250]\n",
            "3081: [discriminator loss: 0.6162452101707458, acc: 0.390625] [gan loss: 1.189287, acc: 0.031250]\n",
            "3082: [discriminator loss: 0.6679287552833557, acc: 0.3203125] [gan loss: 1.271658, acc: 0.000000]\n",
            "3083: [discriminator loss: 0.6087932586669922, acc: 0.359375] [gan loss: 1.379942, acc: 0.031250]\n",
            "3084: [discriminator loss: 0.6439152359962463, acc: 0.296875] [gan loss: 1.308987, acc: 0.000000]\n",
            "3085: [discriminator loss: 0.6200211048126221, acc: 0.3671875] [gan loss: 1.223354, acc: 0.015625]\n",
            "3086: [discriminator loss: 0.6261271238327026, acc: 0.2734375] [gan loss: 1.504954, acc: 0.000000]\n",
            "3087: [discriminator loss: 0.6124962568283081, acc: 0.4609375] [gan loss: 0.969425, acc: 0.125000]\n",
            "3088: [discriminator loss: 0.6631181240081787, acc: 0.171875] [gan loss: 1.685958, acc: 0.000000]\n",
            "3089: [discriminator loss: 0.5989514589309692, acc: 0.453125] [gan loss: 0.776409, acc: 0.328125]\n",
            "3090: [discriminator loss: 0.6599332094192505, acc: 0.140625] [gan loss: 1.678183, acc: 0.000000]\n",
            "3091: [discriminator loss: 0.6150088310241699, acc: 0.4765625] [gan loss: 0.754996, acc: 0.484375]\n",
            "3092: [discriminator loss: 0.7028622031211853, acc: 0.0625] [gan loss: 1.764021, acc: 0.015625]\n",
            "3093: [discriminator loss: 0.6225917339324951, acc: 0.4453125] [gan loss: 0.766309, acc: 0.421875]\n",
            "3094: [discriminator loss: 0.7420575022697449, acc: 0.09375] [gan loss: 1.713373, acc: 0.000000]\n",
            "3095: [discriminator loss: 0.6349562406539917, acc: 0.4609375] [gan loss: 0.816891, acc: 0.343750]\n",
            "3096: [discriminator loss: 0.6685642004013062, acc: 0.140625] [gan loss: 1.318857, acc: 0.046875]\n",
            "3097: [discriminator loss: 0.6198005676269531, acc: 0.421875] [gan loss: 1.024000, acc: 0.125000]\n",
            "3098: [discriminator loss: 0.65349280834198, acc: 0.1875] [gan loss: 1.422349, acc: 0.015625]\n",
            "3099: [discriminator loss: 0.6263520121574402, acc: 0.375] [gan loss: 1.194219, acc: 0.046875]\n",
            "3100: [discriminator loss: 0.6355728507041931, acc: 0.265625] [gan loss: 1.289089, acc: 0.046875]\n",
            "3101: [discriminator loss: 0.6081535220146179, acc: 0.3671875] [gan loss: 1.092369, acc: 0.031250]\n",
            "3102: [discriminator loss: 0.6288453340530396, acc: 0.265625] [gan loss: 1.520792, acc: 0.031250]\n",
            "3103: [discriminator loss: 0.6052412986755371, acc: 0.453125] [gan loss: 0.988671, acc: 0.140625]\n",
            "3104: [discriminator loss: 0.6538472771644592, acc: 0.21875] [gan loss: 1.559760, acc: 0.000000]\n",
            "3105: [discriminator loss: 0.6317138075828552, acc: 0.4296875] [gan loss: 0.890764, acc: 0.203125]\n",
            "3106: [discriminator loss: 0.6632276773452759, acc: 0.140625] [gan loss: 1.664028, acc: 0.000000]\n",
            "3107: [discriminator loss: 0.6261985898017883, acc: 0.4765625] [gan loss: 0.706690, acc: 0.437500]\n",
            "3108: [discriminator loss: 0.7533861398696899, acc: 0.0859375] [gan loss: 1.826777, acc: 0.000000]\n",
            "3109: [discriminator loss: 0.6196293830871582, acc: 0.4765625] [gan loss: 0.884423, acc: 0.218750]\n",
            "3110: [discriminator loss: 0.6850367784500122, acc: 0.078125] [gan loss: 1.485661, acc: 0.031250]\n",
            "3111: [discriminator loss: 0.6163366436958313, acc: 0.40625] [gan loss: 1.030678, acc: 0.140625]\n",
            "3112: [discriminator loss: 0.6489973664283752, acc: 0.171875] [gan loss: 1.337946, acc: 0.000000]\n",
            "3113: [discriminator loss: 0.6259890794754028, acc: 0.375] [gan loss: 1.039635, acc: 0.109375]\n",
            "3114: [discriminator loss: 0.6425671577453613, acc: 0.265625] [gan loss: 1.356394, acc: 0.046875]\n",
            "3115: [discriminator loss: 0.6699666976928711, acc: 0.28125] [gan loss: 1.160943, acc: 0.046875]\n",
            "3116: [discriminator loss: 0.6300787925720215, acc: 0.2265625] [gan loss: 1.415909, acc: 0.015625]\n",
            "3117: [discriminator loss: 0.6143319010734558, acc: 0.390625] [gan loss: 1.191081, acc: 0.078125]\n",
            "3118: [discriminator loss: 0.6635122299194336, acc: 0.2109375] [gan loss: 1.492069, acc: 0.000000]\n",
            "3119: [discriminator loss: 0.6077613830566406, acc: 0.4140625] [gan loss: 1.080665, acc: 0.031250]\n",
            "3120: [discriminator loss: 0.6018915772438049, acc: 0.28125] [gan loss: 1.573025, acc: 0.000000]\n",
            "3121: [discriminator loss: 0.6113784313201904, acc: 0.4140625] [gan loss: 1.207212, acc: 0.031250]\n",
            "3122: [discriminator loss: 0.6354333758354187, acc: 0.265625] [gan loss: 1.499436, acc: 0.000000]\n",
            "3123: [discriminator loss: 0.6048973202705383, acc: 0.40625] [gan loss: 1.129963, acc: 0.046875]\n",
            "3124: [discriminator loss: 0.6066351532936096, acc: 0.2578125] [gan loss: 1.521235, acc: 0.015625]\n",
            "3125: [discriminator loss: 0.6175349950790405, acc: 0.4375] [gan loss: 0.886903, acc: 0.140625]\n",
            "3126: [discriminator loss: 0.7125714421272278, acc: 0.078125] [gan loss: 1.961679, acc: 0.000000]\n",
            "3127: [discriminator loss: 0.633333683013916, acc: 0.5] [gan loss: 0.723372, acc: 0.375000]\n",
            "3128: [discriminator loss: 0.7645572423934937, acc: 0.015625] [gan loss: 1.775427, acc: 0.000000]\n",
            "3129: [discriminator loss: 0.5913630723953247, acc: 0.484375] [gan loss: 0.802246, acc: 0.312500]\n",
            "3130: [discriminator loss: 0.6813899874687195, acc: 0.1171875] [gan loss: 1.398007, acc: 0.046875]\n",
            "3131: [discriminator loss: 0.5979698896408081, acc: 0.4375] [gan loss: 0.952849, acc: 0.125000]\n",
            "3132: [discriminator loss: 0.7035776376724243, acc: 0.15625] [gan loss: 1.513046, acc: 0.000000]\n",
            "3133: [discriminator loss: 0.6158434152603149, acc: 0.4140625] [gan loss: 0.965994, acc: 0.218750]\n",
            "3134: [discriminator loss: 0.6460587978363037, acc: 0.1484375] [gan loss: 1.546859, acc: 0.000000]\n",
            "3135: [discriminator loss: 0.6087777614593506, acc: 0.4453125] [gan loss: 1.024192, acc: 0.093750]\n",
            "3136: [discriminator loss: 0.652702808380127, acc: 0.1796875] [gan loss: 1.672860, acc: 0.000000]\n",
            "3137: [discriminator loss: 0.6329938173294067, acc: 0.4609375] [gan loss: 0.861461, acc: 0.296875]\n",
            "3138: [discriminator loss: 0.7047533988952637, acc: 0.0625] [gan loss: 1.605022, acc: 0.000000]\n",
            "3139: [discriminator loss: 0.6414094567298889, acc: 0.4375] [gan loss: 0.871846, acc: 0.187500]\n",
            "3140: [discriminator loss: 0.6403490304946899, acc: 0.203125] [gan loss: 1.421051, acc: 0.031250]\n",
            "3141: [discriminator loss: 0.6825937628746033, acc: 0.34375] [gan loss: 1.117048, acc: 0.093750]\n",
            "3142: [discriminator loss: 0.6465813517570496, acc: 0.2734375] [gan loss: 1.336699, acc: 0.031250]\n",
            "3143: [discriminator loss: 0.5892133712768555, acc: 0.3828125] [gan loss: 1.150763, acc: 0.062500]\n",
            "3144: [discriminator loss: 0.6296138763427734, acc: 0.265625] [gan loss: 1.300626, acc: 0.000000]\n",
            "3145: [discriminator loss: 0.6490353345870972, acc: 0.34375] [gan loss: 1.126741, acc: 0.046875]\n",
            "3146: [discriminator loss: 0.5959495306015015, acc: 0.359375] [gan loss: 1.357838, acc: 0.046875]\n",
            "3147: [discriminator loss: 0.6625034213066101, acc: 0.328125] [gan loss: 1.194282, acc: 0.000000]\n",
            "3148: [discriminator loss: 0.6473684310913086, acc: 0.265625] [gan loss: 1.380446, acc: 0.015625]\n",
            "3149: [discriminator loss: 0.6386120319366455, acc: 0.3984375] [gan loss: 1.132747, acc: 0.062500]\n",
            "3150: [discriminator loss: 0.6676992177963257, acc: 0.2265625] [gan loss: 1.603265, acc: 0.000000]\n",
            "3151: [discriminator loss: 0.6324946880340576, acc: 0.4296875] [gan loss: 0.953154, acc: 0.093750]\n",
            "3152: [discriminator loss: 0.6810451745986938, acc: 0.125] [gan loss: 1.902604, acc: 0.000000]\n",
            "3153: [discriminator loss: 0.6404060125350952, acc: 0.484375] [gan loss: 0.736211, acc: 0.578125]\n",
            "3154: [discriminator loss: 0.7471214532852173, acc: 0.078125] [gan loss: 1.783667, acc: 0.000000]\n",
            "3155: [discriminator loss: 0.627193808555603, acc: 0.46875] [gan loss: 0.781321, acc: 0.437500]\n",
            "3156: [discriminator loss: 0.6740182638168335, acc: 0.125] [gan loss: 1.497185, acc: 0.000000]\n",
            "3157: [discriminator loss: 0.6153565049171448, acc: 0.4375] [gan loss: 0.905708, acc: 0.234375]\n",
            "3158: [discriminator loss: 0.6709609031677246, acc: 0.1640625] [gan loss: 1.377659, acc: 0.000000]\n",
            "3159: [discriminator loss: 0.625077486038208, acc: 0.4375] [gan loss: 0.933057, acc: 0.187500]\n",
            "3160: [discriminator loss: 0.6176773905754089, acc: 0.2421875] [gan loss: 1.286170, acc: 0.031250]\n",
            "3161: [discriminator loss: 0.6476354598999023, acc: 0.375] [gan loss: 1.127405, acc: 0.000000]\n",
            "3162: [discriminator loss: 0.6526609063148499, acc: 0.21875] [gan loss: 1.450292, acc: 0.000000]\n",
            "3163: [discriminator loss: 0.5929819345474243, acc: 0.4375] [gan loss: 1.030469, acc: 0.125000]\n",
            "3164: [discriminator loss: 0.6616613864898682, acc: 0.21875] [gan loss: 1.461378, acc: 0.015625]\n",
            "3165: [discriminator loss: 0.5897812843322754, acc: 0.4375] [gan loss: 0.909096, acc: 0.250000]\n",
            "3166: [discriminator loss: 0.6485336422920227, acc: 0.203125] [gan loss: 1.691890, acc: 0.000000]\n",
            "3167: [discriminator loss: 0.6290352940559387, acc: 0.421875] [gan loss: 0.905672, acc: 0.187500]\n",
            "3168: [discriminator loss: 0.6776877641677856, acc: 0.109375] [gan loss: 1.569775, acc: 0.031250]\n",
            "3169: [discriminator loss: 0.6798226833343506, acc: 0.34375] [gan loss: 1.084543, acc: 0.093750]\n",
            "3170: [discriminator loss: 0.6294674873352051, acc: 0.2890625] [gan loss: 1.296319, acc: 0.031250]\n",
            "3171: [discriminator loss: 0.5974935293197632, acc: 0.3671875] [gan loss: 1.054592, acc: 0.078125]\n",
            "3172: [discriminator loss: 0.6529120206832886, acc: 0.234375] [gan loss: 1.425256, acc: 0.000000]\n",
            "3173: [discriminator loss: 0.6240573525428772, acc: 0.4140625] [gan loss: 1.126782, acc: 0.031250]\n",
            "3174: [discriminator loss: 0.6470216512680054, acc: 0.234375] [gan loss: 1.485973, acc: 0.000000]\n",
            "3175: [discriminator loss: 0.6126677989959717, acc: 0.390625] [gan loss: 1.033197, acc: 0.125000]\n",
            "3176: [discriminator loss: 0.6997528076171875, acc: 0.1171875] [gan loss: 1.727599, acc: 0.000000]\n",
            "3177: [discriminator loss: 0.6496774554252625, acc: 0.4453125] [gan loss: 0.841747, acc: 0.250000]\n",
            "3178: [discriminator loss: 0.6997610330581665, acc: 0.078125] [gan loss: 1.941579, acc: 0.000000]\n",
            "3179: [discriminator loss: 0.6533533334732056, acc: 0.4765625] [gan loss: 0.707801, acc: 0.500000]\n",
            "3180: [discriminator loss: 0.7550223469734192, acc: 0.03125] [gan loss: 1.778562, acc: 0.000000]\n",
            "3181: [discriminator loss: 0.6285965442657471, acc: 0.46875] [gan loss: 0.832185, acc: 0.234375]\n",
            "3182: [discriminator loss: 0.7136095762252808, acc: 0.140625] [gan loss: 1.396004, acc: 0.031250]\n",
            "3183: [discriminator loss: 0.605621337890625, acc: 0.4453125] [gan loss: 1.013867, acc: 0.078125]\n",
            "3184: [discriminator loss: 0.655290961265564, acc: 0.171875] [gan loss: 1.371587, acc: 0.000000]\n",
            "3185: [discriminator loss: 0.6201598644256592, acc: 0.4296875] [gan loss: 0.965335, acc: 0.125000]\n",
            "3186: [discriminator loss: 0.681677520275116, acc: 0.1484375] [gan loss: 1.496889, acc: 0.000000]\n",
            "3187: [discriminator loss: 0.6140791773796082, acc: 0.484375] [gan loss: 0.904670, acc: 0.187500]\n",
            "3188: [discriminator loss: 0.6738564968109131, acc: 0.171875] [gan loss: 1.454756, acc: 0.015625]\n",
            "3189: [discriminator loss: 0.572805643081665, acc: 0.4375] [gan loss: 1.160479, acc: 0.046875]\n",
            "3190: [discriminator loss: 0.6741361618041992, acc: 0.2421875] [gan loss: 1.292980, acc: 0.000000]\n",
            "3191: [discriminator loss: 0.6169250011444092, acc: 0.375] [gan loss: 1.228559, acc: 0.031250]\n",
            "3192: [discriminator loss: 0.6450604796409607, acc: 0.25] [gan loss: 1.417939, acc: 0.015625]\n",
            "3193: [discriminator loss: 0.6315211653709412, acc: 0.40625] [gan loss: 1.142168, acc: 0.078125]\n",
            "3194: [discriminator loss: 0.6611742377281189, acc: 0.1640625] [gan loss: 1.736935, acc: 0.000000]\n",
            "3195: [discriminator loss: 0.5842989683151245, acc: 0.4765625] [gan loss: 0.896222, acc: 0.218750]\n",
            "3196: [discriminator loss: 0.6828744411468506, acc: 0.125] [gan loss: 1.731541, acc: 0.000000]\n",
            "3197: [discriminator loss: 0.6035428047180176, acc: 0.4765625] [gan loss: 0.834259, acc: 0.328125]\n",
            "3198: [discriminator loss: 0.7041178941726685, acc: 0.1171875] [gan loss: 1.653194, acc: 0.000000]\n",
            "3199: [discriminator loss: 0.6628949046134949, acc: 0.4921875] [gan loss: 0.724316, acc: 0.453125]\n",
            "3200: [discriminator loss: 0.735355019569397, acc: 0.0703125] [gan loss: 1.686836, acc: 0.000000]\n",
            "3201: [discriminator loss: 0.6206165552139282, acc: 0.4765625] [gan loss: 0.868041, acc: 0.218750]\n",
            "3202: [discriminator loss: 0.6896501183509827, acc: 0.1328125] [gan loss: 1.446000, acc: 0.000000]\n",
            "3203: [discriminator loss: 0.611881673336029, acc: 0.390625] [gan loss: 0.992236, acc: 0.093750]\n",
            "3204: [discriminator loss: 0.6410049200057983, acc: 0.234375] [gan loss: 1.337567, acc: 0.000000]\n",
            "3205: [discriminator loss: 0.6386265754699707, acc: 0.3984375] [gan loss: 0.955155, acc: 0.109375]\n",
            "3206: [discriminator loss: 0.6602468490600586, acc: 0.203125] [gan loss: 1.361292, acc: 0.015625]\n",
            "3207: [discriminator loss: 0.6393331289291382, acc: 0.359375] [gan loss: 1.078569, acc: 0.109375]\n",
            "3208: [discriminator loss: 0.6480729579925537, acc: 0.1953125] [gan loss: 1.483088, acc: 0.000000]\n",
            "3209: [discriminator loss: 0.5939936637878418, acc: 0.4375] [gan loss: 1.019540, acc: 0.078125]\n",
            "3210: [discriminator loss: 0.6307384371757507, acc: 0.2578125] [gan loss: 1.595211, acc: 0.000000]\n",
            "3211: [discriminator loss: 0.6331263780593872, acc: 0.46875] [gan loss: 0.891967, acc: 0.265625]\n",
            "3212: [discriminator loss: 0.6905161738395691, acc: 0.109375] [gan loss: 1.615233, acc: 0.000000]\n",
            "3213: [discriminator loss: 0.6186742186546326, acc: 0.4765625] [gan loss: 0.778287, acc: 0.328125]\n",
            "3214: [discriminator loss: 0.66936194896698, acc: 0.09375] [gan loss: 1.665516, acc: 0.000000]\n",
            "3215: [discriminator loss: 0.632678747177124, acc: 0.46875] [gan loss: 0.778975, acc: 0.421875]\n",
            "3216: [discriminator loss: 0.7389965057373047, acc: 0.0859375] [gan loss: 1.642714, acc: 0.000000]\n",
            "3217: [discriminator loss: 0.6020685434341431, acc: 0.46875] [gan loss: 0.873228, acc: 0.250000]\n",
            "3218: [discriminator loss: 0.6897826194763184, acc: 0.078125] [gan loss: 1.517597, acc: 0.000000]\n",
            "3219: [discriminator loss: 0.6603008508682251, acc: 0.4140625] [gan loss: 0.943586, acc: 0.203125]\n",
            "3220: [discriminator loss: 0.6196156144142151, acc: 0.2265625] [gan loss: 1.377233, acc: 0.000000]\n",
            "3221: [discriminator loss: 0.5955212116241455, acc: 0.4296875] [gan loss: 1.094006, acc: 0.125000]\n",
            "3222: [discriminator loss: 0.6458829641342163, acc: 0.234375] [gan loss: 1.418842, acc: 0.015625]\n",
            "3223: [discriminator loss: 0.6000576019287109, acc: 0.3671875] [gan loss: 1.352330, acc: 0.000000]\n",
            "3224: [discriminator loss: 0.5840575695037842, acc: 0.4453125] [gan loss: 0.945426, acc: 0.265625]\n",
            "3225: [discriminator loss: 0.7171554565429688, acc: 0.15625] [gan loss: 1.528976, acc: 0.000000]\n",
            "3226: [discriminator loss: 0.6099672317504883, acc: 0.4453125] [gan loss: 0.951468, acc: 0.093750]\n",
            "3227: [discriminator loss: 0.6619688272476196, acc: 0.1640625] [gan loss: 1.529304, acc: 0.000000]\n",
            "3228: [discriminator loss: 0.5920169353485107, acc: 0.453125] [gan loss: 0.950619, acc: 0.203125]\n",
            "3229: [discriminator loss: 0.6882618069648743, acc: 0.15625] [gan loss: 1.702338, acc: 0.000000]\n",
            "3230: [discriminator loss: 0.6414881944656372, acc: 0.484375] [gan loss: 0.836453, acc: 0.296875]\n",
            "3231: [discriminator loss: 0.7092183232307434, acc: 0.0703125] [gan loss: 1.896770, acc: 0.000000]\n",
            "3232: [discriminator loss: 0.632034957408905, acc: 0.484375] [gan loss: 0.766408, acc: 0.421875]\n",
            "3233: [discriminator loss: 0.7016603350639343, acc: 0.1328125] [gan loss: 1.635337, acc: 0.000000]\n",
            "3234: [discriminator loss: 0.618787407875061, acc: 0.484375] [gan loss: 0.905317, acc: 0.250000]\n",
            "3235: [discriminator loss: 0.67653888463974, acc: 0.1796875] [gan loss: 1.393711, acc: 0.000000]\n",
            "3236: [discriminator loss: 0.6187877655029297, acc: 0.3828125] [gan loss: 0.984829, acc: 0.109375]\n",
            "3237: [discriminator loss: 0.6628423929214478, acc: 0.1796875] [gan loss: 1.503420, acc: 0.000000]\n",
            "3238: [discriminator loss: 0.6272276043891907, acc: 0.390625] [gan loss: 1.244931, acc: 0.046875]\n",
            "3239: [discriminator loss: 0.6390393376350403, acc: 0.3203125] [gan loss: 1.191513, acc: 0.093750]\n",
            "3240: [discriminator loss: 0.6361794471740723, acc: 0.328125] [gan loss: 1.285612, acc: 0.031250]\n",
            "3241: [discriminator loss: 0.6581043004989624, acc: 0.3203125] [gan loss: 1.350194, acc: 0.015625]\n",
            "3242: [discriminator loss: 0.6141328811645508, acc: 0.3515625] [gan loss: 1.250172, acc: 0.015625]\n",
            "3243: [discriminator loss: 0.6019735336303711, acc: 0.34375] [gan loss: 1.214956, acc: 0.062500]\n",
            "3244: [discriminator loss: 0.6128193140029907, acc: 0.265625] [gan loss: 1.596918, acc: 0.000000]\n",
            "3245: [discriminator loss: 0.6077078580856323, acc: 0.4296875] [gan loss: 1.047799, acc: 0.046875]\n",
            "3246: [discriminator loss: 0.6525874137878418, acc: 0.1953125] [gan loss: 1.756136, acc: 0.000000]\n",
            "3247: [discriminator loss: 0.6453169584274292, acc: 0.453125] [gan loss: 0.724981, acc: 0.453125]\n",
            "3248: [discriminator loss: 0.7925658226013184, acc: 0.0703125] [gan loss: 1.997798, acc: 0.000000]\n",
            "3249: [discriminator loss: 0.65008544921875, acc: 0.484375] [gan loss: 0.752881, acc: 0.437500]\n",
            "3250: [discriminator loss: 0.7445812225341797, acc: 0.0546875] [gan loss: 1.711457, acc: 0.000000]\n",
            "3251: [discriminator loss: 0.6390801668167114, acc: 0.453125] [gan loss: 0.955948, acc: 0.156250]\n",
            "3252: [discriminator loss: 0.6570034027099609, acc: 0.15625] [gan loss: 1.551716, acc: 0.000000]\n",
            "3253: [discriminator loss: 0.6330382823944092, acc: 0.4296875] [gan loss: 0.922465, acc: 0.171875]\n",
            "3254: [discriminator loss: 0.6645864248275757, acc: 0.1875] [gan loss: 1.418496, acc: 0.000000]\n",
            "3255: [discriminator loss: 0.6195996403694153, acc: 0.390625] [gan loss: 1.184588, acc: 0.015625]\n",
            "3256: [discriminator loss: 0.6503103971481323, acc: 0.265625] [gan loss: 1.290419, acc: 0.031250]\n",
            "3257: [discriminator loss: 0.633993923664093, acc: 0.3671875] [gan loss: 1.101480, acc: 0.015625]\n",
            "3258: [discriminator loss: 0.6065840721130371, acc: 0.28125] [gan loss: 1.340487, acc: 0.000000]\n",
            "3259: [discriminator loss: 0.6714839935302734, acc: 0.3359375] [gan loss: 1.194654, acc: 0.015625]\n",
            "3260: [discriminator loss: 0.636221170425415, acc: 0.34375] [gan loss: 1.311279, acc: 0.031250]\n",
            "3261: [discriminator loss: 0.6521313190460205, acc: 0.3359375] [gan loss: 1.229311, acc: 0.078125]\n",
            "3262: [discriminator loss: 0.631301999092102, acc: 0.34375] [gan loss: 1.272101, acc: 0.031250]\n",
            "3263: [discriminator loss: 0.6317384243011475, acc: 0.3359375] [gan loss: 1.212289, acc: 0.031250]\n",
            "3264: [discriminator loss: 0.6209887266159058, acc: 0.3828125] [gan loss: 1.070197, acc: 0.156250]\n",
            "3265: [discriminator loss: 0.6559511423110962, acc: 0.2421875] [gan loss: 1.606015, acc: 0.000000]\n",
            "3266: [discriminator loss: 0.5969575047492981, acc: 0.46875] [gan loss: 0.826546, acc: 0.312500]\n",
            "3267: [discriminator loss: 0.7252305746078491, acc: 0.0546875] [gan loss: 1.962255, acc: 0.000000]\n",
            "3268: [discriminator loss: 0.6561394929885864, acc: 0.5] [gan loss: 0.667973, acc: 0.593750]\n",
            "3269: [discriminator loss: 0.798397421836853, acc: 0.0078125] [gan loss: 1.753532, acc: 0.000000]\n",
            "3270: [discriminator loss: 0.622154951095581, acc: 0.4765625] [gan loss: 0.821520, acc: 0.328125]\n",
            "3271: [discriminator loss: 0.7032155990600586, acc: 0.109375] [gan loss: 1.404127, acc: 0.015625]\n",
            "3272: [discriminator loss: 0.605360209941864, acc: 0.4453125] [gan loss: 0.984084, acc: 0.125000]\n",
            "3273: [discriminator loss: 0.6053094863891602, acc: 0.2421875] [gan loss: 1.304236, acc: 0.015625]\n",
            "3274: [discriminator loss: 0.6336584091186523, acc: 0.34375] [gan loss: 1.119884, acc: 0.046875]\n",
            "3275: [discriminator loss: 0.655394434928894, acc: 0.2421875] [gan loss: 1.371575, acc: 0.000000]\n",
            "3276: [discriminator loss: 0.5817447304725647, acc: 0.40625] [gan loss: 1.128201, acc: 0.078125]\n",
            "3277: [discriminator loss: 0.6544298529624939, acc: 0.234375] [gan loss: 1.409970, acc: 0.000000]\n",
            "3278: [discriminator loss: 0.6117755174636841, acc: 0.3828125] [gan loss: 1.210840, acc: 0.015625]\n",
            "3279: [discriminator loss: 0.6346182823181152, acc: 0.296875] [gan loss: 1.396829, acc: 0.046875]\n",
            "3280: [discriminator loss: 0.6320074200630188, acc: 0.3828125] [gan loss: 1.191340, acc: 0.046875]\n",
            "3281: [discriminator loss: 0.6311535835266113, acc: 0.2109375] [gan loss: 1.641553, acc: 0.000000]\n",
            "3282: [discriminator loss: 0.5968379974365234, acc: 0.453125] [gan loss: 0.874033, acc: 0.296875]\n",
            "3283: [discriminator loss: 0.6991713047027588, acc: 0.09375] [gan loss: 1.972110, acc: 0.000000]\n",
            "3284: [discriminator loss: 0.612712562084198, acc: 0.5] [gan loss: 0.719770, acc: 0.500000]\n",
            "3285: [discriminator loss: 0.7567448019981384, acc: 0.0546875] [gan loss: 1.807850, acc: 0.000000]\n",
            "3286: [discriminator loss: 0.6399629712104797, acc: 0.5] [gan loss: 0.724617, acc: 0.515625]\n",
            "3287: [discriminator loss: 0.7111135721206665, acc: 0.0703125] [gan loss: 1.493075, acc: 0.015625]\n",
            "3288: [discriminator loss: 0.6296364068984985, acc: 0.40625] [gan loss: 1.069516, acc: 0.031250]\n",
            "3289: [discriminator loss: 0.6386893391609192, acc: 0.296875] [gan loss: 1.369377, acc: 0.015625]\n",
            "3290: [discriminator loss: 0.6099540591239929, acc: 0.328125] [gan loss: 1.292317, acc: 0.062500]\n",
            "3291: [discriminator loss: 0.5915783643722534, acc: 0.390625] [gan loss: 1.328345, acc: 0.015625]\n",
            "3292: [discriminator loss: 0.6132489442825317, acc: 0.3046875] [gan loss: 1.268486, acc: 0.046875]\n",
            "3293: [discriminator loss: 0.6053915619850159, acc: 0.3515625] [gan loss: 1.409155, acc: 0.000000]\n",
            "3294: [discriminator loss: 0.6518568396568298, acc: 0.3125] [gan loss: 1.270292, acc: 0.062500]\n",
            "3295: [discriminator loss: 0.6634712815284729, acc: 0.296875] [gan loss: 1.152985, acc: 0.046875]\n",
            "3296: [discriminator loss: 0.5954693555831909, acc: 0.34375] [gan loss: 1.359531, acc: 0.046875]\n",
            "3297: [discriminator loss: 0.6502081751823425, acc: 0.2890625] [gan loss: 1.345871, acc: 0.000000]\n",
            "3298: [discriminator loss: 0.6418509483337402, acc: 0.3125] [gan loss: 1.285042, acc: 0.031250]\n",
            "3299: [discriminator loss: 0.64605712890625, acc: 0.3046875] [gan loss: 1.342562, acc: 0.046875]\n",
            "3300: [discriminator loss: 0.6256411075592041, acc: 0.3515625] [gan loss: 1.289594, acc: 0.031250]\n",
            "3301: [discriminator loss: 0.6249876022338867, acc: 0.328125] [gan loss: 1.500040, acc: 0.000000]\n",
            "3302: [discriminator loss: 0.6043047904968262, acc: 0.453125] [gan loss: 0.832128, acc: 0.359375]\n",
            "3303: [discriminator loss: 0.7091442346572876, acc: 0.125] [gan loss: 1.874724, acc: 0.000000]\n",
            "3304: [discriminator loss: 0.657035768032074, acc: 0.4921875] [gan loss: 0.617994, acc: 0.703125]\n",
            "3305: [discriminator loss: 0.7750337719917297, acc: 0.0390625] [gan loss: 2.081263, acc: 0.000000]\n",
            "3306: [discriminator loss: 0.6470304727554321, acc: 0.5] [gan loss: 0.724707, acc: 0.468750]\n",
            "3307: [discriminator loss: 0.7250515222549438, acc: 0.0625] [gan loss: 1.511521, acc: 0.015625]\n",
            "3308: [discriminator loss: 0.6034553050994873, acc: 0.4375] [gan loss: 0.998057, acc: 0.171875]\n",
            "3309: [discriminator loss: 0.6498215198516846, acc: 0.2578125] [gan loss: 1.287093, acc: 0.000000]\n",
            "3310: [discriminator loss: 0.6212013959884644, acc: 0.3671875] [gan loss: 1.155846, acc: 0.031250]\n",
            "3311: [discriminator loss: 0.662010133266449, acc: 0.2109375] [gan loss: 1.528519, acc: 0.031250]\n",
            "3312: [discriminator loss: 0.6199759244918823, acc: 0.390625] [gan loss: 0.964607, acc: 0.187500]\n",
            "3313: [discriminator loss: 0.6377623081207275, acc: 0.2265625] [gan loss: 1.313102, acc: 0.046875]\n",
            "3314: [discriminator loss: 0.6212137937545776, acc: 0.3671875] [gan loss: 1.104823, acc: 0.093750]\n",
            "3315: [discriminator loss: 0.6121660470962524, acc: 0.265625] [gan loss: 1.376184, acc: 0.000000]\n",
            "3316: [discriminator loss: 0.6014825701713562, acc: 0.40625] [gan loss: 1.069975, acc: 0.062500]\n",
            "3317: [discriminator loss: 0.6308419704437256, acc: 0.265625] [gan loss: 1.451672, acc: 0.000000]\n",
            "3318: [discriminator loss: 0.6149647831916809, acc: 0.3984375] [gan loss: 1.005989, acc: 0.078125]\n",
            "3319: [discriminator loss: 0.6675899028778076, acc: 0.171875] [gan loss: 1.775443, acc: 0.000000]\n",
            "3320: [discriminator loss: 0.6194860339164734, acc: 0.453125] [gan loss: 0.844750, acc: 0.250000]\n",
            "3321: [discriminator loss: 0.7139521837234497, acc: 0.0703125] [gan loss: 2.002924, acc: 0.000000]\n",
            "3322: [discriminator loss: 0.614502489566803, acc: 0.484375] [gan loss: 0.812474, acc: 0.328125]\n",
            "3323: [discriminator loss: 0.6808513402938843, acc: 0.1328125] [gan loss: 1.782992, acc: 0.000000]\n",
            "3324: [discriminator loss: 0.6007516384124756, acc: 0.4375] [gan loss: 0.871261, acc: 0.250000]\n",
            "3325: [discriminator loss: 0.6529896259307861, acc: 0.171875] [gan loss: 1.659394, acc: 0.015625]\n",
            "3326: [discriminator loss: 0.6295439004898071, acc: 0.453125] [gan loss: 0.854976, acc: 0.312500]\n",
            "3327: [discriminator loss: 0.6760513782501221, acc: 0.09375] [gan loss: 1.676455, acc: 0.000000]\n",
            "3328: [discriminator loss: 0.6292970776557922, acc: 0.46875] [gan loss: 0.842059, acc: 0.328125]\n",
            "3329: [discriminator loss: 0.7208715677261353, acc: 0.1015625] [gan loss: 1.685210, acc: 0.000000]\n",
            "3330: [discriminator loss: 0.5974678993225098, acc: 0.4609375] [gan loss: 0.948771, acc: 0.109375]\n",
            "3331: [discriminator loss: 0.6541812419891357, acc: 0.1484375] [gan loss: 1.576438, acc: 0.000000]\n",
            "3332: [discriminator loss: 0.6249160170555115, acc: 0.453125] [gan loss: 0.845518, acc: 0.312500]\n",
            "3333: [discriminator loss: 0.6891169548034668, acc: 0.125] [gan loss: 1.581320, acc: 0.000000]\n",
            "3334: [discriminator loss: 0.6194139719009399, acc: 0.4140625] [gan loss: 0.941727, acc: 0.156250]\n",
            "3335: [discriminator loss: 0.6598888635635376, acc: 0.1875] [gan loss: 1.359427, acc: 0.000000]\n",
            "3336: [discriminator loss: 0.6026744842529297, acc: 0.375] [gan loss: 1.139779, acc: 0.109375]\n",
            "3337: [discriminator loss: 0.6329407095909119, acc: 0.28125] [gan loss: 1.408474, acc: 0.031250]\n",
            "3338: [discriminator loss: 0.5759820342063904, acc: 0.4375] [gan loss: 1.311921, acc: 0.062500]\n",
            "3339: [discriminator loss: 0.6034238338470459, acc: 0.3515625] [gan loss: 1.388498, acc: 0.015625]\n",
            "3340: [discriminator loss: 0.5675991177558899, acc: 0.421875] [gan loss: 1.145576, acc: 0.015625]\n",
            "3341: [discriminator loss: 0.6329279541969299, acc: 0.265625] [gan loss: 1.559635, acc: 0.000000]\n",
            "3342: [discriminator loss: 0.591432511806488, acc: 0.4609375] [gan loss: 0.969292, acc: 0.109375]\n",
            "3343: [discriminator loss: 0.6480584144592285, acc: 0.1328125] [gan loss: 2.077526, acc: 0.000000]\n",
            "3344: [discriminator loss: 0.6447882056236267, acc: 0.4609375] [gan loss: 0.687952, acc: 0.578125]\n",
            "3345: [discriminator loss: 0.7374117970466614, acc: 0.078125] [gan loss: 2.009194, acc: 0.000000]\n",
            "3346: [discriminator loss: 0.6382845640182495, acc: 0.484375] [gan loss: 0.665472, acc: 0.609375]\n",
            "3347: [discriminator loss: 0.7778076529502869, acc: 0.0625] [gan loss: 1.579542, acc: 0.000000]\n",
            "3348: [discriminator loss: 0.6457233428955078, acc: 0.453125] [gan loss: 0.879706, acc: 0.250000]\n",
            "3349: [discriminator loss: 0.6861491203308105, acc: 0.140625] [gan loss: 1.475368, acc: 0.000000]\n",
            "3350: [discriminator loss: 0.6097376942634583, acc: 0.375] [gan loss: 1.185101, acc: 0.015625]\n",
            "3351: [discriminator loss: 0.6244903802871704, acc: 0.3125] [gan loss: 1.289297, acc: 0.062500]\n",
            "3352: [discriminator loss: 0.6106051206588745, acc: 0.3515625] [gan loss: 0.994944, acc: 0.140625]\n",
            "3353: [discriminator loss: 0.6603479385375977, acc: 0.1953125] [gan loss: 1.461579, acc: 0.000000]\n",
            "3354: [discriminator loss: 0.6439810395240784, acc: 0.34375] [gan loss: 1.130814, acc: 0.015625]\n",
            "3355: [discriminator loss: 0.6667313575744629, acc: 0.2265625] [gan loss: 1.593981, acc: 0.000000]\n",
            "3356: [discriminator loss: 0.6053345203399658, acc: 0.46875] [gan loss: 0.932754, acc: 0.187500]\n",
            "3357: [discriminator loss: 0.692797064781189, acc: 0.1328125] [gan loss: 1.840874, acc: 0.000000]\n",
            "3358: [discriminator loss: 0.5717977285385132, acc: 0.46875] [gan loss: 0.859061, acc: 0.265625]\n",
            "3359: [discriminator loss: 0.7111523747444153, acc: 0.0859375] [gan loss: 1.661353, acc: 0.000000]\n",
            "3360: [discriminator loss: 0.6171400547027588, acc: 0.5] [gan loss: 0.805310, acc: 0.234375]\n",
            "3361: [discriminator loss: 0.7017476558685303, acc: 0.0703125] [gan loss: 1.692475, acc: 0.000000]\n",
            "3362: [discriminator loss: 0.6260484457015991, acc: 0.4453125] [gan loss: 0.923394, acc: 0.218750]\n",
            "3363: [discriminator loss: 0.693697452545166, acc: 0.125] [gan loss: 1.547943, acc: 0.000000]\n",
            "3364: [discriminator loss: 0.6306926608085632, acc: 0.4453125] [gan loss: 0.843846, acc: 0.265625]\n",
            "3365: [discriminator loss: 0.6676085591316223, acc: 0.1015625] [gan loss: 1.584248, acc: 0.000000]\n",
            "3366: [discriminator loss: 0.6421355605125427, acc: 0.46875] [gan loss: 0.840237, acc: 0.296875]\n",
            "3367: [discriminator loss: 0.7255394458770752, acc: 0.109375] [gan loss: 1.516355, acc: 0.000000]\n",
            "3368: [discriminator loss: 0.6068739891052246, acc: 0.4296875] [gan loss: 0.881968, acc: 0.281250]\n",
            "3369: [discriminator loss: 0.7397841215133667, acc: 0.1015625] [gan loss: 1.572057, acc: 0.000000]\n",
            "3370: [discriminator loss: 0.6481132507324219, acc: 0.4375] [gan loss: 0.854101, acc: 0.250000]\n",
            "3371: [discriminator loss: 0.6681769490242004, acc: 0.140625] [gan loss: 1.492591, acc: 0.015625]\n",
            "3372: [discriminator loss: 0.5907918810844421, acc: 0.4765625] [gan loss: 0.888016, acc: 0.234375]\n",
            "3373: [discriminator loss: 0.6595097184181213, acc: 0.171875] [gan loss: 1.479115, acc: 0.015625]\n",
            "3374: [discriminator loss: 0.6648765802383423, acc: 0.4140625] [gan loss: 0.951881, acc: 0.140625]\n",
            "3375: [discriminator loss: 0.6735891103744507, acc: 0.1640625] [gan loss: 1.597588, acc: 0.000000]\n",
            "3376: [discriminator loss: 0.6143283247947693, acc: 0.4609375] [gan loss: 0.915892, acc: 0.265625]\n",
            "3377: [discriminator loss: 0.6697279214859009, acc: 0.1875] [gan loss: 1.536682, acc: 0.015625]\n",
            "3378: [discriminator loss: 0.6341821551322937, acc: 0.421875] [gan loss: 0.955487, acc: 0.078125]\n",
            "3379: [discriminator loss: 0.7019194960594177, acc: 0.15625] [gan loss: 1.598201, acc: 0.000000]\n",
            "3380: [discriminator loss: 0.6107686758041382, acc: 0.3984375] [gan loss: 0.932131, acc: 0.328125]\n",
            "3381: [discriminator loss: 0.6838952302932739, acc: 0.171875] [gan loss: 1.529485, acc: 0.000000]\n",
            "3382: [discriminator loss: 0.6322557330131531, acc: 0.4453125] [gan loss: 1.027442, acc: 0.109375]\n",
            "3383: [discriminator loss: 0.6717049479484558, acc: 0.1640625] [gan loss: 1.670630, acc: 0.000000]\n",
            "3384: [discriminator loss: 0.6433031558990479, acc: 0.4296875] [gan loss: 1.081268, acc: 0.062500]\n",
            "3385: [discriminator loss: 0.6199759244918823, acc: 0.265625] [gan loss: 1.422567, acc: 0.046875]\n",
            "3386: [discriminator loss: 0.5960971713066101, acc: 0.375] [gan loss: 1.303908, acc: 0.015625]\n",
            "3387: [discriminator loss: 0.6209139227867126, acc: 0.3203125] [gan loss: 1.355453, acc: 0.015625]\n",
            "3388: [discriminator loss: 0.6283771395683289, acc: 0.3671875] [gan loss: 1.280143, acc: 0.000000]\n",
            "3389: [discriminator loss: 0.5944418907165527, acc: 0.3671875] [gan loss: 1.398806, acc: 0.000000]\n",
            "3390: [discriminator loss: 0.5934180617332458, acc: 0.375] [gan loss: 1.285072, acc: 0.000000]\n",
            "3391: [discriminator loss: 0.6002522706985474, acc: 0.3125] [gan loss: 1.458550, acc: 0.000000]\n",
            "3392: [discriminator loss: 0.6251533627510071, acc: 0.3984375] [gan loss: 1.085507, acc: 0.109375]\n",
            "3393: [discriminator loss: 0.6441318988800049, acc: 0.2421875] [gan loss: 1.677943, acc: 0.000000]\n",
            "3394: [discriminator loss: 0.6078783273696899, acc: 0.46875] [gan loss: 0.679492, acc: 0.562500]\n",
            "3395: [discriminator loss: 0.790732741355896, acc: 0.09375] [gan loss: 2.006929, acc: 0.000000]\n",
            "3396: [discriminator loss: 0.6581727266311646, acc: 0.5] [gan loss: 0.721768, acc: 0.453125]\n",
            "3397: [discriminator loss: 0.7884975075721741, acc: 0.03125] [gan loss: 1.790077, acc: 0.000000]\n",
            "3398: [discriminator loss: 0.6341745853424072, acc: 0.4765625] [gan loss: 0.824077, acc: 0.296875]\n",
            "3399: [discriminator loss: 0.7360697984695435, acc: 0.140625] [gan loss: 1.611377, acc: 0.000000]\n",
            "3400: [discriminator loss: 0.6132068037986755, acc: 0.4453125] [gan loss: 1.034419, acc: 0.171875]\n",
            "3401: [discriminator loss: 0.6782298684120178, acc: 0.1796875] [gan loss: 1.446957, acc: 0.062500]\n",
            "3402: [discriminator loss: 0.6300583481788635, acc: 0.4140625] [gan loss: 1.034129, acc: 0.093750]\n",
            "3403: [discriminator loss: 0.6951401829719543, acc: 0.140625] [gan loss: 1.523933, acc: 0.000000]\n",
            "3404: [discriminator loss: 0.6285536885261536, acc: 0.4375] [gan loss: 0.910032, acc: 0.187500]\n",
            "3405: [discriminator loss: 0.6842988729476929, acc: 0.109375] [gan loss: 1.581304, acc: 0.000000]\n",
            "3406: [discriminator loss: 0.6156796216964722, acc: 0.421875] [gan loss: 1.063665, acc: 0.031250]\n",
            "3407: [discriminator loss: 0.6851572394371033, acc: 0.1484375] [gan loss: 1.647002, acc: 0.000000]\n",
            "3408: [discriminator loss: 0.6301187872886658, acc: 0.4453125] [gan loss: 0.899829, acc: 0.156250]\n",
            "3409: [discriminator loss: 0.6723888516426086, acc: 0.1484375] [gan loss: 1.478917, acc: 0.000000]\n",
            "3410: [discriminator loss: 0.5979361534118652, acc: 0.46875] [gan loss: 0.962718, acc: 0.156250]\n",
            "3411: [discriminator loss: 0.651565432548523, acc: 0.1875] [gan loss: 1.750632, acc: 0.000000]\n",
            "3412: [discriminator loss: 0.6185257434844971, acc: 0.4375] [gan loss: 1.024272, acc: 0.218750]\n",
            "3413: [discriminator loss: 0.7012553215026855, acc: 0.1796875] [gan loss: 1.621331, acc: 0.000000]\n",
            "3414: [discriminator loss: 0.6437128782272339, acc: 0.4609375] [gan loss: 0.837300, acc: 0.359375]\n",
            "3415: [discriminator loss: 0.711670458316803, acc: 0.109375] [gan loss: 1.602076, acc: 0.000000]\n",
            "3416: [discriminator loss: 0.5739912986755371, acc: 0.46875] [gan loss: 0.899706, acc: 0.265625]\n",
            "3417: [discriminator loss: 0.680931806564331, acc: 0.1328125] [gan loss: 1.688336, acc: 0.000000]\n",
            "3418: [discriminator loss: 0.5958307981491089, acc: 0.453125] [gan loss: 0.895708, acc: 0.234375]\n",
            "3419: [discriminator loss: 0.7041928172111511, acc: 0.125] [gan loss: 1.704222, acc: 0.000000]\n",
            "3420: [discriminator loss: 0.5951337814331055, acc: 0.46875] [gan loss: 0.909166, acc: 0.218750]\n",
            "3421: [discriminator loss: 0.6703766584396362, acc: 0.140625] [gan loss: 1.585374, acc: 0.000000]\n",
            "3422: [discriminator loss: 0.5905433297157288, acc: 0.484375] [gan loss: 0.937335, acc: 0.250000]\n",
            "3423: [discriminator loss: 0.6994825005531311, acc: 0.140625] [gan loss: 1.607065, acc: 0.000000]\n",
            "3424: [discriminator loss: 0.649770975112915, acc: 0.4375] [gan loss: 0.873861, acc: 0.250000]\n",
            "3425: [discriminator loss: 0.6947447061538696, acc: 0.1484375] [gan loss: 1.531743, acc: 0.000000]\n",
            "3426: [discriminator loss: 0.5959611535072327, acc: 0.4765625] [gan loss: 0.950800, acc: 0.171875]\n",
            "3427: [discriminator loss: 0.6587501764297485, acc: 0.125] [gan loss: 1.582423, acc: 0.000000]\n",
            "3428: [discriminator loss: 0.6465688943862915, acc: 0.4140625] [gan loss: 0.909466, acc: 0.265625]\n",
            "3429: [discriminator loss: 0.6671414971351624, acc: 0.1875] [gan loss: 1.510203, acc: 0.000000]\n",
            "3430: [discriminator loss: 0.6243664622306824, acc: 0.4140625] [gan loss: 0.938889, acc: 0.125000]\n",
            "3431: [discriminator loss: 0.7025752067565918, acc: 0.1484375] [gan loss: 1.526852, acc: 0.015625]\n",
            "3432: [discriminator loss: 0.636631429195404, acc: 0.4296875] [gan loss: 0.866191, acc: 0.265625]\n",
            "3433: [discriminator loss: 0.6648415327072144, acc: 0.2109375] [gan loss: 1.399927, acc: 0.000000]\n",
            "3434: [discriminator loss: 0.5913653373718262, acc: 0.4375] [gan loss: 0.980612, acc: 0.156250]\n",
            "3435: [discriminator loss: 0.6257025003433228, acc: 0.25] [gan loss: 1.489869, acc: 0.000000]\n",
            "3436: [discriminator loss: 0.6603978872299194, acc: 0.3828125] [gan loss: 1.149641, acc: 0.031250]\n",
            "3437: [discriminator loss: 0.7038061022758484, acc: 0.1953125] [gan loss: 1.621169, acc: 0.000000]\n",
            "3438: [discriminator loss: 0.645262598991394, acc: 0.4140625] [gan loss: 1.126186, acc: 0.125000]\n",
            "3439: [discriminator loss: 0.6494749784469604, acc: 0.1875] [gan loss: 1.678953, acc: 0.000000]\n",
            "3440: [discriminator loss: 0.5926552414894104, acc: 0.4765625] [gan loss: 0.686164, acc: 0.546875]\n",
            "3441: [discriminator loss: 0.7115626335144043, acc: 0.046875] [gan loss: 1.898790, acc: 0.000000]\n",
            "3442: [discriminator loss: 0.6594523191452026, acc: 0.4921875] [gan loss: 0.749727, acc: 0.468750]\n",
            "3443: [discriminator loss: 0.6970869302749634, acc: 0.109375] [gan loss: 1.536334, acc: 0.000000]\n",
            "3444: [discriminator loss: 0.6069781184196472, acc: 0.4453125] [gan loss: 0.851140, acc: 0.343750]\n",
            "3445: [discriminator loss: 0.6833516359329224, acc: 0.1015625] [gan loss: 1.647977, acc: 0.015625]\n",
            "3446: [discriminator loss: 0.6067335605621338, acc: 0.421875] [gan loss: 1.046887, acc: 0.062500]\n",
            "3447: [discriminator loss: 0.6229052543640137, acc: 0.234375] [gan loss: 1.369693, acc: 0.031250]\n",
            "3448: [discriminator loss: 0.5932782888412476, acc: 0.359375] [gan loss: 1.284084, acc: 0.046875]\n",
            "3449: [discriminator loss: 0.6167253255844116, acc: 0.34375] [gan loss: 1.164941, acc: 0.062500]\n",
            "3450: [discriminator loss: 0.6689144372940063, acc: 0.2109375] [gan loss: 1.421628, acc: 0.015625]\n",
            "3451: [discriminator loss: 0.6338868141174316, acc: 0.3671875] [gan loss: 1.161399, acc: 0.031250]\n",
            "3452: [discriminator loss: 0.6362351775169373, acc: 0.3515625] [gan loss: 1.401434, acc: 0.031250]\n",
            "3453: [discriminator loss: 0.6203384399414062, acc: 0.3203125] [gan loss: 1.296097, acc: 0.015625]\n",
            "3454: [discriminator loss: 0.6579827070236206, acc: 0.296875] [gan loss: 1.323513, acc: 0.031250]\n",
            "3455: [discriminator loss: 0.6362901329994202, acc: 0.296875] [gan loss: 1.252358, acc: 0.015625]\n",
            "3456: [discriminator loss: 0.5965402126312256, acc: 0.3359375] [gan loss: 1.459991, acc: 0.000000]\n",
            "3457: [discriminator loss: 0.6067764759063721, acc: 0.4140625] [gan loss: 1.222138, acc: 0.062500]\n",
            "3458: [discriminator loss: 0.684554398059845, acc: 0.2890625] [gan loss: 1.416780, acc: 0.046875]\n",
            "3459: [discriminator loss: 0.630363941192627, acc: 0.34375] [gan loss: 1.198791, acc: 0.031250]\n",
            "3460: [discriminator loss: 0.6625963449478149, acc: 0.2421875] [gan loss: 1.721186, acc: 0.000000]\n",
            "3461: [discriminator loss: 0.6039692759513855, acc: 0.4609375] [gan loss: 0.829834, acc: 0.281250]\n",
            "3462: [discriminator loss: 0.6939512491226196, acc: 0.0703125] [gan loss: 2.041064, acc: 0.000000]\n",
            "3463: [discriminator loss: 0.652458667755127, acc: 0.5] [gan loss: 0.559448, acc: 0.781250]\n",
            "3464: [discriminator loss: 0.7931802272796631, acc: 0.0390625] [gan loss: 1.926050, acc: 0.015625]\n",
            "3465: [discriminator loss: 0.6334266066551208, acc: 0.5] [gan loss: 0.835485, acc: 0.375000]\n",
            "3466: [discriminator loss: 0.7022602558135986, acc: 0.1015625] [gan loss: 1.579600, acc: 0.000000]\n",
            "3467: [discriminator loss: 0.622359037399292, acc: 0.4609375] [gan loss: 0.950589, acc: 0.187500]\n",
            "3468: [discriminator loss: 0.6323832273483276, acc: 0.265625] [gan loss: 1.508767, acc: 0.000000]\n",
            "3469: [discriminator loss: 0.5990099906921387, acc: 0.4140625] [gan loss: 1.004870, acc: 0.125000]\n",
            "3470: [discriminator loss: 0.6696536540985107, acc: 0.1796875] [gan loss: 1.433031, acc: 0.015625]\n",
            "3471: [discriminator loss: 0.6289936304092407, acc: 0.4140625] [gan loss: 1.019674, acc: 0.140625]\n",
            "3472: [discriminator loss: 0.6943919062614441, acc: 0.15625] [gan loss: 1.731840, acc: 0.000000]\n",
            "3473: [discriminator loss: 0.6392859816551208, acc: 0.453125] [gan loss: 0.821748, acc: 0.312500]\n",
            "3474: [discriminator loss: 0.6986908912658691, acc: 0.09375] [gan loss: 1.600634, acc: 0.015625]\n",
            "3475: [discriminator loss: 0.6155680418014526, acc: 0.4765625] [gan loss: 0.961964, acc: 0.125000]\n",
            "3476: [discriminator loss: 0.6463569402694702, acc: 0.1796875] [gan loss: 1.552924, acc: 0.000000]\n",
            "3477: [discriminator loss: 0.6106213331222534, acc: 0.375] [gan loss: 1.016511, acc: 0.140625]\n",
            "3478: [discriminator loss: 0.6661421060562134, acc: 0.1875] [gan loss: 1.494995, acc: 0.000000]\n",
            "3479: [discriminator loss: 0.5870018005371094, acc: 0.4140625] [gan loss: 0.960247, acc: 0.156250]\n",
            "3480: [discriminator loss: 0.6679338812828064, acc: 0.1640625] [gan loss: 1.541023, acc: 0.000000]\n",
            "3481: [discriminator loss: 0.5848987102508545, acc: 0.421875] [gan loss: 0.988213, acc: 0.156250]\n",
            "3482: [discriminator loss: 0.7089313864707947, acc: 0.1796875] [gan loss: 1.932597, acc: 0.000000]\n",
            "3483: [discriminator loss: 0.6039333343505859, acc: 0.4921875] [gan loss: 0.809032, acc: 0.437500]\n",
            "3484: [discriminator loss: 0.7301640510559082, acc: 0.0703125] [gan loss: 1.775910, acc: 0.000000]\n",
            "3485: [discriminator loss: 0.6450979709625244, acc: 0.4453125] [gan loss: 0.766611, acc: 0.421875]\n",
            "3486: [discriminator loss: 0.6846483945846558, acc: 0.09375] [gan loss: 1.705715, acc: 0.000000]\n",
            "3487: [discriminator loss: 0.6207508444786072, acc: 0.484375] [gan loss: 0.840032, acc: 0.312500]\n",
            "3488: [discriminator loss: 0.669390082359314, acc: 0.1640625] [gan loss: 1.475702, acc: 0.031250]\n",
            "3489: [discriminator loss: 0.6314821839332581, acc: 0.3828125] [gan loss: 1.097711, acc: 0.078125]\n",
            "3490: [discriminator loss: 0.6498147249221802, acc: 0.25] [gan loss: 1.390356, acc: 0.015625]\n",
            "3491: [discriminator loss: 0.6205950379371643, acc: 0.359375] [gan loss: 1.079352, acc: 0.062500]\n",
            "3492: [discriminator loss: 0.6070842742919922, acc: 0.265625] [gan loss: 1.394461, acc: 0.015625]\n",
            "3493: [discriminator loss: 0.638595700263977, acc: 0.3671875] [gan loss: 1.087771, acc: 0.046875]\n",
            "3494: [discriminator loss: 0.6516362428665161, acc: 0.234375] [gan loss: 1.259902, acc: 0.000000]\n",
            "3495: [discriminator loss: 0.6205604672431946, acc: 0.375] [gan loss: 1.116748, acc: 0.031250]\n",
            "3496: [discriminator loss: 0.6700326204299927, acc: 0.234375] [gan loss: 1.325059, acc: 0.046875]\n",
            "3497: [discriminator loss: 0.6199285984039307, acc: 0.359375] [gan loss: 1.143733, acc: 0.109375]\n",
            "3498: [discriminator loss: 0.6236148476600647, acc: 0.2890625] [gan loss: 1.413590, acc: 0.015625]\n",
            "3499: [discriminator loss: 0.6381782293319702, acc: 0.34375] [gan loss: 1.282158, acc: 0.015625]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "3500: [discriminator loss: 0.6451181173324585, acc: 0.28125] [gan loss: 1.289328, acc: 0.000000]\n",
            "3501: [discriminator loss: 0.6268255710601807, acc: 0.296875] [gan loss: 1.519427, acc: 0.000000]\n",
            "3502: [discriminator loss: 0.6461215019226074, acc: 0.3671875] [gan loss: 1.125192, acc: 0.109375]\n",
            "3503: [discriminator loss: 0.6479521989822388, acc: 0.25] [gan loss: 1.510633, acc: 0.015625]\n",
            "3504: [discriminator loss: 0.6111022233963013, acc: 0.453125] [gan loss: 0.849397, acc: 0.296875]\n",
            "3505: [discriminator loss: 0.7369681596755981, acc: 0.1171875] [gan loss: 2.130880, acc: 0.000000]\n",
            "3506: [discriminator loss: 0.6752094626426697, acc: 0.4921875] [gan loss: 0.609050, acc: 0.703125]\n",
            "3507: [discriminator loss: 0.8486014008522034, acc: 0.0] [gan loss: 1.891546, acc: 0.000000]\n",
            "3508: [discriminator loss: 0.6408402323722839, acc: 0.484375] [gan loss: 0.833233, acc: 0.281250]\n",
            "3509: [discriminator loss: 0.7158929109573364, acc: 0.0859375] [gan loss: 1.854084, acc: 0.000000]\n",
            "3510: [discriminator loss: 0.6136680841445923, acc: 0.46875] [gan loss: 0.852691, acc: 0.296875]\n",
            "3511: [discriminator loss: 0.7107584476470947, acc: 0.1171875] [gan loss: 1.560854, acc: 0.000000]\n",
            "3512: [discriminator loss: 0.6363544464111328, acc: 0.40625] [gan loss: 1.128052, acc: 0.062500]\n",
            "3513: [discriminator loss: 0.6855827569961548, acc: 0.1796875] [gan loss: 1.320236, acc: 0.015625]\n",
            "3514: [discriminator loss: 0.6492967009544373, acc: 0.3671875] [gan loss: 1.002766, acc: 0.109375]\n",
            "3515: [discriminator loss: 0.6812819242477417, acc: 0.1484375] [gan loss: 1.445233, acc: 0.000000]\n",
            "3516: [discriminator loss: 0.6024788618087769, acc: 0.4609375] [gan loss: 0.960294, acc: 0.125000]\n",
            "3517: [discriminator loss: 0.7014274597167969, acc: 0.140625] [gan loss: 1.564430, acc: 0.015625]\n",
            "3518: [discriminator loss: 0.6074594259262085, acc: 0.4453125] [gan loss: 0.893017, acc: 0.281250]\n",
            "3519: [discriminator loss: 0.6427972316741943, acc: 0.265625] [gan loss: 1.314030, acc: 0.031250]\n",
            "3520: [discriminator loss: 0.6548094749450684, acc: 0.3203125] [gan loss: 1.256493, acc: 0.015625]\n",
            "3521: [discriminator loss: 0.6401806473731995, acc: 0.2890625] [gan loss: 1.353747, acc: 0.031250]\n",
            "3522: [discriminator loss: 0.5984880924224854, acc: 0.359375] [gan loss: 1.334897, acc: 0.015625]\n",
            "3523: [discriminator loss: 0.6530581712722778, acc: 0.328125] [gan loss: 1.209806, acc: 0.031250]\n",
            "3524: [discriminator loss: 0.6242252588272095, acc: 0.2890625] [gan loss: 1.572005, acc: 0.000000]\n",
            "3525: [discriminator loss: 0.6161245107650757, acc: 0.4296875] [gan loss: 1.018191, acc: 0.125000]\n",
            "3526: [discriminator loss: 0.7028236985206604, acc: 0.1015625] [gan loss: 2.052217, acc: 0.000000]\n",
            "3527: [discriminator loss: 0.655154287815094, acc: 0.5] [gan loss: 0.565879, acc: 0.734375]\n",
            "3528: [discriminator loss: 0.8515098690986633, acc: 0.0234375] [gan loss: 1.984306, acc: 0.000000]\n",
            "3529: [discriminator loss: 0.6539359092712402, acc: 0.4921875] [gan loss: 0.804672, acc: 0.390625]\n",
            "3530: [discriminator loss: 0.7124098539352417, acc: 0.078125] [gan loss: 1.551876, acc: 0.000000]\n",
            "3531: [discriminator loss: 0.6186755895614624, acc: 0.4609375] [gan loss: 0.735723, acc: 0.453125]\n",
            "3532: [discriminator loss: 0.723770260810852, acc: 0.078125] [gan loss: 1.613260, acc: 0.000000]\n",
            "3533: [discriminator loss: 0.623508632183075, acc: 0.4765625] [gan loss: 0.836062, acc: 0.250000]\n",
            "3534: [discriminator loss: 0.7410768270492554, acc: 0.0625] [gan loss: 1.735227, acc: 0.000000]\n",
            "3535: [discriminator loss: 0.6233962178230286, acc: 0.4453125] [gan loss: 0.821551, acc: 0.328125]\n",
            "3536: [discriminator loss: 0.6720253825187683, acc: 0.1171875] [gan loss: 1.451472, acc: 0.000000]\n",
            "3537: [discriminator loss: 0.6303867697715759, acc: 0.40625] [gan loss: 1.037977, acc: 0.109375]\n",
            "3538: [discriminator loss: 0.6912952661514282, acc: 0.109375] [gan loss: 1.489589, acc: 0.000000]\n",
            "3539: [discriminator loss: 0.6220279932022095, acc: 0.4375] [gan loss: 1.085299, acc: 0.031250]\n",
            "3540: [discriminator loss: 0.675717830657959, acc: 0.1875] [gan loss: 1.612595, acc: 0.000000]\n",
            "3541: [discriminator loss: 0.6258130073547363, acc: 0.4296875] [gan loss: 0.997406, acc: 0.140625]\n",
            "3542: [discriminator loss: 0.6654640436172485, acc: 0.1796875] [gan loss: 1.542429, acc: 0.000000]\n",
            "3543: [discriminator loss: 0.630264401435852, acc: 0.390625] [gan loss: 0.929147, acc: 0.171875]\n",
            "3544: [discriminator loss: 0.633872389793396, acc: 0.1953125] [gan loss: 1.513758, acc: 0.000000]\n",
            "3545: [discriminator loss: 0.6159940958023071, acc: 0.4296875] [gan loss: 1.155646, acc: 0.031250]\n",
            "3546: [discriminator loss: 0.6354607939720154, acc: 0.265625] [gan loss: 1.565101, acc: 0.000000]\n",
            "3547: [discriminator loss: 0.6225103139877319, acc: 0.40625] [gan loss: 1.108967, acc: 0.062500]\n",
            "3548: [discriminator loss: 0.6586607098579407, acc: 0.1953125] [gan loss: 1.527178, acc: 0.015625]\n",
            "3549: [discriminator loss: 0.620444655418396, acc: 0.4375] [gan loss: 0.955459, acc: 0.140625]\n",
            "3550: [discriminator loss: 0.6664664149284363, acc: 0.171875] [gan loss: 1.556174, acc: 0.000000]\n",
            "3551: [discriminator loss: 0.6030353903770447, acc: 0.4765625] [gan loss: 0.917322, acc: 0.281250]\n",
            "3552: [discriminator loss: 0.6419175863265991, acc: 0.171875] [gan loss: 1.568575, acc: 0.000000]\n",
            "3553: [discriminator loss: 0.6332699060440063, acc: 0.4296875] [gan loss: 1.013020, acc: 0.203125]\n",
            "3554: [discriminator loss: 0.694344162940979, acc: 0.1640625] [gan loss: 1.714432, acc: 0.000000]\n",
            "3555: [discriminator loss: 0.6547393798828125, acc: 0.4921875] [gan loss: 0.758654, acc: 0.406250]\n",
            "3556: [discriminator loss: 0.7865257263183594, acc: 0.046875] [gan loss: 2.146963, acc: 0.000000]\n",
            "3557: [discriminator loss: 0.691179633140564, acc: 0.4921875] [gan loss: 0.707564, acc: 0.468750]\n",
            "3558: [discriminator loss: 0.7550203800201416, acc: 0.03125] [gan loss: 1.691720, acc: 0.015625]\n",
            "3559: [discriminator loss: 0.6172059774398804, acc: 0.4921875] [gan loss: 0.792150, acc: 0.375000]\n",
            "3560: [discriminator loss: 0.7055361270904541, acc: 0.140625] [gan loss: 1.548571, acc: 0.000000]\n",
            "3561: [discriminator loss: 0.6242442727088928, acc: 0.4296875] [gan loss: 0.872964, acc: 0.312500]\n",
            "3562: [discriminator loss: 0.7036042213439941, acc: 0.15625] [gan loss: 1.311659, acc: 0.015625]\n",
            "3563: [discriminator loss: 0.5977482199668884, acc: 0.3671875] [gan loss: 1.019867, acc: 0.046875]\n",
            "3564: [discriminator loss: 0.6634052991867065, acc: 0.234375] [gan loss: 1.216920, acc: 0.015625]\n",
            "3565: [discriminator loss: 0.6332566738128662, acc: 0.328125] [gan loss: 1.160368, acc: 0.125000]\n",
            "3566: [discriminator loss: 0.6854537725448608, acc: 0.25] [gan loss: 1.257602, acc: 0.015625]\n",
            "3567: [discriminator loss: 0.6434239149093628, acc: 0.328125] [gan loss: 1.058045, acc: 0.062500]\n",
            "3568: [discriminator loss: 0.6323557496070862, acc: 0.3359375] [gan loss: 1.115218, acc: 0.046875]\n",
            "3569: [discriminator loss: 0.6351867914199829, acc: 0.3125] [gan loss: 1.157531, acc: 0.046875]\n",
            "3570: [discriminator loss: 0.6465471982955933, acc: 0.2890625] [gan loss: 1.384961, acc: 0.015625]\n",
            "3571: [discriminator loss: 0.6111665368080139, acc: 0.34375] [gan loss: 1.404989, acc: 0.015625]\n",
            "3572: [discriminator loss: 0.6349009871482849, acc: 0.234375] [gan loss: 1.558688, acc: 0.000000]\n",
            "3573: [discriminator loss: 0.6521064639091492, acc: 0.3671875] [gan loss: 1.089242, acc: 0.109375]\n",
            "3574: [discriminator loss: 0.630311131477356, acc: 0.203125] [gan loss: 1.919542, acc: 0.000000]\n",
            "3575: [discriminator loss: 0.602969229221344, acc: 0.4921875] [gan loss: 0.596759, acc: 0.765625]\n",
            "3576: [discriminator loss: 0.765623927116394, acc: 0.0390625] [gan loss: 2.122337, acc: 0.000000]\n",
            "3577: [discriminator loss: 0.624323844909668, acc: 0.5] [gan loss: 0.686613, acc: 0.531250]\n",
            "3578: [discriminator loss: 0.7160699367523193, acc: 0.0390625] [gan loss: 1.597827, acc: 0.000000]\n",
            "3579: [discriminator loss: 0.6216586232185364, acc: 0.453125] [gan loss: 0.941236, acc: 0.234375]\n",
            "3580: [discriminator loss: 0.6839590072631836, acc: 0.1328125] [gan loss: 1.564868, acc: 0.000000]\n",
            "3581: [discriminator loss: 0.6321653127670288, acc: 0.40625] [gan loss: 0.961352, acc: 0.140625]\n",
            "3582: [discriminator loss: 0.6438361406326294, acc: 0.1640625] [gan loss: 1.444123, acc: 0.000000]\n",
            "3583: [discriminator loss: 0.5878723859786987, acc: 0.4296875] [gan loss: 1.136173, acc: 0.046875]\n",
            "3584: [discriminator loss: 0.6523936986923218, acc: 0.1875] [gan loss: 1.459396, acc: 0.015625]\n",
            "3585: [discriminator loss: 0.6238409876823425, acc: 0.3515625] [gan loss: 1.092947, acc: 0.062500]\n",
            "3586: [discriminator loss: 0.6380520462989807, acc: 0.2109375] [gan loss: 1.580998, acc: 0.000000]\n",
            "3587: [discriminator loss: 0.6567640900611877, acc: 0.4375] [gan loss: 0.906529, acc: 0.140625]\n",
            "3588: [discriminator loss: 0.6949861645698547, acc: 0.09375] [gan loss: 2.045822, acc: 0.000000]\n",
            "3589: [discriminator loss: 0.6197922229766846, acc: 0.4921875] [gan loss: 0.800820, acc: 0.281250]\n",
            "3590: [discriminator loss: 0.7224968075752258, acc: 0.078125] [gan loss: 1.777473, acc: 0.000000]\n",
            "3591: [discriminator loss: 0.641208827495575, acc: 0.46875] [gan loss: 0.888896, acc: 0.218750]\n",
            "3592: [discriminator loss: 0.7349735498428345, acc: 0.0703125] [gan loss: 1.666527, acc: 0.015625]\n",
            "3593: [discriminator loss: 0.609173595905304, acc: 0.484375] [gan loss: 0.857979, acc: 0.265625]\n",
            "3594: [discriminator loss: 0.6778583526611328, acc: 0.125] [gan loss: 1.648704, acc: 0.000000]\n",
            "3595: [discriminator loss: 0.6146374344825745, acc: 0.4609375] [gan loss: 1.052431, acc: 0.125000]\n",
            "3596: [discriminator loss: 0.7008708715438843, acc: 0.1484375] [gan loss: 1.763611, acc: 0.000000]\n",
            "3597: [discriminator loss: 0.631036639213562, acc: 0.453125] [gan loss: 0.899228, acc: 0.218750]\n",
            "3598: [discriminator loss: 0.6739654541015625, acc: 0.171875] [gan loss: 1.578116, acc: 0.000000]\n",
            "3599: [discriminator loss: 0.6108688712120056, acc: 0.359375] [gan loss: 1.026730, acc: 0.109375]\n",
            "3600: [discriminator loss: 0.6556969881057739, acc: 0.2265625] [gan loss: 1.401834, acc: 0.015625]\n",
            "3601: [discriminator loss: 0.641672670841217, acc: 0.359375] [gan loss: 1.176147, acc: 0.109375]\n",
            "3602: [discriminator loss: 0.618334174156189, acc: 0.3046875] [gan loss: 1.348454, acc: 0.000000]\n",
            "3603: [discriminator loss: 0.6062553524971008, acc: 0.3671875] [gan loss: 1.241196, acc: 0.031250]\n",
            "3604: [discriminator loss: 0.6274991035461426, acc: 0.3203125] [gan loss: 1.207024, acc: 0.015625]\n",
            "3605: [discriminator loss: 0.6555137634277344, acc: 0.2265625] [gan loss: 1.503160, acc: 0.000000]\n",
            "3606: [discriminator loss: 0.6034836769104004, acc: 0.453125] [gan loss: 0.889004, acc: 0.296875]\n",
            "3607: [discriminator loss: 0.74025559425354, acc: 0.140625] [gan loss: 1.931999, acc: 0.000000]\n",
            "3608: [discriminator loss: 0.6416342854499817, acc: 0.46875] [gan loss: 0.668896, acc: 0.593750]\n",
            "3609: [discriminator loss: 0.7768615484237671, acc: 0.0625] [gan loss: 1.899113, acc: 0.000000]\n",
            "3610: [discriminator loss: 0.6084231734275818, acc: 0.4765625] [gan loss: 0.748501, acc: 0.484375]\n",
            "3611: [discriminator loss: 0.7409312725067139, acc: 0.09375] [gan loss: 1.807844, acc: 0.015625]\n",
            "3612: [discriminator loss: 0.6029351949691772, acc: 0.4921875] [gan loss: 0.814798, acc: 0.421875]\n",
            "3613: [discriminator loss: 0.727439284324646, acc: 0.0859375] [gan loss: 1.728495, acc: 0.000000]\n",
            "3614: [discriminator loss: 0.6229164004325867, acc: 0.421875] [gan loss: 0.893529, acc: 0.203125]\n",
            "3615: [discriminator loss: 0.6854491233825684, acc: 0.203125] [gan loss: 1.353285, acc: 0.031250]\n",
            "3616: [discriminator loss: 0.6380023956298828, acc: 0.3125] [gan loss: 1.286667, acc: 0.015625]\n",
            "3617: [discriminator loss: 0.6246665120124817, acc: 0.328125] [gan loss: 1.191364, acc: 0.046875]\n",
            "3618: [discriminator loss: 0.6893757581710815, acc: 0.2734375] [gan loss: 1.479083, acc: 0.015625]\n",
            "3619: [discriminator loss: 0.6120943427085876, acc: 0.3828125] [gan loss: 1.150236, acc: 0.046875]\n",
            "3620: [discriminator loss: 0.676087498664856, acc: 0.2265625] [gan loss: 1.450593, acc: 0.000000]\n",
            "3621: [discriminator loss: 0.6347651481628418, acc: 0.3671875] [gan loss: 1.094201, acc: 0.125000]\n",
            "3622: [discriminator loss: 0.6702690124511719, acc: 0.1796875] [gan loss: 1.422975, acc: 0.015625]\n",
            "3623: [discriminator loss: 0.6603469252586365, acc: 0.40625] [gan loss: 1.065866, acc: 0.078125]\n",
            "3624: [discriminator loss: 0.6767288446426392, acc: 0.1484375] [gan loss: 1.697212, acc: 0.000000]\n",
            "3625: [discriminator loss: 0.6355610489845276, acc: 0.4453125] [gan loss: 0.765263, acc: 0.406250]\n",
            "3626: [discriminator loss: 0.7514996528625488, acc: 0.046875] [gan loss: 2.028971, acc: 0.000000]\n",
            "3627: [discriminator loss: 0.6669425964355469, acc: 0.4921875] [gan loss: 0.695455, acc: 0.593750]\n",
            "3628: [discriminator loss: 0.7674820423126221, acc: 0.0234375] [gan loss: 1.891246, acc: 0.000000]\n",
            "3629: [discriminator loss: 0.678699254989624, acc: 0.4453125] [gan loss: 0.819522, acc: 0.359375]\n",
            "3630: [discriminator loss: 0.7634894847869873, acc: 0.046875] [gan loss: 1.621804, acc: 0.000000]\n",
            "3631: [discriminator loss: 0.6387412548065186, acc: 0.4296875] [gan loss: 1.071548, acc: 0.109375]\n",
            "3632: [discriminator loss: 0.6490918397903442, acc: 0.28125] [gan loss: 1.211115, acc: 0.046875]\n",
            "3633: [discriminator loss: 0.6214629411697388, acc: 0.3359375] [gan loss: 1.106570, acc: 0.078125]\n",
            "3634: [discriminator loss: 0.6401365399360657, acc: 0.2421875] [gan loss: 1.281896, acc: 0.015625]\n",
            "3635: [discriminator loss: 0.6070148944854736, acc: 0.328125] [gan loss: 1.256876, acc: 0.000000]\n",
            "3636: [discriminator loss: 0.6428667306900024, acc: 0.3359375] [gan loss: 1.212252, acc: 0.062500]\n",
            "3637: [discriminator loss: 0.6338855624198914, acc: 0.2890625] [gan loss: 1.222593, acc: 0.015625]\n",
            "3638: [discriminator loss: 0.652398407459259, acc: 0.2578125] [gan loss: 1.420304, acc: 0.000000]\n",
            "3639: [discriminator loss: 0.625887393951416, acc: 0.359375] [gan loss: 1.184862, acc: 0.000000]\n",
            "3640: [discriminator loss: 0.6500481367111206, acc: 0.2578125] [gan loss: 1.612377, acc: 0.015625]\n",
            "3641: [discriminator loss: 0.655217170715332, acc: 0.40625] [gan loss: 0.997507, acc: 0.078125]\n",
            "3642: [discriminator loss: 0.6944889426231384, acc: 0.1640625] [gan loss: 1.837312, acc: 0.000000]\n",
            "3643: [discriminator loss: 0.6284305453300476, acc: 0.484375] [gan loss: 0.623331, acc: 0.718750]\n",
            "3644: [discriminator loss: 0.8115824460983276, acc: 0.015625] [gan loss: 1.945019, acc: 0.000000]\n",
            "3645: [discriminator loss: 0.6800451278686523, acc: 0.4765625] [gan loss: 0.741988, acc: 0.343750]\n",
            "3646: [discriminator loss: 0.7073816061019897, acc: 0.0859375] [gan loss: 1.715196, acc: 0.000000]\n",
            "3647: [discriminator loss: 0.6116570830345154, acc: 0.4453125] [gan loss: 0.941943, acc: 0.109375]\n",
            "3648: [discriminator loss: 0.6778244972229004, acc: 0.140625] [gan loss: 1.407746, acc: 0.015625]\n",
            "3649: [discriminator loss: 0.6477874517440796, acc: 0.390625] [gan loss: 0.938742, acc: 0.203125]\n",
            "3650: [discriminator loss: 0.6795398592948914, acc: 0.1328125] [gan loss: 1.482358, acc: 0.031250]\n",
            "3651: [discriminator loss: 0.6056332588195801, acc: 0.40625] [gan loss: 0.920256, acc: 0.218750]\n",
            "3652: [discriminator loss: 0.6587256193161011, acc: 0.1015625] [gan loss: 1.517129, acc: 0.000000]\n",
            "3653: [discriminator loss: 0.6194499135017395, acc: 0.3984375] [gan loss: 1.112062, acc: 0.109375]\n",
            "3654: [discriminator loss: 0.6585327386856079, acc: 0.234375] [gan loss: 1.578570, acc: 0.000000]\n",
            "3655: [discriminator loss: 0.6243388652801514, acc: 0.4453125] [gan loss: 0.971541, acc: 0.140625]\n",
            "3656: [discriminator loss: 0.6851338148117065, acc: 0.140625] [gan loss: 1.728580, acc: 0.000000]\n",
            "3657: [discriminator loss: 0.6118226051330566, acc: 0.4609375] [gan loss: 0.823078, acc: 0.343750]\n",
            "3658: [discriminator loss: 0.7069743871688843, acc: 0.0859375] [gan loss: 1.713187, acc: 0.000000]\n",
            "3659: [discriminator loss: 0.5805087685585022, acc: 0.484375] [gan loss: 0.880486, acc: 0.281250]\n",
            "3660: [discriminator loss: 0.703252375125885, acc: 0.1015625] [gan loss: 1.762655, acc: 0.015625]\n",
            "3661: [discriminator loss: 0.6460066437721252, acc: 0.40625] [gan loss: 1.017829, acc: 0.062500]\n",
            "3662: [discriminator loss: 0.6493803262710571, acc: 0.1875] [gan loss: 1.427715, acc: 0.015625]\n",
            "3663: [discriminator loss: 0.6681463122367859, acc: 0.3125] [gan loss: 1.198708, acc: 0.046875]\n",
            "3664: [discriminator loss: 0.6185293197631836, acc: 0.2890625] [gan loss: 1.354218, acc: 0.031250]\n",
            "3665: [discriminator loss: 0.6248233914375305, acc: 0.296875] [gan loss: 1.429263, acc: 0.000000]\n",
            "3666: [discriminator loss: 0.6212177276611328, acc: 0.3671875] [gan loss: 1.193839, acc: 0.031250]\n",
            "3667: [discriminator loss: 0.6835692524909973, acc: 0.234375] [gan loss: 1.382075, acc: 0.015625]\n",
            "3668: [discriminator loss: 0.6381497383117676, acc: 0.4375] [gan loss: 0.947627, acc: 0.187500]\n",
            "3669: [discriminator loss: 0.648674488067627, acc: 0.21875] [gan loss: 1.653068, acc: 0.000000]\n",
            "3670: [discriminator loss: 0.6401252746582031, acc: 0.453125] [gan loss: 0.735811, acc: 0.437500]\n",
            "3671: [discriminator loss: 0.7815310955047607, acc: 0.0546875] [gan loss: 2.063190, acc: 0.000000]\n",
            "3672: [discriminator loss: 0.7033639550209045, acc: 0.5] [gan loss: 0.609758, acc: 0.718750]\n",
            "3673: [discriminator loss: 0.8088635206222534, acc: 0.0234375] [gan loss: 1.794172, acc: 0.000000]\n",
            "3674: [discriminator loss: 0.6514634490013123, acc: 0.453125] [gan loss: 0.984750, acc: 0.234375]\n",
            "3675: [discriminator loss: 0.6585382223129272, acc: 0.15625] [gan loss: 1.466470, acc: 0.000000]\n",
            "3676: [discriminator loss: 0.5972696542739868, acc: 0.421875] [gan loss: 1.008203, acc: 0.140625]\n",
            "3677: [discriminator loss: 0.6653254628181458, acc: 0.203125] [gan loss: 1.388099, acc: 0.000000]\n",
            "3678: [discriminator loss: 0.6478427648544312, acc: 0.4140625] [gan loss: 1.046544, acc: 0.078125]\n",
            "3679: [discriminator loss: 0.6536437273025513, acc: 0.2265625] [gan loss: 1.362875, acc: 0.000000]\n",
            "3680: [discriminator loss: 0.6341003179550171, acc: 0.375] [gan loss: 1.188473, acc: 0.093750]\n",
            "3681: [discriminator loss: 0.6279969215393066, acc: 0.296875] [gan loss: 1.498760, acc: 0.000000]\n",
            "3682: [discriminator loss: 0.6160037517547607, acc: 0.3828125] [gan loss: 1.015692, acc: 0.171875]\n",
            "3683: [discriminator loss: 0.6928122639656067, acc: 0.1953125] [gan loss: 1.492972, acc: 0.000000]\n",
            "3684: [discriminator loss: 0.6229285597801208, acc: 0.4140625] [gan loss: 1.018937, acc: 0.187500]\n",
            "3685: [discriminator loss: 0.6595215201377869, acc: 0.15625] [gan loss: 1.636559, acc: 0.000000]\n",
            "3686: [discriminator loss: 0.6493978500366211, acc: 0.4140625] [gan loss: 1.022014, acc: 0.046875]\n",
            "3687: [discriminator loss: 0.6635441780090332, acc: 0.140625] [gan loss: 1.767900, acc: 0.000000]\n",
            "3688: [discriminator loss: 0.6445469856262207, acc: 0.4609375] [gan loss: 0.837856, acc: 0.359375]\n",
            "3689: [discriminator loss: 0.6945875883102417, acc: 0.1171875] [gan loss: 1.622044, acc: 0.000000]\n",
            "3690: [discriminator loss: 0.6406437158584595, acc: 0.46875] [gan loss: 0.760331, acc: 0.437500]\n",
            "3691: [discriminator loss: 0.7475718259811401, acc: 0.109375] [gan loss: 1.782859, acc: 0.000000]\n",
            "3692: [discriminator loss: 0.6193397045135498, acc: 0.484375] [gan loss: 0.769730, acc: 0.406250]\n",
            "3693: [discriminator loss: 0.7521214485168457, acc: 0.09375] [gan loss: 1.766980, acc: 0.015625]\n",
            "3694: [discriminator loss: 0.6130161881446838, acc: 0.46875] [gan loss: 0.855722, acc: 0.265625]\n",
            "3695: [discriminator loss: 0.7063090801239014, acc: 0.1015625] [gan loss: 1.812018, acc: 0.000000]\n",
            "3696: [discriminator loss: 0.6019402146339417, acc: 0.4609375] [gan loss: 0.835722, acc: 0.312500]\n",
            "3697: [discriminator loss: 0.6837871074676514, acc: 0.125] [gan loss: 1.581520, acc: 0.000000]\n",
            "3698: [discriminator loss: 0.6238299608230591, acc: 0.4609375] [gan loss: 0.913517, acc: 0.156250]\n",
            "3699: [discriminator loss: 0.6753644347190857, acc: 0.1328125] [gan loss: 1.584460, acc: 0.000000]\n",
            "3700: [discriminator loss: 0.6305873394012451, acc: 0.4453125] [gan loss: 0.875213, acc: 0.218750]\n",
            "3701: [discriminator loss: 0.6725383400917053, acc: 0.109375] [gan loss: 1.593284, acc: 0.000000]\n",
            "3702: [discriminator loss: 0.6146336793899536, acc: 0.4140625] [gan loss: 0.804153, acc: 0.359375]\n",
            "3703: [discriminator loss: 0.7031729221343994, acc: 0.1015625] [gan loss: 1.631859, acc: 0.000000]\n",
            "3704: [discriminator loss: 0.6328984498977661, acc: 0.390625] [gan loss: 0.955271, acc: 0.171875]\n",
            "3705: [discriminator loss: 0.6549140214920044, acc: 0.1875] [gan loss: 1.418215, acc: 0.015625]\n",
            "3706: [discriminator loss: 0.6201410293579102, acc: 0.3671875] [gan loss: 1.190165, acc: 0.046875]\n",
            "3707: [discriminator loss: 0.6603593230247498, acc: 0.2578125] [gan loss: 1.370215, acc: 0.062500]\n",
            "3708: [discriminator loss: 0.6138025522232056, acc: 0.3984375] [gan loss: 1.078225, acc: 0.125000]\n",
            "3709: [discriminator loss: 0.6655440330505371, acc: 0.2265625] [gan loss: 1.515580, acc: 0.000000]\n",
            "3710: [discriminator loss: 0.6087902784347534, acc: 0.4609375] [gan loss: 0.946626, acc: 0.156250]\n",
            "3711: [discriminator loss: 0.6817750930786133, acc: 0.1484375] [gan loss: 1.886143, acc: 0.000000]\n",
            "3712: [discriminator loss: 0.6065174341201782, acc: 0.4921875] [gan loss: 0.607852, acc: 0.703125]\n",
            "3713: [discriminator loss: 0.8161249756813049, acc: 0.015625] [gan loss: 1.861991, acc: 0.000000]\n",
            "3714: [discriminator loss: 0.6594537496566772, acc: 0.5] [gan loss: 0.658259, acc: 0.562500]\n",
            "3715: [discriminator loss: 0.7832012176513672, acc: 0.0703125] [gan loss: 1.677522, acc: 0.000000]\n",
            "3716: [discriminator loss: 0.6160918474197388, acc: 0.46875] [gan loss: 0.810411, acc: 0.328125]\n",
            "3717: [discriminator loss: 0.7050830125808716, acc: 0.09375] [gan loss: 1.415484, acc: 0.000000]\n",
            "3718: [discriminator loss: 0.6232447624206543, acc: 0.4453125] [gan loss: 0.921900, acc: 0.218750]\n",
            "3719: [discriminator loss: 0.6514475345611572, acc: 0.1640625] [gan loss: 1.282979, acc: 0.062500]\n",
            "3720: [discriminator loss: 0.6559527516365051, acc: 0.296875] [gan loss: 1.180221, acc: 0.031250]\n",
            "3721: [discriminator loss: 0.6641501188278198, acc: 0.234375] [gan loss: 1.463588, acc: 0.015625]\n",
            "3722: [discriminator loss: 0.6140397191047668, acc: 0.4140625] [gan loss: 1.076082, acc: 0.078125]\n",
            "3723: [discriminator loss: 0.6364206075668335, acc: 0.21875] [gan loss: 1.608303, acc: 0.000000]\n",
            "3724: [discriminator loss: 0.5781218409538269, acc: 0.4375] [gan loss: 1.019119, acc: 0.125000]\n",
            "3725: [discriminator loss: 0.6924846768379211, acc: 0.1171875] [gan loss: 1.576171, acc: 0.015625]\n",
            "3726: [discriminator loss: 0.6321734189987183, acc: 0.4140625] [gan loss: 0.945252, acc: 0.250000]\n",
            "3727: [discriminator loss: 0.6646393537521362, acc: 0.15625] [gan loss: 1.506299, acc: 0.000000]\n",
            "3728: [discriminator loss: 0.6251938343048096, acc: 0.390625] [gan loss: 1.081780, acc: 0.093750]\n",
            "3729: [discriminator loss: 0.659864068031311, acc: 0.1953125] [gan loss: 1.511225, acc: 0.000000]\n",
            "3730: [discriminator loss: 0.638275682926178, acc: 0.4140625] [gan loss: 0.999019, acc: 0.203125]\n",
            "3731: [discriminator loss: 0.6810333132743835, acc: 0.1953125] [gan loss: 1.753974, acc: 0.000000]\n",
            "3732: [discriminator loss: 0.6433840394020081, acc: 0.46875] [gan loss: 0.701451, acc: 0.546875]\n",
            "3733: [discriminator loss: 0.7350069284439087, acc: 0.046875] [gan loss: 1.798428, acc: 0.000000]\n",
            "3734: [discriminator loss: 0.6488598585128784, acc: 0.4609375] [gan loss: 0.760817, acc: 0.437500]\n",
            "3735: [discriminator loss: 0.7539801001548767, acc: 0.046875] [gan loss: 1.751199, acc: 0.000000]\n",
            "3736: [discriminator loss: 0.6545813083648682, acc: 0.4453125] [gan loss: 0.693688, acc: 0.546875]\n",
            "3737: [discriminator loss: 0.7408961653709412, acc: 0.0546875] [gan loss: 1.602671, acc: 0.015625]\n",
            "3738: [discriminator loss: 0.6406604647636414, acc: 0.4453125] [gan loss: 1.023112, acc: 0.125000]\n",
            "3739: [discriminator loss: 0.6971471905708313, acc: 0.1171875] [gan loss: 1.642040, acc: 0.000000]\n",
            "3740: [discriminator loss: 0.6290533542633057, acc: 0.4609375] [gan loss: 0.802871, acc: 0.328125]\n",
            "3741: [discriminator loss: 0.7031474113464355, acc: 0.109375] [gan loss: 1.737195, acc: 0.000000]\n",
            "3742: [discriminator loss: 0.601649284362793, acc: 0.4140625] [gan loss: 0.979386, acc: 0.171875]\n",
            "3743: [discriminator loss: 0.6976270079612732, acc: 0.1484375] [gan loss: 1.630021, acc: 0.000000]\n",
            "3744: [discriminator loss: 0.6365723013877869, acc: 0.3828125] [gan loss: 0.938887, acc: 0.125000]\n",
            "3745: [discriminator loss: 0.6554144024848938, acc: 0.1484375] [gan loss: 1.543553, acc: 0.015625]\n",
            "3746: [discriminator loss: 0.6389802694320679, acc: 0.4375] [gan loss: 0.876281, acc: 0.281250]\n",
            "3747: [discriminator loss: 0.7112369537353516, acc: 0.078125] [gan loss: 1.763897, acc: 0.000000]\n",
            "3748: [discriminator loss: 0.642905056476593, acc: 0.5] [gan loss: 0.775786, acc: 0.328125]\n",
            "3749: [discriminator loss: 0.7593050003051758, acc: 0.0546875] [gan loss: 1.726080, acc: 0.000000]\n",
            "3750: [discriminator loss: 0.6195650100708008, acc: 0.4375] [gan loss: 0.963014, acc: 0.140625]\n",
            "3751: [discriminator loss: 0.6803531646728516, acc: 0.1484375] [gan loss: 1.474199, acc: 0.015625]\n",
            "3752: [discriminator loss: 0.5960195064544678, acc: 0.4453125] [gan loss: 0.911312, acc: 0.203125]\n",
            "3753: [discriminator loss: 0.7198103666305542, acc: 0.125] [gan loss: 1.577563, acc: 0.000000]\n",
            "3754: [discriminator loss: 0.6013020277023315, acc: 0.4375] [gan loss: 0.850981, acc: 0.328125]\n",
            "3755: [discriminator loss: 0.6820644736289978, acc: 0.1328125] [gan loss: 1.604615, acc: 0.000000]\n",
            "3756: [discriminator loss: 0.6151229739189148, acc: 0.421875] [gan loss: 0.872235, acc: 0.250000]\n",
            "3757: [discriminator loss: 0.6664409637451172, acc: 0.1953125] [gan loss: 1.617412, acc: 0.000000]\n",
            "3758: [discriminator loss: 0.6101735830307007, acc: 0.4375] [gan loss: 1.022225, acc: 0.109375]\n",
            "3759: [discriminator loss: 0.6440850496292114, acc: 0.203125] [gan loss: 1.597649, acc: 0.000000]\n",
            "3760: [discriminator loss: 0.6323484182357788, acc: 0.3828125] [gan loss: 1.024626, acc: 0.187500]\n",
            "3761: [discriminator loss: 0.7070865631103516, acc: 0.125] [gan loss: 1.717210, acc: 0.000000]\n",
            "3762: [discriminator loss: 0.6074703931808472, acc: 0.4375] [gan loss: 0.917947, acc: 0.187500]\n",
            "3763: [discriminator loss: 0.6743831634521484, acc: 0.1875] [gan loss: 1.652215, acc: 0.000000]\n",
            "3764: [discriminator loss: 0.5955764055252075, acc: 0.4765625] [gan loss: 0.747432, acc: 0.468750]\n",
            "3765: [discriminator loss: 0.7503757476806641, acc: 0.0703125] [gan loss: 1.993464, acc: 0.000000]\n",
            "3766: [discriminator loss: 0.6790348291397095, acc: 0.4765625] [gan loss: 0.681226, acc: 0.546875]\n",
            "3767: [discriminator loss: 0.7546818256378174, acc: 0.0703125] [gan loss: 1.659628, acc: 0.000000]\n",
            "3768: [discriminator loss: 0.645482063293457, acc: 0.4375] [gan loss: 0.916602, acc: 0.171875]\n",
            "3769: [discriminator loss: 0.6904779672622681, acc: 0.125] [gan loss: 1.580571, acc: 0.000000]\n",
            "3770: [discriminator loss: 0.6106922626495361, acc: 0.453125] [gan loss: 0.945303, acc: 0.250000]\n",
            "3771: [discriminator loss: 0.6617865562438965, acc: 0.1875] [gan loss: 1.575308, acc: 0.015625]\n",
            "3772: [discriminator loss: 0.6300960779190063, acc: 0.40625] [gan loss: 1.085903, acc: 0.093750]\n",
            "3773: [discriminator loss: 0.6520475745201111, acc: 0.21875] [gan loss: 1.396338, acc: 0.015625]\n",
            "3774: [discriminator loss: 0.6308416724205017, acc: 0.4140625] [gan loss: 0.994081, acc: 0.171875]\n",
            "3775: [discriminator loss: 0.6488024592399597, acc: 0.1171875] [gan loss: 1.564412, acc: 0.015625]\n",
            "3776: [discriminator loss: 0.6254299879074097, acc: 0.4453125] [gan loss: 0.916330, acc: 0.218750]\n",
            "3777: [discriminator loss: 0.705629825592041, acc: 0.171875] [gan loss: 1.675168, acc: 0.000000]\n",
            "3778: [discriminator loss: 0.647622287273407, acc: 0.4140625] [gan loss: 0.931632, acc: 0.234375]\n",
            "3779: [discriminator loss: 0.6654845476150513, acc: 0.1796875] [gan loss: 1.487590, acc: 0.015625]\n",
            "3780: [discriminator loss: 0.6173301935195923, acc: 0.40625] [gan loss: 0.968090, acc: 0.140625]\n",
            "3781: [discriminator loss: 0.6599039435386658, acc: 0.203125] [gan loss: 1.422577, acc: 0.015625]\n",
            "3782: [discriminator loss: 0.619703471660614, acc: 0.390625] [gan loss: 1.026073, acc: 0.125000]\n",
            "3783: [discriminator loss: 0.7425025105476379, acc: 0.125] [gan loss: 1.796605, acc: 0.000000]\n",
            "3784: [discriminator loss: 0.627979040145874, acc: 0.4921875] [gan loss: 0.686637, acc: 0.546875]\n",
            "3785: [discriminator loss: 0.7222639918327332, acc: 0.0390625] [gan loss: 1.891048, acc: 0.000000]\n",
            "3786: [discriminator loss: 0.6415296196937561, acc: 0.4921875] [gan loss: 0.779163, acc: 0.406250]\n",
            "3787: [discriminator loss: 0.7632516026496887, acc: 0.015625] [gan loss: 1.921092, acc: 0.000000]\n",
            "3788: [discriminator loss: 0.6308985948562622, acc: 0.4453125] [gan loss: 0.815166, acc: 0.406250]\n",
            "3789: [discriminator loss: 0.748540461063385, acc: 0.109375] [gan loss: 1.578065, acc: 0.000000]\n",
            "3790: [discriminator loss: 0.607144832611084, acc: 0.4375] [gan loss: 0.886942, acc: 0.265625]\n",
            "3791: [discriminator loss: 0.6834812164306641, acc: 0.1484375] [gan loss: 1.483993, acc: 0.000000]\n",
            "3792: [discriminator loss: 0.6242989301681519, acc: 0.40625] [gan loss: 1.041055, acc: 0.140625]\n",
            "3793: [discriminator loss: 0.6651806831359863, acc: 0.2109375] [gan loss: 1.344844, acc: 0.031250]\n",
            "3794: [discriminator loss: 0.6397165656089783, acc: 0.359375] [gan loss: 1.120195, acc: 0.046875]\n",
            "3795: [discriminator loss: 0.7070032358169556, acc: 0.1796875] [gan loss: 1.362983, acc: 0.000000]\n",
            "3796: [discriminator loss: 0.6230412721633911, acc: 0.359375] [gan loss: 1.181114, acc: 0.062500]\n",
            "3797: [discriminator loss: 0.651396632194519, acc: 0.2109375] [gan loss: 1.748161, acc: 0.000000]\n",
            "3798: [discriminator loss: 0.6308388710021973, acc: 0.46875] [gan loss: 0.868221, acc: 0.328125]\n",
            "3799: [discriminator loss: 0.7317928075790405, acc: 0.0703125] [gan loss: 1.836630, acc: 0.000000]\n",
            "3800: [discriminator loss: 0.648717999458313, acc: 0.484375] [gan loss: 0.699008, acc: 0.546875]\n",
            "3801: [discriminator loss: 0.7696918249130249, acc: 0.078125] [gan loss: 1.929638, acc: 0.000000]\n",
            "3802: [discriminator loss: 0.6500594019889832, acc: 0.46875] [gan loss: 0.720401, acc: 0.484375]\n",
            "3803: [discriminator loss: 0.7154359221458435, acc: 0.0703125] [gan loss: 1.485303, acc: 0.015625]\n",
            "3804: [discriminator loss: 0.6164343357086182, acc: 0.4296875] [gan loss: 1.007791, acc: 0.187500]\n",
            "3805: [discriminator loss: 0.6652956008911133, acc: 0.2109375] [gan loss: 1.390689, acc: 0.031250]\n",
            "3806: [discriminator loss: 0.6058443188667297, acc: 0.4296875] [gan loss: 1.030483, acc: 0.156250]\n",
            "3807: [discriminator loss: 0.6636126041412354, acc: 0.1875] [gan loss: 1.462643, acc: 0.000000]\n",
            "3808: [discriminator loss: 0.6466948986053467, acc: 0.4375] [gan loss: 0.998073, acc: 0.171875]\n",
            "3809: [discriminator loss: 0.6885901689529419, acc: 0.140625] [gan loss: 1.462820, acc: 0.015625]\n",
            "3810: [discriminator loss: 0.63701993227005, acc: 0.421875] [gan loss: 0.820063, acc: 0.328125]\n",
            "3811: [discriminator loss: 0.7539215683937073, acc: 0.078125] [gan loss: 1.875053, acc: 0.000000]\n",
            "3812: [discriminator loss: 0.6888790130615234, acc: 0.453125] [gan loss: 0.804542, acc: 0.343750]\n",
            "3813: [discriminator loss: 0.6990283727645874, acc: 0.078125] [gan loss: 1.642300, acc: 0.000000]\n",
            "3814: [discriminator loss: 0.6366804838180542, acc: 0.421875] [gan loss: 0.833805, acc: 0.312500]\n",
            "3815: [discriminator loss: 0.7022385597229004, acc: 0.1484375] [gan loss: 1.513539, acc: 0.000000]\n",
            "3816: [discriminator loss: 0.5980345010757446, acc: 0.4765625] [gan loss: 0.932105, acc: 0.218750]\n",
            "3817: [discriminator loss: 0.6732022762298584, acc: 0.125] [gan loss: 1.585634, acc: 0.015625]\n",
            "3818: [discriminator loss: 0.6134335398674011, acc: 0.4453125] [gan loss: 0.853352, acc: 0.343750]\n",
            "3819: [discriminator loss: 0.7178921699523926, acc: 0.0703125] [gan loss: 1.782614, acc: 0.000000]\n",
            "3820: [discriminator loss: 0.6444869637489319, acc: 0.4375] [gan loss: 0.901445, acc: 0.187500]\n",
            "3821: [discriminator loss: 0.6764150857925415, acc: 0.125] [gan loss: 1.539989, acc: 0.000000]\n",
            "3822: [discriminator loss: 0.6286395788192749, acc: 0.4609375] [gan loss: 0.901030, acc: 0.250000]\n",
            "3823: [discriminator loss: 0.6447910070419312, acc: 0.1796875] [gan loss: 1.584254, acc: 0.015625]\n",
            "3824: [discriminator loss: 0.6221963167190552, acc: 0.390625] [gan loss: 1.005969, acc: 0.156250]\n",
            "3825: [discriminator loss: 0.6699662208557129, acc: 0.1640625] [gan loss: 1.472998, acc: 0.000000]\n",
            "3826: [discriminator loss: 0.6277387142181396, acc: 0.375] [gan loss: 1.067674, acc: 0.078125]\n",
            "3827: [discriminator loss: 0.6418584585189819, acc: 0.265625] [gan loss: 1.287268, acc: 0.015625]\n",
            "3828: [discriminator loss: 0.6269762516021729, acc: 0.3828125] [gan loss: 1.239819, acc: 0.062500]\n",
            "3829: [discriminator loss: 0.6303404569625854, acc: 0.25] [gan loss: 1.559904, acc: 0.000000]\n",
            "3830: [discriminator loss: 0.644773542881012, acc: 0.390625] [gan loss: 1.007892, acc: 0.140625]\n",
            "3831: [discriminator loss: 0.6461329460144043, acc: 0.1875] [gan loss: 1.828723, acc: 0.000000]\n",
            "3832: [discriminator loss: 0.6368029117584229, acc: 0.4375] [gan loss: 0.869839, acc: 0.312500]\n",
            "3833: [discriminator loss: 0.7340006828308105, acc: 0.09375] [gan loss: 1.984834, acc: 0.000000]\n",
            "3834: [discriminator loss: 0.6599341630935669, acc: 0.5] [gan loss: 0.700023, acc: 0.531250]\n",
            "3835: [discriminator loss: 0.8228910565376282, acc: 0.0234375] [gan loss: 1.878915, acc: 0.000000]\n",
            "3836: [discriminator loss: 0.6729222536087036, acc: 0.4921875] [gan loss: 0.817143, acc: 0.390625]\n",
            "3837: [discriminator loss: 0.744491457939148, acc: 0.0390625] [gan loss: 1.730368, acc: 0.000000]\n",
            "3838: [discriminator loss: 0.6157690286636353, acc: 0.4296875] [gan loss: 0.928263, acc: 0.218750]\n",
            "3839: [discriminator loss: 0.6952404975891113, acc: 0.140625] [gan loss: 1.601704, acc: 0.000000]\n",
            "3840: [discriminator loss: 0.6410473585128784, acc: 0.4375] [gan loss: 0.964973, acc: 0.171875]\n",
            "3841: [discriminator loss: 0.6521819233894348, acc: 0.1875] [gan loss: 1.432051, acc: 0.015625]\n",
            "3842: [discriminator loss: 0.599814772605896, acc: 0.390625] [gan loss: 1.032082, acc: 0.109375]\n",
            "3843: [discriminator loss: 0.690839409828186, acc: 0.1875] [gan loss: 1.178304, acc: 0.062500]\n",
            "3844: [discriminator loss: 0.6550198793411255, acc: 0.3203125] [gan loss: 1.063496, acc: 0.093750]\n",
            "3845: [discriminator loss: 0.6315897107124329, acc: 0.265625] [gan loss: 1.342374, acc: 0.000000]\n",
            "3846: [discriminator loss: 0.6334108710289001, acc: 0.359375] [gan loss: 1.150125, acc: 0.046875]\n",
            "3847: [discriminator loss: 0.6760354042053223, acc: 0.234375] [gan loss: 1.396692, acc: 0.015625]\n",
            "3848: [discriminator loss: 0.6313408017158508, acc: 0.375] [gan loss: 1.185956, acc: 0.031250]\n",
            "3849: [discriminator loss: 0.6307721734046936, acc: 0.3125] [gan loss: 1.266785, acc: 0.015625]\n",
            "3850: [discriminator loss: 0.6204988360404968, acc: 0.3671875] [gan loss: 1.244519, acc: 0.062500]\n",
            "3851: [discriminator loss: 0.6582794189453125, acc: 0.2734375] [gan loss: 1.277813, acc: 0.031250]\n",
            "3852: [discriminator loss: 0.6236754655838013, acc: 0.359375] [gan loss: 1.251252, acc: 0.015625]\n",
            "3853: [discriminator loss: 0.6605004668235779, acc: 0.2421875] [gan loss: 1.651619, acc: 0.000000]\n",
            "3854: [discriminator loss: 0.6197479963302612, acc: 0.453125] [gan loss: 0.715089, acc: 0.546875]\n",
            "3855: [discriminator loss: 0.706918478012085, acc: 0.125] [gan loss: 2.009684, acc: 0.000000]\n",
            "3856: [discriminator loss: 0.6752220988273621, acc: 0.484375] [gan loss: 0.572397, acc: 0.687500]\n",
            "3857: [discriminator loss: 0.8043583631515503, acc: 0.046875] [gan loss: 1.987243, acc: 0.000000]\n",
            "3858: [discriminator loss: 0.6780326962471008, acc: 0.4765625] [gan loss: 0.877713, acc: 0.281250]\n",
            "3859: [discriminator loss: 0.7399787902832031, acc: 0.0703125] [gan loss: 1.708856, acc: 0.000000]\n",
            "3860: [discriminator loss: 0.6094005107879639, acc: 0.4609375] [gan loss: 0.919525, acc: 0.218750]\n",
            "3861: [discriminator loss: 0.7134851217269897, acc: 0.1328125] [gan loss: 1.479717, acc: 0.000000]\n",
            "3862: [discriminator loss: 0.6046565771102905, acc: 0.4453125] [gan loss: 1.023093, acc: 0.140625]\n",
            "3863: [discriminator loss: 0.7148962020874023, acc: 0.140625] [gan loss: 1.495221, acc: 0.000000]\n",
            "3864: [discriminator loss: 0.6218724250793457, acc: 0.40625] [gan loss: 0.961228, acc: 0.140625]\n",
            "3865: [discriminator loss: 0.7059453129768372, acc: 0.1484375] [gan loss: 1.654912, acc: 0.000000]\n",
            "3866: [discriminator loss: 0.639613926410675, acc: 0.4453125] [gan loss: 0.938239, acc: 0.171875]\n",
            "3867: [discriminator loss: 0.6723145842552185, acc: 0.125] [gan loss: 1.774385, acc: 0.000000]\n",
            "3868: [discriminator loss: 0.6389234662055969, acc: 0.4609375] [gan loss: 0.710287, acc: 0.515625]\n",
            "3869: [discriminator loss: 0.7321261167526245, acc: 0.046875] [gan loss: 1.731504, acc: 0.015625]\n",
            "3870: [discriminator loss: 0.6844013929367065, acc: 0.46875] [gan loss: 0.700598, acc: 0.515625]\n",
            "3871: [discriminator loss: 0.75567626953125, acc: 0.0390625] [gan loss: 1.780619, acc: 0.000000]\n",
            "3872: [discriminator loss: 0.6384134292602539, acc: 0.4609375] [gan loss: 0.856497, acc: 0.265625]\n",
            "3873: [discriminator loss: 0.6643146276473999, acc: 0.1484375] [gan loss: 1.432245, acc: 0.015625]\n",
            "3874: [discriminator loss: 0.6211230754852295, acc: 0.4140625] [gan loss: 1.049954, acc: 0.156250]\n",
            "3875: [discriminator loss: 0.6326569318771362, acc: 0.2578125] [gan loss: 1.318671, acc: 0.062500]\n",
            "3876: [discriminator loss: 0.6259701251983643, acc: 0.359375] [gan loss: 0.995826, acc: 0.218750]\n",
            "3877: [discriminator loss: 0.6629312038421631, acc: 0.21875] [gan loss: 1.287748, acc: 0.031250]\n",
            "3878: [discriminator loss: 0.6283384561538696, acc: 0.34375] [gan loss: 1.240218, acc: 0.031250]\n",
            "3879: [discriminator loss: 0.6270725727081299, acc: 0.2890625] [gan loss: 1.366437, acc: 0.015625]\n",
            "3880: [discriminator loss: 0.6182756423950195, acc: 0.375] [gan loss: 1.137467, acc: 0.031250]\n",
            "3881: [discriminator loss: 0.6529382467269897, acc: 0.2109375] [gan loss: 1.527942, acc: 0.015625]\n",
            "3882: [discriminator loss: 0.6319206357002258, acc: 0.3828125] [gan loss: 1.073426, acc: 0.093750]\n",
            "3883: [discriminator loss: 0.6643491983413696, acc: 0.1796875] [gan loss: 1.720063, acc: 0.000000]\n",
            "3884: [discriminator loss: 0.6139816641807556, acc: 0.484375] [gan loss: 0.816049, acc: 0.328125]\n",
            "3885: [discriminator loss: 0.7150900363922119, acc: 0.0390625] [gan loss: 2.078787, acc: 0.000000]\n",
            "3886: [discriminator loss: 0.6494612097740173, acc: 0.5] [gan loss: 0.651136, acc: 0.593750]\n",
            "3887: [discriminator loss: 0.7758724689483643, acc: 0.0546875] [gan loss: 1.809492, acc: 0.000000]\n",
            "3888: [discriminator loss: 0.6304086446762085, acc: 0.4453125] [gan loss: 0.879809, acc: 0.265625]\n",
            "3889: [discriminator loss: 0.7112935781478882, acc: 0.09375] [gan loss: 1.708473, acc: 0.000000]\n",
            "3890: [discriminator loss: 0.6204330325126648, acc: 0.4375] [gan loss: 1.029235, acc: 0.109375]\n",
            "3891: [discriminator loss: 0.6187684535980225, acc: 0.25] [gan loss: 1.212247, acc: 0.062500]\n",
            "3892: [discriminator loss: 0.6400359869003296, acc: 0.328125] [gan loss: 1.098372, acc: 0.062500]\n",
            "3893: [discriminator loss: 0.6695519089698792, acc: 0.2109375] [gan loss: 1.400269, acc: 0.031250]\n",
            "3894: [discriminator loss: 0.6038863062858582, acc: 0.3515625] [gan loss: 1.156102, acc: 0.046875]\n",
            "3895: [discriminator loss: 0.6198580265045166, acc: 0.2421875] [gan loss: 1.412040, acc: 0.031250]\n",
            "3896: [discriminator loss: 0.654591977596283, acc: 0.3828125] [gan loss: 1.134085, acc: 0.093750]\n",
            "3897: [discriminator loss: 0.6723371148109436, acc: 0.21875] [gan loss: 1.504488, acc: 0.015625]\n",
            "3898: [discriminator loss: 0.6323175430297852, acc: 0.421875] [gan loss: 0.804337, acc: 0.328125]\n",
            "3899: [discriminator loss: 0.7115828990936279, acc: 0.125] [gan loss: 1.860848, acc: 0.000000]\n",
            "3900: [discriminator loss: 0.6428065896034241, acc: 0.4375] [gan loss: 0.945420, acc: 0.093750]\n",
            "3901: [discriminator loss: 0.6864722967147827, acc: 0.140625] [gan loss: 1.590801, acc: 0.000000]\n",
            "3902: [discriminator loss: 0.6578946113586426, acc: 0.4296875] [gan loss: 0.933201, acc: 0.218750]\n",
            "3903: [discriminator loss: 0.6789752244949341, acc: 0.1328125] [gan loss: 1.950921, acc: 0.000000]\n",
            "3904: [discriminator loss: 0.621404230594635, acc: 0.453125] [gan loss: 0.891521, acc: 0.218750]\n",
            "3905: [discriminator loss: 0.7313134670257568, acc: 0.0625] [gan loss: 1.979173, acc: 0.000000]\n",
            "3906: [discriminator loss: 0.6641904711723328, acc: 0.46875] [gan loss: 0.630801, acc: 0.578125]\n",
            "3907: [discriminator loss: 0.7789262533187866, acc: 0.0703125] [gan loss: 1.707109, acc: 0.000000]\n",
            "3908: [discriminator loss: 0.6491106748580933, acc: 0.453125] [gan loss: 0.868712, acc: 0.218750]\n",
            "3909: [discriminator loss: 0.7347626090049744, acc: 0.0703125] [gan loss: 1.686157, acc: 0.000000]\n",
            "3910: [discriminator loss: 0.6730027794837952, acc: 0.4375] [gan loss: 0.902098, acc: 0.171875]\n",
            "3911: [discriminator loss: 0.6948291063308716, acc: 0.1328125] [gan loss: 1.621824, acc: 0.015625]\n",
            "3912: [discriminator loss: 0.6057774424552917, acc: 0.4453125] [gan loss: 0.966976, acc: 0.109375]\n",
            "3913: [discriminator loss: 0.6895122528076172, acc: 0.078125] [gan loss: 1.724126, acc: 0.015625]\n",
            "3914: [discriminator loss: 0.6425816416740417, acc: 0.4375] [gan loss: 0.925042, acc: 0.203125]\n",
            "3915: [discriminator loss: 0.6896994709968567, acc: 0.125] [gan loss: 1.424217, acc: 0.000000]\n",
            "3916: [discriminator loss: 0.5726331472396851, acc: 0.453125] [gan loss: 1.029694, acc: 0.109375]\n",
            "3917: [discriminator loss: 0.6493791937828064, acc: 0.234375] [gan loss: 1.442171, acc: 0.015625]\n",
            "3918: [discriminator loss: 0.6113027334213257, acc: 0.421875] [gan loss: 0.981964, acc: 0.125000]\n",
            "3919: [discriminator loss: 0.6681252121925354, acc: 0.1328125] [gan loss: 1.533205, acc: 0.000000]\n",
            "3920: [discriminator loss: 0.6256762742996216, acc: 0.453125] [gan loss: 0.926628, acc: 0.171875]\n",
            "3921: [discriminator loss: 0.6930845975875854, acc: 0.140625] [gan loss: 1.766471, acc: 0.000000]\n",
            "3922: [discriminator loss: 0.6660701036453247, acc: 0.453125] [gan loss: 0.709960, acc: 0.546875]\n",
            "3923: [discriminator loss: 0.7318428754806519, acc: 0.109375] [gan loss: 1.797202, acc: 0.015625]\n",
            "3924: [discriminator loss: 0.6354233622550964, acc: 0.484375] [gan loss: 0.734635, acc: 0.390625]\n",
            "3925: [discriminator loss: 0.7502804398536682, acc: 0.0390625] [gan loss: 1.677319, acc: 0.000000]\n",
            "3926: [discriminator loss: 0.6311644911766052, acc: 0.4375] [gan loss: 0.843993, acc: 0.312500]\n",
            "3927: [discriminator loss: 0.7242023944854736, acc: 0.1015625] [gan loss: 1.501594, acc: 0.000000]\n",
            "3928: [discriminator loss: 0.5867491960525513, acc: 0.4609375] [gan loss: 0.878117, acc: 0.281250]\n",
            "3929: [discriminator loss: 0.7208349704742432, acc: 0.1015625] [gan loss: 1.731492, acc: 0.000000]\n",
            "3930: [discriminator loss: 0.6201859712600708, acc: 0.421875] [gan loss: 1.059654, acc: 0.078125]\n",
            "3931: [discriminator loss: 0.6599726676940918, acc: 0.2265625] [gan loss: 1.650163, acc: 0.031250]\n",
            "3932: [discriminator loss: 0.6715933680534363, acc: 0.40625] [gan loss: 1.054984, acc: 0.093750]\n",
            "3933: [discriminator loss: 0.6356832385063171, acc: 0.2265625] [gan loss: 1.473230, acc: 0.000000]\n",
            "3934: [discriminator loss: 0.6426882743835449, acc: 0.3671875] [gan loss: 1.128763, acc: 0.031250]\n",
            "3935: [discriminator loss: 0.6099802851676941, acc: 0.296875] [gan loss: 1.437311, acc: 0.015625]\n",
            "3936: [discriminator loss: 0.6462141275405884, acc: 0.375] [gan loss: 1.070487, acc: 0.078125]\n",
            "3937: [discriminator loss: 0.6499921083450317, acc: 0.21875] [gan loss: 1.567730, acc: 0.000000]\n",
            "3938: [discriminator loss: 0.6312899589538574, acc: 0.3984375] [gan loss: 0.908963, acc: 0.203125]\n",
            "3939: [discriminator loss: 0.6430483460426331, acc: 0.1875] [gan loss: 1.747523, acc: 0.000000]\n",
            "3940: [discriminator loss: 0.6277464628219604, acc: 0.4765625] [gan loss: 0.719915, acc: 0.531250]\n",
            "3941: [discriminator loss: 0.7690199613571167, acc: 0.0546875] [gan loss: 1.938592, acc: 0.000000]\n",
            "3942: [discriminator loss: 0.6427568197250366, acc: 0.484375] [gan loss: 0.686501, acc: 0.562500]\n",
            "3943: [discriminator loss: 0.750487208366394, acc: 0.046875] [gan loss: 1.668751, acc: 0.000000]\n",
            "3944: [discriminator loss: 0.6367043852806091, acc: 0.453125] [gan loss: 0.743492, acc: 0.437500]\n",
            "3945: [discriminator loss: 0.7034915685653687, acc: 0.09375] [gan loss: 1.420087, acc: 0.000000]\n",
            "3946: [discriminator loss: 0.6026608943939209, acc: 0.484375] [gan loss: 0.851401, acc: 0.265625]\n",
            "3947: [discriminator loss: 0.6886770725250244, acc: 0.125] [gan loss: 1.427199, acc: 0.046875]\n",
            "3948: [discriminator loss: 0.6459208726882935, acc: 0.390625] [gan loss: 0.945681, acc: 0.203125]\n",
            "3949: [discriminator loss: 0.6791903972625732, acc: 0.1171875] [gan loss: 1.455404, acc: 0.015625]\n",
            "3950: [discriminator loss: 0.6229496598243713, acc: 0.390625] [gan loss: 1.049157, acc: 0.109375]\n",
            "3951: [discriminator loss: 0.6718580722808838, acc: 0.1796875] [gan loss: 1.455488, acc: 0.000000]\n",
            "3952: [discriminator loss: 0.6597105264663696, acc: 0.3671875] [gan loss: 1.127355, acc: 0.078125]\n",
            "3953: [discriminator loss: 0.6657133102416992, acc: 0.1875] [gan loss: 1.729012, acc: 0.000000]\n",
            "3954: [discriminator loss: 0.6498079895973206, acc: 0.40625] [gan loss: 0.983304, acc: 0.125000]\n",
            "3955: [discriminator loss: 0.7266039848327637, acc: 0.09375] [gan loss: 1.677992, acc: 0.000000]\n",
            "3956: [discriminator loss: 0.6089567542076111, acc: 0.4765625] [gan loss: 0.821886, acc: 0.296875]\n",
            "3957: [discriminator loss: 0.7028334140777588, acc: 0.1015625] [gan loss: 1.771345, acc: 0.000000]\n",
            "3958: [discriminator loss: 0.6124923229217529, acc: 0.484375] [gan loss: 0.904415, acc: 0.265625]\n",
            "3959: [discriminator loss: 0.7101269364356995, acc: 0.1015625] [gan loss: 1.830631, acc: 0.000000]\n",
            "3960: [discriminator loss: 0.642876386642456, acc: 0.453125] [gan loss: 0.873517, acc: 0.281250]\n",
            "3961: [discriminator loss: 0.7239460349082947, acc: 0.125] [gan loss: 1.745832, acc: 0.000000]\n",
            "3962: [discriminator loss: 0.6299470663070679, acc: 0.4453125] [gan loss: 0.893274, acc: 0.234375]\n",
            "3963: [discriminator loss: 0.7127407193183899, acc: 0.1015625] [gan loss: 1.657696, acc: 0.000000]\n",
            "3964: [discriminator loss: 0.6040002703666687, acc: 0.4609375] [gan loss: 0.931206, acc: 0.234375]\n",
            "3965: [discriminator loss: 0.7253566980361938, acc: 0.140625] [gan loss: 1.649220, acc: 0.015625]\n",
            "3966: [discriminator loss: 0.5965031981468201, acc: 0.4296875] [gan loss: 0.976724, acc: 0.062500]\n",
            "3967: [discriminator loss: 0.6603215932846069, acc: 0.1875] [gan loss: 1.577133, acc: 0.000000]\n",
            "3968: [discriminator loss: 0.6311174035072327, acc: 0.4375] [gan loss: 0.804048, acc: 0.328125]\n",
            "3969: [discriminator loss: 0.7207850217819214, acc: 0.1171875] [gan loss: 1.791933, acc: 0.000000]\n",
            "3970: [discriminator loss: 0.6625028848648071, acc: 0.4453125] [gan loss: 0.872467, acc: 0.218750]\n",
            "3971: [discriminator loss: 0.6972176432609558, acc: 0.078125] [gan loss: 1.834919, acc: 0.015625]\n",
            "3972: [discriminator loss: 0.6398136019706726, acc: 0.4453125] [gan loss: 0.758340, acc: 0.406250]\n",
            "3973: [discriminator loss: 0.7092427611351013, acc: 0.0859375] [gan loss: 1.884752, acc: 0.000000]\n",
            "3974: [discriminator loss: 0.6336014866828918, acc: 0.453125] [gan loss: 0.822159, acc: 0.359375]\n",
            "3975: [discriminator loss: 0.6931046843528748, acc: 0.1328125] [gan loss: 1.480489, acc: 0.000000]\n",
            "3976: [discriminator loss: 0.6098606586456299, acc: 0.3828125] [gan loss: 0.917293, acc: 0.187500]\n",
            "3977: [discriminator loss: 0.6825854778289795, acc: 0.140625] [gan loss: 1.512667, acc: 0.031250]\n",
            "3978: [discriminator loss: 0.5896201133728027, acc: 0.4296875] [gan loss: 0.878035, acc: 0.250000]\n",
            "3979: [discriminator loss: 0.6672073602676392, acc: 0.1875] [gan loss: 1.762344, acc: 0.000000]\n",
            "3980: [discriminator loss: 0.6593945622444153, acc: 0.4296875] [gan loss: 0.801793, acc: 0.390625]\n",
            "3981: [discriminator loss: 0.7745084762573242, acc: 0.046875] [gan loss: 1.773272, acc: 0.000000]\n",
            "3982: [discriminator loss: 0.6794154644012451, acc: 0.453125] [gan loss: 0.775393, acc: 0.453125]\n",
            "3983: [discriminator loss: 0.7473195791244507, acc: 0.0390625] [gan loss: 1.674564, acc: 0.000000]\n",
            "3984: [discriminator loss: 0.6260495781898499, acc: 0.453125] [gan loss: 0.836308, acc: 0.312500]\n",
            "3985: [discriminator loss: 0.6902738809585571, acc: 0.1171875] [gan loss: 1.694149, acc: 0.015625]\n",
            "3986: [discriminator loss: 0.6710379123687744, acc: 0.4453125] [gan loss: 0.916836, acc: 0.203125]\n",
            "3987: [discriminator loss: 0.6826581358909607, acc: 0.171875] [gan loss: 1.402344, acc: 0.000000]\n",
            "3988: [discriminator loss: 0.6313315629959106, acc: 0.3828125] [gan loss: 0.980460, acc: 0.187500]\n",
            "3989: [discriminator loss: 0.6429662704467773, acc: 0.2109375] [gan loss: 1.232757, acc: 0.046875]\n",
            "3990: [discriminator loss: 0.6635953187942505, acc: 0.3359375] [gan loss: 0.971609, acc: 0.203125]\n",
            "3991: [discriminator loss: 0.6859097480773926, acc: 0.1328125] [gan loss: 1.493975, acc: 0.000000]\n",
            "3992: [discriminator loss: 0.689414918422699, acc: 0.3828125] [gan loss: 0.948116, acc: 0.218750]\n",
            "3993: [discriminator loss: 0.6681585907936096, acc: 0.1953125] [gan loss: 1.635570, acc: 0.000000]\n",
            "3994: [discriminator loss: 0.6415464878082275, acc: 0.4140625] [gan loss: 0.936818, acc: 0.140625]\n",
            "3995: [discriminator loss: 0.6261664628982544, acc: 0.234375] [gan loss: 1.546869, acc: 0.000000]\n",
            "3996: [discriminator loss: 0.6251659393310547, acc: 0.4453125] [gan loss: 0.912765, acc: 0.187500]\n",
            "3997: [discriminator loss: 0.7161836624145508, acc: 0.0859375] [gan loss: 1.870833, acc: 0.000000]\n",
            "3998: [discriminator loss: 0.6221252679824829, acc: 0.46875] [gan loss: 0.836521, acc: 0.250000]\n",
            "3999: [discriminator loss: 0.7695307731628418, acc: 0.046875] [gan loss: 1.662392, acc: 0.015625]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "4000: [discriminator loss: 0.6198815107345581, acc: 0.4765625] [gan loss: 0.804599, acc: 0.421875]\n",
            "4001: [discriminator loss: 0.6729018688201904, acc: 0.125] [gan loss: 1.710731, acc: 0.000000]\n",
            "4002: [discriminator loss: 0.611988365650177, acc: 0.4296875] [gan loss: 0.884606, acc: 0.281250]\n",
            "4003: [discriminator loss: 0.7145911455154419, acc: 0.1015625] [gan loss: 1.630308, acc: 0.000000]\n",
            "4004: [discriminator loss: 0.6376556158065796, acc: 0.4609375] [gan loss: 0.981794, acc: 0.187500]\n",
            "4005: [discriminator loss: 0.7035498023033142, acc: 0.125] [gan loss: 1.617265, acc: 0.000000]\n",
            "4006: [discriminator loss: 0.6452739834785461, acc: 0.4453125] [gan loss: 0.865730, acc: 0.234375]\n",
            "4007: [discriminator loss: 0.6847935914993286, acc: 0.09375] [gan loss: 1.802569, acc: 0.015625]\n",
            "4008: [discriminator loss: 0.6818904280662537, acc: 0.4609375] [gan loss: 0.774415, acc: 0.453125]\n",
            "4009: [discriminator loss: 0.7521687150001526, acc: 0.0625] [gan loss: 1.770937, acc: 0.000000]\n",
            "4010: [discriminator loss: 0.6698440909385681, acc: 0.4375] [gan loss: 0.741822, acc: 0.453125]\n",
            "4011: [discriminator loss: 0.8055794835090637, acc: 0.0078125] [gan loss: 1.790700, acc: 0.000000]\n",
            "4012: [discriminator loss: 0.6538140773773193, acc: 0.46875] [gan loss: 0.867374, acc: 0.234375]\n",
            "4013: [discriminator loss: 0.6847922801971436, acc: 0.1171875] [gan loss: 1.373333, acc: 0.000000]\n",
            "4014: [discriminator loss: 0.657360315322876, acc: 0.390625] [gan loss: 1.144447, acc: 0.078125]\n",
            "4015: [discriminator loss: 0.6504440903663635, acc: 0.2265625] [gan loss: 1.442395, acc: 0.000000]\n",
            "4016: [discriminator loss: 0.6366784572601318, acc: 0.3125] [gan loss: 1.206591, acc: 0.015625]\n",
            "4017: [discriminator loss: 0.6431060433387756, acc: 0.28125] [gan loss: 1.260743, acc: 0.046875]\n",
            "4018: [discriminator loss: 0.6647312641143799, acc: 0.2890625] [gan loss: 1.276556, acc: 0.000000]\n",
            "4019: [discriminator loss: 0.6473612785339355, acc: 0.296875] [gan loss: 1.335037, acc: 0.015625]\n",
            "4020: [discriminator loss: 0.6233843564987183, acc: 0.34375] [gan loss: 1.262875, acc: 0.031250]\n",
            "4021: [discriminator loss: 0.6644444465637207, acc: 0.2578125] [gan loss: 1.356637, acc: 0.000000]\n",
            "4022: [discriminator loss: 0.6607625484466553, acc: 0.390625] [gan loss: 1.011308, acc: 0.093750]\n",
            "4023: [discriminator loss: 0.67891526222229, acc: 0.1875] [gan loss: 1.706863, acc: 0.000000]\n",
            "4024: [discriminator loss: 0.6005546450614929, acc: 0.46875] [gan loss: 0.883781, acc: 0.218750]\n",
            "4025: [discriminator loss: 0.6667983531951904, acc: 0.15625] [gan loss: 1.923117, acc: 0.000000]\n",
            "4026: [discriminator loss: 0.6698616743087769, acc: 0.46875] [gan loss: 0.840699, acc: 0.281250]\n",
            "4027: [discriminator loss: 0.679872989654541, acc: 0.125] [gan loss: 1.803020, acc: 0.000000]\n",
            "4028: [discriminator loss: 0.6558887362480164, acc: 0.4453125] [gan loss: 0.929935, acc: 0.234375]\n",
            "4029: [discriminator loss: 0.6771446466445923, acc: 0.09375] [gan loss: 1.854349, acc: 0.015625]\n",
            "4030: [discriminator loss: 0.6875081062316895, acc: 0.4453125] [gan loss: 0.752755, acc: 0.406250]\n",
            "4031: [discriminator loss: 0.706372857093811, acc: 0.109375] [gan loss: 1.640960, acc: 0.000000]\n",
            "4032: [discriminator loss: 0.6307117938995361, acc: 0.40625] [gan loss: 0.975722, acc: 0.171875]\n",
            "4033: [discriminator loss: 0.6971609592437744, acc: 0.1640625] [gan loss: 1.463369, acc: 0.015625]\n",
            "4034: [discriminator loss: 0.5802597403526306, acc: 0.453125] [gan loss: 0.970165, acc: 0.187500]\n",
            "4035: [discriminator loss: 0.6868374943733215, acc: 0.171875] [gan loss: 1.574551, acc: 0.000000]\n",
            "4036: [discriminator loss: 0.6322317123413086, acc: 0.4296875] [gan loss: 0.939030, acc: 0.125000]\n",
            "4037: [discriminator loss: 0.7113970518112183, acc: 0.1015625] [gan loss: 1.729977, acc: 0.000000]\n",
            "4038: [discriminator loss: 0.6731185913085938, acc: 0.4375] [gan loss: 0.781602, acc: 0.406250]\n",
            "4039: [discriminator loss: 0.7002180218696594, acc: 0.1171875] [gan loss: 1.742015, acc: 0.015625]\n",
            "4040: [discriminator loss: 0.6238856315612793, acc: 0.453125] [gan loss: 0.747785, acc: 0.421875]\n",
            "4041: [discriminator loss: 0.7613532543182373, acc: 0.0859375] [gan loss: 1.739127, acc: 0.000000]\n",
            "4042: [discriminator loss: 0.657796323299408, acc: 0.4765625] [gan loss: 0.884337, acc: 0.234375]\n",
            "4043: [discriminator loss: 0.6614307165145874, acc: 0.1875] [gan loss: 1.431786, acc: 0.000000]\n",
            "4044: [discriminator loss: 0.6112710237503052, acc: 0.421875] [gan loss: 1.024678, acc: 0.078125]\n",
            "4045: [discriminator loss: 0.7030125856399536, acc: 0.09375] [gan loss: 1.552845, acc: 0.000000]\n",
            "4046: [discriminator loss: 0.6190364360809326, acc: 0.4453125] [gan loss: 0.873040, acc: 0.265625]\n",
            "4047: [discriminator loss: 0.6956344842910767, acc: 0.125] [gan loss: 1.602841, acc: 0.000000]\n",
            "4048: [discriminator loss: 0.6044079065322876, acc: 0.4609375] [gan loss: 0.816443, acc: 0.234375]\n",
            "4049: [discriminator loss: 0.7022303342819214, acc: 0.0703125] [gan loss: 1.701525, acc: 0.000000]\n",
            "4050: [discriminator loss: 0.6345955729484558, acc: 0.46875] [gan loss: 0.795495, acc: 0.343750]\n",
            "4051: [discriminator loss: 0.7296339273452759, acc: 0.078125] [gan loss: 1.788089, acc: 0.000000]\n",
            "4052: [discriminator loss: 0.6810790300369263, acc: 0.421875] [gan loss: 0.826589, acc: 0.296875]\n",
            "4053: [discriminator loss: 0.6938908696174622, acc: 0.109375] [gan loss: 1.550564, acc: 0.000000]\n",
            "4054: [discriminator loss: 0.5995383262634277, acc: 0.46875] [gan loss: 0.841893, acc: 0.234375]\n",
            "4055: [discriminator loss: 0.7327540516853333, acc: 0.09375] [gan loss: 1.731552, acc: 0.015625]\n",
            "4056: [discriminator loss: 0.6088321208953857, acc: 0.4765625] [gan loss: 0.908685, acc: 0.218750]\n",
            "4057: [discriminator loss: 0.7021318078041077, acc: 0.15625] [gan loss: 1.589387, acc: 0.015625]\n",
            "4058: [discriminator loss: 0.6402238607406616, acc: 0.453125] [gan loss: 0.779745, acc: 0.390625]\n",
            "4059: [discriminator loss: 0.7387076616287231, acc: 0.0546875] [gan loss: 1.622476, acc: 0.000000]\n",
            "4060: [discriminator loss: 0.6520708799362183, acc: 0.4453125] [gan loss: 0.774526, acc: 0.406250]\n",
            "4061: [discriminator loss: 0.7428629398345947, acc: 0.0859375] [gan loss: 1.700705, acc: 0.000000]\n",
            "4062: [discriminator loss: 0.6295861601829529, acc: 0.4453125] [gan loss: 0.762137, acc: 0.437500]\n",
            "4063: [discriminator loss: 0.7076418995857239, acc: 0.0703125] [gan loss: 1.662603, acc: 0.015625]\n",
            "4064: [discriminator loss: 0.612665593624115, acc: 0.4296875] [gan loss: 0.880132, acc: 0.250000]\n",
            "4065: [discriminator loss: 0.7015409469604492, acc: 0.125] [gan loss: 1.638923, acc: 0.000000]\n",
            "4066: [discriminator loss: 0.6556187868118286, acc: 0.3984375] [gan loss: 0.922929, acc: 0.093750]\n",
            "4067: [discriminator loss: 0.6747536063194275, acc: 0.1796875] [gan loss: 1.502567, acc: 0.000000]\n",
            "4068: [discriminator loss: 0.6524810194969177, acc: 0.4375] [gan loss: 0.894495, acc: 0.265625]\n",
            "4069: [discriminator loss: 0.6745194792747498, acc: 0.1171875] [gan loss: 1.490372, acc: 0.015625]\n",
            "4070: [discriminator loss: 0.6533517837524414, acc: 0.3828125] [gan loss: 0.996220, acc: 0.171875]\n",
            "4071: [discriminator loss: 0.7006103992462158, acc: 0.1796875] [gan loss: 1.576040, acc: 0.000000]\n",
            "4072: [discriminator loss: 0.6147336959838867, acc: 0.4375] [gan loss: 0.982416, acc: 0.125000]\n",
            "4073: [discriminator loss: 0.6987268924713135, acc: 0.1484375] [gan loss: 1.634963, acc: 0.000000]\n",
            "4074: [discriminator loss: 0.6234363317489624, acc: 0.4375] [gan loss: 0.969786, acc: 0.171875]\n",
            "4075: [discriminator loss: 0.688193678855896, acc: 0.15625] [gan loss: 1.677752, acc: 0.000000]\n",
            "4076: [discriminator loss: 0.6321940422058105, acc: 0.453125] [gan loss: 0.910266, acc: 0.218750]\n",
            "4077: [discriminator loss: 0.7137169241905212, acc: 0.1171875] [gan loss: 1.679641, acc: 0.000000]\n",
            "4078: [discriminator loss: 0.6502204537391663, acc: 0.421875] [gan loss: 0.939663, acc: 0.203125]\n",
            "4079: [discriminator loss: 0.68843013048172, acc: 0.1484375] [gan loss: 1.546901, acc: 0.015625]\n",
            "4080: [discriminator loss: 0.6215225458145142, acc: 0.4375] [gan loss: 0.850852, acc: 0.375000]\n",
            "4081: [discriminator loss: 0.7092835903167725, acc: 0.1015625] [gan loss: 1.591305, acc: 0.000000]\n",
            "4082: [discriminator loss: 0.6227016448974609, acc: 0.4296875] [gan loss: 0.877425, acc: 0.281250]\n",
            "4083: [discriminator loss: 0.698880136013031, acc: 0.1328125] [gan loss: 1.654075, acc: 0.000000]\n",
            "4084: [discriminator loss: 0.6396403312683105, acc: 0.40625] [gan loss: 0.972494, acc: 0.156250]\n",
            "4085: [discriminator loss: 0.690775990486145, acc: 0.125] [gan loss: 1.790113, acc: 0.000000]\n",
            "4086: [discriminator loss: 0.6231875419616699, acc: 0.4765625] [gan loss: 0.787116, acc: 0.375000]\n",
            "4087: [discriminator loss: 0.7337318658828735, acc: 0.078125] [gan loss: 1.834469, acc: 0.000000]\n",
            "4088: [discriminator loss: 0.6386981010437012, acc: 0.4453125] [gan loss: 0.829170, acc: 0.250000]\n",
            "4089: [discriminator loss: 0.758065938949585, acc: 0.0234375] [gan loss: 1.801366, acc: 0.000000]\n",
            "4090: [discriminator loss: 0.6453831195831299, acc: 0.46875] [gan loss: 0.836322, acc: 0.281250]\n",
            "4091: [discriminator loss: 0.7225726246833801, acc: 0.046875] [gan loss: 1.684644, acc: 0.000000]\n",
            "4092: [discriminator loss: 0.6270565986633301, acc: 0.4609375] [gan loss: 0.754538, acc: 0.406250]\n",
            "4093: [discriminator loss: 0.7440015077590942, acc: 0.046875] [gan loss: 1.601776, acc: 0.000000]\n",
            "4094: [discriminator loss: 0.6165143847465515, acc: 0.46875] [gan loss: 0.803091, acc: 0.390625]\n",
            "4095: [discriminator loss: 0.7314770817756653, acc: 0.1015625] [gan loss: 1.600612, acc: 0.000000]\n",
            "4096: [discriminator loss: 0.6346877813339233, acc: 0.4609375] [gan loss: 0.861181, acc: 0.296875]\n",
            "4097: [discriminator loss: 0.724348783493042, acc: 0.125] [gan loss: 1.528401, acc: 0.015625]\n",
            "4098: [discriminator loss: 0.6260884404182434, acc: 0.4453125] [gan loss: 0.858429, acc: 0.312500]\n",
            "4099: [discriminator loss: 0.7400572299957275, acc: 0.0703125] [gan loss: 1.605110, acc: 0.000000]\n",
            "4100: [discriminator loss: 0.6413140892982483, acc: 0.4765625] [gan loss: 0.878876, acc: 0.171875]\n",
            "4101: [discriminator loss: 0.6975356340408325, acc: 0.1171875] [gan loss: 1.492134, acc: 0.015625]\n",
            "4102: [discriminator loss: 0.6038623452186584, acc: 0.4296875] [gan loss: 0.958476, acc: 0.109375]\n",
            "4103: [discriminator loss: 0.6603931188583374, acc: 0.2265625] [gan loss: 1.628209, acc: 0.000000]\n",
            "4104: [discriminator loss: 0.6165841817855835, acc: 0.4140625] [gan loss: 1.063924, acc: 0.062500]\n",
            "4105: [discriminator loss: 0.6655486822128296, acc: 0.1796875] [gan loss: 1.526037, acc: 0.015625]\n",
            "4106: [discriminator loss: 0.6528830528259277, acc: 0.375] [gan loss: 0.948580, acc: 0.140625]\n",
            "4107: [discriminator loss: 0.6803637742996216, acc: 0.1328125] [gan loss: 1.636570, acc: 0.000000]\n",
            "4108: [discriminator loss: 0.6656028628349304, acc: 0.4453125] [gan loss: 0.856706, acc: 0.328125]\n",
            "4109: [discriminator loss: 0.707056999206543, acc: 0.09375] [gan loss: 1.844606, acc: 0.015625]\n",
            "4110: [discriminator loss: 0.6364328265190125, acc: 0.4375] [gan loss: 0.806048, acc: 0.234375]\n",
            "4111: [discriminator loss: 0.6982086896896362, acc: 0.109375] [gan loss: 1.693080, acc: 0.000000]\n",
            "4112: [discriminator loss: 0.6144985556602478, acc: 0.4296875] [gan loss: 0.932474, acc: 0.171875]\n",
            "4113: [discriminator loss: 0.6883136034011841, acc: 0.125] [gan loss: 1.573538, acc: 0.015625]\n",
            "4114: [discriminator loss: 0.6146882176399231, acc: 0.4453125] [gan loss: 0.894057, acc: 0.265625]\n",
            "4115: [discriminator loss: 0.7168444395065308, acc: 0.1640625] [gan loss: 1.408073, acc: 0.000000]\n",
            "4116: [discriminator loss: 0.625885009765625, acc: 0.4296875] [gan loss: 0.983379, acc: 0.125000]\n",
            "4117: [discriminator loss: 0.6846471428871155, acc: 0.15625] [gan loss: 1.633373, acc: 0.000000]\n",
            "4118: [discriminator loss: 0.6574232578277588, acc: 0.46875] [gan loss: 0.803271, acc: 0.265625]\n",
            "4119: [discriminator loss: 0.7721129655838013, acc: 0.078125] [gan loss: 1.836623, acc: 0.000000]\n",
            "4120: [discriminator loss: 0.6376698017120361, acc: 0.4765625] [gan loss: 0.779491, acc: 0.375000]\n",
            "4121: [discriminator loss: 0.7823426723480225, acc: 0.046875] [gan loss: 1.676870, acc: 0.000000]\n",
            "4122: [discriminator loss: 0.6357076168060303, acc: 0.4453125] [gan loss: 0.870814, acc: 0.187500]\n",
            "4123: [discriminator loss: 0.6739518642425537, acc: 0.109375] [gan loss: 1.620548, acc: 0.015625]\n",
            "4124: [discriminator loss: 0.6462157368659973, acc: 0.4453125] [gan loss: 0.744043, acc: 0.343750]\n",
            "4125: [discriminator loss: 0.7047933340072632, acc: 0.09375] [gan loss: 1.699047, acc: 0.000000]\n",
            "4126: [discriminator loss: 0.6299823522567749, acc: 0.46875] [gan loss: 0.810477, acc: 0.312500]\n",
            "4127: [discriminator loss: 0.6931739449501038, acc: 0.078125] [gan loss: 1.549771, acc: 0.031250]\n",
            "4128: [discriminator loss: 0.6479994654655457, acc: 0.4375] [gan loss: 0.767289, acc: 0.421875]\n",
            "4129: [discriminator loss: 0.6806477308273315, acc: 0.078125] [gan loss: 1.557458, acc: 0.000000]\n",
            "4130: [discriminator loss: 0.6481497883796692, acc: 0.40625] [gan loss: 0.896817, acc: 0.203125]\n",
            "4131: [discriminator loss: 0.6912383437156677, acc: 0.109375] [gan loss: 1.624382, acc: 0.000000]\n",
            "4132: [discriminator loss: 0.6625171899795532, acc: 0.4375] [gan loss: 0.754834, acc: 0.453125]\n",
            "4133: [discriminator loss: 0.741457998752594, acc: 0.0625] [gan loss: 1.670024, acc: 0.000000]\n",
            "4134: [discriminator loss: 0.6257166266441345, acc: 0.4921875] [gan loss: 0.731922, acc: 0.437500]\n",
            "4135: [discriminator loss: 0.7469896078109741, acc: 0.0625] [gan loss: 1.755495, acc: 0.000000]\n",
            "4136: [discriminator loss: 0.6584962606430054, acc: 0.4453125] [gan loss: 0.911933, acc: 0.203125]\n",
            "4137: [discriminator loss: 0.6911622881889343, acc: 0.1328125] [gan loss: 1.400423, acc: 0.000000]\n",
            "4138: [discriminator loss: 0.6101502776145935, acc: 0.4375] [gan loss: 1.020112, acc: 0.078125]\n",
            "4139: [discriminator loss: 0.6630330085754395, acc: 0.15625] [gan loss: 1.306090, acc: 0.000000]\n",
            "4140: [discriminator loss: 0.6092244386672974, acc: 0.375] [gan loss: 1.150494, acc: 0.046875]\n",
            "4141: [discriminator loss: 0.6862859129905701, acc: 0.1875] [gan loss: 1.418485, acc: 0.000000]\n",
            "4142: [discriminator loss: 0.6524969935417175, acc: 0.3359375] [gan loss: 1.224347, acc: 0.031250]\n",
            "4143: [discriminator loss: 0.6707078218460083, acc: 0.1953125] [gan loss: 1.399810, acc: 0.015625]\n",
            "4144: [discriminator loss: 0.6251667737960815, acc: 0.4140625] [gan loss: 1.078799, acc: 0.015625]\n",
            "4145: [discriminator loss: 0.6963892579078674, acc: 0.203125] [gan loss: 1.497367, acc: 0.015625]\n",
            "4146: [discriminator loss: 0.6255531311035156, acc: 0.4296875] [gan loss: 0.978440, acc: 0.078125]\n",
            "4147: [discriminator loss: 0.7047222852706909, acc: 0.1796875] [gan loss: 1.666019, acc: 0.000000]\n",
            "4148: [discriminator loss: 0.6524882316589355, acc: 0.40625] [gan loss: 0.872840, acc: 0.171875]\n",
            "4149: [discriminator loss: 0.6989274024963379, acc: 0.125] [gan loss: 1.864382, acc: 0.000000]\n",
            "4150: [discriminator loss: 0.6174231767654419, acc: 0.4609375] [gan loss: 0.780440, acc: 0.359375]\n",
            "4151: [discriminator loss: 0.7178608179092407, acc: 0.078125] [gan loss: 1.848455, acc: 0.000000]\n",
            "4152: [discriminator loss: 0.6369295716285706, acc: 0.46875] [gan loss: 0.781116, acc: 0.375000]\n",
            "4153: [discriminator loss: 0.80252605676651, acc: 0.0546875] [gan loss: 1.956435, acc: 0.000000]\n",
            "4154: [discriminator loss: 0.6839095950126648, acc: 0.4765625] [gan loss: 0.616143, acc: 0.718750]\n",
            "4155: [discriminator loss: 0.7873690128326416, acc: 0.015625] [gan loss: 1.798357, acc: 0.000000]\n",
            "4156: [discriminator loss: 0.6455551385879517, acc: 0.453125] [gan loss: 0.857000, acc: 0.265625]\n",
            "4157: [discriminator loss: 0.6584926247596741, acc: 0.125] [gan loss: 1.468228, acc: 0.000000]\n",
            "4158: [discriminator loss: 0.6219767332077026, acc: 0.40625] [gan loss: 1.172172, acc: 0.078125]\n",
            "4159: [discriminator loss: 0.6619908809661865, acc: 0.2265625] [gan loss: 1.412373, acc: 0.031250]\n",
            "4160: [discriminator loss: 0.6231592893600464, acc: 0.421875] [gan loss: 0.890644, acc: 0.218750]\n",
            "4161: [discriminator loss: 0.6835068464279175, acc: 0.1640625] [gan loss: 1.520744, acc: 0.015625]\n",
            "4162: [discriminator loss: 0.6482411026954651, acc: 0.359375] [gan loss: 1.240344, acc: 0.062500]\n",
            "4163: [discriminator loss: 0.6215764880180359, acc: 0.2890625] [gan loss: 1.375432, acc: 0.015625]\n",
            "4164: [discriminator loss: 0.6358176469802856, acc: 0.34375] [gan loss: 1.153883, acc: 0.046875]\n",
            "4165: [discriminator loss: 0.6440399289131165, acc: 0.234375] [gan loss: 1.471516, acc: 0.000000]\n",
            "4166: [discriminator loss: 0.6525932550430298, acc: 0.3203125] [gan loss: 1.291693, acc: 0.015625]\n",
            "4167: [discriminator loss: 0.6319887638092041, acc: 0.3359375] [gan loss: 1.370372, acc: 0.000000]\n",
            "4168: [discriminator loss: 0.6238559484481812, acc: 0.3515625] [gan loss: 1.232746, acc: 0.046875]\n",
            "4169: [discriminator loss: 0.6262996196746826, acc: 0.25] [gan loss: 1.621522, acc: 0.000000]\n",
            "4170: [discriminator loss: 0.6335296630859375, acc: 0.453125] [gan loss: 0.842510, acc: 0.265625]\n",
            "4171: [discriminator loss: 0.7083218097686768, acc: 0.09375] [gan loss: 2.004830, acc: 0.000000]\n",
            "4172: [discriminator loss: 0.6775860786437988, acc: 0.484375] [gan loss: 0.645383, acc: 0.609375]\n",
            "4173: [discriminator loss: 0.8248401880264282, acc: 0.03125] [gan loss: 2.188363, acc: 0.000000]\n",
            "4174: [discriminator loss: 0.6819794178009033, acc: 0.5] [gan loss: 0.569438, acc: 0.750000]\n",
            "4175: [discriminator loss: 0.8022869825363159, acc: 0.0234375] [gan loss: 1.626817, acc: 0.015625]\n",
            "4176: [discriminator loss: 0.6231998801231384, acc: 0.453125] [gan loss: 0.963472, acc: 0.171875]\n",
            "4177: [discriminator loss: 0.6939765214920044, acc: 0.15625] [gan loss: 1.462889, acc: 0.000000]\n",
            "4178: [discriminator loss: 0.6198534965515137, acc: 0.421875] [gan loss: 0.966120, acc: 0.218750]\n",
            "4179: [discriminator loss: 0.648680567741394, acc: 0.2421875] [gan loss: 1.359856, acc: 0.015625]\n",
            "4180: [discriminator loss: 0.6254896521568298, acc: 0.3671875] [gan loss: 1.075892, acc: 0.078125]\n",
            "4181: [discriminator loss: 0.661848783493042, acc: 0.21875] [gan loss: 1.449534, acc: 0.015625]\n",
            "4182: [discriminator loss: 0.6174676418304443, acc: 0.3984375] [gan loss: 0.963884, acc: 0.187500]\n",
            "4183: [discriminator loss: 0.6926981210708618, acc: 0.15625] [gan loss: 1.756522, acc: 0.000000]\n",
            "4184: [discriminator loss: 0.6233023405075073, acc: 0.4609375] [gan loss: 0.835337, acc: 0.375000]\n",
            "4185: [discriminator loss: 0.6935852766036987, acc: 0.0859375] [gan loss: 1.938003, acc: 0.000000]\n",
            "4186: [discriminator loss: 0.6496898531913757, acc: 0.46875] [gan loss: 0.581954, acc: 0.703125]\n",
            "4187: [discriminator loss: 0.8136004209518433, acc: 0.0390625] [gan loss: 1.896542, acc: 0.000000]\n",
            "4188: [discriminator loss: 0.6223168969154358, acc: 0.4765625] [gan loss: 0.816988, acc: 0.328125]\n",
            "4189: [discriminator loss: 0.7242042422294617, acc: 0.1171875] [gan loss: 1.379722, acc: 0.000000]\n",
            "4190: [discriminator loss: 0.6116341352462769, acc: 0.4375] [gan loss: 0.900916, acc: 0.187500]\n",
            "4191: [discriminator loss: 0.6811903119087219, acc: 0.125] [gan loss: 1.590095, acc: 0.000000]\n",
            "4192: [discriminator loss: 0.6309902667999268, acc: 0.4296875] [gan loss: 0.868491, acc: 0.312500]\n",
            "4193: [discriminator loss: 0.7326292395591736, acc: 0.0859375] [gan loss: 1.878159, acc: 0.000000]\n",
            "4194: [discriminator loss: 0.619080126285553, acc: 0.4609375] [gan loss: 0.803372, acc: 0.375000]\n",
            "4195: [discriminator loss: 0.707704484462738, acc: 0.0703125] [gan loss: 1.583486, acc: 0.000000]\n",
            "4196: [discriminator loss: 0.6243075728416443, acc: 0.4375] [gan loss: 0.916209, acc: 0.203125]\n",
            "4197: [discriminator loss: 0.6605092883110046, acc: 0.171875] [gan loss: 1.465252, acc: 0.015625]\n",
            "4198: [discriminator loss: 0.6494144797325134, acc: 0.359375] [gan loss: 1.116914, acc: 0.078125]\n",
            "4199: [discriminator loss: 0.6380545496940613, acc: 0.3125] [gan loss: 1.240934, acc: 0.109375]\n",
            "4200: [discriminator loss: 0.6815026998519897, acc: 0.2734375] [gan loss: 1.288182, acc: 0.031250]\n",
            "4201: [discriminator loss: 0.6484564542770386, acc: 0.3515625] [gan loss: 1.191334, acc: 0.046875]\n",
            "4202: [discriminator loss: 0.6610044240951538, acc: 0.25] [gan loss: 1.622829, acc: 0.000000]\n",
            "4203: [discriminator loss: 0.6336467266082764, acc: 0.359375] [gan loss: 1.174631, acc: 0.046875]\n",
            "4204: [discriminator loss: 0.640292763710022, acc: 0.2578125] [gan loss: 1.282408, acc: 0.015625]\n",
            "4205: [discriminator loss: 0.6367079019546509, acc: 0.328125] [gan loss: 1.034451, acc: 0.140625]\n",
            "4206: [discriminator loss: 0.6573797464370728, acc: 0.2578125] [gan loss: 1.545737, acc: 0.000000]\n",
            "4207: [discriminator loss: 0.6393535137176514, acc: 0.3984375] [gan loss: 1.047944, acc: 0.125000]\n",
            "4208: [discriminator loss: 0.6929736137390137, acc: 0.1328125] [gan loss: 1.998537, acc: 0.000000]\n",
            "4209: [discriminator loss: 0.6651499271392822, acc: 0.5] [gan loss: 0.540030, acc: 0.812500]\n",
            "4210: [discriminator loss: 0.8287881016731262, acc: 0.015625] [gan loss: 2.212445, acc: 0.000000]\n",
            "4211: [discriminator loss: 0.6751006841659546, acc: 0.484375] [gan loss: 0.633205, acc: 0.593750]\n",
            "4212: [discriminator loss: 0.8162004947662354, acc: 0.03125] [gan loss: 1.863299, acc: 0.000000]\n",
            "4213: [discriminator loss: 0.6385151743888855, acc: 0.4609375] [gan loss: 0.830964, acc: 0.296875]\n",
            "4214: [discriminator loss: 0.7171782851219177, acc: 0.15625] [gan loss: 1.391873, acc: 0.046875]\n",
            "4215: [discriminator loss: 0.6436047554016113, acc: 0.3671875] [gan loss: 1.026225, acc: 0.078125]\n",
            "4216: [discriminator loss: 0.6531953811645508, acc: 0.1796875] [gan loss: 1.309294, acc: 0.000000]\n",
            "4217: [discriminator loss: 0.6240435838699341, acc: 0.3515625] [gan loss: 1.117199, acc: 0.078125]\n",
            "4218: [discriminator loss: 0.6184554100036621, acc: 0.234375] [gan loss: 1.517949, acc: 0.000000]\n",
            "4219: [discriminator loss: 0.6537498235702515, acc: 0.3671875] [gan loss: 1.088160, acc: 0.078125]\n",
            "4220: [discriminator loss: 0.6689186096191406, acc: 0.203125] [gan loss: 1.472955, acc: 0.015625]\n",
            "4221: [discriminator loss: 0.6073830723762512, acc: 0.4375] [gan loss: 0.969816, acc: 0.093750]\n",
            "4222: [discriminator loss: 0.6792641878128052, acc: 0.1640625] [gan loss: 1.695170, acc: 0.000000]\n",
            "4223: [discriminator loss: 0.6758402585983276, acc: 0.4609375] [gan loss: 0.642073, acc: 0.671875]\n",
            "4224: [discriminator loss: 0.7686313390731812, acc: 0.046875] [gan loss: 1.797349, acc: 0.000000]\n",
            "4225: [discriminator loss: 0.6205251812934875, acc: 0.4921875] [gan loss: 0.731603, acc: 0.437500]\n",
            "4226: [discriminator loss: 0.7353962063789368, acc: 0.046875] [gan loss: 1.741775, acc: 0.000000]\n",
            "4227: [discriminator loss: 0.6722316741943359, acc: 0.4453125] [gan loss: 0.868625, acc: 0.218750]\n",
            "4228: [discriminator loss: 0.7094623446464539, acc: 0.0703125] [gan loss: 1.678125, acc: 0.000000]\n",
            "4229: [discriminator loss: 0.6138436198234558, acc: 0.4453125] [gan loss: 0.928968, acc: 0.203125]\n",
            "4230: [discriminator loss: 0.6676669120788574, acc: 0.15625] [gan loss: 1.504113, acc: 0.000000]\n",
            "4231: [discriminator loss: 0.6120368242263794, acc: 0.3984375] [gan loss: 1.101408, acc: 0.031250]\n",
            "4232: [discriminator loss: 0.6128851175308228, acc: 0.2265625] [gan loss: 1.427761, acc: 0.046875]\n",
            "4233: [discriminator loss: 0.664038360118866, acc: 0.34375] [gan loss: 1.080213, acc: 0.109375]\n",
            "4234: [discriminator loss: 0.6677327156066895, acc: 0.1875] [gan loss: 1.431469, acc: 0.000000]\n",
            "4235: [discriminator loss: 0.643036961555481, acc: 0.3828125] [gan loss: 1.040497, acc: 0.125000]\n",
            "4236: [discriminator loss: 0.6477810740470886, acc: 0.2265625] [gan loss: 1.635640, acc: 0.015625]\n",
            "4237: [discriminator loss: 0.6537094116210938, acc: 0.4375] [gan loss: 0.879896, acc: 0.203125]\n",
            "4238: [discriminator loss: 0.6798442602157593, acc: 0.09375] [gan loss: 1.915423, acc: 0.000000]\n",
            "4239: [discriminator loss: 0.6461912393569946, acc: 0.4921875] [gan loss: 0.658668, acc: 0.609375]\n",
            "4240: [discriminator loss: 0.7727707624435425, acc: 0.0546875] [gan loss: 1.926546, acc: 0.000000]\n",
            "4241: [discriminator loss: 0.6519134640693665, acc: 0.46875] [gan loss: 0.740961, acc: 0.468750]\n",
            "4242: [discriminator loss: 0.7437615990638733, acc: 0.0625] [gan loss: 1.652640, acc: 0.000000]\n",
            "4243: [discriminator loss: 0.6879923343658447, acc: 0.40625] [gan loss: 0.813965, acc: 0.406250]\n",
            "4244: [discriminator loss: 0.7008562088012695, acc: 0.15625] [gan loss: 1.383753, acc: 0.015625]\n",
            "4245: [discriminator loss: 0.645464301109314, acc: 0.34375] [gan loss: 1.311786, acc: 0.000000]\n",
            "4246: [discriminator loss: 0.6308214664459229, acc: 0.3125] [gan loss: 1.203829, acc: 0.031250]\n",
            "4247: [discriminator loss: 0.6425957679748535, acc: 0.203125] [gan loss: 1.365860, acc: 0.000000]\n",
            "4248: [discriminator loss: 0.6519350409507751, acc: 0.390625] [gan loss: 0.952705, acc: 0.171875]\n",
            "4249: [discriminator loss: 0.7071287035942078, acc: 0.1171875] [gan loss: 1.637498, acc: 0.000000]\n",
            "4250: [discriminator loss: 0.6353238821029663, acc: 0.453125] [gan loss: 0.916456, acc: 0.187500]\n",
            "4251: [discriminator loss: 0.7361992597579956, acc: 0.1015625] [gan loss: 1.858909, acc: 0.000000]\n",
            "4252: [discriminator loss: 0.6317293047904968, acc: 0.484375] [gan loss: 0.682560, acc: 0.609375]\n",
            "4253: [discriminator loss: 0.8109130859375, acc: 0.015625] [gan loss: 2.020734, acc: 0.000000]\n",
            "4254: [discriminator loss: 0.6776244044303894, acc: 0.484375] [gan loss: 0.801337, acc: 0.328125]\n",
            "4255: [discriminator loss: 0.7501559257507324, acc: 0.0625] [gan loss: 1.616386, acc: 0.000000]\n",
            "4256: [discriminator loss: 0.5922077298164368, acc: 0.4765625] [gan loss: 0.862326, acc: 0.218750]\n",
            "4257: [discriminator loss: 0.6926993727684021, acc: 0.140625] [gan loss: 1.520685, acc: 0.000000]\n",
            "4258: [discriminator loss: 0.6185274124145508, acc: 0.4296875] [gan loss: 0.883196, acc: 0.234375]\n",
            "4259: [discriminator loss: 0.688554048538208, acc: 0.1328125] [gan loss: 1.475385, acc: 0.031250]\n",
            "4260: [discriminator loss: 0.6493796706199646, acc: 0.3984375] [gan loss: 0.998198, acc: 0.093750]\n",
            "4261: [discriminator loss: 0.6574513912200928, acc: 0.1640625] [gan loss: 1.501873, acc: 0.000000]\n",
            "4262: [discriminator loss: 0.6331191658973694, acc: 0.3984375] [gan loss: 1.135187, acc: 0.046875]\n",
            "4263: [discriminator loss: 0.6626111268997192, acc: 0.171875] [gan loss: 1.513311, acc: 0.000000]\n",
            "4264: [discriminator loss: 0.607048511505127, acc: 0.40625] [gan loss: 0.974970, acc: 0.125000]\n",
            "4265: [discriminator loss: 0.7303082942962646, acc: 0.1640625] [gan loss: 1.781917, acc: 0.000000]\n",
            "4266: [discriminator loss: 0.6636874079704285, acc: 0.4140625] [gan loss: 0.944475, acc: 0.187500]\n",
            "4267: [discriminator loss: 0.6761478185653687, acc: 0.1171875] [gan loss: 1.710433, acc: 0.000000]\n",
            "4268: [discriminator loss: 0.6495234370231628, acc: 0.46875] [gan loss: 0.708055, acc: 0.515625]\n",
            "4269: [discriminator loss: 0.7598214149475098, acc: 0.0703125] [gan loss: 2.085040, acc: 0.000000]\n",
            "4270: [discriminator loss: 0.6322367191314697, acc: 0.5] [gan loss: 0.626182, acc: 0.656250]\n",
            "4271: [discriminator loss: 0.7679003477096558, acc: 0.03125] [gan loss: 1.793414, acc: 0.000000]\n",
            "4272: [discriminator loss: 0.6292842626571655, acc: 0.4375] [gan loss: 0.961355, acc: 0.140625]\n",
            "4273: [discriminator loss: 0.7156314253807068, acc: 0.1328125] [gan loss: 1.557002, acc: 0.000000]\n",
            "4274: [discriminator loss: 0.6351395845413208, acc: 0.3984375] [gan loss: 0.988015, acc: 0.109375]\n",
            "4275: [discriminator loss: 0.6702374815940857, acc: 0.203125] [gan loss: 1.368476, acc: 0.031250]\n",
            "4276: [discriminator loss: 0.6467820405960083, acc: 0.3203125] [gan loss: 1.308190, acc: 0.031250]\n",
            "4277: [discriminator loss: 0.6379255056381226, acc: 0.3046875] [gan loss: 1.232246, acc: 0.046875]\n",
            "4278: [discriminator loss: 0.6312834620475769, acc: 0.359375] [gan loss: 1.286103, acc: 0.015625]\n",
            "4279: [discriminator loss: 0.6372618079185486, acc: 0.296875] [gan loss: 1.492634, acc: 0.031250]\n",
            "4280: [discriminator loss: 0.6273648738861084, acc: 0.3671875] [gan loss: 1.017311, acc: 0.140625]\n",
            "4281: [discriminator loss: 0.7277607321739197, acc: 0.109375] [gan loss: 1.736519, acc: 0.000000]\n",
            "4282: [discriminator loss: 0.6101490259170532, acc: 0.4921875] [gan loss: 0.733895, acc: 0.468750]\n",
            "4283: [discriminator loss: 0.7383661866188049, acc: 0.046875] [gan loss: 1.903306, acc: 0.000000]\n",
            "4284: [discriminator loss: 0.6740097403526306, acc: 0.46875] [gan loss: 0.833571, acc: 0.343750]\n",
            "4285: [discriminator loss: 0.7209209203720093, acc: 0.078125] [gan loss: 1.697797, acc: 0.000000]\n",
            "4286: [discriminator loss: 0.6210548281669617, acc: 0.4765625] [gan loss: 0.765374, acc: 0.375000]\n",
            "4287: [discriminator loss: 0.7177128195762634, acc: 0.109375] [gan loss: 1.654243, acc: 0.000000]\n",
            "4288: [discriminator loss: 0.617703914642334, acc: 0.4765625] [gan loss: 0.890100, acc: 0.234375]\n",
            "4289: [discriminator loss: 0.7180867791175842, acc: 0.0625] [gan loss: 1.820139, acc: 0.000000]\n",
            "4290: [discriminator loss: 0.6559494137763977, acc: 0.4765625] [gan loss: 0.818783, acc: 0.375000]\n",
            "4291: [discriminator loss: 0.7177931070327759, acc: 0.0703125] [gan loss: 1.692760, acc: 0.015625]\n",
            "4292: [discriminator loss: 0.6777704358100891, acc: 0.453125] [gan loss: 0.726398, acc: 0.359375]\n",
            "4293: [discriminator loss: 0.7035046815872192, acc: 0.0703125] [gan loss: 1.538195, acc: 0.015625]\n",
            "4294: [discriminator loss: 0.6162261962890625, acc: 0.4375] [gan loss: 0.897849, acc: 0.156250]\n",
            "4295: [discriminator loss: 0.7030202150344849, acc: 0.109375] [gan loss: 1.744554, acc: 0.000000]\n",
            "4296: [discriminator loss: 0.6693717837333679, acc: 0.3828125] [gan loss: 0.851789, acc: 0.296875]\n",
            "4297: [discriminator loss: 0.7397297620773315, acc: 0.1015625] [gan loss: 1.628191, acc: 0.000000]\n",
            "4298: [discriminator loss: 0.6479184031486511, acc: 0.4375] [gan loss: 0.967934, acc: 0.218750]\n",
            "4299: [discriminator loss: 0.7125456929206848, acc: 0.109375] [gan loss: 1.652633, acc: 0.000000]\n",
            "4300: [discriminator loss: 0.6736624240875244, acc: 0.4453125] [gan loss: 0.929324, acc: 0.234375]\n",
            "4301: [discriminator loss: 0.7190800905227661, acc: 0.078125] [gan loss: 1.789790, acc: 0.000000]\n",
            "4302: [discriminator loss: 0.6438812017440796, acc: 0.453125] [gan loss: 0.830684, acc: 0.359375]\n",
            "4303: [discriminator loss: 0.7031664252281189, acc: 0.1015625] [gan loss: 1.740006, acc: 0.000000]\n",
            "4304: [discriminator loss: 0.6556671857833862, acc: 0.421875] [gan loss: 0.844927, acc: 0.250000]\n",
            "4305: [discriminator loss: 0.705100953578949, acc: 0.1328125] [gan loss: 1.528179, acc: 0.000000]\n",
            "4306: [discriminator loss: 0.6712664365768433, acc: 0.3515625] [gan loss: 1.173186, acc: 0.015625]\n",
            "4307: [discriminator loss: 0.6562211513519287, acc: 0.25] [gan loss: 1.446504, acc: 0.031250]\n",
            "4308: [discriminator loss: 0.6317494511604309, acc: 0.34375] [gan loss: 1.241448, acc: 0.031250]\n",
            "4309: [discriminator loss: 0.6555367708206177, acc: 0.2890625] [gan loss: 1.272793, acc: 0.031250]\n",
            "4310: [discriminator loss: 0.6293599009513855, acc: 0.3046875] [gan loss: 1.225200, acc: 0.031250]\n",
            "4311: [discriminator loss: 0.642017126083374, acc: 0.3203125] [gan loss: 1.282582, acc: 0.031250]\n",
            "4312: [discriminator loss: 0.6266997456550598, acc: 0.375] [gan loss: 1.048795, acc: 0.093750]\n",
            "4313: [discriminator loss: 0.6698889136314392, acc: 0.1796875] [gan loss: 1.540940, acc: 0.000000]\n",
            "4314: [discriminator loss: 0.6100457310676575, acc: 0.453125] [gan loss: 0.732147, acc: 0.484375]\n",
            "4315: [discriminator loss: 0.7633376121520996, acc: 0.0859375] [gan loss: 2.025751, acc: 0.000000]\n",
            "4316: [discriminator loss: 0.6326469779014587, acc: 0.5] [gan loss: 0.676546, acc: 0.593750]\n",
            "4317: [discriminator loss: 0.7922759652137756, acc: 0.0703125] [gan loss: 1.891810, acc: 0.000000]\n",
            "4318: [discriminator loss: 0.6881771087646484, acc: 0.4921875] [gan loss: 0.662473, acc: 0.531250]\n",
            "4319: [discriminator loss: 0.7592102289199829, acc: 0.0546875] [gan loss: 1.961741, acc: 0.000000]\n",
            "4320: [discriminator loss: 0.6175587773323059, acc: 0.484375] [gan loss: 0.800361, acc: 0.359375]\n",
            "4321: [discriminator loss: 0.7275521159172058, acc: 0.09375] [gan loss: 1.568750, acc: 0.000000]\n",
            "4322: [discriminator loss: 0.6768453121185303, acc: 0.4453125] [gan loss: 0.758219, acc: 0.390625]\n",
            "4323: [discriminator loss: 0.744943380355835, acc: 0.03125] [gan loss: 1.593422, acc: 0.000000]\n",
            "4324: [discriminator loss: 0.6378005743026733, acc: 0.4453125] [gan loss: 0.930557, acc: 0.125000]\n",
            "4325: [discriminator loss: 0.6740254163742065, acc: 0.1171875] [gan loss: 1.605068, acc: 0.000000]\n",
            "4326: [discriminator loss: 0.647321343421936, acc: 0.4609375] [gan loss: 0.857373, acc: 0.250000]\n",
            "4327: [discriminator loss: 0.7059515714645386, acc: 0.078125] [gan loss: 1.682004, acc: 0.000000]\n",
            "4328: [discriminator loss: 0.6380127668380737, acc: 0.4296875] [gan loss: 0.823727, acc: 0.312500]\n",
            "4329: [discriminator loss: 0.6685795783996582, acc: 0.1640625] [gan loss: 1.374654, acc: 0.015625]\n",
            "4330: [discriminator loss: 0.6323909163475037, acc: 0.3984375] [gan loss: 0.960487, acc: 0.156250]\n",
            "4331: [discriminator loss: 0.725059986114502, acc: 0.15625] [gan loss: 1.604711, acc: 0.000000]\n",
            "4332: [discriminator loss: 0.6090928316116333, acc: 0.4375] [gan loss: 1.020433, acc: 0.125000]\n",
            "4333: [discriminator loss: 0.6904237270355225, acc: 0.171875] [gan loss: 1.583035, acc: 0.031250]\n",
            "4334: [discriminator loss: 0.6237702965736389, acc: 0.453125] [gan loss: 1.019083, acc: 0.125000]\n",
            "4335: [discriminator loss: 0.6745597720146179, acc: 0.1953125] [gan loss: 1.680603, acc: 0.000000]\n",
            "4336: [discriminator loss: 0.6593183875083923, acc: 0.3828125] [gan loss: 0.906209, acc: 0.171875]\n",
            "4337: [discriminator loss: 0.6994069218635559, acc: 0.15625] [gan loss: 1.469107, acc: 0.000000]\n",
            "4338: [discriminator loss: 0.6510091423988342, acc: 0.40625] [gan loss: 0.867514, acc: 0.250000]\n",
            "4339: [discriminator loss: 0.7088411450386047, acc: 0.125] [gan loss: 1.851042, acc: 0.000000]\n",
            "4340: [discriminator loss: 0.6484946608543396, acc: 0.4453125] [gan loss: 0.762173, acc: 0.437500]\n",
            "4341: [discriminator loss: 0.7645014524459839, acc: 0.0546875] [gan loss: 2.073736, acc: 0.000000]\n",
            "4342: [discriminator loss: 0.6928404569625854, acc: 0.484375] [gan loss: 0.638434, acc: 0.609375]\n",
            "4343: [discriminator loss: 0.784613311290741, acc: 0.03125] [gan loss: 1.696464, acc: 0.000000]\n",
            "4344: [discriminator loss: 0.682459831237793, acc: 0.4453125] [gan loss: 0.741908, acc: 0.453125]\n",
            "4345: [discriminator loss: 0.7673928737640381, acc: 0.0703125] [gan loss: 1.684349, acc: 0.000000]\n",
            "4346: [discriminator loss: 0.6487572193145752, acc: 0.46875] [gan loss: 0.734920, acc: 0.468750]\n",
            "4347: [discriminator loss: 0.7332909107208252, acc: 0.0390625] [gan loss: 1.789179, acc: 0.000000]\n",
            "4348: [discriminator loss: 0.677932858467102, acc: 0.4375] [gan loss: 0.889859, acc: 0.156250]\n",
            "4349: [discriminator loss: 0.7364457845687866, acc: 0.1171875] [gan loss: 1.410170, acc: 0.031250]\n",
            "4350: [discriminator loss: 0.6343456506729126, acc: 0.375] [gan loss: 1.057235, acc: 0.078125]\n",
            "4351: [discriminator loss: 0.6846637725830078, acc: 0.1796875] [gan loss: 1.263690, acc: 0.015625]\n",
            "4352: [discriminator loss: 0.654177188873291, acc: 0.265625] [gan loss: 1.195231, acc: 0.031250]\n",
            "4353: [discriminator loss: 0.6777796745300293, acc: 0.2265625] [gan loss: 1.367434, acc: 0.000000]\n",
            "4354: [discriminator loss: 0.6610565185546875, acc: 0.3203125] [gan loss: 1.158211, acc: 0.046875]\n",
            "4355: [discriminator loss: 0.6856275796890259, acc: 0.2109375] [gan loss: 1.231774, acc: 0.015625]\n",
            "4356: [discriminator loss: 0.6459882855415344, acc: 0.3203125] [gan loss: 1.207127, acc: 0.062500]\n",
            "4357: [discriminator loss: 0.6553972959518433, acc: 0.3046875] [gan loss: 1.236632, acc: 0.046875]\n",
            "4358: [discriminator loss: 0.6465127468109131, acc: 0.296875] [gan loss: 1.314232, acc: 0.078125]\n",
            "4359: [discriminator loss: 0.6949281692504883, acc: 0.3359375] [gan loss: 1.103277, acc: 0.078125]\n",
            "4360: [discriminator loss: 0.693526029586792, acc: 0.140625] [gan loss: 1.763211, acc: 0.000000]\n",
            "4361: [discriminator loss: 0.6291770339012146, acc: 0.4453125] [gan loss: 0.933481, acc: 0.187500]\n",
            "4362: [discriminator loss: 0.6629549264907837, acc: 0.1796875] [gan loss: 1.506642, acc: 0.015625]\n",
            "4363: [discriminator loss: 0.6692069172859192, acc: 0.3671875] [gan loss: 0.875968, acc: 0.203125]\n",
            "4364: [discriminator loss: 0.7127697467803955, acc: 0.0703125] [gan loss: 2.055655, acc: 0.000000]\n",
            "4365: [discriminator loss: 0.7259269952774048, acc: 0.453125] [gan loss: 0.505627, acc: 0.781250]\n",
            "4366: [discriminator loss: 0.8137374520301819, acc: 0.0078125] [gan loss: 1.930178, acc: 0.000000]\n",
            "4367: [discriminator loss: 0.6744571328163147, acc: 0.484375] [gan loss: 0.629652, acc: 0.671875]\n",
            "4368: [discriminator loss: 0.7744007110595703, acc: 0.0390625] [gan loss: 1.640095, acc: 0.000000]\n",
            "4369: [discriminator loss: 0.6331197023391724, acc: 0.4375] [gan loss: 0.845354, acc: 0.296875]\n",
            "4370: [discriminator loss: 0.6944898366928101, acc: 0.109375] [gan loss: 1.479929, acc: 0.015625]\n",
            "4371: [discriminator loss: 0.6272897720336914, acc: 0.3828125] [gan loss: 1.049244, acc: 0.093750]\n",
            "4372: [discriminator loss: 0.6973617672920227, acc: 0.1171875] [gan loss: 1.350796, acc: 0.031250]\n",
            "4373: [discriminator loss: 0.671859860420227, acc: 0.3984375] [gan loss: 1.023246, acc: 0.093750]\n",
            "4374: [discriminator loss: 0.6612176895141602, acc: 0.203125] [gan loss: 1.343514, acc: 0.000000]\n",
            "4375: [discriminator loss: 0.6543819904327393, acc: 0.359375] [gan loss: 1.138549, acc: 0.062500]\n",
            "4376: [discriminator loss: 0.6684852838516235, acc: 0.2265625] [gan loss: 1.566007, acc: 0.000000]\n",
            "4377: [discriminator loss: 0.6378567218780518, acc: 0.3828125] [gan loss: 1.042770, acc: 0.031250]\n",
            "4378: [discriminator loss: 0.650288462638855, acc: 0.21875] [gan loss: 1.452437, acc: 0.000000]\n",
            "4379: [discriminator loss: 0.6280109882354736, acc: 0.3984375] [gan loss: 1.133938, acc: 0.062500]\n",
            "4380: [discriminator loss: 0.6855892539024353, acc: 0.1484375] [gan loss: 1.827323, acc: 0.000000]\n",
            "4381: [discriminator loss: 0.661541223526001, acc: 0.46875] [gan loss: 0.762172, acc: 0.437500]\n",
            "4382: [discriminator loss: 0.7317169904708862, acc: 0.046875] [gan loss: 1.932708, acc: 0.000000]\n",
            "4383: [discriminator loss: 0.6501151919364929, acc: 0.5] [gan loss: 0.614479, acc: 0.671875]\n",
            "4384: [discriminator loss: 0.8079122304916382, acc: 0.0234375] [gan loss: 1.968945, acc: 0.000000]\n",
            "4385: [discriminator loss: 0.6588157415390015, acc: 0.5] [gan loss: 0.689437, acc: 0.609375]\n",
            "4386: [discriminator loss: 0.744138240814209, acc: 0.0390625] [gan loss: 1.730336, acc: 0.000000]\n",
            "4387: [discriminator loss: 0.6677868366241455, acc: 0.4453125] [gan loss: 0.828873, acc: 0.265625]\n",
            "4388: [discriminator loss: 0.7683676481246948, acc: 0.0703125] [gan loss: 1.627595, acc: 0.000000]\n",
            "4389: [discriminator loss: 0.642413854598999, acc: 0.3984375] [gan loss: 0.958925, acc: 0.203125]\n",
            "4390: [discriminator loss: 0.6785686016082764, acc: 0.171875] [gan loss: 1.534045, acc: 0.000000]\n",
            "4391: [discriminator loss: 0.6161668300628662, acc: 0.3984375] [gan loss: 1.107049, acc: 0.109375]\n",
            "4392: [discriminator loss: 0.6683420538902283, acc: 0.2734375] [gan loss: 1.244852, acc: 0.015625]\n",
            "4393: [discriminator loss: 0.6224971413612366, acc: 0.3984375] [gan loss: 1.064072, acc: 0.093750]\n",
            "4394: [discriminator loss: 0.6940180659294128, acc: 0.21875] [gan loss: 1.361587, acc: 0.000000]\n",
            "4395: [discriminator loss: 0.6558045148849487, acc: 0.375] [gan loss: 0.992771, acc: 0.156250]\n",
            "4396: [discriminator loss: 0.7264142036437988, acc: 0.0859375] [gan loss: 1.822695, acc: 0.000000]\n",
            "4397: [discriminator loss: 0.6496548056602478, acc: 0.4765625] [gan loss: 0.734342, acc: 0.515625]\n",
            "4398: [discriminator loss: 0.7935068607330322, acc: 0.046875] [gan loss: 1.855936, acc: 0.000000]\n",
            "4399: [discriminator loss: 0.6539842486381531, acc: 0.453125] [gan loss: 0.774061, acc: 0.468750]\n",
            "4400: [discriminator loss: 0.7511401772499084, acc: 0.0546875] [gan loss: 1.694877, acc: 0.000000]\n",
            "4401: [discriminator loss: 0.6324820518493652, acc: 0.4921875] [gan loss: 0.703386, acc: 0.578125]\n",
            "4402: [discriminator loss: 0.7536059021949768, acc: 0.0390625] [gan loss: 1.545909, acc: 0.015625]\n",
            "4403: [discriminator loss: 0.632920503616333, acc: 0.390625] [gan loss: 0.856883, acc: 0.234375]\n",
            "4404: [discriminator loss: 0.7082144021987915, acc: 0.0859375] [gan loss: 1.467181, acc: 0.031250]\n",
            "4405: [discriminator loss: 0.6568677425384521, acc: 0.3984375] [gan loss: 1.008957, acc: 0.187500]\n",
            "4406: [discriminator loss: 0.6761307716369629, acc: 0.1640625] [gan loss: 1.451983, acc: 0.015625]\n",
            "4407: [discriminator loss: 0.6511272192001343, acc: 0.3671875] [gan loss: 1.095678, acc: 0.062500]\n",
            "4408: [discriminator loss: 0.7003353834152222, acc: 0.171875] [gan loss: 1.439261, acc: 0.015625]\n",
            "4409: [discriminator loss: 0.6120870113372803, acc: 0.375] [gan loss: 1.127662, acc: 0.125000]\n",
            "4410: [discriminator loss: 0.6397349238395691, acc: 0.2265625] [gan loss: 1.540564, acc: 0.000000]\n",
            "4411: [discriminator loss: 0.6262087821960449, acc: 0.390625] [gan loss: 1.061037, acc: 0.062500]\n",
            "4412: [discriminator loss: 0.6843446493148804, acc: 0.1640625] [gan loss: 1.579753, acc: 0.015625]\n",
            "4413: [discriminator loss: 0.6314327716827393, acc: 0.421875] [gan loss: 1.033148, acc: 0.078125]\n",
            "4414: [discriminator loss: 0.666695237159729, acc: 0.1953125] [gan loss: 1.414397, acc: 0.000000]\n",
            "4415: [discriminator loss: 0.6157270669937134, acc: 0.4140625] [gan loss: 1.048606, acc: 0.078125]\n",
            "4416: [discriminator loss: 0.6394399404525757, acc: 0.2109375] [gan loss: 1.682102, acc: 0.000000]\n",
            "4417: [discriminator loss: 0.6344501972198486, acc: 0.4453125] [gan loss: 0.901052, acc: 0.171875]\n",
            "4418: [discriminator loss: 0.7019048929214478, acc: 0.1171875] [gan loss: 1.794804, acc: 0.000000]\n",
            "4419: [discriminator loss: 0.6837652325630188, acc: 0.4375] [gan loss: 0.832137, acc: 0.296875]\n",
            "4420: [discriminator loss: 0.7409754991531372, acc: 0.0703125] [gan loss: 1.902463, acc: 0.000000]\n",
            "4421: [discriminator loss: 0.6524090766906738, acc: 0.4765625] [gan loss: 0.692117, acc: 0.515625]\n",
            "4422: [discriminator loss: 0.8003381490707397, acc: 0.0390625] [gan loss: 1.980653, acc: 0.000000]\n",
            "4423: [discriminator loss: 0.6572656035423279, acc: 0.4765625] [gan loss: 0.803888, acc: 0.343750]\n",
            "4424: [discriminator loss: 0.7275711894035339, acc: 0.078125] [gan loss: 1.699590, acc: 0.000000]\n",
            "4425: [discriminator loss: 0.6576013565063477, acc: 0.484375] [gan loss: 0.723868, acc: 0.515625]\n",
            "4426: [discriminator loss: 0.7697858810424805, acc: 0.0546875] [gan loss: 1.533101, acc: 0.000000]\n",
            "4427: [discriminator loss: 0.6745091676712036, acc: 0.421875] [gan loss: 0.827107, acc: 0.375000]\n",
            "4428: [discriminator loss: 0.6962924599647522, acc: 0.140625] [gan loss: 1.453595, acc: 0.000000]\n",
            "4429: [discriminator loss: 0.5970960855484009, acc: 0.4140625] [gan loss: 0.971732, acc: 0.156250]\n",
            "4430: [discriminator loss: 0.7430842518806458, acc: 0.09375] [gan loss: 1.709935, acc: 0.000000]\n",
            "4431: [discriminator loss: 0.6462016701698303, acc: 0.421875] [gan loss: 1.060418, acc: 0.062500]\n",
            "4432: [discriminator loss: 0.6569554209709167, acc: 0.203125] [gan loss: 1.354167, acc: 0.000000]\n",
            "4433: [discriminator loss: 0.6641858816146851, acc: 0.34375] [gan loss: 0.965412, acc: 0.187500]\n",
            "4434: [discriminator loss: 0.6659639477729797, acc: 0.1640625] [gan loss: 1.584825, acc: 0.000000]\n",
            "4435: [discriminator loss: 0.6345943808555603, acc: 0.4453125] [gan loss: 0.922797, acc: 0.187500]\n",
            "4436: [discriminator loss: 0.6789234280586243, acc: 0.1171875] [gan loss: 1.611584, acc: 0.000000]\n",
            "4437: [discriminator loss: 0.6253546476364136, acc: 0.4765625] [gan loss: 0.716967, acc: 0.453125]\n",
            "4438: [discriminator loss: 0.7442420721054077, acc: 0.0625] [gan loss: 1.851021, acc: 0.000000]\n",
            "4439: [discriminator loss: 0.6850953102111816, acc: 0.4296875] [gan loss: 0.693580, acc: 0.546875]\n",
            "4440: [discriminator loss: 0.76819908618927, acc: 0.03125] [gan loss: 1.759866, acc: 0.000000]\n",
            "4441: [discriminator loss: 0.6062794923782349, acc: 0.484375] [gan loss: 0.757720, acc: 0.500000]\n",
            "4442: [discriminator loss: 0.790743350982666, acc: 0.0390625] [gan loss: 1.750849, acc: 0.000000]\n",
            "4443: [discriminator loss: 0.6514102816581726, acc: 0.4609375] [gan loss: 0.732209, acc: 0.484375]\n",
            "4444: [discriminator loss: 0.7291514277458191, acc: 0.09375] [gan loss: 1.621858, acc: 0.000000]\n",
            "4445: [discriminator loss: 0.6615152359008789, acc: 0.3984375] [gan loss: 0.925292, acc: 0.203125]\n",
            "4446: [discriminator loss: 0.7257088422775269, acc: 0.109375] [gan loss: 1.490808, acc: 0.000000]\n",
            "4447: [discriminator loss: 0.6267577409744263, acc: 0.4296875] [gan loss: 1.022707, acc: 0.093750]\n",
            "4448: [discriminator loss: 0.6466412544250488, acc: 0.2265625] [gan loss: 1.259661, acc: 0.031250]\n",
            "4449: [discriminator loss: 0.6554180979728699, acc: 0.265625] [gan loss: 1.252528, acc: 0.078125]\n",
            "4450: [discriminator loss: 0.6378743648529053, acc: 0.3359375] [gan loss: 1.193848, acc: 0.062500]\n",
            "4451: [discriminator loss: 0.6527525186538696, acc: 0.28125] [gan loss: 1.192151, acc: 0.062500]\n",
            "4452: [discriminator loss: 0.6581271290779114, acc: 0.265625] [gan loss: 1.239265, acc: 0.015625]\n",
            "4453: [discriminator loss: 0.6792595982551575, acc: 0.3046875] [gan loss: 1.259655, acc: 0.046875]\n",
            "4454: [discriminator loss: 0.6874833106994629, acc: 0.1875] [gan loss: 1.540367, acc: 0.000000]\n",
            "4455: [discriminator loss: 0.6329686045646667, acc: 0.421875] [gan loss: 0.839055, acc: 0.343750]\n",
            "4456: [discriminator loss: 0.6995408535003662, acc: 0.1328125] [gan loss: 1.859681, acc: 0.000000]\n",
            "4457: [discriminator loss: 0.6196600794792175, acc: 0.4765625] [gan loss: 0.684286, acc: 0.625000]\n",
            "4458: [discriminator loss: 0.7925658226013184, acc: 0.0390625] [gan loss: 2.055454, acc: 0.000000]\n",
            "4459: [discriminator loss: 0.6675902009010315, acc: 0.4921875] [gan loss: 0.591265, acc: 0.718750]\n",
            "4460: [discriminator loss: 0.8486354351043701, acc: 0.0078125] [gan loss: 1.867546, acc: 0.000000]\n",
            "4461: [discriminator loss: 0.6602921485900879, acc: 0.5] [gan loss: 0.668952, acc: 0.578125]\n",
            "4462: [discriminator loss: 0.7991620302200317, acc: 0.03125] [gan loss: 1.654047, acc: 0.000000]\n",
            "4463: [discriminator loss: 0.6437681913375854, acc: 0.4609375] [gan loss: 0.828721, acc: 0.296875]\n",
            "4464: [discriminator loss: 0.6943905353546143, acc: 0.125] [gan loss: 1.485299, acc: 0.000000]\n",
            "4465: [discriminator loss: 0.5984202027320862, acc: 0.453125] [gan loss: 0.958357, acc: 0.171875]\n",
            "4466: [discriminator loss: 0.6659693717956543, acc: 0.171875] [gan loss: 1.402694, acc: 0.000000]\n",
            "4467: [discriminator loss: 0.6360769271850586, acc: 0.3671875] [gan loss: 1.043636, acc: 0.031250]\n",
            "4468: [discriminator loss: 0.6736702919006348, acc: 0.171875] [gan loss: 1.372890, acc: 0.000000]\n",
            "4469: [discriminator loss: 0.6593515276908875, acc: 0.3203125] [gan loss: 1.130524, acc: 0.078125]\n",
            "4470: [discriminator loss: 0.6631511449813843, acc: 0.2265625] [gan loss: 1.327844, acc: 0.015625]\n",
            "4471: [discriminator loss: 0.6551876068115234, acc: 0.3125] [gan loss: 1.165595, acc: 0.078125]\n",
            "4472: [discriminator loss: 0.6610416769981384, acc: 0.2109375] [gan loss: 1.477848, acc: 0.015625]\n",
            "4473: [discriminator loss: 0.6309057474136353, acc: 0.40625] [gan loss: 1.020544, acc: 0.125000]\n",
            "4474: [discriminator loss: 0.6534945964813232, acc: 0.234375] [gan loss: 1.789312, acc: 0.015625]\n",
            "4475: [discriminator loss: 0.6498689651489258, acc: 0.40625] [gan loss: 0.773855, acc: 0.343750]\n",
            "4476: [discriminator loss: 0.7059066295623779, acc: 0.0859375] [gan loss: 1.814652, acc: 0.000000]\n",
            "4477: [discriminator loss: 0.6645078659057617, acc: 0.4453125] [gan loss: 0.746452, acc: 0.468750]\n",
            "4478: [discriminator loss: 0.7765355110168457, acc: 0.015625] [gan loss: 2.031599, acc: 0.000000]\n",
            "4479: [discriminator loss: 0.6649069786071777, acc: 0.46875] [gan loss: 0.715041, acc: 0.562500]\n",
            "4480: [discriminator loss: 0.7434622049331665, acc: 0.0703125] [gan loss: 1.649052, acc: 0.000000]\n",
            "4481: [discriminator loss: 0.6164183020591736, acc: 0.4765625] [gan loss: 0.725304, acc: 0.500000]\n",
            "4482: [discriminator loss: 0.7537277936935425, acc: 0.140625] [gan loss: 1.562225, acc: 0.000000]\n",
            "4483: [discriminator loss: 0.6406564116477966, acc: 0.421875] [gan loss: 1.009858, acc: 0.125000]\n",
            "4484: [discriminator loss: 0.6858450174331665, acc: 0.1484375] [gan loss: 1.452106, acc: 0.000000]\n",
            "4485: [discriminator loss: 0.6366642713546753, acc: 0.390625] [gan loss: 0.869341, acc: 0.281250]\n",
            "4486: [discriminator loss: 0.7270171642303467, acc: 0.1171875] [gan loss: 1.670819, acc: 0.000000]\n",
            "4487: [discriminator loss: 0.6430248618125916, acc: 0.4375] [gan loss: 0.808015, acc: 0.343750]\n",
            "4488: [discriminator loss: 0.7068281173706055, acc: 0.0859375] [gan loss: 1.731525, acc: 0.000000]\n",
            "4489: [discriminator loss: 0.6699717044830322, acc: 0.453125] [gan loss: 0.738730, acc: 0.437500]\n",
            "4490: [discriminator loss: 0.7815588116645813, acc: 0.0078125] [gan loss: 1.820662, acc: 0.000000]\n",
            "4491: [discriminator loss: 0.6594194173812866, acc: 0.484375] [gan loss: 0.638169, acc: 0.640625]\n",
            "4492: [discriminator loss: 0.8100650310516357, acc: 0.03125] [gan loss: 1.658841, acc: 0.015625]\n",
            "4493: [discriminator loss: 0.6601129770278931, acc: 0.4765625] [gan loss: 0.727781, acc: 0.484375]\n",
            "4494: [discriminator loss: 0.7480282187461853, acc: 0.0625] [gan loss: 1.453315, acc: 0.000000]\n",
            "4495: [discriminator loss: 0.6262578964233398, acc: 0.421875] [gan loss: 0.821154, acc: 0.296875]\n",
            "4496: [discriminator loss: 0.7149662375450134, acc: 0.140625] [gan loss: 1.413751, acc: 0.015625]\n",
            "4497: [discriminator loss: 0.6628126502037048, acc: 0.375] [gan loss: 0.986684, acc: 0.140625]\n",
            "4498: [discriminator loss: 0.6892281770706177, acc: 0.171875] [gan loss: 1.252302, acc: 0.046875]\n",
            "4499: [discriminator loss: 0.6207388639450073, acc: 0.3515625] [gan loss: 1.184692, acc: 0.015625]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "4500: [discriminator loss: 0.657129168510437, acc: 0.1875] [gan loss: 1.390857, acc: 0.000000]\n",
            "4501: [discriminator loss: 0.6492931842803955, acc: 0.328125] [gan loss: 1.070754, acc: 0.046875]\n",
            "4502: [discriminator loss: 0.7030918598175049, acc: 0.2109375] [gan loss: 1.355588, acc: 0.015625]\n",
            "4503: [discriminator loss: 0.6460800766944885, acc: 0.34375] [gan loss: 1.043229, acc: 0.109375]\n",
            "4504: [discriminator loss: 0.667250394821167, acc: 0.1875] [gan loss: 1.601878, acc: 0.000000]\n",
            "4505: [discriminator loss: 0.6154779195785522, acc: 0.4453125] [gan loss: 0.775227, acc: 0.375000]\n",
            "4506: [discriminator loss: 0.7013792991638184, acc: 0.1328125] [gan loss: 1.618836, acc: 0.000000]\n",
            "4507: [discriminator loss: 0.6713918447494507, acc: 0.421875] [gan loss: 0.824740, acc: 0.359375]\n",
            "4508: [discriminator loss: 0.7656893134117126, acc: 0.109375] [gan loss: 1.820812, acc: 0.000000]\n",
            "4509: [discriminator loss: 0.6889396905899048, acc: 0.46875] [gan loss: 0.724961, acc: 0.484375]\n",
            "4510: [discriminator loss: 0.776620090007782, acc: 0.046875] [gan loss: 1.822447, acc: 0.000000]\n",
            "4511: [discriminator loss: 0.6515734195709229, acc: 0.5] [gan loss: 0.683462, acc: 0.546875]\n",
            "4512: [discriminator loss: 0.7918096780776978, acc: 0.0234375] [gan loss: 1.811475, acc: 0.000000]\n",
            "4513: [discriminator loss: 0.6760993003845215, acc: 0.4296875] [gan loss: 0.833236, acc: 0.281250]\n",
            "4514: [discriminator loss: 0.7022400498390198, acc: 0.0859375] [gan loss: 1.448857, acc: 0.000000]\n",
            "4515: [discriminator loss: 0.6320948600769043, acc: 0.359375] [gan loss: 1.030353, acc: 0.078125]\n",
            "4516: [discriminator loss: 0.6939730644226074, acc: 0.140625] [gan loss: 1.550673, acc: 0.000000]\n",
            "4517: [discriminator loss: 0.6558899879455566, acc: 0.390625] [gan loss: 1.051464, acc: 0.109375]\n",
            "4518: [discriminator loss: 0.738250195980072, acc: 0.109375] [gan loss: 1.740824, acc: 0.000000]\n",
            "4519: [discriminator loss: 0.6409116983413696, acc: 0.46875] [gan loss: 0.741682, acc: 0.390625]\n",
            "4520: [discriminator loss: 0.744255781173706, acc: 0.046875] [gan loss: 1.631244, acc: 0.000000]\n",
            "4521: [discriminator loss: 0.6236681342124939, acc: 0.4765625] [gan loss: 0.807416, acc: 0.328125]\n",
            "4522: [discriminator loss: 0.7111682891845703, acc: 0.09375] [gan loss: 1.440863, acc: 0.000000]\n",
            "4523: [discriminator loss: 0.6388487219810486, acc: 0.3984375] [gan loss: 1.004484, acc: 0.156250]\n",
            "4524: [discriminator loss: 0.6934106349945068, acc: 0.125] [gan loss: 1.636968, acc: 0.000000]\n",
            "4525: [discriminator loss: 0.6190327405929565, acc: 0.4609375] [gan loss: 0.796694, acc: 0.375000]\n",
            "4526: [discriminator loss: 0.749218225479126, acc: 0.0703125] [gan loss: 1.896193, acc: 0.000000]\n",
            "4527: [discriminator loss: 0.6427789926528931, acc: 0.484375] [gan loss: 0.826777, acc: 0.296875]\n",
            "4528: [discriminator loss: 0.7122303247451782, acc: 0.0546875] [gan loss: 1.892105, acc: 0.000000]\n",
            "4529: [discriminator loss: 0.689159095287323, acc: 0.453125] [gan loss: 0.656023, acc: 0.734375]\n",
            "4530: [discriminator loss: 0.7397976517677307, acc: 0.0625] [gan loss: 1.591841, acc: 0.000000]\n",
            "4531: [discriminator loss: 0.6533623337745667, acc: 0.421875] [gan loss: 0.887100, acc: 0.265625]\n",
            "4532: [discriminator loss: 0.7094632387161255, acc: 0.09375] [gan loss: 1.709155, acc: 0.000000]\n",
            "4533: [discriminator loss: 0.6342223882675171, acc: 0.4609375] [gan loss: 0.764729, acc: 0.421875]\n",
            "4534: [discriminator loss: 0.6938544511795044, acc: 0.109375] [gan loss: 1.642654, acc: 0.000000]\n",
            "4535: [discriminator loss: 0.7052090167999268, acc: 0.421875] [gan loss: 0.776307, acc: 0.375000]\n",
            "4536: [discriminator loss: 0.7654537558555603, acc: 0.0703125] [gan loss: 1.659828, acc: 0.000000]\n",
            "4537: [discriminator loss: 0.6267894506454468, acc: 0.4609375] [gan loss: 0.847734, acc: 0.296875]\n",
            "4538: [discriminator loss: 0.7288405299186707, acc: 0.0625] [gan loss: 1.586297, acc: 0.000000]\n",
            "4539: [discriminator loss: 0.6652334332466125, acc: 0.40625] [gan loss: 0.916326, acc: 0.171875]\n",
            "4540: [discriminator loss: 0.7077622413635254, acc: 0.1015625] [gan loss: 1.542930, acc: 0.000000]\n",
            "4541: [discriminator loss: 0.6616392135620117, acc: 0.3828125] [gan loss: 0.895249, acc: 0.140625]\n",
            "4542: [discriminator loss: 0.7117347121238708, acc: 0.0703125] [gan loss: 1.641848, acc: 0.000000]\n",
            "4543: [discriminator loss: 0.6305762529373169, acc: 0.4453125] [gan loss: 0.802250, acc: 0.312500]\n",
            "4544: [discriminator loss: 0.7093712091445923, acc: 0.0703125] [gan loss: 1.567891, acc: 0.000000]\n",
            "4545: [discriminator loss: 0.6490687131881714, acc: 0.4296875] [gan loss: 0.815067, acc: 0.265625]\n",
            "4546: [discriminator loss: 0.7478223443031311, acc: 0.0625] [gan loss: 1.695426, acc: 0.000000]\n",
            "4547: [discriminator loss: 0.6846685409545898, acc: 0.4375] [gan loss: 0.747342, acc: 0.468750]\n",
            "4548: [discriminator loss: 0.7061176896095276, acc: 0.09375] [gan loss: 1.714598, acc: 0.000000]\n",
            "4549: [discriminator loss: 0.6181440949440002, acc: 0.484375] [gan loss: 0.837992, acc: 0.359375]\n",
            "4550: [discriminator loss: 0.7415454983711243, acc: 0.0546875] [gan loss: 1.637866, acc: 0.000000]\n",
            "4551: [discriminator loss: 0.6284561157226562, acc: 0.40625] [gan loss: 0.930106, acc: 0.156250]\n",
            "4552: [discriminator loss: 0.7109042406082153, acc: 0.125] [gan loss: 1.555702, acc: 0.015625]\n",
            "4553: [discriminator loss: 0.6665292382240295, acc: 0.3984375] [gan loss: 0.922599, acc: 0.187500]\n",
            "4554: [discriminator loss: 0.6738898754119873, acc: 0.15625] [gan loss: 1.640650, acc: 0.000000]\n",
            "4555: [discriminator loss: 0.668753981590271, acc: 0.4140625] [gan loss: 1.001332, acc: 0.171875]\n",
            "4556: [discriminator loss: 0.704118013381958, acc: 0.1171875] [gan loss: 1.740464, acc: 0.000000]\n",
            "4557: [discriminator loss: 0.6620367765426636, acc: 0.453125] [gan loss: 0.650655, acc: 0.546875]\n",
            "4558: [discriminator loss: 0.769245982170105, acc: 0.0625] [gan loss: 1.789371, acc: 0.000000]\n",
            "4559: [discriminator loss: 0.660670280456543, acc: 0.421875] [gan loss: 0.811427, acc: 0.265625]\n",
            "4560: [discriminator loss: 0.740437924861908, acc: 0.046875] [gan loss: 1.639038, acc: 0.000000]\n",
            "4561: [discriminator loss: 0.6403502821922302, acc: 0.453125] [gan loss: 0.747551, acc: 0.375000]\n",
            "4562: [discriminator loss: 0.7358993887901306, acc: 0.0546875] [gan loss: 1.902022, acc: 0.000000]\n",
            "4563: [discriminator loss: 0.6659125685691833, acc: 0.46875] [gan loss: 0.824037, acc: 0.281250]\n",
            "4564: [discriminator loss: 0.7112155556678772, acc: 0.109375] [gan loss: 1.638056, acc: 0.015625]\n",
            "4565: [discriminator loss: 0.6161275506019592, acc: 0.4453125] [gan loss: 0.859622, acc: 0.281250]\n",
            "4566: [discriminator loss: 0.7379058599472046, acc: 0.03125] [gan loss: 1.637787, acc: 0.000000]\n",
            "4567: [discriminator loss: 0.6377646327018738, acc: 0.4765625] [gan loss: 0.767922, acc: 0.406250]\n",
            "4568: [discriminator loss: 0.7258073687553406, acc: 0.0703125] [gan loss: 1.622076, acc: 0.000000]\n",
            "4569: [discriminator loss: 0.6451221704483032, acc: 0.4296875] [gan loss: 0.818244, acc: 0.359375]\n",
            "4570: [discriminator loss: 0.7295263409614563, acc: 0.0859375] [gan loss: 1.628429, acc: 0.000000]\n",
            "4571: [discriminator loss: 0.6643514633178711, acc: 0.375] [gan loss: 0.996877, acc: 0.140625]\n",
            "4572: [discriminator loss: 0.7038884162902832, acc: 0.171875] [gan loss: 1.437948, acc: 0.015625]\n",
            "4573: [discriminator loss: 0.6189079284667969, acc: 0.40625] [gan loss: 0.927766, acc: 0.109375]\n",
            "4574: [discriminator loss: 0.6748861074447632, acc: 0.1796875] [gan loss: 1.418380, acc: 0.015625]\n",
            "4575: [discriminator loss: 0.5982029438018799, acc: 0.3984375] [gan loss: 1.017926, acc: 0.156250]\n",
            "4576: [discriminator loss: 0.7196600437164307, acc: 0.1484375] [gan loss: 1.484411, acc: 0.000000]\n",
            "4577: [discriminator loss: 0.605364203453064, acc: 0.4453125] [gan loss: 0.835413, acc: 0.250000]\n",
            "4578: [discriminator loss: 0.7308424711227417, acc: 0.09375] [gan loss: 1.805960, acc: 0.000000]\n",
            "4579: [discriminator loss: 0.6614395976066589, acc: 0.453125] [gan loss: 0.715867, acc: 0.484375]\n",
            "4580: [discriminator loss: 0.7677091956138611, acc: 0.03125] [gan loss: 1.972128, acc: 0.000000]\n",
            "4581: [discriminator loss: 0.6923009753227234, acc: 0.484375] [gan loss: 0.610233, acc: 0.718750]\n",
            "4582: [discriminator loss: 0.7718662023544312, acc: 0.046875] [gan loss: 1.718317, acc: 0.000000]\n",
            "4583: [discriminator loss: 0.6468446254730225, acc: 0.4375] [gan loss: 0.715270, acc: 0.421875]\n",
            "4584: [discriminator loss: 0.7451827526092529, acc: 0.0546875] [gan loss: 1.527641, acc: 0.000000]\n",
            "4585: [discriminator loss: 0.631350040435791, acc: 0.4140625] [gan loss: 0.917251, acc: 0.187500]\n",
            "4586: [discriminator loss: 0.7146918773651123, acc: 0.1015625] [gan loss: 1.376435, acc: 0.015625]\n",
            "4587: [discriminator loss: 0.6312317252159119, acc: 0.3828125] [gan loss: 1.082767, acc: 0.062500]\n",
            "4588: [discriminator loss: 0.682628333568573, acc: 0.1640625] [gan loss: 1.498514, acc: 0.000000]\n",
            "4589: [discriminator loss: 0.6230434775352478, acc: 0.40625] [gan loss: 0.875891, acc: 0.281250]\n",
            "4590: [discriminator loss: 0.7116512060165405, acc: 0.0859375] [gan loss: 1.552617, acc: 0.000000]\n",
            "4591: [discriminator loss: 0.6363549828529358, acc: 0.4375] [gan loss: 0.858747, acc: 0.296875]\n",
            "4592: [discriminator loss: 0.7272372245788574, acc: 0.0546875] [gan loss: 1.815541, acc: 0.000000]\n",
            "4593: [discriminator loss: 0.6032543182373047, acc: 0.46875] [gan loss: 0.748034, acc: 0.406250]\n",
            "4594: [discriminator loss: 0.7273890972137451, acc: 0.09375] [gan loss: 1.842958, acc: 0.000000]\n",
            "4595: [discriminator loss: 0.6339679956436157, acc: 0.4609375] [gan loss: 0.736035, acc: 0.531250]\n",
            "4596: [discriminator loss: 0.765961229801178, acc: 0.0859375] [gan loss: 1.722847, acc: 0.015625]\n",
            "4597: [discriminator loss: 0.6456835269927979, acc: 0.453125] [gan loss: 0.780616, acc: 0.484375]\n",
            "4598: [discriminator loss: 0.7174541354179382, acc: 0.0703125] [gan loss: 1.675117, acc: 0.015625]\n",
            "4599: [discriminator loss: 0.6734323501586914, acc: 0.4296875] [gan loss: 0.897026, acc: 0.281250]\n",
            "4600: [discriminator loss: 0.7373942136764526, acc: 0.0625] [gan loss: 1.743082, acc: 0.000000]\n",
            "4601: [discriminator loss: 0.6640066504478455, acc: 0.4375] [gan loss: 0.874054, acc: 0.234375]\n",
            "4602: [discriminator loss: 0.7292503714561462, acc: 0.0703125] [gan loss: 1.541270, acc: 0.000000]\n",
            "4603: [discriminator loss: 0.6296640038490295, acc: 0.4453125] [gan loss: 0.813474, acc: 0.281250]\n",
            "4604: [discriminator loss: 0.7710789442062378, acc: 0.03125] [gan loss: 1.642427, acc: 0.000000]\n",
            "4605: [discriminator loss: 0.619525134563446, acc: 0.4609375] [gan loss: 0.880730, acc: 0.156250]\n",
            "4606: [discriminator loss: 0.7227040529251099, acc: 0.1171875] [gan loss: 1.616714, acc: 0.000000]\n",
            "4607: [discriminator loss: 0.6160950660705566, acc: 0.453125] [gan loss: 0.887632, acc: 0.234375]\n",
            "4608: [discriminator loss: 0.7376295328140259, acc: 0.109375] [gan loss: 1.583168, acc: 0.000000]\n",
            "4609: [discriminator loss: 0.6147464513778687, acc: 0.4375] [gan loss: 0.892187, acc: 0.234375]\n",
            "4610: [discriminator loss: 0.6893278360366821, acc: 0.1171875] [gan loss: 1.567834, acc: 0.000000]\n",
            "4611: [discriminator loss: 0.6456407308578491, acc: 0.453125] [gan loss: 0.792646, acc: 0.312500]\n",
            "4612: [discriminator loss: 0.7141513824462891, acc: 0.109375] [gan loss: 1.731879, acc: 0.000000]\n",
            "4613: [discriminator loss: 0.6753722429275513, acc: 0.421875] [gan loss: 0.839070, acc: 0.312500]\n",
            "4614: [discriminator loss: 0.7460885047912598, acc: 0.078125] [gan loss: 1.729253, acc: 0.000000]\n",
            "4615: [discriminator loss: 0.6569072008132935, acc: 0.453125] [gan loss: 0.761918, acc: 0.406250]\n",
            "4616: [discriminator loss: 0.7396005988121033, acc: 0.0546875] [gan loss: 1.731854, acc: 0.015625]\n",
            "4617: [discriminator loss: 0.6581019759178162, acc: 0.4453125] [gan loss: 0.821819, acc: 0.281250]\n",
            "4618: [discriminator loss: 0.7235910892486572, acc: 0.0625] [gan loss: 1.737838, acc: 0.000000]\n",
            "4619: [discriminator loss: 0.6835471391677856, acc: 0.453125] [gan loss: 0.787157, acc: 0.406250]\n",
            "4620: [discriminator loss: 0.7386810779571533, acc: 0.09375] [gan loss: 1.550367, acc: 0.000000]\n",
            "4621: [discriminator loss: 0.6311755180358887, acc: 0.40625] [gan loss: 0.904657, acc: 0.156250]\n",
            "4622: [discriminator loss: 0.6933013200759888, acc: 0.140625] [gan loss: 1.505844, acc: 0.000000]\n",
            "4623: [discriminator loss: 0.6873854994773865, acc: 0.3828125] [gan loss: 0.968903, acc: 0.156250]\n",
            "4624: [discriminator loss: 0.6949766874313354, acc: 0.125] [gan loss: 1.510580, acc: 0.015625]\n",
            "4625: [discriminator loss: 0.6863553524017334, acc: 0.359375] [gan loss: 0.937621, acc: 0.187500]\n",
            "4626: [discriminator loss: 0.7344516515731812, acc: 0.0859375] [gan loss: 1.633637, acc: 0.000000]\n",
            "4627: [discriminator loss: 0.6202188730239868, acc: 0.4453125] [gan loss: 0.816627, acc: 0.343750]\n",
            "4628: [discriminator loss: 0.7066277861595154, acc: 0.09375] [gan loss: 1.873844, acc: 0.000000]\n",
            "4629: [discriminator loss: 0.6564289331436157, acc: 0.484375] [gan loss: 0.752350, acc: 0.453125]\n",
            "4630: [discriminator loss: 0.7951313257217407, acc: 0.0390625] [gan loss: 1.861144, acc: 0.000000]\n",
            "4631: [discriminator loss: 0.6691727638244629, acc: 0.4921875] [gan loss: 0.663905, acc: 0.609375]\n",
            "4632: [discriminator loss: 0.7644703388214111, acc: 0.03125] [gan loss: 1.746145, acc: 0.000000]\n",
            "4633: [discriminator loss: 0.6546168327331543, acc: 0.4453125] [gan loss: 0.874642, acc: 0.281250]\n",
            "4634: [discriminator loss: 0.7273455858230591, acc: 0.0546875] [gan loss: 1.612821, acc: 0.000000]\n",
            "4635: [discriminator loss: 0.665218710899353, acc: 0.3828125] [gan loss: 0.961292, acc: 0.171875]\n",
            "4636: [discriminator loss: 0.710146427154541, acc: 0.171875] [gan loss: 1.364679, acc: 0.000000]\n",
            "4637: [discriminator loss: 0.6519252061843872, acc: 0.4140625] [gan loss: 0.936024, acc: 0.171875]\n",
            "4638: [discriminator loss: 0.6731294989585876, acc: 0.15625] [gan loss: 1.542163, acc: 0.000000]\n",
            "4639: [discriminator loss: 0.6616369485855103, acc: 0.4140625] [gan loss: 0.932595, acc: 0.187500]\n",
            "4640: [discriminator loss: 0.7077348828315735, acc: 0.1015625] [gan loss: 1.707268, acc: 0.000000]\n",
            "4641: [discriminator loss: 0.6226902008056641, acc: 0.453125] [gan loss: 0.728957, acc: 0.500000]\n",
            "4642: [discriminator loss: 0.7279968857765198, acc: 0.0703125] [gan loss: 1.715087, acc: 0.015625]\n",
            "4643: [discriminator loss: 0.647441029548645, acc: 0.4609375] [gan loss: 0.675128, acc: 0.578125]\n",
            "4644: [discriminator loss: 0.7692168951034546, acc: 0.046875] [gan loss: 1.724150, acc: 0.000000]\n",
            "4645: [discriminator loss: 0.6752490997314453, acc: 0.484375] [gan loss: 0.778085, acc: 0.359375]\n",
            "4646: [discriminator loss: 0.7914475202560425, acc: 0.0390625] [gan loss: 1.789111, acc: 0.000000]\n",
            "4647: [discriminator loss: 0.6697331666946411, acc: 0.4609375] [gan loss: 0.781036, acc: 0.390625]\n",
            "4648: [discriminator loss: 0.7710475921630859, acc: 0.0546875] [gan loss: 1.714191, acc: 0.000000]\n",
            "4649: [discriminator loss: 0.6584666967391968, acc: 0.46875] [gan loss: 0.858470, acc: 0.265625]\n",
            "4650: [discriminator loss: 0.7089832425117493, acc: 0.0859375] [gan loss: 1.535532, acc: 0.015625]\n",
            "4651: [discriminator loss: 0.6631151437759399, acc: 0.4296875] [gan loss: 0.923074, acc: 0.140625]\n",
            "4652: [discriminator loss: 0.7086919546127319, acc: 0.09375] [gan loss: 1.453361, acc: 0.000000]\n",
            "4653: [discriminator loss: 0.634554386138916, acc: 0.390625] [gan loss: 0.962329, acc: 0.171875]\n",
            "4654: [discriminator loss: 0.6276566982269287, acc: 0.2890625] [gan loss: 1.279167, acc: 0.062500]\n",
            "4655: [discriminator loss: 0.6526422500610352, acc: 0.328125] [gan loss: 1.093687, acc: 0.125000]\n",
            "4656: [discriminator loss: 0.7105045318603516, acc: 0.1484375] [gan loss: 1.551112, acc: 0.000000]\n",
            "4657: [discriminator loss: 0.6365838646888733, acc: 0.421875] [gan loss: 0.924254, acc: 0.250000]\n",
            "4658: [discriminator loss: 0.7089847326278687, acc: 0.125] [gan loss: 1.562451, acc: 0.000000]\n",
            "4659: [discriminator loss: 0.6462457180023193, acc: 0.40625] [gan loss: 0.799169, acc: 0.375000]\n",
            "4660: [discriminator loss: 0.7447795867919922, acc: 0.1015625] [gan loss: 1.853101, acc: 0.000000]\n",
            "4661: [discriminator loss: 0.6889501810073853, acc: 0.4296875] [gan loss: 0.853388, acc: 0.375000]\n",
            "4662: [discriminator loss: 0.7314057350158691, acc: 0.0625] [gan loss: 1.948664, acc: 0.000000]\n",
            "4663: [discriminator loss: 0.6985947489738464, acc: 0.4765625] [gan loss: 0.680681, acc: 0.562500]\n",
            "4664: [discriminator loss: 0.8201625943183899, acc: 0.0234375] [gan loss: 1.956506, acc: 0.000000]\n",
            "4665: [discriminator loss: 0.6431141495704651, acc: 0.484375] [gan loss: 0.734600, acc: 0.421875]\n",
            "4666: [discriminator loss: 0.7519405484199524, acc: 0.015625] [gan loss: 1.710664, acc: 0.000000]\n",
            "4667: [discriminator loss: 0.6428182125091553, acc: 0.4765625] [gan loss: 0.685339, acc: 0.500000]\n",
            "4668: [discriminator loss: 0.7594529390335083, acc: 0.0078125] [gan loss: 1.593655, acc: 0.000000]\n",
            "4669: [discriminator loss: 0.6470726132392883, acc: 0.390625] [gan loss: 0.971758, acc: 0.125000]\n",
            "4670: [discriminator loss: 0.699195384979248, acc: 0.1484375] [gan loss: 1.361766, acc: 0.000000]\n",
            "4671: [discriminator loss: 0.6503371000289917, acc: 0.359375] [gan loss: 0.991750, acc: 0.156250]\n",
            "4672: [discriminator loss: 0.6882361769676208, acc: 0.109375] [gan loss: 1.540290, acc: 0.000000]\n",
            "4673: [discriminator loss: 0.6261078119277954, acc: 0.421875] [gan loss: 1.072409, acc: 0.062500]\n",
            "4674: [discriminator loss: 0.6779658794403076, acc: 0.171875] [gan loss: 1.541402, acc: 0.000000]\n",
            "4675: [discriminator loss: 0.6569360494613647, acc: 0.3828125] [gan loss: 0.908283, acc: 0.125000]\n",
            "4676: [discriminator loss: 0.7518371343612671, acc: 0.1328125] [gan loss: 1.624200, acc: 0.000000]\n",
            "4677: [discriminator loss: 0.6417049169540405, acc: 0.4453125] [gan loss: 0.721296, acc: 0.468750]\n",
            "4678: [discriminator loss: 0.7491934895515442, acc: 0.0078125] [gan loss: 1.847453, acc: 0.000000]\n",
            "4679: [discriminator loss: 0.6411198377609253, acc: 0.4921875] [gan loss: 0.624920, acc: 0.718750]\n",
            "4680: [discriminator loss: 0.7979384660720825, acc: 0.015625] [gan loss: 1.715136, acc: 0.000000]\n",
            "4681: [discriminator loss: 0.6585919260978699, acc: 0.40625] [gan loss: 1.063048, acc: 0.109375]\n",
            "4682: [discriminator loss: 0.6828024387359619, acc: 0.203125] [gan loss: 1.469351, acc: 0.015625]\n",
            "4683: [discriminator loss: 0.6641484498977661, acc: 0.375] [gan loss: 0.954468, acc: 0.203125]\n",
            "4684: [discriminator loss: 0.7139598727226257, acc: 0.09375] [gan loss: 1.673378, acc: 0.015625]\n",
            "4685: [discriminator loss: 0.6783449649810791, acc: 0.4296875] [gan loss: 0.969267, acc: 0.125000]\n",
            "4686: [discriminator loss: 0.7096623182296753, acc: 0.1171875] [gan loss: 1.794016, acc: 0.000000]\n",
            "4687: [discriminator loss: 0.6472129225730896, acc: 0.4453125] [gan loss: 0.870780, acc: 0.187500]\n",
            "4688: [discriminator loss: 0.6885157823562622, acc: 0.0859375] [gan loss: 1.475296, acc: 0.015625]\n",
            "4689: [discriminator loss: 0.664667546749115, acc: 0.3359375] [gan loss: 0.980173, acc: 0.140625]\n",
            "4690: [discriminator loss: 0.6701372265815735, acc: 0.21875] [gan loss: 1.480488, acc: 0.000000]\n",
            "4691: [discriminator loss: 0.6471185684204102, acc: 0.40625] [gan loss: 0.963727, acc: 0.140625]\n",
            "4692: [discriminator loss: 0.7207375168800354, acc: 0.1484375] [gan loss: 1.649072, acc: 0.000000]\n",
            "4693: [discriminator loss: 0.6547919511795044, acc: 0.4296875] [gan loss: 0.813821, acc: 0.281250]\n",
            "4694: [discriminator loss: 0.6858078241348267, acc: 0.1171875] [gan loss: 1.701703, acc: 0.000000]\n",
            "4695: [discriminator loss: 0.6233861446380615, acc: 0.4609375] [gan loss: 0.716277, acc: 0.546875]\n",
            "4696: [discriminator loss: 0.7569020986557007, acc: 0.0703125] [gan loss: 1.897030, acc: 0.000000]\n",
            "4697: [discriminator loss: 0.7048892974853516, acc: 0.4765625] [gan loss: 0.597463, acc: 0.703125]\n",
            "4698: [discriminator loss: 0.7727746367454529, acc: 0.046875] [gan loss: 1.701070, acc: 0.000000]\n",
            "4699: [discriminator loss: 0.7033609747886658, acc: 0.453125] [gan loss: 0.748944, acc: 0.406250]\n",
            "4700: [discriminator loss: 0.7728474140167236, acc: 0.0078125] [gan loss: 1.829163, acc: 0.000000]\n",
            "4701: [discriminator loss: 0.6431249380111694, acc: 0.453125] [gan loss: 0.829965, acc: 0.312500]\n",
            "4702: [discriminator loss: 0.7304647564888, acc: 0.09375] [gan loss: 1.470154, acc: 0.000000]\n",
            "4703: [discriminator loss: 0.6400399208068848, acc: 0.421875] [gan loss: 1.055233, acc: 0.093750]\n",
            "4704: [discriminator loss: 0.6887981295585632, acc: 0.140625] [gan loss: 1.419390, acc: 0.015625]\n",
            "4705: [discriminator loss: 0.6377769708633423, acc: 0.421875] [gan loss: 0.901073, acc: 0.187500]\n",
            "4706: [discriminator loss: 0.6789200901985168, acc: 0.1484375] [gan loss: 1.474328, acc: 0.000000]\n",
            "4707: [discriminator loss: 0.6282552480697632, acc: 0.4140625] [gan loss: 0.908716, acc: 0.218750]\n",
            "4708: [discriminator loss: 0.6799503564834595, acc: 0.140625] [gan loss: 1.638665, acc: 0.000000]\n",
            "4709: [discriminator loss: 0.6694721579551697, acc: 0.4296875] [gan loss: 0.771822, acc: 0.421875]\n",
            "4710: [discriminator loss: 0.7337794303894043, acc: 0.0625] [gan loss: 1.878342, acc: 0.000000]\n",
            "4711: [discriminator loss: 0.6743911504745483, acc: 0.4765625] [gan loss: 0.663125, acc: 0.578125]\n",
            "4712: [discriminator loss: 0.7752668857574463, acc: 0.0390625] [gan loss: 1.810601, acc: 0.000000]\n",
            "4713: [discriminator loss: 0.6686612963676453, acc: 0.46875] [gan loss: 0.710997, acc: 0.484375]\n",
            "4714: [discriminator loss: 0.7445306777954102, acc: 0.0390625] [gan loss: 1.624930, acc: 0.000000]\n",
            "4715: [discriminator loss: 0.6441773176193237, acc: 0.453125] [gan loss: 0.725541, acc: 0.468750]\n",
            "4716: [discriminator loss: 0.7759692668914795, acc: 0.046875] [gan loss: 1.760337, acc: 0.000000]\n",
            "4717: [discriminator loss: 0.6754980683326721, acc: 0.46875] [gan loss: 0.747889, acc: 0.531250]\n",
            "4718: [discriminator loss: 0.7387233376502991, acc: 0.09375] [gan loss: 1.545083, acc: 0.000000]\n",
            "4719: [discriminator loss: 0.6878824234008789, acc: 0.3828125] [gan loss: 0.887592, acc: 0.156250]\n",
            "4720: [discriminator loss: 0.7126768827438354, acc: 0.09375] [gan loss: 1.601527, acc: 0.000000]\n",
            "4721: [discriminator loss: 0.6748279929161072, acc: 0.4453125] [gan loss: 0.813527, acc: 0.328125]\n",
            "4722: [discriminator loss: 0.7083932161331177, acc: 0.0859375] [gan loss: 1.655168, acc: 0.015625]\n",
            "4723: [discriminator loss: 0.6414414644241333, acc: 0.4453125] [gan loss: 0.857495, acc: 0.312500]\n",
            "4724: [discriminator loss: 0.7185533046722412, acc: 0.09375] [gan loss: 1.531057, acc: 0.046875]\n",
            "4725: [discriminator loss: 0.6886613368988037, acc: 0.375] [gan loss: 0.939646, acc: 0.187500]\n",
            "4726: [discriminator loss: 0.6802802085876465, acc: 0.125] [gan loss: 1.613565, acc: 0.000000]\n",
            "4727: [discriminator loss: 0.6755597591400146, acc: 0.3515625] [gan loss: 1.051059, acc: 0.046875]\n",
            "4728: [discriminator loss: 0.6616097688674927, acc: 0.15625] [gan loss: 1.540061, acc: 0.000000]\n",
            "4729: [discriminator loss: 0.6333253383636475, acc: 0.421875] [gan loss: 0.903814, acc: 0.203125]\n",
            "4730: [discriminator loss: 0.6965159177780151, acc: 0.1875] [gan loss: 1.426063, acc: 0.000000]\n",
            "4731: [discriminator loss: 0.6558146476745605, acc: 0.3984375] [gan loss: 0.845442, acc: 0.265625]\n",
            "4732: [discriminator loss: 0.7246989011764526, acc: 0.0859375] [gan loss: 1.736818, acc: 0.000000]\n",
            "4733: [discriminator loss: 0.6457067728042603, acc: 0.4921875] [gan loss: 0.622729, acc: 0.671875]\n",
            "4734: [discriminator loss: 0.7691466808319092, acc: 0.03125] [gan loss: 1.828086, acc: 0.000000]\n",
            "4735: [discriminator loss: 0.7132563591003418, acc: 0.4609375] [gan loss: 0.667814, acc: 0.562500]\n",
            "4736: [discriminator loss: 0.7889243364334106, acc: 0.03125] [gan loss: 1.861384, acc: 0.000000]\n",
            "4737: [discriminator loss: 0.6320903301239014, acc: 0.4921875] [gan loss: 0.722628, acc: 0.515625]\n",
            "4738: [discriminator loss: 0.7548636198043823, acc: 0.0625] [gan loss: 1.681059, acc: 0.000000]\n",
            "4739: [discriminator loss: 0.6799236536026001, acc: 0.4296875] [gan loss: 0.792571, acc: 0.375000]\n",
            "4740: [discriminator loss: 0.736543595790863, acc: 0.046875] [gan loss: 1.487946, acc: 0.000000]\n",
            "4741: [discriminator loss: 0.6717228293418884, acc: 0.453125] [gan loss: 0.797470, acc: 0.343750]\n",
            "4742: [discriminator loss: 0.7149602770805359, acc: 0.1015625] [gan loss: 1.518185, acc: 0.015625]\n",
            "4743: [discriminator loss: 0.652866780757904, acc: 0.40625] [gan loss: 0.847819, acc: 0.234375]\n",
            "4744: [discriminator loss: 0.7122834920883179, acc: 0.1328125] [gan loss: 1.422467, acc: 0.000000]\n",
            "4745: [discriminator loss: 0.6492237448692322, acc: 0.3828125] [gan loss: 1.112868, acc: 0.078125]\n",
            "4746: [discriminator loss: 0.677108883857727, acc: 0.2421875] [gan loss: 1.322029, acc: 0.015625]\n",
            "4747: [discriminator loss: 0.6440649032592773, acc: 0.3359375] [gan loss: 1.153969, acc: 0.078125]\n",
            "4748: [discriminator loss: 0.6727660298347473, acc: 0.1875] [gan loss: 1.409834, acc: 0.000000]\n",
            "4749: [discriminator loss: 0.6494650840759277, acc: 0.3671875] [gan loss: 0.959946, acc: 0.187500]\n",
            "4750: [discriminator loss: 0.6687527894973755, acc: 0.1796875] [gan loss: 1.508805, acc: 0.000000]\n",
            "4751: [discriminator loss: 0.6678057909011841, acc: 0.421875] [gan loss: 0.844460, acc: 0.296875]\n",
            "4752: [discriminator loss: 0.7181910276412964, acc: 0.1171875] [gan loss: 1.652260, acc: 0.000000]\n",
            "4753: [discriminator loss: 0.6623443365097046, acc: 0.4296875] [gan loss: 0.764934, acc: 0.421875]\n",
            "4754: [discriminator loss: 0.7126917839050293, acc: 0.1015625] [gan loss: 1.546914, acc: 0.000000]\n",
            "4755: [discriminator loss: 0.6284904479980469, acc: 0.4375] [gan loss: 0.840184, acc: 0.265625]\n",
            "4756: [discriminator loss: 0.7318924069404602, acc: 0.0703125] [gan loss: 1.839936, acc: 0.000000]\n",
            "4757: [discriminator loss: 0.6438025236129761, acc: 0.484375] [gan loss: 0.731816, acc: 0.453125]\n",
            "4758: [discriminator loss: 0.7792127132415771, acc: 0.015625] [gan loss: 1.903730, acc: 0.000000]\n",
            "4759: [discriminator loss: 0.6624304056167603, acc: 0.5] [gan loss: 0.534219, acc: 0.812500]\n",
            "4760: [discriminator loss: 0.8400105237960815, acc: 0.015625] [gan loss: 1.752561, acc: 0.000000]\n",
            "4761: [discriminator loss: 0.6802177429199219, acc: 0.5] [gan loss: 0.672777, acc: 0.625000]\n",
            "4762: [discriminator loss: 0.7301838397979736, acc: 0.0625] [gan loss: 1.524192, acc: 0.000000]\n",
            "4763: [discriminator loss: 0.6378037929534912, acc: 0.421875] [gan loss: 0.895707, acc: 0.187500]\n",
            "4764: [discriminator loss: 0.7189105749130249, acc: 0.0859375] [gan loss: 1.566031, acc: 0.000000]\n",
            "4765: [discriminator loss: 0.6235189437866211, acc: 0.4296875] [gan loss: 0.967022, acc: 0.125000]\n",
            "4766: [discriminator loss: 0.7444206476211548, acc: 0.1015625] [gan loss: 1.652873, acc: 0.015625]\n",
            "4767: [discriminator loss: 0.6815789341926575, acc: 0.4296875] [gan loss: 0.897223, acc: 0.218750]\n",
            "4768: [discriminator loss: 0.7098872661590576, acc: 0.0625] [gan loss: 1.670421, acc: 0.000000]\n",
            "4769: [discriminator loss: 0.6372373700141907, acc: 0.4453125] [gan loss: 0.843842, acc: 0.265625]\n",
            "4770: [discriminator loss: 0.7168713212013245, acc: 0.0625] [gan loss: 1.502373, acc: 0.000000]\n",
            "4771: [discriminator loss: 0.6445040702819824, acc: 0.4375] [gan loss: 0.899917, acc: 0.281250]\n",
            "4772: [discriminator loss: 0.7340426445007324, acc: 0.0703125] [gan loss: 1.623509, acc: 0.000000]\n",
            "4773: [discriminator loss: 0.6674380302429199, acc: 0.40625] [gan loss: 0.960187, acc: 0.093750]\n",
            "4774: [discriminator loss: 0.7055838108062744, acc: 0.1328125] [gan loss: 1.390652, acc: 0.031250]\n",
            "4775: [discriminator loss: 0.6574666500091553, acc: 0.390625] [gan loss: 1.003816, acc: 0.093750]\n",
            "4776: [discriminator loss: 0.6881005167961121, acc: 0.15625] [gan loss: 1.618980, acc: 0.000000]\n",
            "4777: [discriminator loss: 0.6516710519790649, acc: 0.40625] [gan loss: 0.918569, acc: 0.203125]\n",
            "4778: [discriminator loss: 0.7524703145027161, acc: 0.0546875] [gan loss: 1.779992, acc: 0.000000]\n",
            "4779: [discriminator loss: 0.5981969833374023, acc: 0.484375] [gan loss: 0.663777, acc: 0.656250]\n",
            "4780: [discriminator loss: 0.793359637260437, acc: 0.0390625] [gan loss: 1.900651, acc: 0.000000]\n",
            "4781: [discriminator loss: 0.6502736806869507, acc: 0.4921875] [gan loss: 0.676503, acc: 0.562500]\n",
            "4782: [discriminator loss: 0.7565314769744873, acc: 0.0234375] [gan loss: 1.611076, acc: 0.000000]\n",
            "4783: [discriminator loss: 0.6668038964271545, acc: 0.421875] [gan loss: 0.743735, acc: 0.468750]\n",
            "4784: [discriminator loss: 0.7780658602714539, acc: 0.0390625] [gan loss: 1.750738, acc: 0.000000]\n",
            "4785: [discriminator loss: 0.6490215063095093, acc: 0.453125] [gan loss: 0.806652, acc: 0.328125]\n",
            "4786: [discriminator loss: 0.7336444854736328, acc: 0.078125] [gan loss: 1.658715, acc: 0.000000]\n",
            "4787: [discriminator loss: 0.6346049308776855, acc: 0.4609375] [gan loss: 0.749580, acc: 0.390625]\n",
            "4788: [discriminator loss: 0.7185559272766113, acc: 0.0703125] [gan loss: 1.594808, acc: 0.000000]\n",
            "4789: [discriminator loss: 0.6605477333068848, acc: 0.4296875] [gan loss: 0.805614, acc: 0.296875]\n",
            "4790: [discriminator loss: 0.7634536027908325, acc: 0.0703125] [gan loss: 1.640727, acc: 0.000000]\n",
            "4791: [discriminator loss: 0.6545851826667786, acc: 0.453125] [gan loss: 0.825624, acc: 0.281250]\n",
            "4792: [discriminator loss: 0.702163577079773, acc: 0.109375] [gan loss: 1.623893, acc: 0.000000]\n",
            "4793: [discriminator loss: 0.6416338086128235, acc: 0.46875] [gan loss: 0.801653, acc: 0.328125]\n",
            "4794: [discriminator loss: 0.701575756072998, acc: 0.1015625] [gan loss: 1.449512, acc: 0.000000]\n",
            "4795: [discriminator loss: 0.6683071255683899, acc: 0.375] [gan loss: 0.945460, acc: 0.171875]\n",
            "4796: [discriminator loss: 0.7466602325439453, acc: 0.09375] [gan loss: 1.507416, acc: 0.000000]\n",
            "4797: [discriminator loss: 0.6097722053527832, acc: 0.421875] [gan loss: 0.928609, acc: 0.171875]\n",
            "4798: [discriminator loss: 0.7174433469772339, acc: 0.1328125] [gan loss: 1.614715, acc: 0.000000]\n",
            "4799: [discriminator loss: 0.6501944661140442, acc: 0.4140625] [gan loss: 0.970066, acc: 0.125000]\n",
            "4800: [discriminator loss: 0.7015230059623718, acc: 0.1328125] [gan loss: 1.690407, acc: 0.000000]\n",
            "4801: [discriminator loss: 0.6695946455001831, acc: 0.4375] [gan loss: 0.800523, acc: 0.375000]\n",
            "4802: [discriminator loss: 0.7437760233879089, acc: 0.125] [gan loss: 1.722619, acc: 0.000000]\n",
            "4803: [discriminator loss: 0.6247128844261169, acc: 0.46875] [gan loss: 0.738240, acc: 0.531250]\n",
            "4804: [discriminator loss: 0.7833923101425171, acc: 0.0390625] [gan loss: 2.036345, acc: 0.000000]\n",
            "4805: [discriminator loss: 0.6938527822494507, acc: 0.4765625] [gan loss: 0.761783, acc: 0.453125]\n",
            "4806: [discriminator loss: 0.7639265656471252, acc: 0.03125] [gan loss: 1.716073, acc: 0.000000]\n",
            "4807: [discriminator loss: 0.6560556292533875, acc: 0.484375] [gan loss: 0.621567, acc: 0.625000]\n",
            "4808: [discriminator loss: 0.7781149744987488, acc: 0.0234375] [gan loss: 1.671152, acc: 0.000000]\n",
            "4809: [discriminator loss: 0.6475892663002014, acc: 0.4453125] [gan loss: 0.812037, acc: 0.328125]\n",
            "4810: [discriminator loss: 0.7314030528068542, acc: 0.0625] [gan loss: 1.600546, acc: 0.000000]\n",
            "4811: [discriminator loss: 0.5987207889556885, acc: 0.4609375] [gan loss: 0.922988, acc: 0.171875]\n",
            "4812: [discriminator loss: 0.703576385974884, acc: 0.1171875] [gan loss: 1.406164, acc: 0.015625]\n",
            "4813: [discriminator loss: 0.6567674875259399, acc: 0.3359375] [gan loss: 1.037926, acc: 0.140625]\n",
            "4814: [discriminator loss: 0.6761465072631836, acc: 0.171875] [gan loss: 1.405406, acc: 0.000000]\n",
            "4815: [discriminator loss: 0.6581131815910339, acc: 0.359375] [gan loss: 0.996848, acc: 0.140625]\n",
            "4816: [discriminator loss: 0.6823256015777588, acc: 0.1328125] [gan loss: 1.568434, acc: 0.000000]\n",
            "4817: [discriminator loss: 0.6614711284637451, acc: 0.421875] [gan loss: 0.863953, acc: 0.281250]\n",
            "4818: [discriminator loss: 0.7221506834030151, acc: 0.0703125] [gan loss: 1.693304, acc: 0.000000]\n",
            "4819: [discriminator loss: 0.6706328988075256, acc: 0.4453125] [gan loss: 0.609448, acc: 0.718750]\n",
            "4820: [discriminator loss: 0.7950469255447388, acc: 0.0234375] [gan loss: 1.986158, acc: 0.000000]\n",
            "4821: [discriminator loss: 0.6696028113365173, acc: 0.4921875] [gan loss: 0.604186, acc: 0.687500]\n",
            "4822: [discriminator loss: 0.8184884786605835, acc: 0.0234375] [gan loss: 1.635041, acc: 0.000000]\n",
            "4823: [discriminator loss: 0.6765806674957275, acc: 0.4453125] [gan loss: 0.744454, acc: 0.484375]\n",
            "4824: [discriminator loss: 0.7483696937561035, acc: 0.0703125] [gan loss: 1.557826, acc: 0.000000]\n",
            "4825: [discriminator loss: 0.6673635244369507, acc: 0.40625] [gan loss: 0.967053, acc: 0.125000]\n",
            "4826: [discriminator loss: 0.7401424050331116, acc: 0.078125] [gan loss: 1.404304, acc: 0.000000]\n",
            "4827: [discriminator loss: 0.6856827735900879, acc: 0.3671875] [gan loss: 0.924785, acc: 0.156250]\n",
            "4828: [discriminator loss: 0.6865453124046326, acc: 0.1328125] [gan loss: 1.374743, acc: 0.000000]\n",
            "4829: [discriminator loss: 0.6337035298347473, acc: 0.40625] [gan loss: 0.989681, acc: 0.093750]\n",
            "4830: [discriminator loss: 0.6857259273529053, acc: 0.171875] [gan loss: 1.432672, acc: 0.000000]\n",
            "4831: [discriminator loss: 0.6462520360946655, acc: 0.34375] [gan loss: 1.031533, acc: 0.093750]\n",
            "4832: [discriminator loss: 0.6790562868118286, acc: 0.1640625] [gan loss: 1.475510, acc: 0.000000]\n",
            "4833: [discriminator loss: 0.6264689564704895, acc: 0.3671875] [gan loss: 1.017643, acc: 0.156250]\n",
            "4834: [discriminator loss: 0.6709549427032471, acc: 0.1640625] [gan loss: 1.540277, acc: 0.000000]\n",
            "4835: [discriminator loss: 0.646361768245697, acc: 0.4375] [gan loss: 0.832346, acc: 0.312500]\n",
            "4836: [discriminator loss: 0.7320384383201599, acc: 0.09375] [gan loss: 1.802279, acc: 0.000000]\n",
            "4837: [discriminator loss: 0.6689468026161194, acc: 0.4765625] [gan loss: 0.687094, acc: 0.515625]\n",
            "4838: [discriminator loss: 0.7575047016143799, acc: 0.1015625] [gan loss: 1.814399, acc: 0.000000]\n",
            "4839: [discriminator loss: 0.6669471859931946, acc: 0.4765625] [gan loss: 0.678378, acc: 0.593750]\n",
            "4840: [discriminator loss: 0.7429966926574707, acc: 0.0546875] [gan loss: 1.647930, acc: 0.000000]\n",
            "4841: [discriminator loss: 0.6486414670944214, acc: 0.4296875] [gan loss: 0.807865, acc: 0.375000]\n",
            "4842: [discriminator loss: 0.724880039691925, acc: 0.09375] [gan loss: 1.499763, acc: 0.000000]\n",
            "4843: [discriminator loss: 0.6860464811325073, acc: 0.34375] [gan loss: 1.080022, acc: 0.140625]\n",
            "4844: [discriminator loss: 0.7268110513687134, acc: 0.125] [gan loss: 1.656733, acc: 0.000000]\n",
            "4845: [discriminator loss: 0.6043949127197266, acc: 0.4609375] [gan loss: 0.836182, acc: 0.390625]\n",
            "4846: [discriminator loss: 0.731616199016571, acc: 0.078125] [gan loss: 1.811305, acc: 0.000000]\n",
            "4847: [discriminator loss: 0.6519670486450195, acc: 0.453125] [gan loss: 0.760698, acc: 0.437500]\n",
            "4848: [discriminator loss: 0.7526017427444458, acc: 0.0234375] [gan loss: 1.697782, acc: 0.000000]\n",
            "4849: [discriminator loss: 0.6211215257644653, acc: 0.4453125] [gan loss: 0.817071, acc: 0.296875]\n",
            "4850: [discriminator loss: 0.7237943410873413, acc: 0.0625] [gan loss: 1.644999, acc: 0.000000]\n",
            "4851: [discriminator loss: 0.6460042595863342, acc: 0.4296875] [gan loss: 0.777837, acc: 0.437500]\n",
            "4852: [discriminator loss: 0.7723115682601929, acc: 0.0546875] [gan loss: 1.766212, acc: 0.000000]\n",
            "4853: [discriminator loss: 0.6810766458511353, acc: 0.46875] [gan loss: 0.736740, acc: 0.468750]\n",
            "4854: [discriminator loss: 0.7980977296829224, acc: 0.015625] [gan loss: 1.824457, acc: 0.000000]\n",
            "4855: [discriminator loss: 0.6905324459075928, acc: 0.4609375] [gan loss: 0.690239, acc: 0.531250]\n",
            "4856: [discriminator loss: 0.7550830841064453, acc: 0.0546875] [gan loss: 1.608499, acc: 0.015625]\n",
            "4857: [discriminator loss: 0.6598690152168274, acc: 0.46875] [gan loss: 0.819225, acc: 0.296875]\n",
            "4858: [discriminator loss: 0.7214713096618652, acc: 0.0546875] [gan loss: 1.541072, acc: 0.015625]\n",
            "4859: [discriminator loss: 0.6672123074531555, acc: 0.3828125] [gan loss: 0.908629, acc: 0.234375]\n",
            "4860: [discriminator loss: 0.6861156225204468, acc: 0.15625] [gan loss: 1.432635, acc: 0.000000]\n",
            "4861: [discriminator loss: 0.6744008660316467, acc: 0.3671875] [gan loss: 1.037957, acc: 0.046875]\n",
            "4862: [discriminator loss: 0.7046565413475037, acc: 0.1171875] [gan loss: 1.516069, acc: 0.000000]\n",
            "4863: [discriminator loss: 0.6584614515304565, acc: 0.3671875] [gan loss: 0.868030, acc: 0.234375]\n",
            "4864: [discriminator loss: 0.6843583583831787, acc: 0.15625] [gan loss: 1.540694, acc: 0.000000]\n",
            "4865: [discriminator loss: 0.6615826487541199, acc: 0.421875] [gan loss: 1.011129, acc: 0.093750]\n",
            "4866: [discriminator loss: 0.7033354640007019, acc: 0.1328125] [gan loss: 1.647664, acc: 0.031250]\n",
            "4867: [discriminator loss: 0.6439225673675537, acc: 0.421875] [gan loss: 0.821143, acc: 0.250000]\n",
            "4868: [discriminator loss: 0.7021231055259705, acc: 0.0859375] [gan loss: 1.759336, acc: 0.000000]\n",
            "4869: [discriminator loss: 0.6436524987220764, acc: 0.4609375] [gan loss: 0.754713, acc: 0.406250]\n",
            "4870: [discriminator loss: 0.7573726773262024, acc: 0.0703125] [gan loss: 1.912712, acc: 0.000000]\n",
            "4871: [discriminator loss: 0.6665164232254028, acc: 0.4609375] [gan loss: 0.743870, acc: 0.468750]\n",
            "4872: [discriminator loss: 0.7654336094856262, acc: 0.0625] [gan loss: 1.674583, acc: 0.000000]\n",
            "4873: [discriminator loss: 0.6742511987686157, acc: 0.4609375] [gan loss: 0.739551, acc: 0.437500]\n",
            "4874: [discriminator loss: 0.7152366042137146, acc: 0.0859375] [gan loss: 1.694737, acc: 0.015625]\n",
            "4875: [discriminator loss: 0.6694828271865845, acc: 0.421875] [gan loss: 0.877714, acc: 0.171875]\n",
            "4876: [discriminator loss: 0.6752397418022156, acc: 0.1953125] [gan loss: 1.271368, acc: 0.000000]\n",
            "4877: [discriminator loss: 0.6445415616035461, acc: 0.34375] [gan loss: 1.099001, acc: 0.062500]\n",
            "4878: [discriminator loss: 0.6816518306732178, acc: 0.1953125] [gan loss: 1.361737, acc: 0.000000]\n",
            "4879: [discriminator loss: 0.6731810569763184, acc: 0.3828125] [gan loss: 1.046036, acc: 0.078125]\n",
            "4880: [discriminator loss: 0.7556451559066772, acc: 0.109375] [gan loss: 1.576422, acc: 0.000000]\n",
            "4881: [discriminator loss: 0.6452776193618774, acc: 0.4375] [gan loss: 0.800733, acc: 0.390625]\n",
            "4882: [discriminator loss: 0.7342447638511658, acc: 0.09375] [gan loss: 1.854396, acc: 0.000000]\n",
            "4883: [discriminator loss: 0.6467186808586121, acc: 0.4609375] [gan loss: 0.734526, acc: 0.515625]\n",
            "4884: [discriminator loss: 0.7679286003112793, acc: 0.0546875] [gan loss: 1.926244, acc: 0.000000]\n",
            "4885: [discriminator loss: 0.6332902908325195, acc: 0.4765625] [gan loss: 0.672268, acc: 0.609375]\n",
            "4886: [discriminator loss: 0.7765272855758667, acc: 0.0703125] [gan loss: 1.596444, acc: 0.015625]\n",
            "4887: [discriminator loss: 0.6763592958450317, acc: 0.4375] [gan loss: 0.881552, acc: 0.203125]\n",
            "4888: [discriminator loss: 0.7077940702438354, acc: 0.109375] [gan loss: 1.513787, acc: 0.000000]\n",
            "4889: [discriminator loss: 0.6146866679191589, acc: 0.453125] [gan loss: 0.844803, acc: 0.265625]\n",
            "4890: [discriminator loss: 0.7255756855010986, acc: 0.0859375] [gan loss: 1.690174, acc: 0.000000]\n",
            "4891: [discriminator loss: 0.6345834732055664, acc: 0.4375] [gan loss: 0.829529, acc: 0.218750]\n",
            "4892: [discriminator loss: 0.735059916973114, acc: 0.078125] [gan loss: 1.680202, acc: 0.000000]\n",
            "4893: [discriminator loss: 0.6765396595001221, acc: 0.421875] [gan loss: 0.771342, acc: 0.375000]\n",
            "4894: [discriminator loss: 0.7466368079185486, acc: 0.0703125] [gan loss: 1.639695, acc: 0.000000]\n",
            "4895: [discriminator loss: 0.6548423767089844, acc: 0.421875] [gan loss: 0.791393, acc: 0.375000]\n",
            "4896: [discriminator loss: 0.8105493783950806, acc: 0.0546875] [gan loss: 1.926731, acc: 0.000000]\n",
            "4897: [discriminator loss: 0.6859369277954102, acc: 0.4921875] [gan loss: 0.676768, acc: 0.609375]\n",
            "4898: [discriminator loss: 0.7648761868476868, acc: 0.03125] [gan loss: 1.760410, acc: 0.000000]\n",
            "4899: [discriminator loss: 0.6673827767372131, acc: 0.484375] [gan loss: 0.748102, acc: 0.500000]\n",
            "4900: [discriminator loss: 0.7905020713806152, acc: 0.03125] [gan loss: 1.712714, acc: 0.000000]\n",
            "4901: [discriminator loss: 0.652540922164917, acc: 0.484375] [gan loss: 0.778632, acc: 0.406250]\n",
            "4902: [discriminator loss: 0.7515972852706909, acc: 0.046875] [gan loss: 1.636129, acc: 0.000000]\n",
            "4903: [discriminator loss: 0.6635658740997314, acc: 0.4375] [gan loss: 0.933202, acc: 0.187500]\n",
            "4904: [discriminator loss: 0.7229118347167969, acc: 0.0859375] [gan loss: 1.373452, acc: 0.015625]\n",
            "4905: [discriminator loss: 0.6495926976203918, acc: 0.421875] [gan loss: 0.844661, acc: 0.187500]\n",
            "4906: [discriminator loss: 0.7205325365066528, acc: 0.078125] [gan loss: 1.433506, acc: 0.015625]\n",
            "4907: [discriminator loss: 0.6490706205368042, acc: 0.4140625] [gan loss: 0.857224, acc: 0.265625]\n",
            "4908: [discriminator loss: 0.7410485744476318, acc: 0.0546875] [gan loss: 1.747103, acc: 0.000000]\n",
            "4909: [discriminator loss: 0.6768152713775635, acc: 0.453125] [gan loss: 0.683267, acc: 0.531250]\n",
            "4910: [discriminator loss: 0.7816961407661438, acc: 0.0078125] [gan loss: 1.716289, acc: 0.000000]\n",
            "4911: [discriminator loss: 0.6416028738021851, acc: 0.4609375] [gan loss: 0.786563, acc: 0.296875]\n",
            "4912: [discriminator loss: 0.7179727554321289, acc: 0.0703125] [gan loss: 1.545782, acc: 0.000000]\n",
            "4913: [discriminator loss: 0.6472983956336975, acc: 0.3984375] [gan loss: 0.875561, acc: 0.250000]\n",
            "4914: [discriminator loss: 0.6961166262626648, acc: 0.109375] [gan loss: 1.602137, acc: 0.000000]\n",
            "4915: [discriminator loss: 0.6436431407928467, acc: 0.4609375] [gan loss: 0.865410, acc: 0.250000]\n",
            "4916: [discriminator loss: 0.7589543461799622, acc: 0.0859375] [gan loss: 1.654134, acc: 0.000000]\n",
            "4917: [discriminator loss: 0.6638399958610535, acc: 0.4453125] [gan loss: 0.821118, acc: 0.375000]\n",
            "4918: [discriminator loss: 0.7216783165931702, acc: 0.09375] [gan loss: 1.588962, acc: 0.000000]\n",
            "4919: [discriminator loss: 0.6715959310531616, acc: 0.4140625] [gan loss: 0.941140, acc: 0.156250]\n",
            "4920: [discriminator loss: 0.7461605072021484, acc: 0.078125] [gan loss: 1.708028, acc: 0.000000]\n",
            "4921: [discriminator loss: 0.6760081648826599, acc: 0.4453125] [gan loss: 0.730580, acc: 0.453125]\n",
            "4922: [discriminator loss: 0.7625802159309387, acc: 0.046875] [gan loss: 1.649680, acc: 0.000000]\n",
            "4923: [discriminator loss: 0.6885430812835693, acc: 0.4609375] [gan loss: 0.699266, acc: 0.546875]\n",
            "4924: [discriminator loss: 0.7623493075370789, acc: 0.0234375] [gan loss: 1.712856, acc: 0.000000]\n",
            "4925: [discriminator loss: 0.6324650645256042, acc: 0.4765625] [gan loss: 0.756492, acc: 0.375000]\n",
            "4926: [discriminator loss: 0.7360976934432983, acc: 0.0625] [gan loss: 1.540151, acc: 0.015625]\n",
            "4927: [discriminator loss: 0.6454647779464722, acc: 0.4609375] [gan loss: 0.795004, acc: 0.296875]\n",
            "4928: [discriminator loss: 0.7329740524291992, acc: 0.1015625] [gan loss: 1.487955, acc: 0.000000]\n",
            "4929: [discriminator loss: 0.6184039115905762, acc: 0.4140625] [gan loss: 0.929188, acc: 0.203125]\n",
            "4930: [discriminator loss: 0.7330642342567444, acc: 0.140625] [gan loss: 1.366436, acc: 0.000000]\n",
            "4931: [discriminator loss: 0.6746528148651123, acc: 0.3203125] [gan loss: 0.974489, acc: 0.109375]\n",
            "4932: [discriminator loss: 0.7079950571060181, acc: 0.1171875] [gan loss: 1.574330, acc: 0.000000]\n",
            "4933: [discriminator loss: 0.6793230772018433, acc: 0.3671875] [gan loss: 0.967129, acc: 0.125000]\n",
            "4934: [discriminator loss: 0.7254370450973511, acc: 0.09375] [gan loss: 1.651069, acc: 0.000000]\n",
            "4935: [discriminator loss: 0.6518950462341309, acc: 0.4453125] [gan loss: 0.655752, acc: 0.671875]\n",
            "4936: [discriminator loss: 0.7558014392852783, acc: 0.0625] [gan loss: 1.864243, acc: 0.000000]\n",
            "4937: [discriminator loss: 0.686482846736908, acc: 0.46875] [gan loss: 0.760512, acc: 0.406250]\n",
            "4938: [discriminator loss: 0.7284600138664246, acc: 0.109375] [gan loss: 1.855734, acc: 0.000000]\n",
            "4939: [discriminator loss: 0.6766833066940308, acc: 0.4375] [gan loss: 0.811270, acc: 0.359375]\n",
            "4940: [discriminator loss: 0.7249074578285217, acc: 0.0390625] [gan loss: 1.559918, acc: 0.000000]\n",
            "4941: [discriminator loss: 0.6739845871925354, acc: 0.4296875] [gan loss: 0.854990, acc: 0.218750]\n",
            "4942: [discriminator loss: 0.7039180994033813, acc: 0.078125] [gan loss: 1.578894, acc: 0.000000]\n",
            "4943: [discriminator loss: 0.6557457447052002, acc: 0.4296875] [gan loss: 0.726767, acc: 0.500000]\n",
            "4944: [discriminator loss: 0.7490490078926086, acc: 0.0390625] [gan loss: 1.887323, acc: 0.000000]\n",
            "4945: [discriminator loss: 0.6817481517791748, acc: 0.4765625] [gan loss: 0.595740, acc: 0.718750]\n",
            "4946: [discriminator loss: 0.7955263257026672, acc: 0.03125] [gan loss: 1.650538, acc: 0.000000]\n",
            "4947: [discriminator loss: 0.6736156940460205, acc: 0.4453125] [gan loss: 0.765585, acc: 0.515625]\n",
            "4948: [discriminator loss: 0.7616937756538391, acc: 0.0625] [gan loss: 1.649064, acc: 0.000000]\n",
            "4949: [discriminator loss: 0.613153874874115, acc: 0.453125] [gan loss: 0.779946, acc: 0.421875]\n",
            "4950: [discriminator loss: 0.7717945575714111, acc: 0.0625] [gan loss: 1.477510, acc: 0.015625]\n",
            "4951: [discriminator loss: 0.6652008295059204, acc: 0.421875] [gan loss: 0.793567, acc: 0.343750]\n",
            "4952: [discriminator loss: 0.7287095189094543, acc: 0.078125] [gan loss: 1.500293, acc: 0.000000]\n",
            "4953: [discriminator loss: 0.6513959765434265, acc: 0.4140625] [gan loss: 0.937660, acc: 0.109375]\n",
            "4954: [discriminator loss: 0.7359488606452942, acc: 0.046875] [gan loss: 1.550006, acc: 0.000000]\n",
            "4955: [discriminator loss: 0.6561595797538757, acc: 0.4296875] [gan loss: 1.018971, acc: 0.125000]\n",
            "4956: [discriminator loss: 0.7246014475822449, acc: 0.1484375] [gan loss: 1.307993, acc: 0.015625]\n",
            "4957: [discriminator loss: 0.650298535823822, acc: 0.3515625] [gan loss: 1.098168, acc: 0.046875]\n",
            "4958: [discriminator loss: 0.7055841684341431, acc: 0.1328125] [gan loss: 1.394858, acc: 0.015625]\n",
            "4959: [discriminator loss: 0.6511322855949402, acc: 0.390625] [gan loss: 0.958737, acc: 0.171875]\n",
            "4960: [discriminator loss: 0.6919301748275757, acc: 0.15625] [gan loss: 1.582483, acc: 0.000000]\n",
            "4961: [discriminator loss: 0.6786719560623169, acc: 0.4375] [gan loss: 0.728853, acc: 0.421875]\n",
            "4962: [discriminator loss: 0.7337672710418701, acc: 0.0390625] [gan loss: 1.830505, acc: 0.000000]\n",
            "4963: [discriminator loss: 0.6466057896614075, acc: 0.4609375] [gan loss: 0.688882, acc: 0.515625]\n",
            "4964: [discriminator loss: 0.7468392848968506, acc: 0.0546875] [gan loss: 1.660498, acc: 0.000000]\n",
            "4965: [discriminator loss: 0.6424391269683838, acc: 0.46875] [gan loss: 0.924819, acc: 0.218750]\n",
            "4966: [discriminator loss: 0.7390344738960266, acc: 0.0625] [gan loss: 1.747813, acc: 0.000000]\n",
            "4967: [discriminator loss: 0.6477088928222656, acc: 0.4765625] [gan loss: 0.727832, acc: 0.406250]\n",
            "4968: [discriminator loss: 0.7824273109436035, acc: 0.0234375] [gan loss: 2.012049, acc: 0.000000]\n",
            "4969: [discriminator loss: 0.6700834035873413, acc: 0.4921875] [gan loss: 0.668571, acc: 0.562500]\n",
            "4970: [discriminator loss: 0.7951098680496216, acc: 0.046875] [gan loss: 1.792118, acc: 0.000000]\n",
            "4971: [discriminator loss: 0.657758355140686, acc: 0.484375] [gan loss: 0.686974, acc: 0.546875]\n",
            "4972: [discriminator loss: 0.7463256120681763, acc: 0.03125] [gan loss: 1.663022, acc: 0.000000]\n",
            "4973: [discriminator loss: 0.6660533547401428, acc: 0.390625] [gan loss: 0.788195, acc: 0.375000]\n",
            "4974: [discriminator loss: 0.7801742553710938, acc: 0.0625] [gan loss: 1.690192, acc: 0.000000]\n",
            "4975: [discriminator loss: 0.6434274911880493, acc: 0.4765625] [gan loss: 0.783304, acc: 0.421875]\n",
            "4976: [discriminator loss: 0.7724181413650513, acc: 0.0546875] [gan loss: 1.514170, acc: 0.000000]\n",
            "4977: [discriminator loss: 0.6430844068527222, acc: 0.453125] [gan loss: 0.758364, acc: 0.375000]\n",
            "4978: [discriminator loss: 0.7471414804458618, acc: 0.0546875] [gan loss: 1.590803, acc: 0.000000]\n",
            "4979: [discriminator loss: 0.6434909701347351, acc: 0.4609375] [gan loss: 0.911522, acc: 0.156250]\n",
            "4980: [discriminator loss: 0.7305889129638672, acc: 0.0390625] [gan loss: 1.591984, acc: 0.000000]\n",
            "4981: [discriminator loss: 0.6350833773612976, acc: 0.4453125] [gan loss: 0.877272, acc: 0.265625]\n",
            "4982: [discriminator loss: 0.6850155591964722, acc: 0.1171875] [gan loss: 1.528422, acc: 0.015625]\n",
            "4983: [discriminator loss: 0.6601969003677368, acc: 0.375] [gan loss: 0.990891, acc: 0.140625]\n",
            "4984: [discriminator loss: 0.663902997970581, acc: 0.171875] [gan loss: 1.552916, acc: 0.000000]\n",
            "4985: [discriminator loss: 0.6614220142364502, acc: 0.46875] [gan loss: 0.699612, acc: 0.484375]\n",
            "4986: [discriminator loss: 0.7613195180892944, acc: 0.0546875] [gan loss: 1.829877, acc: 0.000000]\n",
            "4987: [discriminator loss: 0.6918782591819763, acc: 0.453125] [gan loss: 0.675634, acc: 0.515625]\n",
            "4988: [discriminator loss: 0.7697245478630066, acc: 0.03125] [gan loss: 1.697956, acc: 0.000000]\n",
            "4989: [discriminator loss: 0.6332324743270874, acc: 0.4765625] [gan loss: 0.743383, acc: 0.437500]\n",
            "4990: [discriminator loss: 0.7294084429740906, acc: 0.046875] [gan loss: 1.683153, acc: 0.000000]\n",
            "4991: [discriminator loss: 0.652682363986969, acc: 0.4296875] [gan loss: 0.810807, acc: 0.328125]\n",
            "4992: [discriminator loss: 0.7271403074264526, acc: 0.109375] [gan loss: 1.498491, acc: 0.031250]\n",
            "4993: [discriminator loss: 0.7005014419555664, acc: 0.3515625] [gan loss: 0.900151, acc: 0.281250]\n",
            "4994: [discriminator loss: 0.7081212401390076, acc: 0.1484375] [gan loss: 1.406157, acc: 0.031250]\n",
            "4995: [discriminator loss: 0.6548177003860474, acc: 0.34375] [gan loss: 1.005203, acc: 0.171875]\n",
            "4996: [discriminator loss: 0.7038785219192505, acc: 0.1640625] [gan loss: 1.521288, acc: 0.000000]\n",
            "4997: [discriminator loss: 0.6766959428787231, acc: 0.3984375] [gan loss: 0.924991, acc: 0.140625]\n",
            "4998: [discriminator loss: 0.727191686630249, acc: 0.09375] [gan loss: 1.679419, acc: 0.000000]\n",
            "4999: [discriminator loss: 0.6380534172058105, acc: 0.4453125] [gan loss: 0.790898, acc: 0.375000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5000: [discriminator loss: 0.7283541560173035, acc: 0.0859375] [gan loss: 1.719476, acc: 0.000000]\n",
            "5001: [discriminator loss: 0.6407639384269714, acc: 0.453125] [gan loss: 0.702372, acc: 0.484375]\n",
            "5002: [discriminator loss: 0.778745174407959, acc: 0.015625] [gan loss: 1.782570, acc: 0.000000]\n",
            "5003: [discriminator loss: 0.6738384366035461, acc: 0.4921875] [gan loss: 0.734407, acc: 0.468750]\n",
            "5004: [discriminator loss: 0.7364515066146851, acc: 0.0234375] [gan loss: 1.736596, acc: 0.000000]\n",
            "5005: [discriminator loss: 0.6503865122795105, acc: 0.4375] [gan loss: 0.824330, acc: 0.312500]\n",
            "5006: [discriminator loss: 0.7150939702987671, acc: 0.078125] [gan loss: 1.684381, acc: 0.000000]\n",
            "5007: [discriminator loss: 0.6860384941101074, acc: 0.3515625] [gan loss: 0.938082, acc: 0.140625]\n",
            "5008: [discriminator loss: 0.7033729553222656, acc: 0.0703125] [gan loss: 1.620145, acc: 0.000000]\n",
            "5009: [discriminator loss: 0.6201589703559875, acc: 0.5] [gan loss: 0.835419, acc: 0.218750]\n",
            "5010: [discriminator loss: 0.687639594078064, acc: 0.09375] [gan loss: 1.585103, acc: 0.000000]\n",
            "5011: [discriminator loss: 0.6473243832588196, acc: 0.453125] [gan loss: 1.004424, acc: 0.093750]\n",
            "5012: [discriminator loss: 0.7221654057502747, acc: 0.09375] [gan loss: 1.632784, acc: 0.000000]\n",
            "5013: [discriminator loss: 0.6751875877380371, acc: 0.4375] [gan loss: 0.839653, acc: 0.218750]\n",
            "5014: [discriminator loss: 0.7456420660018921, acc: 0.0703125] [gan loss: 1.618195, acc: 0.000000]\n",
            "5015: [discriminator loss: 0.7001215219497681, acc: 0.4609375] [gan loss: 0.734339, acc: 0.437500]\n",
            "5016: [discriminator loss: 0.7746463418006897, acc: 0.015625] [gan loss: 1.809593, acc: 0.000000]\n",
            "5017: [discriminator loss: 0.6648133993148804, acc: 0.4609375] [gan loss: 0.696044, acc: 0.546875]\n",
            "5018: [discriminator loss: 0.7431936860084534, acc: 0.0546875] [gan loss: 1.721901, acc: 0.000000]\n",
            "5019: [discriminator loss: 0.6557822823524475, acc: 0.4453125] [gan loss: 0.829272, acc: 0.234375]\n",
            "5020: [discriminator loss: 0.781616747379303, acc: 0.0546875] [gan loss: 1.696418, acc: 0.000000]\n",
            "5021: [discriminator loss: 0.6792148351669312, acc: 0.3828125] [gan loss: 0.942006, acc: 0.093750]\n",
            "5022: [discriminator loss: 0.7058442234992981, acc: 0.09375] [gan loss: 1.530837, acc: 0.000000]\n",
            "5023: [discriminator loss: 0.6371902227401733, acc: 0.40625] [gan loss: 0.926033, acc: 0.156250]\n",
            "5024: [discriminator loss: 0.7094415426254272, acc: 0.1328125] [gan loss: 1.475526, acc: 0.000000]\n",
            "5025: [discriminator loss: 0.6534561514854431, acc: 0.4375] [gan loss: 0.916430, acc: 0.156250]\n",
            "5026: [discriminator loss: 0.7136989831924438, acc: 0.125] [gan loss: 1.677546, acc: 0.000000]\n",
            "5027: [discriminator loss: 0.6721029281616211, acc: 0.421875] [gan loss: 0.850106, acc: 0.296875]\n",
            "5028: [discriminator loss: 0.7257522940635681, acc: 0.1171875] [gan loss: 1.547296, acc: 0.000000]\n",
            "5029: [discriminator loss: 0.6475688219070435, acc: 0.4453125] [gan loss: 0.771261, acc: 0.343750]\n",
            "5030: [discriminator loss: 0.7574508190155029, acc: 0.0390625] [gan loss: 1.788780, acc: 0.000000]\n",
            "5031: [discriminator loss: 0.6622491478919983, acc: 0.484375] [gan loss: 0.623714, acc: 0.718750]\n",
            "5032: [discriminator loss: 0.7933441400527954, acc: 0.015625] [gan loss: 1.953044, acc: 0.000000]\n",
            "5033: [discriminator loss: 0.682194709777832, acc: 0.484375] [gan loss: 0.555484, acc: 0.812500]\n",
            "5034: [discriminator loss: 0.7896482944488525, acc: 0.015625] [gan loss: 1.561393, acc: 0.000000]\n",
            "5035: [discriminator loss: 0.6540578007698059, acc: 0.4375] [gan loss: 0.871835, acc: 0.234375]\n",
            "5036: [discriminator loss: 0.7489681839942932, acc: 0.0625] [gan loss: 1.379698, acc: 0.000000]\n",
            "5037: [discriminator loss: 0.6214109659194946, acc: 0.3671875] [gan loss: 0.974737, acc: 0.171875]\n",
            "5038: [discriminator loss: 0.6958314180374146, acc: 0.125] [gan loss: 1.452802, acc: 0.015625]\n",
            "5039: [discriminator loss: 0.6568765044212341, acc: 0.4375] [gan loss: 0.877967, acc: 0.234375]\n",
            "5040: [discriminator loss: 0.7495779991149902, acc: 0.0859375] [gan loss: 1.583521, acc: 0.000000]\n",
            "5041: [discriminator loss: 0.6274462342262268, acc: 0.4375] [gan loss: 0.832937, acc: 0.296875]\n",
            "5042: [discriminator loss: 0.7407503724098206, acc: 0.1171875] [gan loss: 1.670508, acc: 0.000000]\n",
            "5043: [discriminator loss: 0.6651951670646667, acc: 0.3984375] [gan loss: 1.007313, acc: 0.078125]\n",
            "5044: [discriminator loss: 0.6909682750701904, acc: 0.1015625] [gan loss: 1.414356, acc: 0.015625]\n",
            "5045: [discriminator loss: 0.6370071172714233, acc: 0.34375] [gan loss: 1.138254, acc: 0.046875]\n",
            "5046: [discriminator loss: 0.6600838899612427, acc: 0.2265625] [gan loss: 1.503327, acc: 0.000000]\n",
            "5047: [discriminator loss: 0.6555210947990417, acc: 0.3828125] [gan loss: 0.995285, acc: 0.109375]\n",
            "5048: [discriminator loss: 0.7024836540222168, acc: 0.1015625] [gan loss: 1.703371, acc: 0.000000]\n",
            "5049: [discriminator loss: 0.6616930365562439, acc: 0.46875] [gan loss: 0.748345, acc: 0.406250]\n",
            "5050: [discriminator loss: 0.7437307834625244, acc: 0.03125] [gan loss: 1.968221, acc: 0.000000]\n",
            "5051: [discriminator loss: 0.7047304511070251, acc: 0.46875] [gan loss: 0.549802, acc: 0.750000]\n",
            "5052: [discriminator loss: 0.873603343963623, acc: 0.0078125] [gan loss: 1.869210, acc: 0.000000]\n",
            "5053: [discriminator loss: 0.6573696732521057, acc: 0.4765625] [gan loss: 0.746189, acc: 0.453125]\n",
            "5054: [discriminator loss: 0.7481176257133484, acc: 0.0546875] [gan loss: 1.675051, acc: 0.000000]\n",
            "5055: [discriminator loss: 0.6558335423469543, acc: 0.40625] [gan loss: 0.969938, acc: 0.234375]\n",
            "5056: [discriminator loss: 0.7180958986282349, acc: 0.0859375] [gan loss: 1.395677, acc: 0.000000]\n",
            "5057: [discriminator loss: 0.6570967435836792, acc: 0.34375] [gan loss: 1.063207, acc: 0.062500]\n",
            "5058: [discriminator loss: 0.7335615754127502, acc: 0.15625] [gan loss: 1.581365, acc: 0.000000]\n",
            "5059: [discriminator loss: 0.6749347448348999, acc: 0.3203125] [gan loss: 1.051088, acc: 0.109375]\n",
            "5060: [discriminator loss: 0.6876068711280823, acc: 0.1875] [gan loss: 1.442127, acc: 0.000000]\n",
            "5061: [discriminator loss: 0.6648895144462585, acc: 0.3359375] [gan loss: 0.947544, acc: 0.062500]\n",
            "5062: [discriminator loss: 0.6730992794036865, acc: 0.1875] [gan loss: 1.528424, acc: 0.000000]\n",
            "5063: [discriminator loss: 0.6509770750999451, acc: 0.4140625] [gan loss: 0.879672, acc: 0.250000]\n",
            "5064: [discriminator loss: 0.7520576119422913, acc: 0.078125] [gan loss: 1.759749, acc: 0.000000]\n",
            "5065: [discriminator loss: 0.6665332913398743, acc: 0.4765625] [gan loss: 0.673259, acc: 0.593750]\n",
            "5066: [discriminator loss: 0.7589387893676758, acc: 0.0390625] [gan loss: 1.843984, acc: 0.000000]\n",
            "5067: [discriminator loss: 0.6844881772994995, acc: 0.484375] [gan loss: 0.724289, acc: 0.468750]\n",
            "5068: [discriminator loss: 0.7836612462997437, acc: 0.0234375] [gan loss: 1.852042, acc: 0.000000]\n",
            "5069: [discriminator loss: 0.687612771987915, acc: 0.4921875] [gan loss: 0.580780, acc: 0.765625]\n",
            "5070: [discriminator loss: 0.8099494576454163, acc: 0.0234375] [gan loss: 1.741407, acc: 0.000000]\n",
            "5071: [discriminator loss: 0.6493016481399536, acc: 0.46875] [gan loss: 0.901046, acc: 0.125000]\n",
            "5072: [discriminator loss: 0.7365759611129761, acc: 0.0625] [gan loss: 1.475710, acc: 0.015625]\n",
            "5073: [discriminator loss: 0.664609432220459, acc: 0.390625] [gan loss: 0.930197, acc: 0.156250]\n",
            "5074: [discriminator loss: 0.7101658582687378, acc: 0.1015625] [gan loss: 1.429317, acc: 0.000000]\n",
            "5075: [discriminator loss: 0.6762423515319824, acc: 0.375] [gan loss: 0.982633, acc: 0.109375]\n",
            "5076: [discriminator loss: 0.6509769558906555, acc: 0.203125] [gan loss: 1.455004, acc: 0.031250]\n",
            "5077: [discriminator loss: 0.6833388805389404, acc: 0.3359375] [gan loss: 0.996911, acc: 0.125000]\n",
            "5078: [discriminator loss: 0.6913610100746155, acc: 0.1953125] [gan loss: 1.369732, acc: 0.000000]\n",
            "5079: [discriminator loss: 0.6613824367523193, acc: 0.34375] [gan loss: 0.921974, acc: 0.093750]\n",
            "5080: [discriminator loss: 0.6907145380973816, acc: 0.1640625] [gan loss: 1.432690, acc: 0.015625]\n",
            "5081: [discriminator loss: 0.6727875471115112, acc: 0.3046875] [gan loss: 1.175022, acc: 0.000000]\n",
            "5082: [discriminator loss: 0.6948778629302979, acc: 0.1875] [gan loss: 1.499314, acc: 0.015625]\n",
            "5083: [discriminator loss: 0.6474111080169678, acc: 0.3359375] [gan loss: 1.078268, acc: 0.046875]\n",
            "5084: [discriminator loss: 0.7004743814468384, acc: 0.1640625] [gan loss: 1.564392, acc: 0.000000]\n",
            "5085: [discriminator loss: 0.6433353424072266, acc: 0.421875] [gan loss: 0.815000, acc: 0.312500]\n",
            "5086: [discriminator loss: 0.7260963916778564, acc: 0.1171875] [gan loss: 2.011797, acc: 0.000000]\n",
            "5087: [discriminator loss: 0.6320721507072449, acc: 0.484375] [gan loss: 0.776367, acc: 0.343750]\n",
            "5088: [discriminator loss: 0.7753665447235107, acc: 0.015625] [gan loss: 2.033227, acc: 0.000000]\n",
            "5089: [discriminator loss: 0.6797162294387817, acc: 0.4921875] [gan loss: 0.522568, acc: 0.859375]\n",
            "5090: [discriminator loss: 0.8464763760566711, acc: 0.0] [gan loss: 1.847147, acc: 0.000000]\n",
            "5091: [discriminator loss: 0.6580051183700562, acc: 0.484375] [gan loss: 0.627805, acc: 0.656250]\n",
            "5092: [discriminator loss: 0.7695502042770386, acc: 0.1015625] [gan loss: 1.558950, acc: 0.000000]\n",
            "5093: [discriminator loss: 0.6520752906799316, acc: 0.46875] [gan loss: 0.776435, acc: 0.421875]\n",
            "5094: [discriminator loss: 0.7242974042892456, acc: 0.09375] [gan loss: 1.539444, acc: 0.000000]\n",
            "5095: [discriminator loss: 0.655418872833252, acc: 0.4609375] [gan loss: 0.800410, acc: 0.281250]\n",
            "5096: [discriminator loss: 0.727617621421814, acc: 0.125] [gan loss: 1.557753, acc: 0.015625]\n",
            "5097: [discriminator loss: 0.6763520836830139, acc: 0.328125] [gan loss: 0.941128, acc: 0.156250]\n",
            "5098: [discriminator loss: 0.7079889178276062, acc: 0.1328125] [gan loss: 1.391564, acc: 0.000000]\n",
            "5099: [discriminator loss: 0.6160348057746887, acc: 0.4375] [gan loss: 0.889101, acc: 0.203125]\n",
            "5100: [discriminator loss: 0.7100749015808105, acc: 0.1171875] [gan loss: 1.764611, acc: 0.000000]\n",
            "5101: [discriminator loss: 0.6454251408576965, acc: 0.4453125] [gan loss: 0.780324, acc: 0.453125]\n",
            "5102: [discriminator loss: 0.7825428247451782, acc: 0.046875] [gan loss: 1.735787, acc: 0.000000]\n",
            "5103: [discriminator loss: 0.6348415613174438, acc: 0.4609375] [gan loss: 0.862436, acc: 0.234375]\n",
            "5104: [discriminator loss: 0.7443252205848694, acc: 0.0546875] [gan loss: 1.658583, acc: 0.000000]\n",
            "5105: [discriminator loss: 0.6730190515518188, acc: 0.453125] [gan loss: 0.674710, acc: 0.625000]\n",
            "5106: [discriminator loss: 0.8011849522590637, acc: 0.0234375] [gan loss: 1.763610, acc: 0.000000]\n",
            "5107: [discriminator loss: 0.6511785984039307, acc: 0.4765625] [gan loss: 0.708034, acc: 0.562500]\n",
            "5108: [discriminator loss: 0.7806190252304077, acc: 0.0546875] [gan loss: 1.523606, acc: 0.000000]\n",
            "5109: [discriminator loss: 0.662604570388794, acc: 0.40625] [gan loss: 0.822330, acc: 0.218750]\n",
            "5110: [discriminator loss: 0.7312738299369812, acc: 0.046875] [gan loss: 1.520714, acc: 0.000000]\n",
            "5111: [discriminator loss: 0.6523386836051941, acc: 0.421875] [gan loss: 0.774913, acc: 0.375000]\n",
            "5112: [discriminator loss: 0.7543858885765076, acc: 0.0625] [gan loss: 1.522618, acc: 0.000000]\n",
            "5113: [discriminator loss: 0.6426388025283813, acc: 0.4296875] [gan loss: 0.761165, acc: 0.390625]\n",
            "5114: [discriminator loss: 0.7423298358917236, acc: 0.046875] [gan loss: 1.735169, acc: 0.000000]\n",
            "5115: [discriminator loss: 0.6626721620559692, acc: 0.421875] [gan loss: 0.826828, acc: 0.312500]\n",
            "5116: [discriminator loss: 0.7383012771606445, acc: 0.0703125] [gan loss: 1.434400, acc: 0.015625]\n",
            "5117: [discriminator loss: 0.6363703012466431, acc: 0.46875] [gan loss: 0.789980, acc: 0.343750]\n",
            "5118: [discriminator loss: 0.736758828163147, acc: 0.09375] [gan loss: 1.578530, acc: 0.000000]\n",
            "5119: [discriminator loss: 0.6538538336753845, acc: 0.4453125] [gan loss: 0.804901, acc: 0.359375]\n",
            "5120: [discriminator loss: 0.7587329745292664, acc: 0.0546875] [gan loss: 1.812268, acc: 0.000000]\n",
            "5121: [discriminator loss: 0.633790135383606, acc: 0.4609375] [gan loss: 0.835682, acc: 0.281250]\n",
            "5122: [discriminator loss: 0.807841956615448, acc: 0.0390625] [gan loss: 1.770464, acc: 0.000000]\n",
            "5123: [discriminator loss: 0.6691712141036987, acc: 0.4765625] [gan loss: 0.736706, acc: 0.468750]\n",
            "5124: [discriminator loss: 0.7239134311676025, acc: 0.0859375] [gan loss: 1.423164, acc: 0.000000]\n",
            "5125: [discriminator loss: 0.6237024068832397, acc: 0.4140625] [gan loss: 0.837160, acc: 0.328125]\n",
            "5126: [discriminator loss: 0.7157469391822815, acc: 0.140625] [gan loss: 1.643840, acc: 0.000000]\n",
            "5127: [discriminator loss: 0.6781949400901794, acc: 0.3984375] [gan loss: 0.832589, acc: 0.281250]\n",
            "5128: [discriminator loss: 0.7406995892524719, acc: 0.046875] [gan loss: 1.802339, acc: 0.000000]\n",
            "5129: [discriminator loss: 0.638802170753479, acc: 0.4921875] [gan loss: 0.708707, acc: 0.453125]\n",
            "5130: [discriminator loss: 0.7635340690612793, acc: 0.0625] [gan loss: 1.678422, acc: 0.000000]\n",
            "5131: [discriminator loss: 0.6543319821357727, acc: 0.4375] [gan loss: 0.789158, acc: 0.343750]\n",
            "5132: [discriminator loss: 0.7458508014678955, acc: 0.0703125] [gan loss: 1.528812, acc: 0.000000]\n",
            "5133: [discriminator loss: 0.6708834171295166, acc: 0.3671875] [gan loss: 1.003111, acc: 0.140625]\n",
            "5134: [discriminator loss: 0.6955903768539429, acc: 0.1953125] [gan loss: 1.403515, acc: 0.015625]\n",
            "5135: [discriminator loss: 0.6711132526397705, acc: 0.296875] [gan loss: 1.148522, acc: 0.062500]\n",
            "5136: [discriminator loss: 0.6858963370323181, acc: 0.25] [gan loss: 1.272163, acc: 0.000000]\n",
            "5137: [discriminator loss: 0.6543817520141602, acc: 0.34375] [gan loss: 1.246251, acc: 0.046875]\n",
            "5138: [discriminator loss: 0.6793382167816162, acc: 0.21875] [gan loss: 1.368992, acc: 0.000000]\n",
            "5139: [discriminator loss: 0.6444827318191528, acc: 0.3828125] [gan loss: 1.076272, acc: 0.031250]\n",
            "5140: [discriminator loss: 0.6707977652549744, acc: 0.15625] [gan loss: 1.668410, acc: 0.000000]\n",
            "5141: [discriminator loss: 0.6541032791137695, acc: 0.453125] [gan loss: 0.767948, acc: 0.359375]\n",
            "5142: [discriminator loss: 0.7327134609222412, acc: 0.046875] [gan loss: 1.912691, acc: 0.000000]\n",
            "5143: [discriminator loss: 0.6723892688751221, acc: 0.4765625] [gan loss: 0.678168, acc: 0.531250]\n",
            "5144: [discriminator loss: 0.7786240577697754, acc: 0.0234375] [gan loss: 2.094540, acc: 0.000000]\n",
            "5145: [discriminator loss: 0.6762415766716003, acc: 0.4921875] [gan loss: 0.596189, acc: 0.687500]\n",
            "5146: [discriminator loss: 0.846301794052124, acc: 0.0] [gan loss: 2.053036, acc: 0.000000]\n",
            "5147: [discriminator loss: 0.6567045450210571, acc: 0.46875] [gan loss: 0.671160, acc: 0.578125]\n",
            "5148: [discriminator loss: 0.7619143724441528, acc: 0.0546875] [gan loss: 1.608200, acc: 0.000000]\n",
            "5149: [discriminator loss: 0.6725426912307739, acc: 0.453125] [gan loss: 0.712367, acc: 0.500000]\n",
            "5150: [discriminator loss: 0.7930550575256348, acc: 0.0390625] [gan loss: 1.579128, acc: 0.000000]\n",
            "5151: [discriminator loss: 0.6605527400970459, acc: 0.4453125] [gan loss: 0.811587, acc: 0.250000]\n",
            "5152: [discriminator loss: 0.7244158983230591, acc: 0.0859375] [gan loss: 1.480218, acc: 0.031250]\n",
            "5153: [discriminator loss: 0.665206253528595, acc: 0.4140625] [gan loss: 0.868310, acc: 0.234375]\n",
            "5154: [discriminator loss: 0.7416294813156128, acc: 0.0703125] [gan loss: 1.582631, acc: 0.000000]\n",
            "5155: [discriminator loss: 0.6662592887878418, acc: 0.4296875] [gan loss: 0.801509, acc: 0.343750]\n",
            "5156: [discriminator loss: 0.7439157962799072, acc: 0.0390625] [gan loss: 1.630829, acc: 0.000000]\n",
            "5157: [discriminator loss: 0.6638193130493164, acc: 0.4375] [gan loss: 0.748621, acc: 0.437500]\n",
            "5158: [discriminator loss: 0.7464893460273743, acc: 0.0625] [gan loss: 1.693024, acc: 0.000000]\n",
            "5159: [discriminator loss: 0.6860802173614502, acc: 0.4375] [gan loss: 0.817845, acc: 0.312500]\n",
            "5160: [discriminator loss: 0.7390269041061401, acc: 0.0234375] [gan loss: 1.783474, acc: 0.000000]\n",
            "5161: [discriminator loss: 0.6392147541046143, acc: 0.4609375] [gan loss: 0.821062, acc: 0.328125]\n",
            "5162: [discriminator loss: 0.7718470096588135, acc: 0.0390625] [gan loss: 1.809511, acc: 0.000000]\n",
            "5163: [discriminator loss: 0.6984010338783264, acc: 0.4140625] [gan loss: 0.741610, acc: 0.484375]\n",
            "5164: [discriminator loss: 0.7234614491462708, acc: 0.0703125] [gan loss: 1.492035, acc: 0.000000]\n",
            "5165: [discriminator loss: 0.6733952760696411, acc: 0.390625] [gan loss: 0.817699, acc: 0.296875]\n",
            "5166: [discriminator loss: 0.7737674713134766, acc: 0.0546875] [gan loss: 1.874288, acc: 0.000000]\n",
            "5167: [discriminator loss: 0.6698181629180908, acc: 0.46875] [gan loss: 0.667791, acc: 0.562500]\n",
            "5168: [discriminator loss: 0.7694408893585205, acc: 0.03125] [gan loss: 1.697270, acc: 0.000000]\n",
            "5169: [discriminator loss: 0.6632887721061707, acc: 0.4296875] [gan loss: 0.855534, acc: 0.218750]\n",
            "5170: [discriminator loss: 0.7236013412475586, acc: 0.078125] [gan loss: 1.601185, acc: 0.000000]\n",
            "5171: [discriminator loss: 0.6480082869529724, acc: 0.453125] [gan loss: 0.835070, acc: 0.218750]\n",
            "5172: [discriminator loss: 0.7296385169029236, acc: 0.1015625] [gan loss: 1.569901, acc: 0.000000]\n",
            "5173: [discriminator loss: 0.6368802189826965, acc: 0.421875] [gan loss: 0.927817, acc: 0.093750]\n",
            "5174: [discriminator loss: 0.7239373922348022, acc: 0.0859375] [gan loss: 1.520148, acc: 0.000000]\n",
            "5175: [discriminator loss: 0.6652043461799622, acc: 0.4296875] [gan loss: 0.890622, acc: 0.187500]\n",
            "5176: [discriminator loss: 0.6972664594650269, acc: 0.1328125] [gan loss: 1.668824, acc: 0.000000]\n",
            "5177: [discriminator loss: 0.6577638387680054, acc: 0.4140625] [gan loss: 0.870062, acc: 0.234375]\n",
            "5178: [discriminator loss: 0.75185227394104, acc: 0.0546875] [gan loss: 1.894762, acc: 0.000000]\n",
            "5179: [discriminator loss: 0.6813304424285889, acc: 0.4765625] [gan loss: 0.637142, acc: 0.640625]\n",
            "5180: [discriminator loss: 0.8120386600494385, acc: 0.0] [gan loss: 1.826357, acc: 0.000000]\n",
            "5181: [discriminator loss: 0.6748580932617188, acc: 0.4765625] [gan loss: 0.692997, acc: 0.500000]\n",
            "5182: [discriminator loss: 0.7940183281898499, acc: 0.0390625] [gan loss: 1.864754, acc: 0.000000]\n",
            "5183: [discriminator loss: 0.6904470324516296, acc: 0.4609375] [gan loss: 0.665142, acc: 0.625000]\n",
            "5184: [discriminator loss: 0.7818419337272644, acc: 0.03125] [gan loss: 1.632908, acc: 0.000000]\n",
            "5185: [discriminator loss: 0.6604049801826477, acc: 0.4140625] [gan loss: 0.824569, acc: 0.296875]\n",
            "5186: [discriminator loss: 0.7531064748764038, acc: 0.0703125] [gan loss: 1.639767, acc: 0.000000]\n",
            "5187: [discriminator loss: 0.6687729954719543, acc: 0.4296875] [gan loss: 0.864566, acc: 0.281250]\n",
            "5188: [discriminator loss: 0.6937947273254395, acc: 0.1484375] [gan loss: 1.350537, acc: 0.000000]\n",
            "5189: [discriminator loss: 0.638334333896637, acc: 0.375] [gan loss: 0.947587, acc: 0.062500]\n",
            "5190: [discriminator loss: 0.6829022169113159, acc: 0.15625] [gan loss: 1.416561, acc: 0.015625]\n",
            "5191: [discriminator loss: 0.6692714691162109, acc: 0.390625] [gan loss: 0.982174, acc: 0.125000]\n",
            "5192: [discriminator loss: 0.674680769443512, acc: 0.125] [gan loss: 1.604894, acc: 0.000000]\n",
            "5193: [discriminator loss: 0.6977464556694031, acc: 0.3828125] [gan loss: 0.757358, acc: 0.375000]\n",
            "5194: [discriminator loss: 0.7438554763793945, acc: 0.078125] [gan loss: 1.671011, acc: 0.000000]\n",
            "5195: [discriminator loss: 0.676220178604126, acc: 0.4765625] [gan loss: 0.706639, acc: 0.468750]\n",
            "5196: [discriminator loss: 0.7784351110458374, acc: 0.0546875] [gan loss: 1.999978, acc: 0.000000]\n",
            "5197: [discriminator loss: 0.6720970869064331, acc: 0.484375] [gan loss: 0.627470, acc: 0.656250]\n",
            "5198: [discriminator loss: 0.8206120729446411, acc: 0.0078125] [gan loss: 1.835093, acc: 0.000000]\n",
            "5199: [discriminator loss: 0.6419305801391602, acc: 0.484375] [gan loss: 0.798086, acc: 0.375000]\n",
            "5200: [discriminator loss: 0.7293350696563721, acc: 0.03125] [gan loss: 1.592203, acc: 0.015625]\n",
            "5201: [discriminator loss: 0.6365261673927307, acc: 0.4296875] [gan loss: 0.900840, acc: 0.125000]\n",
            "5202: [discriminator loss: 0.7395457029342651, acc: 0.09375] [gan loss: 1.638724, acc: 0.000000]\n",
            "5203: [discriminator loss: 0.6656109094619751, acc: 0.4375] [gan loss: 0.822946, acc: 0.281250]\n",
            "5204: [discriminator loss: 0.7289911508560181, acc: 0.0390625] [gan loss: 1.662268, acc: 0.000000]\n",
            "5205: [discriminator loss: 0.6468374133110046, acc: 0.453125] [gan loss: 0.746397, acc: 0.437500]\n",
            "5206: [discriminator loss: 0.7403809428215027, acc: 0.03125] [gan loss: 1.800445, acc: 0.000000]\n",
            "5207: [discriminator loss: 0.6858156323432922, acc: 0.4609375] [gan loss: 0.688667, acc: 0.578125]\n",
            "5208: [discriminator loss: 0.7769134640693665, acc: 0.03125] [gan loss: 1.789440, acc: 0.000000]\n",
            "5209: [discriminator loss: 0.6482946872711182, acc: 0.46875] [gan loss: 0.797501, acc: 0.328125]\n",
            "5210: [discriminator loss: 0.740801215171814, acc: 0.0390625] [gan loss: 1.503978, acc: 0.000000]\n",
            "5211: [discriminator loss: 0.6345585584640503, acc: 0.375] [gan loss: 1.027434, acc: 0.062500]\n",
            "5212: [discriminator loss: 0.6723494529724121, acc: 0.1953125] [gan loss: 1.297323, acc: 0.015625]\n",
            "5213: [discriminator loss: 0.6458343267440796, acc: 0.3671875] [gan loss: 1.087606, acc: 0.046875]\n",
            "5214: [discriminator loss: 0.7098363637924194, acc: 0.1640625] [gan loss: 1.373066, acc: 0.015625]\n",
            "5215: [discriminator loss: 0.6340055465698242, acc: 0.3359375] [gan loss: 1.193267, acc: 0.046875]\n",
            "5216: [discriminator loss: 0.6833227872848511, acc: 0.203125] [gan loss: 1.401720, acc: 0.000000]\n",
            "5217: [discriminator loss: 0.6678723096847534, acc: 0.375] [gan loss: 0.994787, acc: 0.156250]\n",
            "5218: [discriminator loss: 0.6969529986381531, acc: 0.125] [gan loss: 1.842222, acc: 0.000000]\n",
            "5219: [discriminator loss: 0.6630613803863525, acc: 0.4453125] [gan loss: 0.672135, acc: 0.578125]\n",
            "5220: [discriminator loss: 0.7653050422668457, acc: 0.0625] [gan loss: 1.972839, acc: 0.000000]\n",
            "5221: [discriminator loss: 0.7080075144767761, acc: 0.4921875] [gan loss: 0.559667, acc: 0.781250]\n",
            "5222: [discriminator loss: 0.8387213945388794, acc: 0.03125] [gan loss: 1.881651, acc: 0.000000]\n",
            "5223: [discriminator loss: 0.6741575598716736, acc: 0.4765625] [gan loss: 0.685468, acc: 0.578125]\n",
            "5224: [discriminator loss: 0.7737154960632324, acc: 0.015625] [gan loss: 1.567764, acc: 0.000000]\n",
            "5225: [discriminator loss: 0.686252236366272, acc: 0.4296875] [gan loss: 0.765206, acc: 0.375000]\n",
            "5226: [discriminator loss: 0.8110545873641968, acc: 0.0] [gan loss: 1.672630, acc: 0.000000]\n",
            "5227: [discriminator loss: 0.6420044302940369, acc: 0.4609375] [gan loss: 0.823963, acc: 0.406250]\n",
            "5228: [discriminator loss: 0.7324331998825073, acc: 0.1015625] [gan loss: 1.472150, acc: 0.000000]\n",
            "5229: [discriminator loss: 0.663032054901123, acc: 0.3671875] [gan loss: 0.927849, acc: 0.156250]\n",
            "5230: [discriminator loss: 0.7295066118240356, acc: 0.078125] [gan loss: 1.363155, acc: 0.015625]\n",
            "5231: [discriminator loss: 0.6407073736190796, acc: 0.40625] [gan loss: 0.897175, acc: 0.265625]\n",
            "5232: [discriminator loss: 0.6964836120605469, acc: 0.09375] [gan loss: 1.391919, acc: 0.000000]\n",
            "5233: [discriminator loss: 0.6506755352020264, acc: 0.3984375] [gan loss: 0.964364, acc: 0.187500]\n",
            "5234: [discriminator loss: 0.7485604286193848, acc: 0.046875] [gan loss: 1.831477, acc: 0.000000]\n",
            "5235: [discriminator loss: 0.6780524253845215, acc: 0.4375] [gan loss: 0.716403, acc: 0.578125]\n",
            "5236: [discriminator loss: 0.7531275749206543, acc: 0.03125] [gan loss: 1.730473, acc: 0.000000]\n",
            "5237: [discriminator loss: 0.676491916179657, acc: 0.4453125] [gan loss: 0.776438, acc: 0.296875]\n",
            "5238: [discriminator loss: 0.7622160911560059, acc: 0.046875] [gan loss: 1.801539, acc: 0.000000]\n",
            "5239: [discriminator loss: 0.6512879133224487, acc: 0.4765625] [gan loss: 0.696963, acc: 0.453125]\n",
            "5240: [discriminator loss: 0.7234823107719421, acc: 0.0625] [gan loss: 1.612567, acc: 0.000000]\n",
            "5241: [discriminator loss: 0.6587408781051636, acc: 0.4375] [gan loss: 0.889703, acc: 0.250000]\n",
            "5242: [discriminator loss: 0.7138971090316772, acc: 0.09375] [gan loss: 1.629157, acc: 0.015625]\n",
            "5243: [discriminator loss: 0.6890482902526855, acc: 0.3828125] [gan loss: 0.951450, acc: 0.140625]\n",
            "5244: [discriminator loss: 0.6996617317199707, acc: 0.1328125] [gan loss: 1.433526, acc: 0.000000]\n",
            "5245: [discriminator loss: 0.6445099115371704, acc: 0.3515625] [gan loss: 0.935454, acc: 0.203125]\n",
            "5246: [discriminator loss: 0.6899380683898926, acc: 0.140625] [gan loss: 1.497584, acc: 0.015625]\n",
            "5247: [discriminator loss: 0.6327248811721802, acc: 0.40625] [gan loss: 0.913655, acc: 0.171875]\n",
            "5248: [discriminator loss: 0.7383796572685242, acc: 0.0546875] [gan loss: 1.650674, acc: 0.000000]\n",
            "5249: [discriminator loss: 0.6497825980186462, acc: 0.46875] [gan loss: 0.813154, acc: 0.437500]\n",
            "5250: [discriminator loss: 0.7318471670150757, acc: 0.0859375] [gan loss: 1.810290, acc: 0.000000]\n",
            "5251: [discriminator loss: 0.6908237338066101, acc: 0.46875] [gan loss: 0.582845, acc: 0.750000]\n",
            "5252: [discriminator loss: 0.7852641344070435, acc: 0.0234375] [gan loss: 1.715239, acc: 0.000000]\n",
            "5253: [discriminator loss: 0.652194619178772, acc: 0.484375] [gan loss: 0.664471, acc: 0.625000]\n",
            "5254: [discriminator loss: 0.8222083449363708, acc: 0.0078125] [gan loss: 1.582947, acc: 0.000000]\n",
            "5255: [discriminator loss: 0.688807487487793, acc: 0.4609375] [gan loss: 0.762102, acc: 0.328125]\n",
            "5256: [discriminator loss: 0.7509525418281555, acc: 0.0390625] [gan loss: 1.720150, acc: 0.000000]\n",
            "5257: [discriminator loss: 0.6431357264518738, acc: 0.4609375] [gan loss: 0.751225, acc: 0.468750]\n",
            "5258: [discriminator loss: 0.7797024250030518, acc: 0.046875] [gan loss: 1.618092, acc: 0.000000]\n",
            "5259: [discriminator loss: 0.66552734375, acc: 0.40625] [gan loss: 0.859621, acc: 0.328125]\n",
            "5260: [discriminator loss: 0.7096840143203735, acc: 0.09375] [gan loss: 1.485963, acc: 0.000000]\n",
            "5261: [discriminator loss: 0.6494640111923218, acc: 0.359375] [gan loss: 1.004195, acc: 0.140625]\n",
            "5262: [discriminator loss: 0.693703293800354, acc: 0.171875] [gan loss: 1.394362, acc: 0.046875]\n",
            "5263: [discriminator loss: 0.6898244619369507, acc: 0.328125] [gan loss: 1.305324, acc: 0.000000]\n",
            "5264: [discriminator loss: 0.6465516090393066, acc: 0.2890625] [gan loss: 1.247912, acc: 0.031250]\n",
            "5265: [discriminator loss: 0.6717028617858887, acc: 0.2109375] [gan loss: 1.595600, acc: 0.000000]\n",
            "5266: [discriminator loss: 0.6555283069610596, acc: 0.4140625] [gan loss: 0.889823, acc: 0.109375]\n",
            "5267: [discriminator loss: 0.736795961856842, acc: 0.0625] [gan loss: 1.808282, acc: 0.000000]\n",
            "5268: [discriminator loss: 0.6727163791656494, acc: 0.4765625] [gan loss: 0.643716, acc: 0.718750]\n",
            "5269: [discriminator loss: 0.7657886147499084, acc: 0.0] [gan loss: 1.948610, acc: 0.000000]\n",
            "5270: [discriminator loss: 0.6644339561462402, acc: 0.5] [gan loss: 0.559543, acc: 0.718750]\n",
            "5271: [discriminator loss: 0.8078681230545044, acc: 0.0078125] [gan loss: 1.780267, acc: 0.000000]\n",
            "5272: [discriminator loss: 0.6730606555938721, acc: 0.484375] [gan loss: 0.668359, acc: 0.578125]\n",
            "5273: [discriminator loss: 0.7953962087631226, acc: 0.015625] [gan loss: 1.764920, acc: 0.000000]\n",
            "5274: [discriminator loss: 0.683983564376831, acc: 0.4921875] [gan loss: 0.695047, acc: 0.484375]\n",
            "5275: [discriminator loss: 0.7570600509643555, acc: 0.0234375] [gan loss: 1.742303, acc: 0.000000]\n",
            "5276: [discriminator loss: 0.6634923219680786, acc: 0.421875] [gan loss: 0.790283, acc: 0.312500]\n",
            "5277: [discriminator loss: 0.7513848543167114, acc: 0.0625] [gan loss: 1.563489, acc: 0.015625]\n",
            "5278: [discriminator loss: 0.6307716965675354, acc: 0.4609375] [gan loss: 0.702279, acc: 0.484375]\n",
            "5279: [discriminator loss: 0.7345659732818604, acc: 0.0546875] [gan loss: 1.536350, acc: 0.000000]\n",
            "5280: [discriminator loss: 0.6461049914360046, acc: 0.4296875] [gan loss: 0.855167, acc: 0.187500]\n",
            "5281: [discriminator loss: 0.7020056843757629, acc: 0.1328125] [gan loss: 1.373861, acc: 0.015625]\n",
            "5282: [discriminator loss: 0.6817964315414429, acc: 0.34375] [gan loss: 1.046872, acc: 0.093750]\n",
            "5283: [discriminator loss: 0.6963764429092407, acc: 0.171875] [gan loss: 1.495746, acc: 0.000000]\n",
            "5284: [discriminator loss: 0.6379382610321045, acc: 0.3515625] [gan loss: 1.047283, acc: 0.078125]\n",
            "5285: [discriminator loss: 0.6668128967285156, acc: 0.21875] [gan loss: 1.393853, acc: 0.015625]\n",
            "5286: [discriminator loss: 0.6805964112281799, acc: 0.2890625] [gan loss: 1.223300, acc: 0.000000]\n",
            "5287: [discriminator loss: 0.6758268475532532, acc: 0.2578125] [gan loss: 1.244128, acc: 0.015625]\n",
            "5288: [discriminator loss: 0.6789951324462891, acc: 0.25] [gan loss: 1.225998, acc: 0.000000]\n",
            "5289: [discriminator loss: 0.6859683990478516, acc: 0.2578125] [gan loss: 1.178313, acc: 0.015625]\n",
            "5290: [discriminator loss: 0.6677868366241455, acc: 0.203125] [gan loss: 1.292618, acc: 0.000000]\n",
            "5291: [discriminator loss: 0.6712566018104553, acc: 0.265625] [gan loss: 1.459328, acc: 0.000000]\n",
            "5292: [discriminator loss: 0.6989398002624512, acc: 0.28125] [gan loss: 1.212361, acc: 0.015625]\n",
            "5293: [discriminator loss: 0.6769136786460876, acc: 0.2109375] [gan loss: 1.628528, acc: 0.000000]\n",
            "5294: [discriminator loss: 0.6795129179954529, acc: 0.4375] [gan loss: 0.710375, acc: 0.531250]\n",
            "5295: [discriminator loss: 0.7221011519432068, acc: 0.078125] [gan loss: 2.130265, acc: 0.000000]\n",
            "5296: [discriminator loss: 0.700153648853302, acc: 0.484375] [gan loss: 0.478141, acc: 0.906250]\n",
            "5297: [discriminator loss: 0.838361382484436, acc: 0.015625] [gan loss: 2.204552, acc: 0.000000]\n",
            "5298: [discriminator loss: 0.7285065054893494, acc: 0.4921875] [gan loss: 0.486645, acc: 0.875000]\n",
            "5299: [discriminator loss: 0.8738975524902344, acc: 0.0] [gan loss: 1.762187, acc: 0.000000]\n",
            "5300: [discriminator loss: 0.7035191059112549, acc: 0.4453125] [gan loss: 0.753758, acc: 0.406250]\n",
            "5301: [discriminator loss: 0.7788619995117188, acc: 0.0078125] [gan loss: 1.621843, acc: 0.000000]\n",
            "5302: [discriminator loss: 0.6338908672332764, acc: 0.4453125] [gan loss: 0.891833, acc: 0.265625]\n",
            "5303: [discriminator loss: 0.7366803288459778, acc: 0.0234375] [gan loss: 1.493227, acc: 0.000000]\n",
            "5304: [discriminator loss: 0.6440565586090088, acc: 0.4296875] [gan loss: 0.877011, acc: 0.218750]\n",
            "5305: [discriminator loss: 0.7190641164779663, acc: 0.09375] [gan loss: 1.390226, acc: 0.031250]\n",
            "5306: [discriminator loss: 0.6761947870254517, acc: 0.3515625] [gan loss: 0.895530, acc: 0.218750]\n",
            "5307: [discriminator loss: 0.730205774307251, acc: 0.140625] [gan loss: 1.571640, acc: 0.000000]\n",
            "5308: [discriminator loss: 0.6506099104881287, acc: 0.4375] [gan loss: 0.937862, acc: 0.140625]\n",
            "5309: [discriminator loss: 0.7450053691864014, acc: 0.046875] [gan loss: 1.589104, acc: 0.000000]\n",
            "5310: [discriminator loss: 0.6492174863815308, acc: 0.4375] [gan loss: 0.812620, acc: 0.343750]\n",
            "5311: [discriminator loss: 0.7080475091934204, acc: 0.0859375] [gan loss: 1.661947, acc: 0.000000]\n",
            "5312: [discriminator loss: 0.653755247592926, acc: 0.4765625] [gan loss: 0.741368, acc: 0.421875]\n",
            "5313: [discriminator loss: 0.7934951782226562, acc: 0.03125] [gan loss: 1.886653, acc: 0.000000]\n",
            "5314: [discriminator loss: 0.6571493148803711, acc: 0.4765625] [gan loss: 0.782639, acc: 0.312500]\n",
            "5315: [discriminator loss: 0.7666041254997253, acc: 0.0546875] [gan loss: 1.840927, acc: 0.000000]\n",
            "5316: [discriminator loss: 0.6674274802207947, acc: 0.46875] [gan loss: 0.715439, acc: 0.437500]\n",
            "5317: [discriminator loss: 0.7652839422225952, acc: 0.0390625] [gan loss: 1.612401, acc: 0.000000]\n",
            "5318: [discriminator loss: 0.6578635573387146, acc: 0.453125] [gan loss: 0.710570, acc: 0.531250]\n",
            "5319: [discriminator loss: 0.7570101022720337, acc: 0.0703125] [gan loss: 1.611757, acc: 0.000000]\n",
            "5320: [discriminator loss: 0.6752363443374634, acc: 0.40625] [gan loss: 0.846760, acc: 0.312500]\n",
            "5321: [discriminator loss: 0.7050125598907471, acc: 0.1171875] [gan loss: 1.622207, acc: 0.000000]\n",
            "5322: [discriminator loss: 0.6318848133087158, acc: 0.421875] [gan loss: 0.969207, acc: 0.093750]\n",
            "5323: [discriminator loss: 0.6898066997528076, acc: 0.1484375] [gan loss: 1.296129, acc: 0.000000]\n",
            "5324: [discriminator loss: 0.6499964594841003, acc: 0.359375] [gan loss: 0.938971, acc: 0.125000]\n",
            "5325: [discriminator loss: 0.7019038200378418, acc: 0.1328125] [gan loss: 1.511943, acc: 0.000000]\n",
            "5326: [discriminator loss: 0.6389646530151367, acc: 0.390625] [gan loss: 1.095700, acc: 0.062500]\n",
            "5327: [discriminator loss: 0.7129011154174805, acc: 0.140625] [gan loss: 1.472739, acc: 0.000000]\n",
            "5328: [discriminator loss: 0.6661831140518188, acc: 0.3984375] [gan loss: 0.818602, acc: 0.265625]\n",
            "5329: [discriminator loss: 0.7330411672592163, acc: 0.0625] [gan loss: 1.849570, acc: 0.000000]\n",
            "5330: [discriminator loss: 0.6817823648452759, acc: 0.4921875] [gan loss: 0.560132, acc: 0.781250]\n",
            "5331: [discriminator loss: 0.8467262983322144, acc: 0.015625] [gan loss: 2.088022, acc: 0.000000]\n",
            "5332: [discriminator loss: 0.6849998235702515, acc: 0.5] [gan loss: 0.618143, acc: 0.656250]\n",
            "5333: [discriminator loss: 0.7993335723876953, acc: 0.0234375] [gan loss: 1.449967, acc: 0.000000]\n",
            "5334: [discriminator loss: 0.6416006684303284, acc: 0.4453125] [gan loss: 0.831452, acc: 0.265625]\n",
            "5335: [discriminator loss: 0.7239106297492981, acc: 0.09375] [gan loss: 1.492021, acc: 0.031250]\n",
            "5336: [discriminator loss: 0.6945250034332275, acc: 0.34375] [gan loss: 1.041737, acc: 0.109375]\n",
            "5337: [discriminator loss: 0.7074337601661682, acc: 0.1796875] [gan loss: 1.501600, acc: 0.015625]\n",
            "5338: [discriminator loss: 0.6442875862121582, acc: 0.3515625] [gan loss: 1.147532, acc: 0.062500]\n",
            "5339: [discriminator loss: 0.6896249055862427, acc: 0.1484375] [gan loss: 1.284677, acc: 0.046875]\n",
            "5340: [discriminator loss: 0.6725959181785583, acc: 0.296875] [gan loss: 1.118863, acc: 0.078125]\n",
            "5341: [discriminator loss: 0.7168763875961304, acc: 0.2109375] [gan loss: 1.476248, acc: 0.000000]\n",
            "5342: [discriminator loss: 0.651543140411377, acc: 0.4375] [gan loss: 0.876750, acc: 0.250000]\n",
            "5343: [discriminator loss: 0.7459731698036194, acc: 0.0703125] [gan loss: 1.786385, acc: 0.000000]\n",
            "5344: [discriminator loss: 0.6870975494384766, acc: 0.4609375] [gan loss: 0.661552, acc: 0.562500]\n",
            "5345: [discriminator loss: 0.7863831520080566, acc: 0.015625] [gan loss: 2.076941, acc: 0.000000]\n",
            "5346: [discriminator loss: 0.6765105724334717, acc: 0.4921875] [gan loss: 0.594919, acc: 0.781250]\n",
            "5347: [discriminator loss: 0.8218628764152527, acc: 0.0078125] [gan loss: 2.023335, acc: 0.000000]\n",
            "5348: [discriminator loss: 0.6910561919212341, acc: 0.46875] [gan loss: 0.593013, acc: 0.671875]\n",
            "5349: [discriminator loss: 0.7812763452529907, acc: 0.03125] [gan loss: 1.586822, acc: 0.000000]\n",
            "5350: [discriminator loss: 0.6773333549499512, acc: 0.4375] [gan loss: 0.754114, acc: 0.390625]\n",
            "5351: [discriminator loss: 0.7698594331741333, acc: 0.0390625] [gan loss: 1.696144, acc: 0.000000]\n",
            "5352: [discriminator loss: 0.6461261510848999, acc: 0.484375] [gan loss: 0.761247, acc: 0.437500]\n",
            "5353: [discriminator loss: 0.7581595182418823, acc: 0.046875] [gan loss: 1.564080, acc: 0.000000]\n",
            "5354: [discriminator loss: 0.6261730194091797, acc: 0.421875] [gan loss: 0.897133, acc: 0.250000]\n",
            "5355: [discriminator loss: 0.7202839255332947, acc: 0.1328125] [gan loss: 1.394083, acc: 0.000000]\n",
            "5356: [discriminator loss: 0.6513187885284424, acc: 0.34375] [gan loss: 0.940009, acc: 0.140625]\n",
            "5357: [discriminator loss: 0.7064951658248901, acc: 0.140625] [gan loss: 1.581491, acc: 0.000000]\n",
            "5358: [discriminator loss: 0.6661970615386963, acc: 0.4375] [gan loss: 0.888824, acc: 0.218750]\n",
            "5359: [discriminator loss: 0.7506431937217712, acc: 0.046875] [gan loss: 1.766969, acc: 0.000000]\n",
            "5360: [discriminator loss: 0.6811401844024658, acc: 0.4765625] [gan loss: 0.623321, acc: 0.656250]\n",
            "5361: [discriminator loss: 0.7643648386001587, acc: 0.0390625] [gan loss: 1.767152, acc: 0.000000]\n",
            "5362: [discriminator loss: 0.6669183969497681, acc: 0.4453125] [gan loss: 0.625880, acc: 0.640625]\n",
            "5363: [discriminator loss: 0.7471729516983032, acc: 0.015625] [gan loss: 1.717467, acc: 0.000000]\n",
            "5364: [discriminator loss: 0.6465381383895874, acc: 0.46875] [gan loss: 0.749308, acc: 0.437500]\n",
            "5365: [discriminator loss: 0.7656117677688599, acc: 0.0546875] [gan loss: 1.697165, acc: 0.000000]\n",
            "5366: [discriminator loss: 0.6643813848495483, acc: 0.4609375] [gan loss: 0.714167, acc: 0.468750]\n",
            "5367: [discriminator loss: 0.7579512596130371, acc: 0.0234375] [gan loss: 1.766849, acc: 0.000000]\n",
            "5368: [discriminator loss: 0.6603168845176697, acc: 0.4296875] [gan loss: 0.764992, acc: 0.421875]\n",
            "5369: [discriminator loss: 0.7392981648445129, acc: 0.046875] [gan loss: 1.684958, acc: 0.000000]\n",
            "5370: [discriminator loss: 0.6866804361343384, acc: 0.4765625] [gan loss: 0.670185, acc: 0.578125]\n",
            "5371: [discriminator loss: 0.8255463242530823, acc: 0.0234375] [gan loss: 1.706428, acc: 0.000000]\n",
            "5372: [discriminator loss: 0.6764355897903442, acc: 0.46875] [gan loss: 0.723709, acc: 0.484375]\n",
            "5373: [discriminator loss: 0.742565393447876, acc: 0.015625] [gan loss: 1.642362, acc: 0.000000]\n",
            "5374: [discriminator loss: 0.6787582039833069, acc: 0.4140625] [gan loss: 0.828335, acc: 0.250000]\n",
            "5375: [discriminator loss: 0.7629919052124023, acc: 0.046875] [gan loss: 1.567643, acc: 0.015625]\n",
            "5376: [discriminator loss: 0.6715285778045654, acc: 0.4140625] [gan loss: 0.819428, acc: 0.312500]\n",
            "5377: [discriminator loss: 0.7538288831710815, acc: 0.046875] [gan loss: 1.633007, acc: 0.000000]\n",
            "5378: [discriminator loss: 0.6526662111282349, acc: 0.4609375] [gan loss: 0.747285, acc: 0.453125]\n",
            "5379: [discriminator loss: 0.7306861877441406, acc: 0.0546875] [gan loss: 1.551256, acc: 0.015625]\n",
            "5380: [discriminator loss: 0.6686925292015076, acc: 0.359375] [gan loss: 0.887299, acc: 0.234375]\n",
            "5381: [discriminator loss: 0.6928254961967468, acc: 0.1328125] [gan loss: 1.397542, acc: 0.015625]\n",
            "5382: [discriminator loss: 0.6505134105682373, acc: 0.3515625] [gan loss: 0.901819, acc: 0.187500]\n",
            "5383: [discriminator loss: 0.708767294883728, acc: 0.1328125] [gan loss: 1.699929, acc: 0.000000]\n",
            "5384: [discriminator loss: 0.6599020957946777, acc: 0.40625] [gan loss: 0.860695, acc: 0.234375]\n",
            "5385: [discriminator loss: 0.7116948366165161, acc: 0.1171875] [gan loss: 1.608193, acc: 0.031250]\n",
            "5386: [discriminator loss: 0.6668770909309387, acc: 0.4296875] [gan loss: 0.765780, acc: 0.375000]\n",
            "5387: [discriminator loss: 0.7445042133331299, acc: 0.03125] [gan loss: 1.845510, acc: 0.000000]\n",
            "5388: [discriminator loss: 0.6650463342666626, acc: 0.4609375] [gan loss: 0.697747, acc: 0.593750]\n",
            "5389: [discriminator loss: 0.7895364165306091, acc: 0.015625] [gan loss: 1.941696, acc: 0.000000]\n",
            "5390: [discriminator loss: 0.687911868095398, acc: 0.484375] [gan loss: 0.730525, acc: 0.500000]\n",
            "5391: [discriminator loss: 0.7823697328567505, acc: 0.0546875] [gan loss: 1.831617, acc: 0.000000]\n",
            "5392: [discriminator loss: 0.6395926475524902, acc: 0.484375] [gan loss: 0.725051, acc: 0.468750]\n",
            "5393: [discriminator loss: 0.7653249502182007, acc: 0.0234375] [gan loss: 1.729033, acc: 0.000000]\n",
            "5394: [discriminator loss: 0.6862495541572571, acc: 0.484375] [gan loss: 0.694926, acc: 0.546875]\n",
            "5395: [discriminator loss: 0.763857364654541, acc: 0.0390625] [gan loss: 1.620375, acc: 0.000000]\n",
            "5396: [discriminator loss: 0.6458541750907898, acc: 0.4453125] [gan loss: 0.775865, acc: 0.375000]\n",
            "5397: [discriminator loss: 0.7477749586105347, acc: 0.0546875] [gan loss: 1.501846, acc: 0.000000]\n",
            "5398: [discriminator loss: 0.6698338985443115, acc: 0.375] [gan loss: 0.959315, acc: 0.125000]\n",
            "5399: [discriminator loss: 0.7136433720588684, acc: 0.109375] [gan loss: 1.434372, acc: 0.000000]\n",
            "5400: [discriminator loss: 0.6798379421234131, acc: 0.328125] [gan loss: 0.986890, acc: 0.062500]\n",
            "5401: [discriminator loss: 0.7156774997711182, acc: 0.1171875] [gan loss: 1.537989, acc: 0.000000]\n",
            "5402: [discriminator loss: 0.6492711901664734, acc: 0.4296875] [gan loss: 0.820670, acc: 0.187500]\n",
            "5403: [discriminator loss: 0.7511566281318665, acc: 0.0546875] [gan loss: 1.934164, acc: 0.000000]\n",
            "5404: [discriminator loss: 0.6815209984779358, acc: 0.4765625] [gan loss: 0.630960, acc: 0.703125]\n",
            "5405: [discriminator loss: 0.7421169877052307, acc: 0.03125] [gan loss: 1.767786, acc: 0.000000]\n",
            "5406: [discriminator loss: 0.6970967054367065, acc: 0.453125] [gan loss: 0.680480, acc: 0.593750]\n",
            "5407: [discriminator loss: 0.7773102521896362, acc: 0.015625] [gan loss: 1.760994, acc: 0.015625]\n",
            "5408: [discriminator loss: 0.6782029271125793, acc: 0.4609375] [gan loss: 0.679410, acc: 0.531250]\n",
            "5409: [discriminator loss: 0.756273627281189, acc: 0.03125] [gan loss: 1.675665, acc: 0.000000]\n",
            "5410: [discriminator loss: 0.6721131801605225, acc: 0.4453125] [gan loss: 0.799242, acc: 0.406250]\n",
            "5411: [discriminator loss: 0.7596297264099121, acc: 0.03125] [gan loss: 1.602543, acc: 0.000000]\n",
            "5412: [discriminator loss: 0.6460514664649963, acc: 0.4140625] [gan loss: 0.831241, acc: 0.281250]\n",
            "5413: [discriminator loss: 0.7816947102546692, acc: 0.046875] [gan loss: 1.588409, acc: 0.000000]\n",
            "5414: [discriminator loss: 0.6441144347190857, acc: 0.46875] [gan loss: 0.738359, acc: 0.468750]\n",
            "5415: [discriminator loss: 0.7588210105895996, acc: 0.03125] [gan loss: 1.607574, acc: 0.000000]\n",
            "5416: [discriminator loss: 0.6605983972549438, acc: 0.453125] [gan loss: 0.883070, acc: 0.218750]\n",
            "5417: [discriminator loss: 0.7589799761772156, acc: 0.0546875] [gan loss: 1.560981, acc: 0.000000]\n",
            "5418: [discriminator loss: 0.6390420198440552, acc: 0.46875] [gan loss: 0.749551, acc: 0.406250]\n",
            "5419: [discriminator loss: 0.7479831576347351, acc: 0.0390625] [gan loss: 1.701118, acc: 0.000000]\n",
            "5420: [discriminator loss: 0.6642360687255859, acc: 0.4296875] [gan loss: 0.710685, acc: 0.468750]\n",
            "5421: [discriminator loss: 0.7445560693740845, acc: 0.0390625] [gan loss: 1.710377, acc: 0.000000]\n",
            "5422: [discriminator loss: 0.6837688088417053, acc: 0.421875] [gan loss: 0.739890, acc: 0.406250]\n",
            "5423: [discriminator loss: 0.7586199045181274, acc: 0.03125] [gan loss: 1.713518, acc: 0.000000]\n",
            "5424: [discriminator loss: 0.6834933757781982, acc: 0.4453125] [gan loss: 0.832190, acc: 0.312500]\n",
            "5425: [discriminator loss: 0.6972823739051819, acc: 0.0703125] [gan loss: 1.492126, acc: 0.015625]\n",
            "5426: [discriminator loss: 0.6649627685546875, acc: 0.4296875] [gan loss: 0.852841, acc: 0.171875]\n",
            "5427: [discriminator loss: 0.7032605409622192, acc: 0.09375] [gan loss: 1.496910, acc: 0.000000]\n",
            "5428: [discriminator loss: 0.6562884449958801, acc: 0.4375] [gan loss: 0.809628, acc: 0.343750]\n",
            "5429: [discriminator loss: 0.7662158012390137, acc: 0.0703125] [gan loss: 1.718707, acc: 0.000000]\n",
            "5430: [discriminator loss: 0.6470203399658203, acc: 0.4765625] [gan loss: 0.741058, acc: 0.421875]\n",
            "5431: [discriminator loss: 0.778773844242096, acc: 0.0390625] [gan loss: 1.583843, acc: 0.000000]\n",
            "5432: [discriminator loss: 0.7039918303489685, acc: 0.34375] [gan loss: 1.057041, acc: 0.078125]\n",
            "5433: [discriminator loss: 0.7002580761909485, acc: 0.1640625] [gan loss: 1.668287, acc: 0.000000]\n",
            "5434: [discriminator loss: 0.6508646011352539, acc: 0.390625] [gan loss: 0.909425, acc: 0.171875]\n",
            "5435: [discriminator loss: 0.7390233874320984, acc: 0.1015625] [gan loss: 1.599941, acc: 0.000000]\n",
            "5436: [discriminator loss: 0.670203447341919, acc: 0.4453125] [gan loss: 0.718677, acc: 0.546875]\n",
            "5437: [discriminator loss: 0.7614966630935669, acc: 0.015625] [gan loss: 1.854737, acc: 0.000000]\n",
            "5438: [discriminator loss: 0.6585373282432556, acc: 0.484375] [gan loss: 0.615134, acc: 0.656250]\n",
            "5439: [discriminator loss: 0.8271839022636414, acc: 0.015625] [gan loss: 1.897259, acc: 0.000000]\n",
            "5440: [discriminator loss: 0.6624154448509216, acc: 0.4765625] [gan loss: 0.705950, acc: 0.437500]\n",
            "5441: [discriminator loss: 0.7733116149902344, acc: 0.015625] [gan loss: 1.829832, acc: 0.000000]\n",
            "5442: [discriminator loss: 0.6655910015106201, acc: 0.4375] [gan loss: 0.762319, acc: 0.390625]\n",
            "5443: [discriminator loss: 0.7715043425559998, acc: 0.0390625] [gan loss: 1.552814, acc: 0.000000]\n",
            "5444: [discriminator loss: 0.645089864730835, acc: 0.4296875] [gan loss: 0.818424, acc: 0.343750]\n",
            "5445: [discriminator loss: 0.7456929683685303, acc: 0.09375] [gan loss: 1.435883, acc: 0.000000]\n",
            "5446: [discriminator loss: 0.6739448308944702, acc: 0.3515625] [gan loss: 1.014023, acc: 0.093750]\n",
            "5447: [discriminator loss: 0.6566720008850098, acc: 0.2265625] [gan loss: 1.346196, acc: 0.015625]\n",
            "5448: [discriminator loss: 0.674761176109314, acc: 0.28125] [gan loss: 1.160846, acc: 0.031250]\n",
            "5449: [discriminator loss: 0.6647664308547974, acc: 0.2578125] [gan loss: 1.252153, acc: 0.031250]\n",
            "5450: [discriminator loss: 0.6898373365402222, acc: 0.2265625] [gan loss: 1.212036, acc: 0.031250]\n",
            "5451: [discriminator loss: 0.6985516548156738, acc: 0.2265625] [gan loss: 1.411251, acc: 0.000000]\n",
            "5452: [discriminator loss: 0.6570056080818176, acc: 0.3125] [gan loss: 1.113843, acc: 0.031250]\n",
            "5453: [discriminator loss: 0.6980273723602295, acc: 0.1640625] [gan loss: 1.565363, acc: 0.000000]\n",
            "5454: [discriminator loss: 0.6648069024085999, acc: 0.390625] [gan loss: 1.007284, acc: 0.046875]\n",
            "5455: [discriminator loss: 0.7163249254226685, acc: 0.1328125] [gan loss: 1.640427, acc: 0.000000]\n",
            "5456: [discriminator loss: 0.6857708692550659, acc: 0.421875] [gan loss: 0.757778, acc: 0.359375]\n",
            "5457: [discriminator loss: 0.7304471135139465, acc: 0.046875] [gan loss: 1.873143, acc: 0.000000]\n",
            "5458: [discriminator loss: 0.6348750591278076, acc: 0.4921875] [gan loss: 0.537958, acc: 0.812500]\n",
            "5459: [discriminator loss: 0.8421412706375122, acc: 0.0078125] [gan loss: 2.121082, acc: 0.000000]\n",
            "5460: [discriminator loss: 0.6807719469070435, acc: 0.4921875] [gan loss: 0.559038, acc: 0.765625]\n",
            "5461: [discriminator loss: 0.8604338765144348, acc: 0.0078125] [gan loss: 1.969016, acc: 0.000000]\n",
            "5462: [discriminator loss: 0.6467971801757812, acc: 0.484375] [gan loss: 0.788150, acc: 0.312500]\n",
            "5463: [discriminator loss: 0.7434311509132385, acc: 0.0625] [gan loss: 1.442684, acc: 0.000000]\n",
            "5464: [discriminator loss: 0.6539727449417114, acc: 0.3984375] [gan loss: 1.000214, acc: 0.078125]\n",
            "5465: [discriminator loss: 0.7275034785270691, acc: 0.1015625] [gan loss: 1.478433, acc: 0.000000]\n",
            "5466: [discriminator loss: 0.6513848304748535, acc: 0.3828125] [gan loss: 0.944570, acc: 0.171875]\n",
            "5467: [discriminator loss: 0.7121866941452026, acc: 0.140625] [gan loss: 1.295782, acc: 0.000000]\n",
            "5468: [discriminator loss: 0.6640158891677856, acc: 0.3671875] [gan loss: 0.995885, acc: 0.078125]\n",
            "5469: [discriminator loss: 0.684711217880249, acc: 0.15625] [gan loss: 1.422971, acc: 0.000000]\n",
            "5470: [discriminator loss: 0.6551315188407898, acc: 0.3828125] [gan loss: 0.969963, acc: 0.156250]\n",
            "5471: [discriminator loss: 0.7477400302886963, acc: 0.09375] [gan loss: 1.769947, acc: 0.000000]\n",
            "5472: [discriminator loss: 0.684964120388031, acc: 0.46875] [gan loss: 0.703508, acc: 0.453125]\n",
            "5473: [discriminator loss: 0.7889106869697571, acc: 0.0078125] [gan loss: 1.955229, acc: 0.000000]\n",
            "5474: [discriminator loss: 0.6695531606674194, acc: 0.4765625] [gan loss: 0.642099, acc: 0.593750]\n",
            "5475: [discriminator loss: 0.8070061206817627, acc: 0.0390625] [gan loss: 1.724451, acc: 0.000000]\n",
            "5476: [discriminator loss: 0.653964638710022, acc: 0.5] [gan loss: 0.669491, acc: 0.625000]\n",
            "5477: [discriminator loss: 0.8256856799125671, acc: 0.0234375] [gan loss: 1.704861, acc: 0.000000]\n",
            "5478: [discriminator loss: 0.6842572689056396, acc: 0.46875] [gan loss: 0.794377, acc: 0.468750]\n",
            "5479: [discriminator loss: 0.8034859299659729, acc: 0.0234375] [gan loss: 1.742109, acc: 0.000000]\n",
            "5480: [discriminator loss: 0.6558541059494019, acc: 0.46875] [gan loss: 0.856061, acc: 0.265625]\n",
            "5481: [discriminator loss: 0.7446705102920532, acc: 0.0625] [gan loss: 1.374983, acc: 0.000000]\n",
            "5482: [discriminator loss: 0.643179178237915, acc: 0.3984375] [gan loss: 0.861243, acc: 0.218750]\n",
            "5483: [discriminator loss: 0.7079499363899231, acc: 0.109375] [gan loss: 1.540209, acc: 0.000000]\n",
            "5484: [discriminator loss: 0.6439329981803894, acc: 0.3671875] [gan loss: 0.912105, acc: 0.187500]\n",
            "5485: [discriminator loss: 0.7532005310058594, acc: 0.1171875] [gan loss: 1.465522, acc: 0.000000]\n",
            "5486: [discriminator loss: 0.6456354856491089, acc: 0.421875] [gan loss: 1.034020, acc: 0.062500]\n",
            "5487: [discriminator loss: 0.702794075012207, acc: 0.125] [gan loss: 1.722858, acc: 0.000000]\n",
            "5488: [discriminator loss: 0.6524972915649414, acc: 0.4140625] [gan loss: 0.883766, acc: 0.171875]\n",
            "5489: [discriminator loss: 0.7184996604919434, acc: 0.078125] [gan loss: 1.800661, acc: 0.000000]\n",
            "5490: [discriminator loss: 0.6826324462890625, acc: 0.453125] [gan loss: 0.727983, acc: 0.484375]\n",
            "5491: [discriminator loss: 0.7762613892555237, acc: 0.03125] [gan loss: 1.896692, acc: 0.000000]\n",
            "5492: [discriminator loss: 0.6990846395492554, acc: 0.4921875] [gan loss: 0.596442, acc: 0.718750]\n",
            "5493: [discriminator loss: 0.8324345946311951, acc: 0.0] [gan loss: 1.790243, acc: 0.000000]\n",
            "5494: [discriminator loss: 0.6731054186820984, acc: 0.46875] [gan loss: 0.727326, acc: 0.468750]\n",
            "5495: [discriminator loss: 0.7534452676773071, acc: 0.0625] [gan loss: 1.526717, acc: 0.000000]\n",
            "5496: [discriminator loss: 0.6614078283309937, acc: 0.3828125] [gan loss: 0.860469, acc: 0.234375]\n",
            "5497: [discriminator loss: 0.7260070443153381, acc: 0.09375] [gan loss: 1.448031, acc: 0.000000]\n",
            "5498: [discriminator loss: 0.6549563407897949, acc: 0.3828125] [gan loss: 0.985922, acc: 0.125000]\n",
            "5499: [discriminator loss: 0.6824056506156921, acc: 0.1796875] [gan loss: 1.278047, acc: 0.015625]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "5500: [discriminator loss: 0.6784108877182007, acc: 0.3046875] [gan loss: 1.052749, acc: 0.093750]\n",
            "5501: [discriminator loss: 0.6913833022117615, acc: 0.1640625] [gan loss: 1.525216, acc: 0.000000]\n",
            "5502: [discriminator loss: 0.6633216738700867, acc: 0.375] [gan loss: 0.869750, acc: 0.187500]\n",
            "5503: [discriminator loss: 0.7392345070838928, acc: 0.0625] [gan loss: 1.677665, acc: 0.000000]\n",
            "5504: [discriminator loss: 0.6433102488517761, acc: 0.46875] [gan loss: 0.701035, acc: 0.546875]\n",
            "5505: [discriminator loss: 0.8066738843917847, acc: 0.0078125] [gan loss: 2.089963, acc: 0.000000]\n",
            "5506: [discriminator loss: 0.6839435696601868, acc: 0.5] [gan loss: 0.550511, acc: 0.828125]\n",
            "5507: [discriminator loss: 0.8049265146255493, acc: 0.0234375] [gan loss: 1.747300, acc: 0.000000]\n",
            "5508: [discriminator loss: 0.7042381763458252, acc: 0.4765625] [gan loss: 0.796752, acc: 0.328125]\n",
            "5509: [discriminator loss: 0.7407562136650085, acc: 0.078125] [gan loss: 1.615873, acc: 0.000000]\n",
            "5510: [discriminator loss: 0.6448920369148254, acc: 0.4453125] [gan loss: 0.746929, acc: 0.406250]\n",
            "5511: [discriminator loss: 0.771770715713501, acc: 0.078125] [gan loss: 1.686344, acc: 0.000000]\n",
            "5512: [discriminator loss: 0.6618996858596802, acc: 0.4296875] [gan loss: 0.765397, acc: 0.328125]\n",
            "5513: [discriminator loss: 0.7717527747154236, acc: 0.03125] [gan loss: 1.573440, acc: 0.000000]\n",
            "5514: [discriminator loss: 0.6771531701087952, acc: 0.4375] [gan loss: 0.778648, acc: 0.328125]\n",
            "5515: [discriminator loss: 0.7420876026153564, acc: 0.0703125] [gan loss: 1.597421, acc: 0.000000]\n",
            "5516: [discriminator loss: 0.6569176912307739, acc: 0.4140625] [gan loss: 0.838976, acc: 0.250000]\n",
            "5517: [discriminator loss: 0.7719111442565918, acc: 0.0546875] [gan loss: 1.867875, acc: 0.000000]\n",
            "5518: [discriminator loss: 0.6450136303901672, acc: 0.46875] [gan loss: 0.731666, acc: 0.468750]\n",
            "5519: [discriminator loss: 0.760955810546875, acc: 0.03125] [gan loss: 1.739485, acc: 0.000000]\n",
            "5520: [discriminator loss: 0.6911401152610779, acc: 0.4375] [gan loss: 0.809600, acc: 0.312500]\n",
            "5521: [discriminator loss: 0.7267687916755676, acc: 0.046875] [gan loss: 1.788957, acc: 0.000000]\n",
            "5522: [discriminator loss: 0.6630153656005859, acc: 0.4453125] [gan loss: 0.759357, acc: 0.390625]\n",
            "5523: [discriminator loss: 0.7343636155128479, acc: 0.03125] [gan loss: 1.633379, acc: 0.000000]\n",
            "5524: [discriminator loss: 0.6593124270439148, acc: 0.421875] [gan loss: 0.754518, acc: 0.406250]\n",
            "5525: [discriminator loss: 0.7432623505592346, acc: 0.03125] [gan loss: 1.694405, acc: 0.000000]\n",
            "5526: [discriminator loss: 0.6499872207641602, acc: 0.46875] [gan loss: 0.743043, acc: 0.421875]\n",
            "5527: [discriminator loss: 0.740679144859314, acc: 0.0546875] [gan loss: 1.500162, acc: 0.000000]\n",
            "5528: [discriminator loss: 0.679496169090271, acc: 0.34375] [gan loss: 1.131575, acc: 0.062500]\n",
            "5529: [discriminator loss: 0.6722004413604736, acc: 0.203125] [gan loss: 1.324028, acc: 0.000000]\n",
            "5530: [discriminator loss: 0.6712273359298706, acc: 0.3125] [gan loss: 1.116479, acc: 0.062500]\n",
            "5531: [discriminator loss: 0.7620148658752441, acc: 0.1328125] [gan loss: 1.360271, acc: 0.000000]\n",
            "5532: [discriminator loss: 0.6550431251525879, acc: 0.3671875] [gan loss: 1.017170, acc: 0.125000]\n",
            "5533: [discriminator loss: 0.7026902437210083, acc: 0.1015625] [gan loss: 1.654504, acc: 0.000000]\n",
            "5534: [discriminator loss: 0.6411322355270386, acc: 0.4609375] [gan loss: 0.760050, acc: 0.390625]\n",
            "5535: [discriminator loss: 0.8046721816062927, acc: 0.0234375] [gan loss: 1.959693, acc: 0.000000]\n",
            "5536: [discriminator loss: 0.6985083818435669, acc: 0.5] [gan loss: 0.494558, acc: 0.843750]\n",
            "5537: [discriminator loss: 0.8336555361747742, acc: 0.0078125] [gan loss: 1.836534, acc: 0.000000]\n",
            "5538: [discriminator loss: 0.6975928544998169, acc: 0.46875] [gan loss: 0.669827, acc: 0.609375]\n",
            "5539: [discriminator loss: 0.7922751307487488, acc: 0.03125] [gan loss: 1.753998, acc: 0.000000]\n",
            "5540: [discriminator loss: 0.6750727891921997, acc: 0.421875] [gan loss: 0.795254, acc: 0.343750]\n",
            "5541: [discriminator loss: 0.7421520948410034, acc: 0.0859375] [gan loss: 1.582469, acc: 0.000000]\n",
            "5542: [discriminator loss: 0.6782195568084717, acc: 0.4375] [gan loss: 0.744307, acc: 0.375000]\n",
            "5543: [discriminator loss: 0.7375206351280212, acc: 0.0625] [gan loss: 1.568784, acc: 0.015625]\n",
            "5544: [discriminator loss: 0.6629818081855774, acc: 0.421875] [gan loss: 0.815116, acc: 0.250000]\n",
            "5545: [discriminator loss: 0.7381981015205383, acc: 0.0703125] [gan loss: 1.635183, acc: 0.000000]\n",
            "5546: [discriminator loss: 0.6531121134757996, acc: 0.4296875] [gan loss: 0.826330, acc: 0.218750]\n",
            "5547: [discriminator loss: 0.7309839725494385, acc: 0.046875] [gan loss: 1.595766, acc: 0.015625]\n",
            "5548: [discriminator loss: 0.684633195400238, acc: 0.421875] [gan loss: 0.820955, acc: 0.250000]\n",
            "5549: [discriminator loss: 0.7279743552207947, acc: 0.0703125] [gan loss: 1.695408, acc: 0.015625]\n",
            "5550: [discriminator loss: 0.6590403914451599, acc: 0.4609375] [gan loss: 0.701289, acc: 0.484375]\n",
            "5551: [discriminator loss: 0.7722500562667847, acc: 0.046875] [gan loss: 1.661048, acc: 0.000000]\n",
            "5552: [discriminator loss: 0.6207841038703918, acc: 0.4375] [gan loss: 0.915922, acc: 0.234375]\n",
            "5553: [discriminator loss: 0.6920702457427979, acc: 0.140625] [gan loss: 1.535410, acc: 0.000000]\n",
            "5554: [discriminator loss: 0.6471834778785706, acc: 0.390625] [gan loss: 1.025701, acc: 0.093750]\n",
            "5555: [discriminator loss: 0.7050099968910217, acc: 0.15625] [gan loss: 1.435341, acc: 0.000000]\n",
            "5556: [discriminator loss: 0.6602791547775269, acc: 0.359375] [gan loss: 0.996707, acc: 0.140625]\n",
            "5557: [discriminator loss: 0.7372397780418396, acc: 0.15625] [gan loss: 1.489035, acc: 0.000000]\n",
            "5558: [discriminator loss: 0.6855738759040833, acc: 0.2890625] [gan loss: 1.125406, acc: 0.031250]\n",
            "5559: [discriminator loss: 0.7168681621551514, acc: 0.1875] [gan loss: 1.513942, acc: 0.000000]\n",
            "5560: [discriminator loss: 0.6824450492858887, acc: 0.328125] [gan loss: 1.151617, acc: 0.046875]\n",
            "5561: [discriminator loss: 0.6539427042007446, acc: 0.25] [gan loss: 1.404791, acc: 0.015625]\n",
            "5562: [discriminator loss: 0.7127557992935181, acc: 0.265625] [gan loss: 1.169974, acc: 0.031250]\n",
            "5563: [discriminator loss: 0.6933669447898865, acc: 0.234375] [gan loss: 1.410869, acc: 0.000000]\n",
            "5564: [discriminator loss: 0.6600699424743652, acc: 0.375] [gan loss: 0.953593, acc: 0.109375]\n",
            "5565: [discriminator loss: 0.7095421552658081, acc: 0.125] [gan loss: 1.765419, acc: 0.000000]\n",
            "5566: [discriminator loss: 0.6841005682945251, acc: 0.46875] [gan loss: 0.515749, acc: 0.890625]\n",
            "5567: [discriminator loss: 0.8520427942276001, acc: 0.0234375] [gan loss: 2.385555, acc: 0.000000]\n",
            "5568: [discriminator loss: 0.7472766041755676, acc: 0.5] [gan loss: 0.395363, acc: 0.968750]\n",
            "5569: [discriminator loss: 0.9158488512039185, acc: 0.0] [gan loss: 1.870195, acc: 0.000000]\n",
            "5570: [discriminator loss: 0.7009834051132202, acc: 0.46875] [gan loss: 0.659158, acc: 0.546875]\n",
            "5571: [discriminator loss: 0.7906845808029175, acc: 0.0390625] [gan loss: 1.499468, acc: 0.000000]\n",
            "5572: [discriminator loss: 0.6754655838012695, acc: 0.390625] [gan loss: 0.895270, acc: 0.250000]\n",
            "5573: [discriminator loss: 0.690828800201416, acc: 0.140625] [gan loss: 1.414236, acc: 0.000000]\n",
            "5574: [discriminator loss: 0.6570354700088501, acc: 0.3828125] [gan loss: 0.870397, acc: 0.218750]\n",
            "5575: [discriminator loss: 0.7141781449317932, acc: 0.109375] [gan loss: 1.578985, acc: 0.000000]\n",
            "5576: [discriminator loss: 0.6451215744018555, acc: 0.4296875] [gan loss: 0.903321, acc: 0.218750]\n",
            "5577: [discriminator loss: 0.7142711281776428, acc: 0.0859375] [gan loss: 1.553783, acc: 0.000000]\n",
            "5578: [discriminator loss: 0.6495909094810486, acc: 0.4140625] [gan loss: 0.867024, acc: 0.218750]\n",
            "5579: [discriminator loss: 0.7177848219871521, acc: 0.0703125] [gan loss: 1.638934, acc: 0.000000]\n",
            "5580: [discriminator loss: 0.6683489084243774, acc: 0.390625] [gan loss: 0.887186, acc: 0.171875]\n",
            "5581: [discriminator loss: 0.7255755662918091, acc: 0.0859375] [gan loss: 1.799549, acc: 0.000000]\n",
            "5582: [discriminator loss: 0.6569485664367676, acc: 0.4453125] [gan loss: 0.773304, acc: 0.421875]\n",
            "5583: [discriminator loss: 0.7379908561706543, acc: 0.046875] [gan loss: 1.707158, acc: 0.000000]\n",
            "5584: [discriminator loss: 0.6519819498062134, acc: 0.4296875] [gan loss: 0.805556, acc: 0.312500]\n",
            "5585: [discriminator loss: 0.7512240409851074, acc: 0.078125] [gan loss: 1.814107, acc: 0.000000]\n",
            "5586: [discriminator loss: 0.6764819622039795, acc: 0.484375] [gan loss: 0.670043, acc: 0.578125]\n",
            "5587: [discriminator loss: 0.817707896232605, acc: 0.03125] [gan loss: 1.865715, acc: 0.000000]\n",
            "5588: [discriminator loss: 0.6590076684951782, acc: 0.4375] [gan loss: 0.773546, acc: 0.312500]\n",
            "5589: [discriminator loss: 0.7805757522583008, acc: 0.046875] [gan loss: 1.808026, acc: 0.000000]\n",
            "5590: [discriminator loss: 0.6639460921287537, acc: 0.453125] [gan loss: 0.788185, acc: 0.359375]\n",
            "5591: [discriminator loss: 0.7592235803604126, acc: 0.0703125] [gan loss: 1.714055, acc: 0.000000]\n",
            "5592: [discriminator loss: 0.6656765341758728, acc: 0.421875] [gan loss: 0.752158, acc: 0.375000]\n",
            "5593: [discriminator loss: 0.7310706377029419, acc: 0.09375] [gan loss: 1.453972, acc: 0.046875]\n",
            "5594: [discriminator loss: 0.6685630083084106, acc: 0.421875] [gan loss: 0.827476, acc: 0.312500]\n",
            "5595: [discriminator loss: 0.7677342891693115, acc: 0.046875] [gan loss: 1.601644, acc: 0.000000]\n",
            "5596: [discriminator loss: 0.6871808767318726, acc: 0.4609375] [gan loss: 0.692594, acc: 0.578125]\n",
            "5597: [discriminator loss: 0.7645282745361328, acc: 0.0546875] [gan loss: 1.745353, acc: 0.000000]\n",
            "5598: [discriminator loss: 0.6563239097595215, acc: 0.453125] [gan loss: 0.788169, acc: 0.406250]\n",
            "5599: [discriminator loss: 0.7813549041748047, acc: 0.0390625] [gan loss: 1.711594, acc: 0.000000]\n",
            "5600: [discriminator loss: 0.6554716229438782, acc: 0.4609375] [gan loss: 0.803569, acc: 0.343750]\n",
            "5601: [discriminator loss: 0.8110894560813904, acc: 0.046875] [gan loss: 1.918905, acc: 0.000000]\n",
            "5602: [discriminator loss: 0.6942843794822693, acc: 0.4921875] [gan loss: 0.607768, acc: 0.718750]\n",
            "5603: [discriminator loss: 0.7760903835296631, acc: 0.0390625] [gan loss: 1.560560, acc: 0.000000]\n",
            "5604: [discriminator loss: 0.677996814250946, acc: 0.4453125] [gan loss: 0.805221, acc: 0.328125]\n",
            "5605: [discriminator loss: 0.7590093016624451, acc: 0.046875] [gan loss: 1.660874, acc: 0.000000]\n",
            "5606: [discriminator loss: 0.6902987957000732, acc: 0.4296875] [gan loss: 0.849659, acc: 0.265625]\n",
            "5607: [discriminator loss: 0.7443972826004028, acc: 0.0625] [gan loss: 1.511758, acc: 0.000000]\n",
            "5608: [discriminator loss: 0.6875578165054321, acc: 0.390625] [gan loss: 0.829634, acc: 0.250000]\n",
            "5609: [discriminator loss: 0.7600530385971069, acc: 0.0390625] [gan loss: 1.596378, acc: 0.000000]\n",
            "5610: [discriminator loss: 0.6723227500915527, acc: 0.3828125] [gan loss: 0.915380, acc: 0.140625]\n",
            "5611: [discriminator loss: 0.7195199728012085, acc: 0.1171875] [gan loss: 1.432853, acc: 0.000000]\n",
            "5612: [discriminator loss: 0.6716465950012207, acc: 0.34375] [gan loss: 1.011292, acc: 0.109375]\n",
            "5613: [discriminator loss: 0.6847712397575378, acc: 0.140625] [gan loss: 1.418341, acc: 0.031250]\n",
            "5614: [discriminator loss: 0.7087339758872986, acc: 0.296875] [gan loss: 1.042351, acc: 0.046875]\n",
            "5615: [discriminator loss: 0.7024247646331787, acc: 0.1875] [gan loss: 1.233981, acc: 0.000000]\n",
            "5616: [discriminator loss: 0.7350541353225708, acc: 0.171875] [gan loss: 1.345652, acc: 0.000000]\n",
            "5617: [discriminator loss: 0.7040120959281921, acc: 0.265625] [gan loss: 1.198667, acc: 0.062500]\n",
            "5618: [discriminator loss: 0.7019315361976624, acc: 0.203125] [gan loss: 1.539488, acc: 0.000000]\n",
            "5619: [discriminator loss: 0.6608282327651978, acc: 0.3984375] [gan loss: 0.745368, acc: 0.406250]\n",
            "5620: [discriminator loss: 0.7359397411346436, acc: 0.078125] [gan loss: 1.843210, acc: 0.000000]\n",
            "5621: [discriminator loss: 0.7080338001251221, acc: 0.4765625] [gan loss: 0.521446, acc: 0.859375]\n",
            "5622: [discriminator loss: 0.8236445784568787, acc: 0.0078125] [gan loss: 1.974625, acc: 0.000000]\n",
            "5623: [discriminator loss: 0.670879602432251, acc: 0.5] [gan loss: 0.607757, acc: 0.687500]\n",
            "5624: [discriminator loss: 0.8605531454086304, acc: 0.015625] [gan loss: 1.785839, acc: 0.000000]\n",
            "5625: [discriminator loss: 0.6605846285820007, acc: 0.453125] [gan loss: 0.762399, acc: 0.453125]\n",
            "5626: [discriminator loss: 0.7652276754379272, acc: 0.046875] [gan loss: 1.377313, acc: 0.000000]\n",
            "5627: [discriminator loss: 0.6550962924957275, acc: 0.421875] [gan loss: 0.860323, acc: 0.265625]\n",
            "5628: [discriminator loss: 0.7359758615493774, acc: 0.0859375] [gan loss: 1.462172, acc: 0.000000]\n",
            "5629: [discriminator loss: 0.6627953052520752, acc: 0.421875] [gan loss: 0.902661, acc: 0.203125]\n",
            "5630: [discriminator loss: 0.7258782386779785, acc: 0.0703125] [gan loss: 1.594915, acc: 0.000000]\n",
            "5631: [discriminator loss: 0.7098097801208496, acc: 0.390625] [gan loss: 0.898835, acc: 0.218750]\n",
            "5632: [discriminator loss: 0.7330056428909302, acc: 0.0703125] [gan loss: 1.567255, acc: 0.000000]\n",
            "5633: [discriminator loss: 0.681892991065979, acc: 0.4296875] [gan loss: 0.889814, acc: 0.234375]\n",
            "5634: [discriminator loss: 0.7335124015808105, acc: 0.1015625] [gan loss: 1.575200, acc: 0.015625]\n",
            "5635: [discriminator loss: 0.66529381275177, acc: 0.453125] [gan loss: 0.787254, acc: 0.390625]\n",
            "5636: [discriminator loss: 0.7553117871284485, acc: 0.0625] [gan loss: 1.730171, acc: 0.000000]\n",
            "5637: [discriminator loss: 0.6885539293289185, acc: 0.40625] [gan loss: 0.858138, acc: 0.187500]\n",
            "5638: [discriminator loss: 0.7197555303573608, acc: 0.1015625] [gan loss: 1.819358, acc: 0.000000]\n",
            "5639: [discriminator loss: 0.6633908748626709, acc: 0.46875] [gan loss: 0.689093, acc: 0.531250]\n",
            "5640: [discriminator loss: 0.7596546411514282, acc: 0.03125] [gan loss: 1.810512, acc: 0.000000]\n",
            "5641: [discriminator loss: 0.6940714120864868, acc: 0.4375] [gan loss: 0.691249, acc: 0.546875]\n",
            "5642: [discriminator loss: 0.777116060256958, acc: 0.03125] [gan loss: 1.766054, acc: 0.000000]\n",
            "5643: [discriminator loss: 0.6846051812171936, acc: 0.453125] [gan loss: 0.798426, acc: 0.296875]\n",
            "5644: [discriminator loss: 0.7409071326255798, acc: 0.046875] [gan loss: 1.604665, acc: 0.000000]\n",
            "5645: [discriminator loss: 0.6668931245803833, acc: 0.4609375] [gan loss: 0.775108, acc: 0.359375]\n",
            "5646: [discriminator loss: 0.7737434506416321, acc: 0.0] [gan loss: 1.905475, acc: 0.000000]\n",
            "5647: [discriminator loss: 0.7084867358207703, acc: 0.4375] [gan loss: 0.787813, acc: 0.343750]\n",
            "5648: [discriminator loss: 0.7679226398468018, acc: 0.0390625] [gan loss: 1.627337, acc: 0.000000]\n",
            "5649: [discriminator loss: 0.6634718179702759, acc: 0.4296875] [gan loss: 0.848276, acc: 0.328125]\n",
            "5650: [discriminator loss: 0.7334755659103394, acc: 0.0625] [gan loss: 1.670012, acc: 0.000000]\n",
            "5651: [discriminator loss: 0.6519685983657837, acc: 0.4453125] [gan loss: 0.729290, acc: 0.437500]\n",
            "5652: [discriminator loss: 0.7425796985626221, acc: 0.046875] [gan loss: 1.587691, acc: 0.000000]\n",
            "5653: [discriminator loss: 0.6600490808486938, acc: 0.4453125] [gan loss: 0.711882, acc: 0.562500]\n",
            "5654: [discriminator loss: 0.7816164493560791, acc: 0.015625] [gan loss: 1.719884, acc: 0.000000]\n",
            "5655: [discriminator loss: 0.6683844923973083, acc: 0.4765625] [gan loss: 0.759055, acc: 0.359375]\n",
            "5656: [discriminator loss: 0.7402092218399048, acc: 0.078125] [gan loss: 1.760168, acc: 0.000000]\n",
            "5657: [discriminator loss: 0.6711066961288452, acc: 0.4296875] [gan loss: 0.722578, acc: 0.484375]\n",
            "5658: [discriminator loss: 0.7885035872459412, acc: 0.0234375] [gan loss: 1.750158, acc: 0.000000]\n",
            "5659: [discriminator loss: 0.6942312121391296, acc: 0.4453125] [gan loss: 0.754584, acc: 0.453125]\n",
            "5660: [discriminator loss: 0.7725402116775513, acc: 0.03125] [gan loss: 1.651520, acc: 0.000000]\n",
            "5661: [discriminator loss: 0.6740593314170837, acc: 0.4453125] [gan loss: 0.723578, acc: 0.468750]\n",
            "5662: [discriminator loss: 0.810894787311554, acc: 0.03125] [gan loss: 1.610818, acc: 0.000000]\n",
            "5663: [discriminator loss: 0.652209460735321, acc: 0.4609375] [gan loss: 0.834422, acc: 0.250000]\n",
            "5664: [discriminator loss: 0.7806228995323181, acc: 0.03125] [gan loss: 1.627792, acc: 0.015625]\n",
            "5665: [discriminator loss: 0.6518272757530212, acc: 0.46875] [gan loss: 0.720538, acc: 0.578125]\n",
            "5666: [discriminator loss: 0.7501583099365234, acc: 0.0546875] [gan loss: 1.641229, acc: 0.000000]\n",
            "5667: [discriminator loss: 0.6641710996627808, acc: 0.4375] [gan loss: 0.768112, acc: 0.390625]\n",
            "5668: [discriminator loss: 0.744887113571167, acc: 0.0859375] [gan loss: 1.597244, acc: 0.000000]\n",
            "5669: [discriminator loss: 0.6600726842880249, acc: 0.4296875] [gan loss: 0.820305, acc: 0.265625]\n",
            "5670: [discriminator loss: 0.7325694561004639, acc: 0.0390625] [gan loss: 1.658532, acc: 0.000000]\n",
            "5671: [discriminator loss: 0.6607646346092224, acc: 0.40625] [gan loss: 0.811132, acc: 0.281250]\n",
            "5672: [discriminator loss: 0.73284512758255, acc: 0.0390625] [gan loss: 1.625785, acc: 0.000000]\n",
            "5673: [discriminator loss: 0.6711387634277344, acc: 0.4375] [gan loss: 0.801776, acc: 0.250000]\n",
            "5674: [discriminator loss: 0.7227683067321777, acc: 0.03125] [gan loss: 1.577697, acc: 0.000000]\n",
            "5675: [discriminator loss: 0.6705673933029175, acc: 0.375] [gan loss: 1.019455, acc: 0.093750]\n",
            "5676: [discriminator loss: 0.7077374458312988, acc: 0.109375] [gan loss: 1.463245, acc: 0.015625]\n",
            "5677: [discriminator loss: 0.6665692925453186, acc: 0.4296875] [gan loss: 0.881022, acc: 0.218750]\n",
            "5678: [discriminator loss: 0.7219634652137756, acc: 0.078125] [gan loss: 1.600461, acc: 0.015625]\n",
            "5679: [discriminator loss: 0.6473881006240845, acc: 0.453125] [gan loss: 0.715968, acc: 0.453125]\n",
            "5680: [discriminator loss: 0.7787599563598633, acc: 0.046875] [gan loss: 1.741818, acc: 0.000000]\n",
            "5681: [discriminator loss: 0.6724696159362793, acc: 0.4609375] [gan loss: 0.653609, acc: 0.625000]\n",
            "5682: [discriminator loss: 0.8058618307113647, acc: 0.0234375] [gan loss: 1.855996, acc: 0.000000]\n",
            "5683: [discriminator loss: 0.683819055557251, acc: 0.4765625] [gan loss: 0.601904, acc: 0.703125]\n",
            "5684: [discriminator loss: 0.8043026924133301, acc: 0.03125] [gan loss: 1.792592, acc: 0.000000]\n",
            "5685: [discriminator loss: 0.7255454659461975, acc: 0.3828125] [gan loss: 0.768331, acc: 0.375000]\n",
            "5686: [discriminator loss: 0.7635505199432373, acc: 0.0390625] [gan loss: 1.574300, acc: 0.000000]\n",
            "5687: [discriminator loss: 0.6925139427185059, acc: 0.4140625] [gan loss: 0.844030, acc: 0.203125]\n",
            "5688: [discriminator loss: 0.7392299175262451, acc: 0.0625] [gan loss: 1.529048, acc: 0.000000]\n",
            "5689: [discriminator loss: 0.6842943429946899, acc: 0.3671875] [gan loss: 0.957432, acc: 0.140625]\n",
            "5690: [discriminator loss: 0.683203935623169, acc: 0.1328125] [gan loss: 1.530430, acc: 0.015625]\n",
            "5691: [discriminator loss: 0.6900741457939148, acc: 0.375] [gan loss: 0.863432, acc: 0.203125]\n",
            "5692: [discriminator loss: 0.7143294215202332, acc: 0.09375] [gan loss: 1.600632, acc: 0.000000]\n",
            "5693: [discriminator loss: 0.6636511087417603, acc: 0.4140625] [gan loss: 0.882234, acc: 0.171875]\n",
            "5694: [discriminator loss: 0.730036735534668, acc: 0.0625] [gan loss: 1.756688, acc: 0.000000]\n",
            "5695: [discriminator loss: 0.6604817509651184, acc: 0.46875] [gan loss: 0.679640, acc: 0.500000]\n",
            "5696: [discriminator loss: 0.8026571273803711, acc: 0.0] [gan loss: 1.911695, acc: 0.000000]\n",
            "5697: [discriminator loss: 0.6616449952125549, acc: 0.484375] [gan loss: 0.641676, acc: 0.625000]\n",
            "5698: [discriminator loss: 0.799540638923645, acc: 0.0234375] [gan loss: 1.759192, acc: 0.000000]\n",
            "5699: [discriminator loss: 0.6830987930297852, acc: 0.4453125] [gan loss: 0.822924, acc: 0.265625]\n",
            "5700: [discriminator loss: 0.7359805107116699, acc: 0.0390625] [gan loss: 1.707786, acc: 0.000000]\n",
            "5701: [discriminator loss: 0.6680602431297302, acc: 0.3828125] [gan loss: 0.908178, acc: 0.265625]\n",
            "5702: [discriminator loss: 0.7831414937973022, acc: 0.046875] [gan loss: 1.593651, acc: 0.000000]\n",
            "5703: [discriminator loss: 0.657474160194397, acc: 0.4296875] [gan loss: 0.903178, acc: 0.234375]\n",
            "5704: [discriminator loss: 0.7440929412841797, acc: 0.0703125] [gan loss: 1.523305, acc: 0.000000]\n",
            "5705: [discriminator loss: 0.6806275844573975, acc: 0.359375] [gan loss: 0.868445, acc: 0.265625]\n",
            "5706: [discriminator loss: 0.7090256810188293, acc: 0.1171875] [gan loss: 1.494773, acc: 0.015625]\n",
            "5707: [discriminator loss: 0.6843022108078003, acc: 0.3671875] [gan loss: 0.959148, acc: 0.109375]\n",
            "5708: [discriminator loss: 0.678254246711731, acc: 0.1328125] [gan loss: 1.420480, acc: 0.000000]\n",
            "5709: [discriminator loss: 0.6686573028564453, acc: 0.3515625] [gan loss: 0.914881, acc: 0.203125]\n",
            "5710: [discriminator loss: 0.6941624283790588, acc: 0.109375] [gan loss: 1.453707, acc: 0.015625]\n",
            "5711: [discriminator loss: 0.6823663115501404, acc: 0.3359375] [gan loss: 0.950794, acc: 0.140625]\n",
            "5712: [discriminator loss: 0.697108268737793, acc: 0.1015625] [gan loss: 1.719826, acc: 0.000000]\n",
            "5713: [discriminator loss: 0.6637687087059021, acc: 0.4609375] [gan loss: 0.643239, acc: 0.656250]\n",
            "5714: [discriminator loss: 0.7908772826194763, acc: 0.0546875] [gan loss: 1.974768, acc: 0.000000]\n",
            "5715: [discriminator loss: 0.6778298616409302, acc: 0.5] [gan loss: 0.521172, acc: 0.812500]\n",
            "5716: [discriminator loss: 0.8540293574333191, acc: 0.0] [gan loss: 1.936437, acc: 0.000000]\n",
            "5717: [discriminator loss: 0.676990270614624, acc: 0.484375] [gan loss: 0.788110, acc: 0.328125]\n",
            "5718: [discriminator loss: 0.7835691571235657, acc: 0.03125] [gan loss: 1.709078, acc: 0.000000]\n",
            "5719: [discriminator loss: 0.6431008577346802, acc: 0.46875] [gan loss: 0.797182, acc: 0.328125]\n",
            "5720: [discriminator loss: 0.765842080116272, acc: 0.03125] [gan loss: 1.606477, acc: 0.015625]\n",
            "5721: [discriminator loss: 0.6834829449653625, acc: 0.4140625] [gan loss: 0.713253, acc: 0.437500]\n",
            "5722: [discriminator loss: 0.7523525953292847, acc: 0.03125] [gan loss: 1.547181, acc: 0.000000]\n",
            "5723: [discriminator loss: 0.6907856464385986, acc: 0.3828125] [gan loss: 0.850506, acc: 0.218750]\n",
            "5724: [discriminator loss: 0.7082862854003906, acc: 0.1015625] [gan loss: 1.604240, acc: 0.000000]\n",
            "5725: [discriminator loss: 0.65789395570755, acc: 0.3984375] [gan loss: 0.935847, acc: 0.156250]\n",
            "5726: [discriminator loss: 0.7142050266265869, acc: 0.078125] [gan loss: 1.446788, acc: 0.000000]\n",
            "5727: [discriminator loss: 0.682613730430603, acc: 0.3359375] [gan loss: 0.903435, acc: 0.218750]\n",
            "5728: [discriminator loss: 0.7313889265060425, acc: 0.09375] [gan loss: 1.571361, acc: 0.000000]\n",
            "5729: [discriminator loss: 0.6268743276596069, acc: 0.4609375] [gan loss: 0.795756, acc: 0.390625]\n",
            "5730: [discriminator loss: 0.7610770463943481, acc: 0.0625] [gan loss: 1.807373, acc: 0.000000]\n",
            "5731: [discriminator loss: 0.6391121745109558, acc: 0.46875] [gan loss: 0.678180, acc: 0.515625]\n",
            "5732: [discriminator loss: 0.7818523645401001, acc: 0.0625] [gan loss: 1.736092, acc: 0.031250]\n",
            "5733: [discriminator loss: 0.7005236148834229, acc: 0.4375] [gan loss: 0.763492, acc: 0.406250]\n",
            "5734: [discriminator loss: 0.7296460866928101, acc: 0.03125] [gan loss: 1.694661, acc: 0.000000]\n",
            "5735: [discriminator loss: 0.6870348453521729, acc: 0.4609375] [gan loss: 0.735274, acc: 0.390625]\n",
            "5736: [discriminator loss: 0.7532107830047607, acc: 0.0625] [gan loss: 1.751726, acc: 0.000000]\n",
            "5737: [discriminator loss: 0.6725872159004211, acc: 0.421875] [gan loss: 0.796456, acc: 0.296875]\n",
            "5738: [discriminator loss: 0.7339369058609009, acc: 0.0625] [gan loss: 1.550637, acc: 0.000000]\n",
            "5739: [discriminator loss: 0.7099482417106628, acc: 0.3828125] [gan loss: 0.906821, acc: 0.109375]\n",
            "5740: [discriminator loss: 0.7323611974716187, acc: 0.078125] [gan loss: 1.608899, acc: 0.000000]\n",
            "5741: [discriminator loss: 0.6692309379577637, acc: 0.390625] [gan loss: 0.902306, acc: 0.171875]\n",
            "5742: [discriminator loss: 0.7153030633926392, acc: 0.09375] [gan loss: 1.603365, acc: 0.015625]\n",
            "5743: [discriminator loss: 0.6624172925949097, acc: 0.4296875] [gan loss: 0.833311, acc: 0.296875]\n",
            "5744: [discriminator loss: 0.7250196933746338, acc: 0.0859375] [gan loss: 1.629516, acc: 0.000000]\n",
            "5745: [discriminator loss: 0.672356128692627, acc: 0.453125] [gan loss: 0.755571, acc: 0.390625]\n",
            "5746: [discriminator loss: 0.7744978666305542, acc: 0.0390625] [gan loss: 1.929872, acc: 0.000000]\n",
            "5747: [discriminator loss: 0.6601316928863525, acc: 0.46875] [gan loss: 0.621086, acc: 0.718750]\n",
            "5748: [discriminator loss: 0.810973048210144, acc: 0.0234375] [gan loss: 1.951083, acc: 0.000000]\n",
            "5749: [discriminator loss: 0.681095540523529, acc: 0.484375] [gan loss: 0.666463, acc: 0.546875]\n",
            "5750: [discriminator loss: 0.8230313062667847, acc: 0.015625] [gan loss: 1.812757, acc: 0.000000]\n",
            "5751: [discriminator loss: 0.6424301862716675, acc: 0.4765625] [gan loss: 0.739935, acc: 0.406250]\n",
            "5752: [discriminator loss: 0.7629190683364868, acc: 0.046875] [gan loss: 1.475864, acc: 0.031250]\n",
            "5753: [discriminator loss: 0.6662562489509583, acc: 0.4140625] [gan loss: 0.865548, acc: 0.171875]\n",
            "5754: [discriminator loss: 0.7988698482513428, acc: 0.015625] [gan loss: 1.619558, acc: 0.031250]\n",
            "5755: [discriminator loss: 0.6630789637565613, acc: 0.4140625] [gan loss: 0.803045, acc: 0.328125]\n",
            "5756: [discriminator loss: 0.7450684309005737, acc: 0.03125] [gan loss: 1.588319, acc: 0.015625]\n",
            "5757: [discriminator loss: 0.6759719848632812, acc: 0.4453125] [gan loss: 0.760587, acc: 0.421875]\n",
            "5758: [discriminator loss: 0.7502961158752441, acc: 0.03125] [gan loss: 1.633576, acc: 0.000000]\n",
            "5759: [discriminator loss: 0.6861453056335449, acc: 0.4453125] [gan loss: 0.829147, acc: 0.312500]\n",
            "5760: [discriminator loss: 0.7527487277984619, acc: 0.0234375] [gan loss: 1.674756, acc: 0.000000]\n",
            "5761: [discriminator loss: 0.6886426210403442, acc: 0.390625] [gan loss: 0.876511, acc: 0.265625]\n",
            "5762: [discriminator loss: 0.7296181917190552, acc: 0.0703125] [gan loss: 1.522058, acc: 0.000000]\n",
            "5763: [discriminator loss: 0.666824221611023, acc: 0.421875] [gan loss: 0.922699, acc: 0.125000]\n",
            "5764: [discriminator loss: 0.6962025165557861, acc: 0.1484375] [gan loss: 1.499174, acc: 0.000000]\n",
            "5765: [discriminator loss: 0.6945469379425049, acc: 0.3046875] [gan loss: 1.007685, acc: 0.203125]\n",
            "5766: [discriminator loss: 0.6746410131454468, acc: 0.1796875] [gan loss: 1.374691, acc: 0.046875]\n",
            "5767: [discriminator loss: 0.6957052946090698, acc: 0.2890625] [gan loss: 1.083586, acc: 0.078125]\n",
            "5768: [discriminator loss: 0.7025989294052124, acc: 0.21875] [gan loss: 1.422950, acc: 0.015625]\n",
            "5769: [discriminator loss: 0.6821810007095337, acc: 0.28125] [gan loss: 1.205003, acc: 0.031250]\n",
            "5770: [discriminator loss: 0.658498227596283, acc: 0.3203125] [gan loss: 1.220571, acc: 0.000000]\n",
            "5771: [discriminator loss: 0.6602166891098022, acc: 0.2734375] [gan loss: 1.409009, acc: 0.000000]\n",
            "5772: [discriminator loss: 0.6595545411109924, acc: 0.3671875] [gan loss: 0.981218, acc: 0.078125]\n",
            "5773: [discriminator loss: 0.7112811207771301, acc: 0.09375] [gan loss: 1.666168, acc: 0.015625]\n",
            "5774: [discriminator loss: 0.6591584086418152, acc: 0.453125] [gan loss: 0.579127, acc: 0.718750]\n",
            "5775: [discriminator loss: 0.8381472826004028, acc: 0.0234375] [gan loss: 2.355203, acc: 0.000000]\n",
            "5776: [discriminator loss: 0.7279048562049866, acc: 0.5] [gan loss: 0.390316, acc: 0.968750]\n",
            "5777: [discriminator loss: 0.9550424814224243, acc: 0.0] [gan loss: 1.944865, acc: 0.000000]\n",
            "5778: [discriminator loss: 0.6895442008972168, acc: 0.4921875] [gan loss: 0.677544, acc: 0.578125]\n",
            "5779: [discriminator loss: 0.7577178478240967, acc: 0.0625] [gan loss: 1.693457, acc: 0.000000]\n",
            "5780: [discriminator loss: 0.6273334622383118, acc: 0.40625] [gan loss: 0.870755, acc: 0.234375]\n",
            "5781: [discriminator loss: 0.7362667322158813, acc: 0.0859375] [gan loss: 1.486702, acc: 0.015625]\n",
            "5782: [discriminator loss: 0.6970136165618896, acc: 0.328125] [gan loss: 1.036938, acc: 0.062500]\n",
            "5783: [discriminator loss: 0.7281349897384644, acc: 0.1328125] [gan loss: 1.369951, acc: 0.000000]\n",
            "5784: [discriminator loss: 0.7130217552185059, acc: 0.328125] [gan loss: 0.972309, acc: 0.125000]\n",
            "5785: [discriminator loss: 0.7301865816116333, acc: 0.078125] [gan loss: 1.551967, acc: 0.000000]\n",
            "5786: [discriminator loss: 0.65705806016922, acc: 0.4453125] [gan loss: 0.919463, acc: 0.171875]\n",
            "5787: [discriminator loss: 0.7401847839355469, acc: 0.1015625] [gan loss: 1.729255, acc: 0.000000]\n",
            "5788: [discriminator loss: 0.6674163341522217, acc: 0.453125] [gan loss: 0.701811, acc: 0.500000]\n",
            "5789: [discriminator loss: 0.7491315603256226, acc: 0.0078125] [gan loss: 1.653114, acc: 0.000000]\n",
            "5790: [discriminator loss: 0.6940538883209229, acc: 0.453125] [gan loss: 0.682380, acc: 0.562500]\n",
            "5791: [discriminator loss: 0.7806196212768555, acc: 0.015625] [gan loss: 1.844455, acc: 0.000000]\n",
            "5792: [discriminator loss: 0.6697330474853516, acc: 0.4765625] [gan loss: 0.655190, acc: 0.625000]\n",
            "5793: [discriminator loss: 0.7398044466972351, acc: 0.0859375] [gan loss: 1.744634, acc: 0.000000]\n",
            "5794: [discriminator loss: 0.6766791343688965, acc: 0.4296875] [gan loss: 0.747850, acc: 0.421875]\n",
            "5795: [discriminator loss: 0.78225177526474, acc: 0.03125] [gan loss: 1.776632, acc: 0.000000]\n",
            "5796: [discriminator loss: 0.6831768155097961, acc: 0.453125] [gan loss: 0.764049, acc: 0.390625]\n",
            "5797: [discriminator loss: 0.7517221570014954, acc: 0.0625] [gan loss: 1.615664, acc: 0.000000]\n",
            "5798: [discriminator loss: 0.6710188388824463, acc: 0.4609375] [gan loss: 0.685769, acc: 0.484375]\n",
            "5799: [discriminator loss: 0.762360155582428, acc: 0.0625] [gan loss: 1.724601, acc: 0.000000]\n",
            "5800: [discriminator loss: 0.6439815759658813, acc: 0.484375] [gan loss: 0.683561, acc: 0.515625]\n",
            "5801: [discriminator loss: 0.7914540767669678, acc: 0.0390625] [gan loss: 1.688136, acc: 0.000000]\n",
            "5802: [discriminator loss: 0.6753772497177124, acc: 0.4375] [gan loss: 0.687784, acc: 0.468750]\n",
            "5803: [discriminator loss: 0.8113210797309875, acc: 0.03125] [gan loss: 1.568951, acc: 0.000000]\n",
            "5804: [discriminator loss: 0.6438674330711365, acc: 0.46875] [gan loss: 0.812804, acc: 0.312500]\n",
            "5805: [discriminator loss: 0.7592476010322571, acc: 0.0703125] [gan loss: 1.608266, acc: 0.000000]\n",
            "5806: [discriminator loss: 0.6690541505813599, acc: 0.4140625] [gan loss: 0.814562, acc: 0.343750]\n",
            "5807: [discriminator loss: 0.7317799925804138, acc: 0.109375] [gan loss: 1.504652, acc: 0.015625]\n",
            "5808: [discriminator loss: 0.7277456521987915, acc: 0.328125] [gan loss: 0.970459, acc: 0.046875]\n",
            "5809: [discriminator loss: 0.7355126142501831, acc: 0.1171875] [gan loss: 1.544730, acc: 0.000000]\n",
            "5810: [discriminator loss: 0.685350775718689, acc: 0.390625] [gan loss: 0.720381, acc: 0.468750]\n",
            "5811: [discriminator loss: 0.8011888861656189, acc: 0.0234375] [gan loss: 1.843397, acc: 0.000000]\n",
            "5812: [discriminator loss: 0.6740880608558655, acc: 0.4453125] [gan loss: 0.766278, acc: 0.421875]\n",
            "5813: [discriminator loss: 0.7784181833267212, acc: 0.0234375] [gan loss: 1.794187, acc: 0.000000]\n",
            "5814: [discriminator loss: 0.671301007270813, acc: 0.46875] [gan loss: 0.647157, acc: 0.625000]\n",
            "5815: [discriminator loss: 0.7783989310264587, acc: 0.0703125] [gan loss: 1.516629, acc: 0.000000]\n",
            "5816: [discriminator loss: 0.6656235456466675, acc: 0.4296875] [gan loss: 0.825524, acc: 0.265625]\n",
            "5817: [discriminator loss: 0.7487602829933167, acc: 0.0625] [gan loss: 1.764056, acc: 0.000000]\n",
            "5818: [discriminator loss: 0.6680314540863037, acc: 0.4296875] [gan loss: 0.719507, acc: 0.531250]\n",
            "5819: [discriminator loss: 0.7419421672821045, acc: 0.0625] [gan loss: 1.748770, acc: 0.000000]\n",
            "5820: [discriminator loss: 0.6899434328079224, acc: 0.4140625] [gan loss: 0.921441, acc: 0.156250]\n",
            "5821: [discriminator loss: 0.7112003564834595, acc: 0.1328125] [gan loss: 1.699624, acc: 0.000000]\n",
            "5822: [discriminator loss: 0.6882020235061646, acc: 0.3515625] [gan loss: 0.893937, acc: 0.218750]\n",
            "5823: [discriminator loss: 0.7644777297973633, acc: 0.125] [gan loss: 1.719877, acc: 0.000000]\n",
            "5824: [discriminator loss: 0.7100222110748291, acc: 0.4453125] [gan loss: 0.623002, acc: 0.625000]\n",
            "5825: [discriminator loss: 0.7705445289611816, acc: 0.0390625] [gan loss: 1.633157, acc: 0.000000]\n",
            "5826: [discriminator loss: 0.6832656264305115, acc: 0.453125] [gan loss: 0.706820, acc: 0.515625]\n",
            "5827: [discriminator loss: 0.7504435777664185, acc: 0.0546875] [gan loss: 1.576891, acc: 0.000000]\n",
            "5828: [discriminator loss: 0.6480399966239929, acc: 0.453125] [gan loss: 0.794683, acc: 0.328125]\n",
            "5829: [discriminator loss: 0.776640772819519, acc: 0.0625] [gan loss: 1.630775, acc: 0.015625]\n",
            "5830: [discriminator loss: 0.6567099094390869, acc: 0.4296875] [gan loss: 0.751936, acc: 0.453125]\n",
            "5831: [discriminator loss: 0.7550749778747559, acc: 0.0390625] [gan loss: 1.759090, acc: 0.000000]\n",
            "5832: [discriminator loss: 0.6711852550506592, acc: 0.4375] [gan loss: 0.854571, acc: 0.234375]\n",
            "5833: [discriminator loss: 0.763981819152832, acc: 0.046875] [gan loss: 1.636195, acc: 0.015625]\n",
            "5834: [discriminator loss: 0.6760165691375732, acc: 0.4609375] [gan loss: 0.745736, acc: 0.468750]\n",
            "5835: [discriminator loss: 0.7748410701751709, acc: 0.0390625] [gan loss: 1.812720, acc: 0.000000]\n",
            "5836: [discriminator loss: 0.652554988861084, acc: 0.4765625] [gan loss: 0.661843, acc: 0.609375]\n",
            "5837: [discriminator loss: 0.8146603107452393, acc: 0.046875] [gan loss: 1.746460, acc: 0.000000]\n",
            "5838: [discriminator loss: 0.6618356108665466, acc: 0.4609375] [gan loss: 0.733443, acc: 0.437500]\n",
            "5839: [discriminator loss: 0.749993622303009, acc: 0.0546875] [gan loss: 1.668999, acc: 0.000000]\n",
            "5840: [discriminator loss: 0.6585603952407837, acc: 0.4296875] [gan loss: 0.797760, acc: 0.343750]\n",
            "5841: [discriminator loss: 0.7867476940155029, acc: 0.0625] [gan loss: 1.753799, acc: 0.000000]\n",
            "5842: [discriminator loss: 0.6691389083862305, acc: 0.4296875] [gan loss: 0.810552, acc: 0.406250]\n",
            "5843: [discriminator loss: 0.7608333230018616, acc: 0.0390625] [gan loss: 1.505706, acc: 0.000000]\n",
            "5844: [discriminator loss: 0.6607080698013306, acc: 0.4140625] [gan loss: 0.771929, acc: 0.390625]\n",
            "5845: [discriminator loss: 0.7428004741668701, acc: 0.0625] [gan loss: 1.672966, acc: 0.000000]\n",
            "5846: [discriminator loss: 0.6682019233703613, acc: 0.453125] [gan loss: 0.809560, acc: 0.281250]\n",
            "5847: [discriminator loss: 0.7343247532844543, acc: 0.0625] [gan loss: 1.729580, acc: 0.000000]\n",
            "5848: [discriminator loss: 0.6820073127746582, acc: 0.40625] [gan loss: 0.882824, acc: 0.156250]\n",
            "5849: [discriminator loss: 0.7168585658073425, acc: 0.09375] [gan loss: 1.643705, acc: 0.000000]\n",
            "5850: [discriminator loss: 0.6463410258293152, acc: 0.484375] [gan loss: 0.691131, acc: 0.625000]\n",
            "5851: [discriminator loss: 0.7834027409553528, acc: 0.0546875] [gan loss: 1.732306, acc: 0.000000]\n",
            "5852: [discriminator loss: 0.6896311044692993, acc: 0.4765625] [gan loss: 0.604307, acc: 0.750000]\n",
            "5853: [discriminator loss: 0.831867516040802, acc: 0.0078125] [gan loss: 1.813666, acc: 0.000000]\n",
            "5854: [discriminator loss: 0.6761058568954468, acc: 0.4765625] [gan loss: 0.600465, acc: 0.750000]\n",
            "5855: [discriminator loss: 0.7712463140487671, acc: 0.015625] [gan loss: 1.518760, acc: 0.000000]\n",
            "5856: [discriminator loss: 0.690018355846405, acc: 0.40625] [gan loss: 0.919560, acc: 0.156250]\n",
            "5857: [discriminator loss: 0.7061852216720581, acc: 0.09375] [gan loss: 1.407717, acc: 0.015625]\n",
            "5858: [discriminator loss: 0.7214421033859253, acc: 0.28125] [gan loss: 1.042934, acc: 0.062500]\n",
            "5859: [discriminator loss: 0.6813587546348572, acc: 0.1484375] [gan loss: 1.300756, acc: 0.031250]\n",
            "5860: [discriminator loss: 0.6944317817687988, acc: 0.25] [gan loss: 1.163910, acc: 0.015625]\n",
            "5861: [discriminator loss: 0.6922150254249573, acc: 0.2265625] [gan loss: 1.285168, acc: 0.000000]\n",
            "5862: [discriminator loss: 0.6602431535720825, acc: 0.3203125] [gan loss: 1.060780, acc: 0.109375]\n",
            "5863: [discriminator loss: 0.6716741323471069, acc: 0.1953125] [gan loss: 1.331760, acc: 0.000000]\n",
            "5864: [discriminator loss: 0.6726875305175781, acc: 0.3125] [gan loss: 1.085737, acc: 0.093750]\n",
            "5865: [discriminator loss: 0.7435632944107056, acc: 0.1953125] [gan loss: 1.255286, acc: 0.015625]\n",
            "5866: [discriminator loss: 0.6798440217971802, acc: 0.28125] [gan loss: 1.060694, acc: 0.015625]\n",
            "5867: [discriminator loss: 0.682077169418335, acc: 0.1640625] [gan loss: 1.586742, acc: 0.000000]\n",
            "5868: [discriminator loss: 0.6504570841789246, acc: 0.4453125] [gan loss: 0.691765, acc: 0.578125]\n",
            "5869: [discriminator loss: 0.752956748008728, acc: 0.0390625] [gan loss: 1.977893, acc: 0.000000]\n",
            "5870: [discriminator loss: 0.6727318167686462, acc: 0.4921875] [gan loss: 0.474189, acc: 0.843750]\n",
            "5871: [discriminator loss: 0.9141284227371216, acc: 0.0234375] [gan loss: 2.080736, acc: 0.000000]\n",
            "5872: [discriminator loss: 0.7183125019073486, acc: 0.4765625] [gan loss: 0.556927, acc: 0.796875]\n",
            "5873: [discriminator loss: 0.8399445414543152, acc: 0.015625] [gan loss: 1.833999, acc: 0.000000]\n",
            "5874: [discriminator loss: 0.6923668384552002, acc: 0.4453125] [gan loss: 0.683703, acc: 0.609375]\n",
            "5875: [discriminator loss: 0.8346886038780212, acc: 0.015625] [gan loss: 1.573487, acc: 0.000000]\n",
            "5876: [discriminator loss: 0.7096181511878967, acc: 0.4453125] [gan loss: 0.824487, acc: 0.296875]\n",
            "5877: [discriminator loss: 0.7422454953193665, acc: 0.03125] [gan loss: 1.569951, acc: 0.000000]\n",
            "5878: [discriminator loss: 0.7010747790336609, acc: 0.40625] [gan loss: 0.864457, acc: 0.281250]\n",
            "5879: [discriminator loss: 0.7105391621589661, acc: 0.140625] [gan loss: 1.384845, acc: 0.000000]\n",
            "5880: [discriminator loss: 0.6669185757637024, acc: 0.34375] [gan loss: 0.976541, acc: 0.078125]\n",
            "5881: [discriminator loss: 0.734972357749939, acc: 0.09375] [gan loss: 1.427583, acc: 0.000000]\n",
            "5882: [discriminator loss: 0.6512929201126099, acc: 0.4609375] [gan loss: 0.813281, acc: 0.343750]\n",
            "5883: [discriminator loss: 0.7821226716041565, acc: 0.0234375] [gan loss: 1.740053, acc: 0.000000]\n",
            "5884: [discriminator loss: 0.6834472417831421, acc: 0.4609375] [gan loss: 0.677243, acc: 0.562500]\n",
            "5885: [discriminator loss: 0.8119412064552307, acc: 0.015625] [gan loss: 1.777715, acc: 0.000000]\n",
            "5886: [discriminator loss: 0.6993948221206665, acc: 0.4453125] [gan loss: 0.662526, acc: 0.546875]\n",
            "5887: [discriminator loss: 0.7953628301620483, acc: 0.015625] [gan loss: 1.689443, acc: 0.000000]\n",
            "5888: [discriminator loss: 0.6743972897529602, acc: 0.46875] [gan loss: 0.780566, acc: 0.296875]\n",
            "5889: [discriminator loss: 0.7954949736595154, acc: 0.015625] [gan loss: 1.694009, acc: 0.000000]\n",
            "5890: [discriminator loss: 0.6702172756195068, acc: 0.4921875] [gan loss: 0.695536, acc: 0.468750]\n",
            "5891: [discriminator loss: 0.8061221241950989, acc: 0.03125] [gan loss: 1.709718, acc: 0.000000]\n",
            "5892: [discriminator loss: 0.6663209199905396, acc: 0.484375] [gan loss: 0.681301, acc: 0.531250]\n",
            "5893: [discriminator loss: 0.7710971832275391, acc: 0.015625] [gan loss: 1.645003, acc: 0.000000]\n",
            "5894: [discriminator loss: 0.7029269337654114, acc: 0.4375] [gan loss: 0.759384, acc: 0.421875]\n",
            "5895: [discriminator loss: 0.7433159947395325, acc: 0.0234375] [gan loss: 1.536576, acc: 0.000000]\n",
            "5896: [discriminator loss: 0.6648741960525513, acc: 0.453125] [gan loss: 0.817767, acc: 0.312500]\n",
            "5897: [discriminator loss: 0.7575098276138306, acc: 0.0234375] [gan loss: 1.552361, acc: 0.000000]\n",
            "5898: [discriminator loss: 0.6620401740074158, acc: 0.4296875] [gan loss: 0.884777, acc: 0.234375]\n",
            "5899: [discriminator loss: 0.7238888740539551, acc: 0.0625] [gan loss: 1.467009, acc: 0.000000]\n",
            "5900: [discriminator loss: 0.6629642248153687, acc: 0.3671875] [gan loss: 0.908658, acc: 0.187500]\n",
            "5901: [discriminator loss: 0.7327628135681152, acc: 0.0859375] [gan loss: 1.659674, acc: 0.000000]\n",
            "5902: [discriminator loss: 0.6537340879440308, acc: 0.4453125] [gan loss: 0.794877, acc: 0.343750]\n",
            "5903: [discriminator loss: 0.772148609161377, acc: 0.03125] [gan loss: 1.669284, acc: 0.000000]\n",
            "5904: [discriminator loss: 0.6838096380233765, acc: 0.4609375] [gan loss: 0.748403, acc: 0.500000]\n",
            "5905: [discriminator loss: 0.790256142616272, acc: 0.0390625] [gan loss: 1.714624, acc: 0.000000]\n",
            "5906: [discriminator loss: 0.6833946704864502, acc: 0.4609375] [gan loss: 0.743663, acc: 0.468750]\n",
            "5907: [discriminator loss: 0.7677529454231262, acc: 0.015625] [gan loss: 1.782071, acc: 0.000000]\n",
            "5908: [discriminator loss: 0.7042745351791382, acc: 0.4375] [gan loss: 0.638054, acc: 0.609375]\n",
            "5909: [discriminator loss: 0.7973897457122803, acc: 0.0078125] [gan loss: 1.684128, acc: 0.000000]\n",
            "5910: [discriminator loss: 0.680163562297821, acc: 0.421875] [gan loss: 0.751064, acc: 0.484375]\n",
            "5911: [discriminator loss: 0.7843111753463745, acc: 0.03125] [gan loss: 1.511990, acc: 0.000000]\n",
            "5912: [discriminator loss: 0.6835297346115112, acc: 0.40625] [gan loss: 0.845627, acc: 0.203125]\n",
            "5913: [discriminator loss: 0.728029727935791, acc: 0.046875] [gan loss: 1.548064, acc: 0.000000]\n",
            "5914: [discriminator loss: 0.6627750396728516, acc: 0.375] [gan loss: 0.884799, acc: 0.281250]\n",
            "5915: [discriminator loss: 0.7198774814605713, acc: 0.140625] [gan loss: 1.483661, acc: 0.015625]\n",
            "5916: [discriminator loss: 0.6854466199874878, acc: 0.3125] [gan loss: 1.068349, acc: 0.015625]\n",
            "5917: [discriminator loss: 0.7406819462776184, acc: 0.1484375] [gan loss: 1.315518, acc: 0.000000]\n",
            "5918: [discriminator loss: 0.6584922075271606, acc: 0.328125] [gan loss: 1.121008, acc: 0.031250]\n",
            "5919: [discriminator loss: 0.6760740876197815, acc: 0.21875] [gan loss: 1.394442, acc: 0.000000]\n",
            "5920: [discriminator loss: 0.6795790195465088, acc: 0.3671875] [gan loss: 1.014426, acc: 0.109375]\n",
            "5921: [discriminator loss: 0.7075443267822266, acc: 0.109375] [gan loss: 1.664091, acc: 0.000000]\n",
            "5922: [discriminator loss: 0.6821497082710266, acc: 0.4609375] [gan loss: 0.711904, acc: 0.484375]\n",
            "5923: [discriminator loss: 0.7798501253128052, acc: 0.046875] [gan loss: 1.932989, acc: 0.000000]\n",
            "5924: [discriminator loss: 0.7208858132362366, acc: 0.484375] [gan loss: 0.487876, acc: 0.828125]\n",
            "5925: [discriminator loss: 0.9084094762802124, acc: 0.0] [gan loss: 1.971813, acc: 0.000000]\n",
            "5926: [discriminator loss: 0.7073375582695007, acc: 0.4921875] [gan loss: 0.685685, acc: 0.531250]\n",
            "5927: [discriminator loss: 0.8096334338188171, acc: 0.0078125] [gan loss: 1.703759, acc: 0.000000]\n",
            "5928: [discriminator loss: 0.6562529802322388, acc: 0.4609375] [gan loss: 0.793392, acc: 0.296875]\n",
            "5929: [discriminator loss: 0.770293116569519, acc: 0.0703125] [gan loss: 1.463610, acc: 0.000000]\n",
            "5930: [discriminator loss: 0.6726464629173279, acc: 0.40625] [gan loss: 0.851371, acc: 0.187500]\n",
            "5931: [discriminator loss: 0.7209036350250244, acc: 0.078125] [gan loss: 1.477013, acc: 0.000000]\n",
            "5932: [discriminator loss: 0.6785697937011719, acc: 0.390625] [gan loss: 0.889370, acc: 0.140625]\n",
            "5933: [discriminator loss: 0.7300639748573303, acc: 0.1171875] [gan loss: 1.448277, acc: 0.000000]\n",
            "5934: [discriminator loss: 0.6460396647453308, acc: 0.3828125] [gan loss: 1.003978, acc: 0.156250]\n",
            "5935: [discriminator loss: 0.7231278419494629, acc: 0.15625] [gan loss: 1.404987, acc: 0.000000]\n",
            "5936: [discriminator loss: 0.6485592722892761, acc: 0.40625] [gan loss: 0.843723, acc: 0.203125]\n",
            "5937: [discriminator loss: 0.7170023918151855, acc: 0.0625] [gan loss: 1.589708, acc: 0.015625]\n",
            "5938: [discriminator loss: 0.6771126985549927, acc: 0.4453125] [gan loss: 0.738268, acc: 0.437500]\n",
            "5939: [discriminator loss: 0.7993460297584534, acc: 0.0234375] [gan loss: 1.821347, acc: 0.000000]\n",
            "5940: [discriminator loss: 0.6848945021629333, acc: 0.484375] [gan loss: 0.714561, acc: 0.468750]\n",
            "5941: [discriminator loss: 0.7875937223434448, acc: 0.03125] [gan loss: 1.717603, acc: 0.000000]\n",
            "5942: [discriminator loss: 0.6673092246055603, acc: 0.484375] [gan loss: 0.637953, acc: 0.656250]\n",
            "5943: [discriminator loss: 0.8480162024497986, acc: 0.0078125] [gan loss: 1.877271, acc: 0.000000]\n",
            "5944: [discriminator loss: 0.6943433880805969, acc: 0.4765625] [gan loss: 0.612180, acc: 0.687500]\n",
            "5945: [discriminator loss: 0.8006353378295898, acc: 0.0078125] [gan loss: 1.535408, acc: 0.000000]\n",
            "5946: [discriminator loss: 0.6693416833877563, acc: 0.4375] [gan loss: 0.765386, acc: 0.359375]\n",
            "5947: [discriminator loss: 0.7544139623641968, acc: 0.0390625] [gan loss: 1.573794, acc: 0.000000]\n",
            "5948: [discriminator loss: 0.6401385068893433, acc: 0.4296875] [gan loss: 0.807846, acc: 0.328125]\n",
            "5949: [discriminator loss: 0.7627425193786621, acc: 0.046875] [gan loss: 1.398675, acc: 0.000000]\n",
            "5950: [discriminator loss: 0.6636266112327576, acc: 0.3828125] [gan loss: 0.974338, acc: 0.140625]\n",
            "5951: [discriminator loss: 0.7342658638954163, acc: 0.09375] [gan loss: 1.528410, acc: 0.000000]\n",
            "5952: [discriminator loss: 0.6831575632095337, acc: 0.34375] [gan loss: 0.980641, acc: 0.062500]\n",
            "5953: [discriminator loss: 0.694566011428833, acc: 0.140625] [gan loss: 1.389040, acc: 0.015625]\n",
            "5954: [discriminator loss: 0.6849573254585266, acc: 0.34375] [gan loss: 1.003862, acc: 0.062500]\n",
            "5955: [discriminator loss: 0.6947807669639587, acc: 0.1796875] [gan loss: 1.486444, acc: 0.000000]\n",
            "5956: [discriminator loss: 0.6716227531433105, acc: 0.4375] [gan loss: 0.823597, acc: 0.281250]\n",
            "5957: [discriminator loss: 0.7435364127159119, acc: 0.0625] [gan loss: 1.695912, acc: 0.000000]\n",
            "5958: [discriminator loss: 0.6963130235671997, acc: 0.453125] [gan loss: 0.756130, acc: 0.375000]\n",
            "5959: [discriminator loss: 0.7616091966629028, acc: 0.046875] [gan loss: 1.676833, acc: 0.000000]\n",
            "5960: [discriminator loss: 0.6811400651931763, acc: 0.46875] [gan loss: 0.677325, acc: 0.546875]\n",
            "5961: [discriminator loss: 0.8123075366020203, acc: 0.015625] [gan loss: 1.937328, acc: 0.000000]\n",
            "5962: [discriminator loss: 0.6984686851501465, acc: 0.5] [gan loss: 0.617424, acc: 0.687500]\n",
            "5963: [discriminator loss: 0.8263241648674011, acc: 0.0] [gan loss: 1.928195, acc: 0.000000]\n",
            "5964: [discriminator loss: 0.6932952404022217, acc: 0.4765625] [gan loss: 0.655872, acc: 0.562500]\n",
            "5965: [discriminator loss: 0.7941948771476746, acc: 0.03125] [gan loss: 1.599523, acc: 0.000000]\n",
            "5966: [discriminator loss: 0.6592993140220642, acc: 0.4921875] [gan loss: 0.725986, acc: 0.500000]\n",
            "5967: [discriminator loss: 0.7784491181373596, acc: 0.0234375] [gan loss: 1.441959, acc: 0.000000]\n",
            "5968: [discriminator loss: 0.6793555021286011, acc: 0.390625] [gan loss: 0.846841, acc: 0.265625]\n",
            "5969: [discriminator loss: 0.7390611171722412, acc: 0.09375] [gan loss: 1.557264, acc: 0.000000]\n",
            "5970: [discriminator loss: 0.6739863753318787, acc: 0.40625] [gan loss: 0.919092, acc: 0.203125]\n",
            "5971: [discriminator loss: 0.7038357853889465, acc: 0.125] [gan loss: 1.519034, acc: 0.015625]\n",
            "5972: [discriminator loss: 0.7028391361236572, acc: 0.375] [gan loss: 0.751166, acc: 0.343750]\n",
            "5973: [discriminator loss: 0.7724520564079285, acc: 0.046875] [gan loss: 1.680934, acc: 0.000000]\n",
            "5974: [discriminator loss: 0.6616632342338562, acc: 0.484375] [gan loss: 0.590707, acc: 0.750000]\n",
            "5975: [discriminator loss: 0.8366526365280151, acc: 0.0078125] [gan loss: 1.969330, acc: 0.000000]\n",
            "5976: [discriminator loss: 0.7184594869613647, acc: 0.4765625] [gan loss: 0.657962, acc: 0.625000]\n",
            "5977: [discriminator loss: 0.8081785440444946, acc: 0.0390625] [gan loss: 1.629006, acc: 0.000000]\n",
            "5978: [discriminator loss: 0.6759728789329529, acc: 0.4375] [gan loss: 0.800963, acc: 0.312500]\n",
            "5979: [discriminator loss: 0.7568837404251099, acc: 0.046875] [gan loss: 1.423239, acc: 0.015625]\n",
            "5980: [discriminator loss: 0.6716546416282654, acc: 0.4140625] [gan loss: 0.817310, acc: 0.250000]\n",
            "5981: [discriminator loss: 0.7533596158027649, acc: 0.0625] [gan loss: 1.653263, acc: 0.000000]\n",
            "5982: [discriminator loss: 0.6657884120941162, acc: 0.4609375] [gan loss: 0.808863, acc: 0.312500]\n",
            "5983: [discriminator loss: 0.7312791347503662, acc: 0.0703125] [gan loss: 1.502181, acc: 0.000000]\n",
            "5984: [discriminator loss: 0.6662459373474121, acc: 0.4609375] [gan loss: 0.761421, acc: 0.375000]\n",
            "5985: [discriminator loss: 0.7473157048225403, acc: 0.078125] [gan loss: 1.490592, acc: 0.000000]\n",
            "5986: [discriminator loss: 0.6663026809692383, acc: 0.421875] [gan loss: 0.860776, acc: 0.218750]\n",
            "5987: [discriminator loss: 0.7161295413970947, acc: 0.1328125] [gan loss: 1.525596, acc: 0.000000]\n",
            "5988: [discriminator loss: 0.6910380721092224, acc: 0.3828125] [gan loss: 0.793621, acc: 0.343750]\n",
            "5989: [discriminator loss: 0.7419859170913696, acc: 0.046875] [gan loss: 1.761998, acc: 0.000000]\n",
            "5990: [discriminator loss: 0.6983466744422913, acc: 0.453125] [gan loss: 0.600924, acc: 0.687500]\n",
            "5991: [discriminator loss: 0.8227096199989319, acc: 0.0234375] [gan loss: 2.014506, acc: 0.000000]\n",
            "5992: [discriminator loss: 0.6858332753181458, acc: 0.4765625] [gan loss: 0.648284, acc: 0.593750]\n",
            "5993: [discriminator loss: 0.7893905639648438, acc: 0.0] [gan loss: 1.730412, acc: 0.000000]\n",
            "5994: [discriminator loss: 0.6613653898239136, acc: 0.46875] [gan loss: 0.700496, acc: 0.453125]\n",
            "5995: [discriminator loss: 0.8200651407241821, acc: 0.0234375] [gan loss: 1.616057, acc: 0.000000]\n",
            "5996: [discriminator loss: 0.6966921091079712, acc: 0.421875] [gan loss: 0.831454, acc: 0.234375]\n",
            "5997: [discriminator loss: 0.7112239599227905, acc: 0.1015625] [gan loss: 1.421299, acc: 0.000000]\n",
            "5998: [discriminator loss: 0.6980284452438354, acc: 0.359375] [gan loss: 0.941229, acc: 0.109375]\n",
            "5999: [discriminator loss: 0.6870750188827515, acc: 0.109375] [gan loss: 1.424555, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "6000: [discriminator loss: 0.6845234632492065, acc: 0.3671875] [gan loss: 0.976411, acc: 0.140625]\n",
            "6001: [discriminator loss: 0.700971245765686, acc: 0.109375] [gan loss: 1.434013, acc: 0.000000]\n",
            "6002: [discriminator loss: 0.6822312474250793, acc: 0.3359375] [gan loss: 0.921173, acc: 0.156250]\n",
            "6003: [discriminator loss: 0.7095932960510254, acc: 0.109375] [gan loss: 1.436819, acc: 0.000000]\n",
            "6004: [discriminator loss: 0.6701407432556152, acc: 0.34375] [gan loss: 1.001444, acc: 0.125000]\n",
            "6005: [discriminator loss: 0.6833071708679199, acc: 0.15625] [gan loss: 1.522094, acc: 0.000000]\n",
            "6006: [discriminator loss: 0.6641517877578735, acc: 0.421875] [gan loss: 0.768671, acc: 0.359375]\n",
            "6007: [discriminator loss: 0.7841814756393433, acc: 0.046875] [gan loss: 1.982322, acc: 0.000000]\n",
            "6008: [discriminator loss: 0.7109779119491577, acc: 0.484375] [gan loss: 0.517506, acc: 0.859375]\n",
            "6009: [discriminator loss: 0.8449737429618835, acc: 0.0078125] [gan loss: 1.784088, acc: 0.000000]\n",
            "6010: [discriminator loss: 0.7035965919494629, acc: 0.4921875] [gan loss: 0.602347, acc: 0.750000]\n",
            "6011: [discriminator loss: 0.8687523007392883, acc: 0.0] [gan loss: 1.784267, acc: 0.000000]\n",
            "6012: [discriminator loss: 0.6741693615913391, acc: 0.484375] [gan loss: 0.655527, acc: 0.578125]\n",
            "6013: [discriminator loss: 0.7727712988853455, acc: 0.0390625] [gan loss: 1.524042, acc: 0.015625]\n",
            "6014: [discriminator loss: 0.6595339775085449, acc: 0.40625] [gan loss: 0.928970, acc: 0.140625]\n",
            "6015: [discriminator loss: 0.6968671083450317, acc: 0.109375] [gan loss: 1.593556, acc: 0.000000]\n",
            "6016: [discriminator loss: 0.6680703163146973, acc: 0.3984375] [gan loss: 0.900953, acc: 0.093750]\n",
            "6017: [discriminator loss: 0.7359988689422607, acc: 0.0859375] [gan loss: 1.524738, acc: 0.000000]\n",
            "6018: [discriminator loss: 0.7024980783462524, acc: 0.3828125] [gan loss: 1.012715, acc: 0.078125]\n",
            "6019: [discriminator loss: 0.6926423907279968, acc: 0.109375] [gan loss: 1.619095, acc: 0.000000]\n",
            "6020: [discriminator loss: 0.6827318668365479, acc: 0.3828125] [gan loss: 0.820349, acc: 0.312500]\n",
            "6021: [discriminator loss: 0.7406215071678162, acc: 0.0859375] [gan loss: 1.591264, acc: 0.000000]\n",
            "6022: [discriminator loss: 0.6672184467315674, acc: 0.4453125] [gan loss: 0.682680, acc: 0.578125]\n",
            "6023: [discriminator loss: 0.814221978187561, acc: 0.0390625] [gan loss: 1.986486, acc: 0.000000]\n",
            "6024: [discriminator loss: 0.6888267993927002, acc: 0.4921875] [gan loss: 0.649330, acc: 0.578125]\n",
            "6025: [discriminator loss: 0.816102147102356, acc: 0.015625] [gan loss: 1.792250, acc: 0.000000]\n",
            "6026: [discriminator loss: 0.7090058326721191, acc: 0.46875] [gan loss: 0.717230, acc: 0.453125]\n",
            "6027: [discriminator loss: 0.78931725025177, acc: 0.015625] [gan loss: 1.652537, acc: 0.000000]\n",
            "6028: [discriminator loss: 0.6639356017112732, acc: 0.453125] [gan loss: 0.677859, acc: 0.593750]\n",
            "6029: [discriminator loss: 0.7585729360580444, acc: 0.0390625] [gan loss: 1.561256, acc: 0.000000]\n",
            "6030: [discriminator loss: 0.6626446843147278, acc: 0.46875] [gan loss: 0.767417, acc: 0.453125]\n",
            "6031: [discriminator loss: 0.7795666456222534, acc: 0.0234375] [gan loss: 1.669317, acc: 0.000000]\n",
            "6032: [discriminator loss: 0.6621952652931213, acc: 0.4453125] [gan loss: 0.753652, acc: 0.359375]\n",
            "6033: [discriminator loss: 0.7301125526428223, acc: 0.0859375] [gan loss: 1.470264, acc: 0.000000]\n",
            "6034: [discriminator loss: 0.6976014375686646, acc: 0.3671875] [gan loss: 0.777791, acc: 0.328125]\n",
            "6035: [discriminator loss: 0.71632981300354, acc: 0.0859375] [gan loss: 1.521217, acc: 0.000000]\n",
            "6036: [discriminator loss: 0.670973539352417, acc: 0.390625] [gan loss: 0.842771, acc: 0.203125]\n",
            "6037: [discriminator loss: 0.7554283142089844, acc: 0.0625] [gan loss: 1.620399, acc: 0.000000]\n",
            "6038: [discriminator loss: 0.6519102454185486, acc: 0.4453125] [gan loss: 0.750667, acc: 0.375000]\n",
            "6039: [discriminator loss: 0.7880833148956299, acc: 0.0546875] [gan loss: 1.706735, acc: 0.000000]\n",
            "6040: [discriminator loss: 0.6765611171722412, acc: 0.4296875] [gan loss: 0.733673, acc: 0.406250]\n",
            "6041: [discriminator loss: 0.7772458791732788, acc: 0.0234375] [gan loss: 1.700879, acc: 0.000000]\n",
            "6042: [discriminator loss: 0.6708214282989502, acc: 0.4375] [gan loss: 0.794086, acc: 0.296875]\n",
            "6043: [discriminator loss: 0.7267597913742065, acc: 0.0546875] [gan loss: 1.679984, acc: 0.000000]\n",
            "6044: [discriminator loss: 0.6754570007324219, acc: 0.4375] [gan loss: 0.673707, acc: 0.625000]\n",
            "6045: [discriminator loss: 0.7729183435440063, acc: 0.015625] [gan loss: 1.728222, acc: 0.000000]\n",
            "6046: [discriminator loss: 0.6598889827728271, acc: 0.4375] [gan loss: 0.839536, acc: 0.281250]\n",
            "6047: [discriminator loss: 0.7439697980880737, acc: 0.0625] [gan loss: 1.669949, acc: 0.000000]\n",
            "6048: [discriminator loss: 0.672353982925415, acc: 0.453125] [gan loss: 0.745010, acc: 0.453125]\n",
            "6049: [discriminator loss: 0.782007098197937, acc: 0.0625] [gan loss: 1.653966, acc: 0.000000]\n",
            "6050: [discriminator loss: 0.6864060163497925, acc: 0.453125] [gan loss: 0.877927, acc: 0.203125]\n",
            "6051: [discriminator loss: 0.7391723394393921, acc: 0.046875] [gan loss: 1.664660, acc: 0.000000]\n",
            "6052: [discriminator loss: 0.6763527989387512, acc: 0.4296875] [gan loss: 0.683776, acc: 0.562500]\n",
            "6053: [discriminator loss: 0.7564277648925781, acc: 0.03125] [gan loss: 1.732472, acc: 0.000000]\n",
            "6054: [discriminator loss: 0.6872326731681824, acc: 0.4453125] [gan loss: 0.676530, acc: 0.531250]\n",
            "6055: [discriminator loss: 0.7963529229164124, acc: 0.046875] [gan loss: 1.681936, acc: 0.000000]\n",
            "6056: [discriminator loss: 0.684877872467041, acc: 0.421875] [gan loss: 0.682255, acc: 0.578125]\n",
            "6057: [discriminator loss: 0.7594475746154785, acc: 0.046875] [gan loss: 1.695457, acc: 0.000000]\n",
            "6058: [discriminator loss: 0.6865270137786865, acc: 0.46875] [gan loss: 0.754506, acc: 0.359375]\n",
            "6059: [discriminator loss: 0.7641128897666931, acc: 0.0234375] [gan loss: 1.594963, acc: 0.000000]\n",
            "6060: [discriminator loss: 0.6519831418991089, acc: 0.4453125] [gan loss: 0.829252, acc: 0.281250]\n",
            "6061: [discriminator loss: 0.7710550427436829, acc: 0.0390625] [gan loss: 1.723507, acc: 0.000000]\n",
            "6062: [discriminator loss: 0.6819504499435425, acc: 0.453125] [gan loss: 0.686064, acc: 0.593750]\n",
            "6063: [discriminator loss: 0.7782105803489685, acc: 0.046875] [gan loss: 1.657233, acc: 0.000000]\n",
            "6064: [discriminator loss: 0.665849506855011, acc: 0.484375] [gan loss: 0.756789, acc: 0.421875]\n",
            "6065: [discriminator loss: 0.7890956401824951, acc: 0.015625] [gan loss: 1.797084, acc: 0.000000]\n",
            "6066: [discriminator loss: 0.6420448422431946, acc: 0.46875] [gan loss: 0.684357, acc: 0.546875]\n",
            "6067: [discriminator loss: 0.7889808416366577, acc: 0.03125] [gan loss: 1.550375, acc: 0.000000]\n",
            "6068: [discriminator loss: 0.6818792223930359, acc: 0.4140625] [gan loss: 0.784965, acc: 0.312500]\n",
            "6069: [discriminator loss: 0.7578304409980774, acc: 0.0859375] [gan loss: 1.596273, acc: 0.000000]\n",
            "6070: [discriminator loss: 0.7039974927902222, acc: 0.3828125] [gan loss: 0.813541, acc: 0.265625]\n",
            "6071: [discriminator loss: 0.7209945321083069, acc: 0.09375] [gan loss: 1.517932, acc: 0.000000]\n",
            "6072: [discriminator loss: 0.6552025079727173, acc: 0.4140625] [gan loss: 0.810704, acc: 0.421875]\n",
            "6073: [discriminator loss: 0.7197834253311157, acc: 0.1171875] [gan loss: 1.398595, acc: 0.000000]\n",
            "6074: [discriminator loss: 0.6604025959968567, acc: 0.359375] [gan loss: 0.799825, acc: 0.296875]\n",
            "6075: [discriminator loss: 0.7501988410949707, acc: 0.078125] [gan loss: 1.793167, acc: 0.000000]\n",
            "6076: [discriminator loss: 0.6687390804290771, acc: 0.4375] [gan loss: 0.689694, acc: 0.562500]\n",
            "6077: [discriminator loss: 0.7658293843269348, acc: 0.0625] [gan loss: 1.745139, acc: 0.000000]\n",
            "6078: [discriminator loss: 0.67673259973526, acc: 0.46875] [gan loss: 0.627766, acc: 0.625000]\n",
            "6079: [discriminator loss: 0.8026543855667114, acc: 0.0234375] [gan loss: 1.722738, acc: 0.000000]\n",
            "6080: [discriminator loss: 0.6673130989074707, acc: 0.46875] [gan loss: 0.690100, acc: 0.500000]\n",
            "6081: [discriminator loss: 0.776092529296875, acc: 0.0078125] [gan loss: 1.531818, acc: 0.000000]\n",
            "6082: [discriminator loss: 0.6689939498901367, acc: 0.40625] [gan loss: 0.836134, acc: 0.250000]\n",
            "6083: [discriminator loss: 0.7634884715080261, acc: 0.046875] [gan loss: 1.611535, acc: 0.000000]\n",
            "6084: [discriminator loss: 0.6832088232040405, acc: 0.3671875] [gan loss: 0.777408, acc: 0.343750]\n",
            "6085: [discriminator loss: 0.7586490511894226, acc: 0.046875] [gan loss: 1.511178, acc: 0.000000]\n",
            "6086: [discriminator loss: 0.6836034059524536, acc: 0.4375] [gan loss: 0.781971, acc: 0.328125]\n",
            "6087: [discriminator loss: 0.7803822159767151, acc: 0.0234375] [gan loss: 1.658011, acc: 0.015625]\n",
            "6088: [discriminator loss: 0.701562762260437, acc: 0.3828125] [gan loss: 0.818458, acc: 0.328125]\n",
            "6089: [discriminator loss: 0.716001570224762, acc: 0.0859375] [gan loss: 1.438428, acc: 0.000000]\n",
            "6090: [discriminator loss: 0.677011251449585, acc: 0.3671875] [gan loss: 0.944196, acc: 0.156250]\n",
            "6091: [discriminator loss: 0.7405511736869812, acc: 0.09375] [gan loss: 1.625327, acc: 0.000000]\n",
            "6092: [discriminator loss: 0.6687474250793457, acc: 0.40625] [gan loss: 0.941175, acc: 0.171875]\n",
            "6093: [discriminator loss: 0.7028505206108093, acc: 0.1015625] [gan loss: 1.605549, acc: 0.000000]\n",
            "6094: [discriminator loss: 0.6835522055625916, acc: 0.4375] [gan loss: 0.825622, acc: 0.343750]\n",
            "6095: [discriminator loss: 0.7576851844787598, acc: 0.03125] [gan loss: 1.854855, acc: 0.000000]\n",
            "6096: [discriminator loss: 0.7069788575172424, acc: 0.4921875] [gan loss: 0.524408, acc: 0.859375]\n",
            "6097: [discriminator loss: 0.8392040729522705, acc: 0.0] [gan loss: 1.670820, acc: 0.000000]\n",
            "6098: [discriminator loss: 0.7024362683296204, acc: 0.40625] [gan loss: 0.817179, acc: 0.375000]\n",
            "6099: [discriminator loss: 0.7576719522476196, acc: 0.1015625] [gan loss: 1.636785, acc: 0.000000]\n",
            "6100: [discriminator loss: 0.6732971668243408, acc: 0.421875] [gan loss: 0.700467, acc: 0.531250]\n",
            "6101: [discriminator loss: 0.7656822204589844, acc: 0.046875] [gan loss: 1.588690, acc: 0.000000]\n",
            "6102: [discriminator loss: 0.6836488842964172, acc: 0.40625] [gan loss: 0.795345, acc: 0.296875]\n",
            "6103: [discriminator loss: 0.7329124212265015, acc: 0.0703125] [gan loss: 1.551479, acc: 0.000000]\n",
            "6104: [discriminator loss: 0.6596430540084839, acc: 0.4375] [gan loss: 0.745475, acc: 0.390625]\n",
            "6105: [discriminator loss: 0.8096097707748413, acc: 0.03125] [gan loss: 1.546423, acc: 0.000000]\n",
            "6106: [discriminator loss: 0.6667325496673584, acc: 0.453125] [gan loss: 0.782850, acc: 0.343750]\n",
            "6107: [discriminator loss: 0.7579470276832581, acc: 0.078125] [gan loss: 1.695106, acc: 0.000000]\n",
            "6108: [discriminator loss: 0.6844993233680725, acc: 0.4765625] [gan loss: 0.713827, acc: 0.500000]\n",
            "6109: [discriminator loss: 0.7876858711242676, acc: 0.0078125] [gan loss: 1.800224, acc: 0.000000]\n",
            "6110: [discriminator loss: 0.6976354122161865, acc: 0.453125] [gan loss: 0.710252, acc: 0.406250]\n",
            "6111: [discriminator loss: 0.748016357421875, acc: 0.03125] [gan loss: 1.560740, acc: 0.000000]\n",
            "6112: [discriminator loss: 0.6653389930725098, acc: 0.4375] [gan loss: 0.793360, acc: 0.328125]\n",
            "6113: [discriminator loss: 0.7442858219146729, acc: 0.0625] [gan loss: 1.514986, acc: 0.015625]\n",
            "6114: [discriminator loss: 0.6859210729598999, acc: 0.40625] [gan loss: 0.777199, acc: 0.328125]\n",
            "6115: [discriminator loss: 0.7474143505096436, acc: 0.046875] [gan loss: 1.707792, acc: 0.000000]\n",
            "6116: [discriminator loss: 0.6817882061004639, acc: 0.4140625] [gan loss: 0.766835, acc: 0.359375]\n",
            "6117: [discriminator loss: 0.73271244764328, acc: 0.078125] [gan loss: 1.537191, acc: 0.000000]\n",
            "6118: [discriminator loss: 0.6676463484764099, acc: 0.4375] [gan loss: 0.743525, acc: 0.500000]\n",
            "6119: [discriminator loss: 0.8081719875335693, acc: 0.0390625] [gan loss: 1.750981, acc: 0.000000]\n",
            "6120: [discriminator loss: 0.6806432008743286, acc: 0.46875] [gan loss: 0.633406, acc: 0.656250]\n",
            "6121: [discriminator loss: 0.7719700932502747, acc: 0.0625] [gan loss: 1.491813, acc: 0.000000]\n",
            "6122: [discriminator loss: 0.6813085079193115, acc: 0.3984375] [gan loss: 0.835072, acc: 0.296875]\n",
            "6123: [discriminator loss: 0.7359135150909424, acc: 0.09375] [gan loss: 1.452361, acc: 0.000000]\n",
            "6124: [discriminator loss: 0.6769739985466003, acc: 0.3828125] [gan loss: 0.926860, acc: 0.109375]\n",
            "6125: [discriminator loss: 0.7258684635162354, acc: 0.09375] [gan loss: 1.496094, acc: 0.000000]\n",
            "6126: [discriminator loss: 0.6671819090843201, acc: 0.3984375] [gan loss: 0.999524, acc: 0.125000]\n",
            "6127: [discriminator loss: 0.7014314532279968, acc: 0.1484375] [gan loss: 1.420288, acc: 0.000000]\n",
            "6128: [discriminator loss: 0.7093213200569153, acc: 0.34375] [gan loss: 1.045881, acc: 0.093750]\n",
            "6129: [discriminator loss: 0.712414026260376, acc: 0.125] [gan loss: 1.519359, acc: 0.000000]\n",
            "6130: [discriminator loss: 0.6894469857215881, acc: 0.3828125] [gan loss: 0.812993, acc: 0.312500]\n",
            "6131: [discriminator loss: 0.7660410404205322, acc: 0.0859375] [gan loss: 2.036496, acc: 0.000000]\n",
            "6132: [discriminator loss: 0.7031956315040588, acc: 0.4609375] [gan loss: 0.469028, acc: 0.921875]\n",
            "6133: [discriminator loss: 0.8458396196365356, acc: 0.0078125] [gan loss: 2.180763, acc: 0.000000]\n",
            "6134: [discriminator loss: 0.7112289667129517, acc: 0.5] [gan loss: 0.541282, acc: 0.796875]\n",
            "6135: [discriminator loss: 0.8590891361236572, acc: 0.015625] [gan loss: 1.433300, acc: 0.015625]\n",
            "6136: [discriminator loss: 0.6897799968719482, acc: 0.4296875] [gan loss: 0.791676, acc: 0.437500]\n",
            "6137: [discriminator loss: 0.7285005450248718, acc: 0.1015625] [gan loss: 1.553006, acc: 0.000000]\n",
            "6138: [discriminator loss: 0.701750636100769, acc: 0.3828125] [gan loss: 0.878612, acc: 0.203125]\n",
            "6139: [discriminator loss: 0.7024092674255371, acc: 0.046875] [gan loss: 1.471113, acc: 0.000000]\n",
            "6140: [discriminator loss: 0.6541457176208496, acc: 0.421875] [gan loss: 0.860086, acc: 0.234375]\n",
            "6141: [discriminator loss: 0.7220348119735718, acc: 0.0703125] [gan loss: 1.401836, acc: 0.000000]\n",
            "6142: [discriminator loss: 0.6603868007659912, acc: 0.359375] [gan loss: 0.912688, acc: 0.140625]\n",
            "6143: [discriminator loss: 0.7182148694992065, acc: 0.0859375] [gan loss: 1.566162, acc: 0.000000]\n",
            "6144: [discriminator loss: 0.6621960401535034, acc: 0.375] [gan loss: 0.937607, acc: 0.140625]\n",
            "6145: [discriminator loss: 0.7016989588737488, acc: 0.140625] [gan loss: 1.453288, acc: 0.000000]\n",
            "6146: [discriminator loss: 0.6916977167129517, acc: 0.3125] [gan loss: 0.979552, acc: 0.140625]\n",
            "6147: [discriminator loss: 0.6931697130203247, acc: 0.1484375] [gan loss: 1.470149, acc: 0.015625]\n",
            "6148: [discriminator loss: 0.6918096542358398, acc: 0.390625] [gan loss: 1.046056, acc: 0.062500]\n",
            "6149: [discriminator loss: 0.714424192905426, acc: 0.109375] [gan loss: 1.655297, acc: 0.000000]\n",
            "6150: [discriminator loss: 0.7088708877563477, acc: 0.375] [gan loss: 0.789844, acc: 0.375000]\n",
            "6151: [discriminator loss: 0.7153147459030151, acc: 0.0703125] [gan loss: 1.637996, acc: 0.000000]\n",
            "6152: [discriminator loss: 0.6850652694702148, acc: 0.453125] [gan loss: 0.746669, acc: 0.421875]\n",
            "6153: [discriminator loss: 0.7783243060112, acc: 0.0390625] [gan loss: 2.046972, acc: 0.000000]\n",
            "6154: [discriminator loss: 0.6802821755409241, acc: 0.4765625] [gan loss: 0.611928, acc: 0.671875]\n",
            "6155: [discriminator loss: 0.8578522801399231, acc: 0.0390625] [gan loss: 1.894085, acc: 0.000000]\n",
            "6156: [discriminator loss: 0.710802435874939, acc: 0.4765625] [gan loss: 0.530676, acc: 0.859375]\n",
            "6157: [discriminator loss: 0.846625566482544, acc: 0.0078125] [gan loss: 1.701070, acc: 0.000000]\n",
            "6158: [discriminator loss: 0.7133135199546814, acc: 0.4765625] [gan loss: 0.667645, acc: 0.531250]\n",
            "6159: [discriminator loss: 0.8257294297218323, acc: 0.0] [gan loss: 1.758213, acc: 0.000000]\n",
            "6160: [discriminator loss: 0.6781550049781799, acc: 0.4765625] [gan loss: 0.759472, acc: 0.453125]\n",
            "6161: [discriminator loss: 0.740558922290802, acc: 0.0546875] [gan loss: 1.563216, acc: 0.000000]\n",
            "6162: [discriminator loss: 0.6784746646881104, acc: 0.3828125] [gan loss: 0.891324, acc: 0.187500]\n",
            "6163: [discriminator loss: 0.733026385307312, acc: 0.0859375] [gan loss: 1.392704, acc: 0.000000]\n",
            "6164: [discriminator loss: 0.7051466703414917, acc: 0.3046875] [gan loss: 0.867484, acc: 0.203125]\n",
            "6165: [discriminator loss: 0.7324198484420776, acc: 0.0859375] [gan loss: 1.413848, acc: 0.015625]\n",
            "6166: [discriminator loss: 0.6718226671218872, acc: 0.4375] [gan loss: 0.826585, acc: 0.312500]\n",
            "6167: [discriminator loss: 0.763995885848999, acc: 0.03125] [gan loss: 1.797881, acc: 0.000000]\n",
            "6168: [discriminator loss: 0.6853576898574829, acc: 0.4921875] [gan loss: 0.568289, acc: 0.750000]\n",
            "6169: [discriminator loss: 0.7833807468414307, acc: 0.0] [gan loss: 1.849433, acc: 0.000000]\n",
            "6170: [discriminator loss: 0.691997766494751, acc: 0.421875] [gan loss: 0.661098, acc: 0.687500]\n",
            "6171: [discriminator loss: 0.787326455116272, acc: 0.046875] [gan loss: 1.504249, acc: 0.000000]\n",
            "6172: [discriminator loss: 0.6861029863357544, acc: 0.3984375] [gan loss: 0.855299, acc: 0.281250]\n",
            "6173: [discriminator loss: 0.748939037322998, acc: 0.046875] [gan loss: 1.414360, acc: 0.000000]\n",
            "6174: [discriminator loss: 0.6947194337844849, acc: 0.3828125] [gan loss: 0.811455, acc: 0.328125]\n",
            "6175: [discriminator loss: 0.771620512008667, acc: 0.0546875] [gan loss: 1.546917, acc: 0.000000]\n",
            "6176: [discriminator loss: 0.6633229851722717, acc: 0.4140625] [gan loss: 0.783065, acc: 0.406250]\n",
            "6177: [discriminator loss: 0.7392892837524414, acc: 0.046875] [gan loss: 1.427276, acc: 0.000000]\n",
            "6178: [discriminator loss: 0.6796801090240479, acc: 0.390625] [gan loss: 0.838141, acc: 0.296875]\n",
            "6179: [discriminator loss: 0.7349591851234436, acc: 0.0859375] [gan loss: 1.573391, acc: 0.000000]\n",
            "6180: [discriminator loss: 0.6854761838912964, acc: 0.421875] [gan loss: 0.869816, acc: 0.218750]\n",
            "6181: [discriminator loss: 0.7255406975746155, acc: 0.078125] [gan loss: 1.540006, acc: 0.000000]\n",
            "6182: [discriminator loss: 0.6883636713027954, acc: 0.3515625] [gan loss: 0.989977, acc: 0.093750]\n",
            "6183: [discriminator loss: 0.6978211402893066, acc: 0.109375] [gan loss: 1.646765, acc: 0.000000]\n",
            "6184: [discriminator loss: 0.700347900390625, acc: 0.4140625] [gan loss: 0.786405, acc: 0.312500]\n",
            "6185: [discriminator loss: 0.7556785345077515, acc: 0.0234375] [gan loss: 1.742134, acc: 0.000000]\n",
            "6186: [discriminator loss: 0.6669609546661377, acc: 0.4921875] [gan loss: 0.552188, acc: 0.859375]\n",
            "6187: [discriminator loss: 0.8698567152023315, acc: 0.0] [gan loss: 2.139622, acc: 0.000000]\n",
            "6188: [discriminator loss: 0.7145480513572693, acc: 0.4765625] [gan loss: 0.579484, acc: 0.703125]\n",
            "6189: [discriminator loss: 0.810481607913971, acc: 0.0] [gan loss: 1.605712, acc: 0.000000]\n",
            "6190: [discriminator loss: 0.6796297430992126, acc: 0.40625] [gan loss: 0.759917, acc: 0.343750]\n",
            "6191: [discriminator loss: 0.8045654892921448, acc: 0.0703125] [gan loss: 1.537166, acc: 0.015625]\n",
            "6192: [discriminator loss: 0.6815159320831299, acc: 0.40625] [gan loss: 0.835159, acc: 0.234375]\n",
            "6193: [discriminator loss: 0.7437466382980347, acc: 0.0390625] [gan loss: 1.559866, acc: 0.000000]\n",
            "6194: [discriminator loss: 0.6627770662307739, acc: 0.375] [gan loss: 0.963034, acc: 0.078125]\n",
            "6195: [discriminator loss: 0.7350736856460571, acc: 0.078125] [gan loss: 1.583778, acc: 0.000000]\n",
            "6196: [discriminator loss: 0.650266170501709, acc: 0.4609375] [gan loss: 0.859632, acc: 0.218750]\n",
            "6197: [discriminator loss: 0.7551525831222534, acc: 0.03125] [gan loss: 1.583855, acc: 0.000000]\n",
            "6198: [discriminator loss: 0.6655229330062866, acc: 0.4296875] [gan loss: 0.842456, acc: 0.218750]\n",
            "6199: [discriminator loss: 0.7524343729019165, acc: 0.0234375] [gan loss: 1.807048, acc: 0.000000]\n",
            "6200: [discriminator loss: 0.6617115139961243, acc: 0.484375] [gan loss: 0.687270, acc: 0.515625]\n",
            "6201: [discriminator loss: 0.7662307024002075, acc: 0.0390625] [gan loss: 1.572417, acc: 0.000000]\n",
            "6202: [discriminator loss: 0.6808149814605713, acc: 0.4296875] [gan loss: 0.832736, acc: 0.312500]\n",
            "6203: [discriminator loss: 0.7283327579498291, acc: 0.0703125] [gan loss: 1.682510, acc: 0.000000]\n",
            "6204: [discriminator loss: 0.6755460500717163, acc: 0.4296875] [gan loss: 0.775349, acc: 0.359375]\n",
            "6205: [discriminator loss: 0.7281519174575806, acc: 0.046875] [gan loss: 1.536337, acc: 0.000000]\n",
            "6206: [discriminator loss: 0.7067930102348328, acc: 0.40625] [gan loss: 0.806162, acc: 0.343750]\n",
            "6207: [discriminator loss: 0.7864620685577393, acc: 0.015625] [gan loss: 1.834975, acc: 0.000000]\n",
            "6208: [discriminator loss: 0.71897292137146, acc: 0.4296875] [gan loss: 0.656773, acc: 0.546875]\n",
            "6209: [discriminator loss: 0.7924532890319824, acc: 0.015625] [gan loss: 1.543982, acc: 0.000000]\n",
            "6210: [discriminator loss: 0.6492575407028198, acc: 0.453125] [gan loss: 0.735852, acc: 0.453125]\n",
            "6211: [discriminator loss: 0.7505483627319336, acc: 0.0546875] [gan loss: 1.649455, acc: 0.000000]\n",
            "6212: [discriminator loss: 0.6682369112968445, acc: 0.4453125] [gan loss: 0.721896, acc: 0.484375]\n",
            "6213: [discriminator loss: 0.763745903968811, acc: 0.046875] [gan loss: 1.538252, acc: 0.000000]\n",
            "6214: [discriminator loss: 0.6742294430732727, acc: 0.40625] [gan loss: 0.876137, acc: 0.265625]\n",
            "6215: [discriminator loss: 0.7272165417671204, acc: 0.1484375] [gan loss: 1.517260, acc: 0.000000]\n",
            "6216: [discriminator loss: 0.6487221717834473, acc: 0.4296875] [gan loss: 0.871952, acc: 0.234375]\n",
            "6217: [discriminator loss: 0.742333173751831, acc: 0.0390625] [gan loss: 1.796966, acc: 0.000000]\n",
            "6218: [discriminator loss: 0.6987650394439697, acc: 0.4453125] [gan loss: 0.742548, acc: 0.437500]\n",
            "6219: [discriminator loss: 0.788176417350769, acc: 0.0234375] [gan loss: 1.769975, acc: 0.000000]\n",
            "6220: [discriminator loss: 0.7242820262908936, acc: 0.4609375] [gan loss: 0.628608, acc: 0.656250]\n",
            "6221: [discriminator loss: 0.7950801849365234, acc: 0.0] [gan loss: 1.746100, acc: 0.000000]\n",
            "6222: [discriminator loss: 0.707268238067627, acc: 0.4375] [gan loss: 0.693774, acc: 0.515625]\n",
            "6223: [discriminator loss: 0.7462735176086426, acc: 0.046875] [gan loss: 1.470714, acc: 0.000000]\n",
            "6224: [discriminator loss: 0.6826767921447754, acc: 0.40625] [gan loss: 0.845004, acc: 0.265625]\n",
            "6225: [discriminator loss: 0.7280805706977844, acc: 0.0703125] [gan loss: 1.613651, acc: 0.000000]\n",
            "6226: [discriminator loss: 0.7252893447875977, acc: 0.3671875] [gan loss: 0.918840, acc: 0.109375]\n",
            "6227: [discriminator loss: 0.7241014242172241, acc: 0.0703125] [gan loss: 1.490280, acc: 0.000000]\n",
            "6228: [discriminator loss: 0.6914671063423157, acc: 0.375] [gan loss: 0.771682, acc: 0.328125]\n",
            "6229: [discriminator loss: 0.7417774200439453, acc: 0.03125] [gan loss: 1.744494, acc: 0.000000]\n",
            "6230: [discriminator loss: 0.6466160416603088, acc: 0.4921875] [gan loss: 0.703022, acc: 0.484375]\n",
            "6231: [discriminator loss: 0.7669618725776672, acc: 0.0234375] [gan loss: 1.733006, acc: 0.015625]\n",
            "6232: [discriminator loss: 0.7053065896034241, acc: 0.390625] [gan loss: 0.646599, acc: 0.656250]\n",
            "6233: [discriminator loss: 0.7924594879150391, acc: 0.015625] [gan loss: 1.736966, acc: 0.000000]\n",
            "6234: [discriminator loss: 0.6541920900344849, acc: 0.4765625] [gan loss: 0.751307, acc: 0.406250]\n",
            "6235: [discriminator loss: 0.7586943507194519, acc: 0.015625] [gan loss: 1.627234, acc: 0.000000]\n",
            "6236: [discriminator loss: 0.6970668435096741, acc: 0.4453125] [gan loss: 0.686213, acc: 0.531250]\n",
            "6237: [discriminator loss: 0.7452365756034851, acc: 0.0546875] [gan loss: 1.525061, acc: 0.000000]\n",
            "6238: [discriminator loss: 0.657231330871582, acc: 0.421875] [gan loss: 0.742607, acc: 0.468750]\n",
            "6239: [discriminator loss: 0.841503918170929, acc: 0.015625] [gan loss: 1.628655, acc: 0.015625]\n",
            "6240: [discriminator loss: 0.7039183974266052, acc: 0.4453125] [gan loss: 0.790543, acc: 0.328125]\n",
            "6241: [discriminator loss: 0.7324228882789612, acc: 0.078125] [gan loss: 1.450776, acc: 0.000000]\n",
            "6242: [discriminator loss: 0.6607229709625244, acc: 0.3828125] [gan loss: 0.895477, acc: 0.203125]\n",
            "6243: [discriminator loss: 0.7567747831344604, acc: 0.1015625] [gan loss: 1.648154, acc: 0.000000]\n",
            "6244: [discriminator loss: 0.6647257208824158, acc: 0.4140625] [gan loss: 0.827243, acc: 0.296875]\n",
            "6245: [discriminator loss: 0.7442789673805237, acc: 0.0703125] [gan loss: 1.388668, acc: 0.000000]\n",
            "6246: [discriminator loss: 0.6865818500518799, acc: 0.4140625] [gan loss: 0.807921, acc: 0.343750]\n",
            "6247: [discriminator loss: 0.7507451176643372, acc: 0.078125] [gan loss: 1.596069, acc: 0.000000]\n",
            "6248: [discriminator loss: 0.6854625940322876, acc: 0.4375] [gan loss: 0.868402, acc: 0.171875]\n",
            "6249: [discriminator loss: 0.7231892347335815, acc: 0.0625] [gan loss: 1.494080, acc: 0.000000]\n",
            "6250: [discriminator loss: 0.6686484813690186, acc: 0.4140625] [gan loss: 0.880123, acc: 0.171875]\n",
            "6251: [discriminator loss: 0.7388153076171875, acc: 0.078125] [gan loss: 1.687430, acc: 0.000000]\n",
            "6252: [discriminator loss: 0.7081843614578247, acc: 0.453125] [gan loss: 0.647224, acc: 0.609375]\n",
            "6253: [discriminator loss: 0.7578691244125366, acc: 0.046875] [gan loss: 1.666794, acc: 0.000000]\n",
            "6254: [discriminator loss: 0.6832250356674194, acc: 0.4609375] [gan loss: 0.616225, acc: 0.703125]\n",
            "6255: [discriminator loss: 0.8060905337333679, acc: 0.0078125] [gan loss: 1.708816, acc: 0.000000]\n",
            "6256: [discriminator loss: 0.6948704719543457, acc: 0.453125] [gan loss: 0.623386, acc: 0.687500]\n",
            "6257: [discriminator loss: 0.7827768325805664, acc: 0.03125] [gan loss: 1.628261, acc: 0.000000]\n",
            "6258: [discriminator loss: 0.6682146787643433, acc: 0.4609375] [gan loss: 0.753640, acc: 0.453125]\n",
            "6259: [discriminator loss: 0.7596720457077026, acc: 0.0625] [gan loss: 1.577681, acc: 0.000000]\n",
            "6260: [discriminator loss: 0.6777216792106628, acc: 0.4453125] [gan loss: 0.780091, acc: 0.328125]\n",
            "6261: [discriminator loss: 0.803459644317627, acc: 0.0390625] [gan loss: 1.660464, acc: 0.000000]\n",
            "6262: [discriminator loss: 0.6914733648300171, acc: 0.421875] [gan loss: 0.786152, acc: 0.296875]\n",
            "6263: [discriminator loss: 0.7460212707519531, acc: 0.0390625] [gan loss: 1.666140, acc: 0.000000]\n",
            "6264: [discriminator loss: 0.6776230335235596, acc: 0.4609375] [gan loss: 0.751239, acc: 0.437500]\n",
            "6265: [discriminator loss: 0.7531453967094421, acc: 0.0546875] [gan loss: 1.604402, acc: 0.000000]\n",
            "6266: [discriminator loss: 0.7027285099029541, acc: 0.4375] [gan loss: 0.707631, acc: 0.468750]\n",
            "6267: [discriminator loss: 0.7997803688049316, acc: 0.0078125] [gan loss: 1.816015, acc: 0.000000]\n",
            "6268: [discriminator loss: 0.6877354979515076, acc: 0.4921875] [gan loss: 0.673582, acc: 0.562500]\n",
            "6269: [discriminator loss: 0.7842797636985779, acc: 0.015625] [gan loss: 1.609165, acc: 0.000000]\n",
            "6270: [discriminator loss: 0.7019037008285522, acc: 0.4375] [gan loss: 0.731925, acc: 0.468750]\n",
            "6271: [discriminator loss: 0.7712518572807312, acc: 0.0] [gan loss: 1.589272, acc: 0.000000]\n",
            "6272: [discriminator loss: 0.6690584421157837, acc: 0.4375] [gan loss: 0.825373, acc: 0.250000]\n",
            "6273: [discriminator loss: 0.7402319312095642, acc: 0.0390625] [gan loss: 1.544765, acc: 0.000000]\n",
            "6274: [discriminator loss: 0.6661550998687744, acc: 0.453125] [gan loss: 0.767884, acc: 0.375000]\n",
            "6275: [discriminator loss: 0.7500139474868774, acc: 0.0390625] [gan loss: 1.503646, acc: 0.000000]\n",
            "6276: [discriminator loss: 0.6647452116012573, acc: 0.46875] [gan loss: 0.754597, acc: 0.406250]\n",
            "6277: [discriminator loss: 0.796028196811676, acc: 0.0234375] [gan loss: 1.511746, acc: 0.000000]\n",
            "6278: [discriminator loss: 0.7111160755157471, acc: 0.3984375] [gan loss: 0.882731, acc: 0.218750]\n",
            "6279: [discriminator loss: 0.7288079857826233, acc: 0.0546875] [gan loss: 1.567399, acc: 0.000000]\n",
            "6280: [discriminator loss: 0.6996337175369263, acc: 0.40625] [gan loss: 0.742468, acc: 0.468750]\n",
            "6281: [discriminator loss: 0.7492046356201172, acc: 0.0625] [gan loss: 1.481251, acc: 0.000000]\n",
            "6282: [discriminator loss: 0.6890581846237183, acc: 0.4609375] [gan loss: 0.770491, acc: 0.390625]\n",
            "6283: [discriminator loss: 0.7456778883934021, acc: 0.0859375] [gan loss: 1.576154, acc: 0.000000]\n",
            "6284: [discriminator loss: 0.6800745129585266, acc: 0.4140625] [gan loss: 0.780539, acc: 0.312500]\n",
            "6285: [discriminator loss: 0.7320113778114319, acc: 0.046875] [gan loss: 1.429860, acc: 0.000000]\n",
            "6286: [discriminator loss: 0.6964284181594849, acc: 0.40625] [gan loss: 0.900735, acc: 0.125000]\n",
            "6287: [discriminator loss: 0.7765133380889893, acc: 0.078125] [gan loss: 1.579561, acc: 0.000000]\n",
            "6288: [discriminator loss: 0.6809341907501221, acc: 0.4453125] [gan loss: 0.823221, acc: 0.250000]\n",
            "6289: [discriminator loss: 0.7638360261917114, acc: 0.03125] [gan loss: 1.679313, acc: 0.000000]\n",
            "6290: [discriminator loss: 0.6604474186897278, acc: 0.4296875] [gan loss: 0.709105, acc: 0.500000]\n",
            "6291: [discriminator loss: 0.787665843963623, acc: 0.0078125] [gan loss: 1.640563, acc: 0.000000]\n",
            "6292: [discriminator loss: 0.6948933005332947, acc: 0.4609375] [gan loss: 0.666619, acc: 0.578125]\n",
            "6293: [discriminator loss: 0.8469401597976685, acc: 0.0078125] [gan loss: 1.625564, acc: 0.000000]\n",
            "6294: [discriminator loss: 0.6839050054550171, acc: 0.46875] [gan loss: 0.646526, acc: 0.687500]\n",
            "6295: [discriminator loss: 0.7826952338218689, acc: 0.046875] [gan loss: 1.583983, acc: 0.000000]\n",
            "6296: [discriminator loss: 0.6932340264320374, acc: 0.4140625] [gan loss: 0.886237, acc: 0.187500]\n",
            "6297: [discriminator loss: 0.7428321242332458, acc: 0.046875] [gan loss: 1.509283, acc: 0.000000]\n",
            "6298: [discriminator loss: 0.6908003091812134, acc: 0.3359375] [gan loss: 0.957551, acc: 0.171875]\n",
            "6299: [discriminator loss: 0.7314855456352234, acc: 0.0703125] [gan loss: 1.371020, acc: 0.000000]\n",
            "6300: [discriminator loss: 0.6817168593406677, acc: 0.3984375] [gan loss: 0.861420, acc: 0.234375]\n",
            "6301: [discriminator loss: 0.7306504249572754, acc: 0.03125] [gan loss: 1.432251, acc: 0.000000]\n",
            "6302: [discriminator loss: 0.6774481534957886, acc: 0.375] [gan loss: 0.890254, acc: 0.140625]\n",
            "6303: [discriminator loss: 0.7344613075256348, acc: 0.0859375] [gan loss: 1.610079, acc: 0.000000]\n",
            "6304: [discriminator loss: 0.7221431732177734, acc: 0.375] [gan loss: 0.793906, acc: 0.328125]\n",
            "6305: [discriminator loss: 0.7641761302947998, acc: 0.0390625] [gan loss: 1.707568, acc: 0.000000]\n",
            "6306: [discriminator loss: 0.7037124633789062, acc: 0.4609375] [gan loss: 0.685393, acc: 0.531250]\n",
            "6307: [discriminator loss: 0.7838099002838135, acc: 0.015625] [gan loss: 1.662527, acc: 0.000000]\n",
            "6308: [discriminator loss: 0.6529468894004822, acc: 0.46875] [gan loss: 0.672292, acc: 0.593750]\n",
            "6309: [discriminator loss: 0.8128897547721863, acc: 0.0234375] [gan loss: 1.738050, acc: 0.000000]\n",
            "6310: [discriminator loss: 0.6814428567886353, acc: 0.46875] [gan loss: 0.682580, acc: 0.531250]\n",
            "6311: [discriminator loss: 0.7602572441101074, acc: 0.0859375] [gan loss: 1.497394, acc: 0.000000]\n",
            "6312: [discriminator loss: 0.6740695834159851, acc: 0.375] [gan loss: 0.902984, acc: 0.203125]\n",
            "6313: [discriminator loss: 0.7071405649185181, acc: 0.140625] [gan loss: 1.488646, acc: 0.000000]\n",
            "6314: [discriminator loss: 0.6701222658157349, acc: 0.390625] [gan loss: 0.900980, acc: 0.265625]\n",
            "6315: [discriminator loss: 0.7383288741111755, acc: 0.109375] [gan loss: 1.446799, acc: 0.000000]\n",
            "6316: [discriminator loss: 0.6524937152862549, acc: 0.40625] [gan loss: 0.912679, acc: 0.203125]\n",
            "6317: [discriminator loss: 0.6844629049301147, acc: 0.15625] [gan loss: 1.504194, acc: 0.000000]\n",
            "6318: [discriminator loss: 0.6554712653160095, acc: 0.375] [gan loss: 0.910999, acc: 0.203125]\n",
            "6319: [discriminator loss: 0.7754131555557251, acc: 0.078125] [gan loss: 1.798867, acc: 0.000000]\n",
            "6320: [discriminator loss: 0.703546404838562, acc: 0.4765625] [gan loss: 0.665113, acc: 0.562500]\n",
            "6321: [discriminator loss: 0.7898313403129578, acc: 0.0] [gan loss: 1.766840, acc: 0.000000]\n",
            "6322: [discriminator loss: 0.6731741428375244, acc: 0.4921875] [gan loss: 0.552177, acc: 0.843750]\n",
            "6323: [discriminator loss: 0.8330899477005005, acc: 0.0234375] [gan loss: 1.831947, acc: 0.000000]\n",
            "6324: [discriminator loss: 0.716328501701355, acc: 0.46875] [gan loss: 0.697626, acc: 0.531250]\n",
            "6325: [discriminator loss: 0.7745332717895508, acc: 0.0234375] [gan loss: 1.516871, acc: 0.000000]\n",
            "6326: [discriminator loss: 0.6932095885276794, acc: 0.3828125] [gan loss: 0.792196, acc: 0.265625]\n",
            "6327: [discriminator loss: 0.7728567123413086, acc: 0.03125] [gan loss: 1.530282, acc: 0.000000]\n",
            "6328: [discriminator loss: 0.6692290902137756, acc: 0.4296875] [gan loss: 0.806522, acc: 0.312500]\n",
            "6329: [discriminator loss: 0.7345595359802246, acc: 0.046875] [gan loss: 1.484127, acc: 0.000000]\n",
            "6330: [discriminator loss: 0.667086660861969, acc: 0.3671875] [gan loss: 0.953454, acc: 0.140625]\n",
            "6331: [discriminator loss: 0.7058328986167908, acc: 0.1484375] [gan loss: 1.473678, acc: 0.000000]\n",
            "6332: [discriminator loss: 0.7000141143798828, acc: 0.375] [gan loss: 0.766732, acc: 0.406250]\n",
            "6333: [discriminator loss: 0.7639133334159851, acc: 0.0546875] [gan loss: 1.595382, acc: 0.000000]\n",
            "6334: [discriminator loss: 0.714741587638855, acc: 0.4453125] [gan loss: 0.756856, acc: 0.406250]\n",
            "6335: [discriminator loss: 0.7840690016746521, acc: 0.0234375] [gan loss: 1.779764, acc: 0.000000]\n",
            "6336: [discriminator loss: 0.68607497215271, acc: 0.46875] [gan loss: 0.689645, acc: 0.546875]\n",
            "6337: [discriminator loss: 0.760979175567627, acc: 0.015625] [gan loss: 1.791311, acc: 0.000000]\n",
            "6338: [discriminator loss: 0.7111579179763794, acc: 0.453125] [gan loss: 0.633092, acc: 0.640625]\n",
            "6339: [discriminator loss: 0.7849317789077759, acc: 0.015625] [gan loss: 1.631372, acc: 0.000000]\n",
            "6340: [discriminator loss: 0.6891160011291504, acc: 0.4609375] [gan loss: 0.689352, acc: 0.578125]\n",
            "6341: [discriminator loss: 0.7508832216262817, acc: 0.0234375] [gan loss: 1.509407, acc: 0.000000]\n",
            "6342: [discriminator loss: 0.6779783368110657, acc: 0.390625] [gan loss: 0.900681, acc: 0.171875]\n",
            "6343: [discriminator loss: 0.7053098082542419, acc: 0.1171875] [gan loss: 1.270427, acc: 0.000000]\n",
            "6344: [discriminator loss: 0.6808900237083435, acc: 0.3671875] [gan loss: 0.872787, acc: 0.171875]\n",
            "6345: [discriminator loss: 0.7350413799285889, acc: 0.0703125] [gan loss: 1.440048, acc: 0.000000]\n",
            "6346: [discriminator loss: 0.7066663503646851, acc: 0.390625] [gan loss: 0.823275, acc: 0.234375]\n",
            "6347: [discriminator loss: 0.7445391416549683, acc: 0.078125] [gan loss: 1.527259, acc: 0.000000]\n",
            "6348: [discriminator loss: 0.680091381072998, acc: 0.4296875] [gan loss: 0.824964, acc: 0.281250]\n",
            "6349: [discriminator loss: 0.7619532346725464, acc: 0.046875] [gan loss: 1.581671, acc: 0.000000]\n",
            "6350: [discriminator loss: 0.6879188418388367, acc: 0.4609375] [gan loss: 0.734202, acc: 0.453125]\n",
            "6351: [discriminator loss: 0.781164824962616, acc: 0.0234375] [gan loss: 1.918349, acc: 0.000000]\n",
            "6352: [discriminator loss: 0.7172533273696899, acc: 0.46875] [gan loss: 0.557493, acc: 0.828125]\n",
            "6353: [discriminator loss: 0.8133033514022827, acc: 0.015625] [gan loss: 1.640079, acc: 0.000000]\n",
            "6354: [discriminator loss: 0.7223453521728516, acc: 0.4609375] [gan loss: 0.714538, acc: 0.546875]\n",
            "6355: [discriminator loss: 0.7604541182518005, acc: 0.046875] [gan loss: 1.539212, acc: 0.000000]\n",
            "6356: [discriminator loss: 0.680385172367096, acc: 0.4375] [gan loss: 0.796720, acc: 0.296875]\n",
            "6357: [discriminator loss: 0.7799512147903442, acc: 0.0703125] [gan loss: 1.446922, acc: 0.000000]\n",
            "6358: [discriminator loss: 0.6790129542350769, acc: 0.359375] [gan loss: 0.931241, acc: 0.125000]\n",
            "6359: [discriminator loss: 0.7507150173187256, acc: 0.078125] [gan loss: 1.658907, acc: 0.000000]\n",
            "6360: [discriminator loss: 0.6729705333709717, acc: 0.4609375] [gan loss: 0.795509, acc: 0.328125]\n",
            "6361: [discriminator loss: 0.7614226341247559, acc: 0.0546875] [gan loss: 1.628909, acc: 0.000000]\n",
            "6362: [discriminator loss: 0.6626188158988953, acc: 0.4453125] [gan loss: 0.785904, acc: 0.312500]\n",
            "6363: [discriminator loss: 0.7412980794906616, acc: 0.046875] [gan loss: 1.649262, acc: 0.000000]\n",
            "6364: [discriminator loss: 0.6908600330352783, acc: 0.46875] [gan loss: 0.676059, acc: 0.500000]\n",
            "6365: [discriminator loss: 0.7489432096481323, acc: 0.015625] [gan loss: 1.662840, acc: 0.000000]\n",
            "6366: [discriminator loss: 0.6947874426841736, acc: 0.4375] [gan loss: 0.720773, acc: 0.484375]\n",
            "6367: [discriminator loss: 0.7886733412742615, acc: 0.0390625] [gan loss: 1.701572, acc: 0.000000]\n",
            "6368: [discriminator loss: 0.6788358688354492, acc: 0.4609375] [gan loss: 0.772572, acc: 0.328125]\n",
            "6369: [discriminator loss: 0.7661214470863342, acc: 0.046875] [gan loss: 1.605205, acc: 0.000000]\n",
            "6370: [discriminator loss: 0.6786304712295532, acc: 0.4140625] [gan loss: 0.763060, acc: 0.375000]\n",
            "6371: [discriminator loss: 0.745625376701355, acc: 0.078125] [gan loss: 1.422123, acc: 0.000000]\n",
            "6372: [discriminator loss: 0.6652373671531677, acc: 0.3984375] [gan loss: 0.860771, acc: 0.203125]\n",
            "6373: [discriminator loss: 0.7293822169303894, acc: 0.15625] [gan loss: 1.382353, acc: 0.000000]\n",
            "6374: [discriminator loss: 0.6922654509544373, acc: 0.34375] [gan loss: 1.045363, acc: 0.078125]\n",
            "6375: [discriminator loss: 0.6744886636734009, acc: 0.2109375] [gan loss: 1.364600, acc: 0.000000]\n",
            "6376: [discriminator loss: 0.6990933418273926, acc: 0.34375] [gan loss: 0.819210, acc: 0.218750]\n",
            "6377: [discriminator loss: 0.7415750622749329, acc: 0.046875] [gan loss: 1.704606, acc: 0.000000]\n",
            "6378: [discriminator loss: 0.681655764579773, acc: 0.4765625] [gan loss: 0.644534, acc: 0.625000]\n",
            "6379: [discriminator loss: 0.8182436227798462, acc: 0.0] [gan loss: 1.894158, acc: 0.000000]\n",
            "6380: [discriminator loss: 0.7289296388626099, acc: 0.484375] [gan loss: 0.522236, acc: 0.812500]\n",
            "6381: [discriminator loss: 0.8558549284934998, acc: 0.0078125] [gan loss: 1.444015, acc: 0.000000]\n",
            "6382: [discriminator loss: 0.6852565407752991, acc: 0.375] [gan loss: 0.778317, acc: 0.406250]\n",
            "6383: [discriminator loss: 0.7792788147926331, acc: 0.0546875] [gan loss: 1.445971, acc: 0.000000]\n",
            "6384: [discriminator loss: 0.6793174147605896, acc: 0.3671875] [gan loss: 0.903983, acc: 0.187500]\n",
            "6385: [discriminator loss: 0.7209170460700989, acc: 0.0625] [gan loss: 1.622642, acc: 0.000000]\n",
            "6386: [discriminator loss: 0.7017041444778442, acc: 0.3828125] [gan loss: 0.793895, acc: 0.296875]\n",
            "6387: [discriminator loss: 0.7285171747207642, acc: 0.078125] [gan loss: 1.481764, acc: 0.000000]\n",
            "6388: [discriminator loss: 0.6886038780212402, acc: 0.4296875] [gan loss: 0.733304, acc: 0.437500]\n",
            "6389: [discriminator loss: 0.7486672401428223, acc: 0.046875] [gan loss: 1.506294, acc: 0.000000]\n",
            "6390: [discriminator loss: 0.7232738733291626, acc: 0.3671875] [gan loss: 0.951101, acc: 0.140625]\n",
            "6391: [discriminator loss: 0.7053510546684265, acc: 0.125] [gan loss: 1.630306, acc: 0.000000]\n",
            "6392: [discriminator loss: 0.6713566780090332, acc: 0.4140625] [gan loss: 0.904131, acc: 0.156250]\n",
            "6393: [discriminator loss: 0.7252994179725647, acc: 0.1015625] [gan loss: 1.359456, acc: 0.000000]\n",
            "6394: [discriminator loss: 0.6680905818939209, acc: 0.34375] [gan loss: 0.946341, acc: 0.125000]\n",
            "6395: [discriminator loss: 0.6871595978736877, acc: 0.1484375] [gan loss: 1.599562, acc: 0.000000]\n",
            "6396: [discriminator loss: 0.6907678842544556, acc: 0.4296875] [gan loss: 0.829113, acc: 0.265625]\n",
            "6397: [discriminator loss: 0.7431852221488953, acc: 0.03125] [gan loss: 1.597006, acc: 0.000000]\n",
            "6398: [discriminator loss: 0.6952791810035706, acc: 0.4375] [gan loss: 0.788566, acc: 0.359375]\n",
            "6399: [discriminator loss: 0.7799986600875854, acc: 0.046875] [gan loss: 1.714664, acc: 0.000000]\n",
            "6400: [discriminator loss: 0.7105884552001953, acc: 0.4609375] [gan loss: 0.610577, acc: 0.718750]\n",
            "6401: [discriminator loss: 0.8267110586166382, acc: 0.0078125] [gan loss: 1.854506, acc: 0.000000]\n",
            "6402: [discriminator loss: 0.7000706195831299, acc: 0.484375] [gan loss: 0.610258, acc: 0.734375]\n",
            "6403: [discriminator loss: 0.8040465116500854, acc: 0.015625] [gan loss: 1.713028, acc: 0.000000]\n",
            "6404: [discriminator loss: 0.677844226360321, acc: 0.4296875] [gan loss: 0.841296, acc: 0.296875]\n",
            "6405: [discriminator loss: 0.7573681473731995, acc: 0.0546875] [gan loss: 1.539650, acc: 0.000000]\n",
            "6406: [discriminator loss: 0.6834668517112732, acc: 0.453125] [gan loss: 0.743273, acc: 0.468750]\n",
            "6407: [discriminator loss: 0.7385973930358887, acc: 0.0390625] [gan loss: 1.451310, acc: 0.000000]\n",
            "6408: [discriminator loss: 0.6733406782150269, acc: 0.3515625] [gan loss: 0.800948, acc: 0.375000]\n",
            "6409: [discriminator loss: 0.7491616010665894, acc: 0.046875] [gan loss: 1.582807, acc: 0.000000]\n",
            "6410: [discriminator loss: 0.6939530372619629, acc: 0.40625] [gan loss: 0.791257, acc: 0.406250]\n",
            "6411: [discriminator loss: 0.7791038751602173, acc: 0.015625] [gan loss: 1.582207, acc: 0.000000]\n",
            "6412: [discriminator loss: 0.6826286315917969, acc: 0.4453125] [gan loss: 0.782395, acc: 0.312500]\n",
            "6413: [discriminator loss: 0.7261068820953369, acc: 0.03125] [gan loss: 1.628116, acc: 0.000000]\n",
            "6414: [discriminator loss: 0.6999003291130066, acc: 0.4375] [gan loss: 0.690749, acc: 0.546875]\n",
            "6415: [discriminator loss: 0.764803409576416, acc: 0.03125] [gan loss: 1.590189, acc: 0.000000]\n",
            "6416: [discriminator loss: 0.6697619557380676, acc: 0.46875] [gan loss: 0.680240, acc: 0.562500]\n",
            "6417: [discriminator loss: 0.8084410429000854, acc: 0.03125] [gan loss: 1.724294, acc: 0.000000]\n",
            "6418: [discriminator loss: 0.7085102200508118, acc: 0.4453125] [gan loss: 0.704506, acc: 0.437500]\n",
            "6419: [discriminator loss: 0.7919954657554626, acc: 0.03125] [gan loss: 1.590714, acc: 0.000000]\n",
            "6420: [discriminator loss: 0.6908655762672424, acc: 0.453125] [gan loss: 0.743532, acc: 0.343750]\n",
            "6421: [discriminator loss: 0.7459127902984619, acc: 0.0390625] [gan loss: 1.631563, acc: 0.000000]\n",
            "6422: [discriminator loss: 0.695737361907959, acc: 0.40625] [gan loss: 0.805096, acc: 0.281250]\n",
            "6423: [discriminator loss: 0.732727587223053, acc: 0.046875] [gan loss: 1.589379, acc: 0.000000]\n",
            "6424: [discriminator loss: 0.7004673480987549, acc: 0.4453125] [gan loss: 0.701280, acc: 0.546875]\n",
            "6425: [discriminator loss: 0.7800294160842896, acc: 0.0234375] [gan loss: 1.590450, acc: 0.000000]\n",
            "6426: [discriminator loss: 0.6826095581054688, acc: 0.453125] [gan loss: 0.695887, acc: 0.500000]\n",
            "6427: [discriminator loss: 0.7472696304321289, acc: 0.015625] [gan loss: 1.582899, acc: 0.015625]\n",
            "6428: [discriminator loss: 0.687866747379303, acc: 0.4296875] [gan loss: 0.773385, acc: 0.406250]\n",
            "6429: [discriminator loss: 0.747068464756012, acc: 0.078125] [gan loss: 1.487698, acc: 0.000000]\n",
            "6430: [discriminator loss: 0.6919012665748596, acc: 0.3984375] [gan loss: 0.713536, acc: 0.484375]\n",
            "6431: [discriminator loss: 0.7836853265762329, acc: 0.0625] [gan loss: 1.580368, acc: 0.000000]\n",
            "6432: [discriminator loss: 0.6959925293922424, acc: 0.4140625] [gan loss: 0.839933, acc: 0.312500]\n",
            "6433: [discriminator loss: 0.7517002820968628, acc: 0.0859375] [gan loss: 1.497933, acc: 0.000000]\n",
            "6434: [discriminator loss: 0.6947123408317566, acc: 0.40625] [gan loss: 0.724366, acc: 0.500000]\n",
            "6435: [discriminator loss: 0.7652382850646973, acc: 0.0234375] [gan loss: 1.467923, acc: 0.000000]\n",
            "6436: [discriminator loss: 0.6718398332595825, acc: 0.421875] [gan loss: 0.844927, acc: 0.187500]\n",
            "6437: [discriminator loss: 0.7257101535797119, acc: 0.1015625] [gan loss: 1.566302, acc: 0.000000]\n",
            "6438: [discriminator loss: 0.6704574823379517, acc: 0.421875] [gan loss: 0.826588, acc: 0.234375]\n",
            "6439: [discriminator loss: 0.7427979707717896, acc: 0.046875] [gan loss: 1.617637, acc: 0.015625]\n",
            "6440: [discriminator loss: 0.6796788573265076, acc: 0.453125] [gan loss: 0.702766, acc: 0.546875]\n",
            "6441: [discriminator loss: 0.7762069702148438, acc: 0.0234375] [gan loss: 1.858985, acc: 0.000000]\n",
            "6442: [discriminator loss: 0.7086577415466309, acc: 0.46875] [gan loss: 0.644419, acc: 0.625000]\n",
            "6443: [discriminator loss: 0.8204103708267212, acc: 0.0] [gan loss: 1.760733, acc: 0.000000]\n",
            "6444: [discriminator loss: 0.6780696511268616, acc: 0.453125] [gan loss: 0.619879, acc: 0.734375]\n",
            "6445: [discriminator loss: 0.793303370475769, acc: 0.0] [gan loss: 1.529999, acc: 0.000000]\n",
            "6446: [discriminator loss: 0.6640495657920837, acc: 0.4375] [gan loss: 0.788439, acc: 0.281250]\n",
            "6447: [discriminator loss: 0.7575914263725281, acc: 0.015625] [gan loss: 1.568621, acc: 0.000000]\n",
            "6448: [discriminator loss: 0.6730941534042358, acc: 0.390625] [gan loss: 0.846101, acc: 0.281250]\n",
            "6449: [discriminator loss: 0.7812654972076416, acc: 0.0390625] [gan loss: 1.526954, acc: 0.000000]\n",
            "6450: [discriminator loss: 0.6838892698287964, acc: 0.453125] [gan loss: 0.788267, acc: 0.328125]\n",
            "6451: [discriminator loss: 0.7454881072044373, acc: 0.03125] [gan loss: 1.457030, acc: 0.000000]\n",
            "6452: [discriminator loss: 0.672742486000061, acc: 0.453125] [gan loss: 0.735964, acc: 0.453125]\n",
            "6453: [discriminator loss: 0.7812680006027222, acc: 0.046875] [gan loss: 1.670080, acc: 0.000000]\n",
            "6454: [discriminator loss: 0.665874719619751, acc: 0.453125] [gan loss: 0.767314, acc: 0.375000]\n",
            "6455: [discriminator loss: 0.766059398651123, acc: 0.0546875] [gan loss: 1.470725, acc: 0.000000]\n",
            "6456: [discriminator loss: 0.7292796969413757, acc: 0.390625] [gan loss: 0.798587, acc: 0.359375]\n",
            "6457: [discriminator loss: 0.7970260977745056, acc: 0.0390625] [gan loss: 1.528862, acc: 0.000000]\n",
            "6458: [discriminator loss: 0.6798584461212158, acc: 0.46875] [gan loss: 0.716276, acc: 0.421875]\n",
            "6459: [discriminator loss: 0.777470052242279, acc: 0.0078125] [gan loss: 1.736542, acc: 0.000000]\n",
            "6460: [discriminator loss: 0.6795847415924072, acc: 0.484375] [gan loss: 0.712284, acc: 0.515625]\n",
            "6461: [discriminator loss: 0.7536457777023315, acc: 0.0078125] [gan loss: 1.543409, acc: 0.000000]\n",
            "6462: [discriminator loss: 0.6898709535598755, acc: 0.40625] [gan loss: 0.759197, acc: 0.328125]\n",
            "6463: [discriminator loss: 0.735682487487793, acc: 0.0703125] [gan loss: 1.464439, acc: 0.000000]\n",
            "6464: [discriminator loss: 0.7012734413146973, acc: 0.3359375] [gan loss: 1.001011, acc: 0.062500]\n",
            "6465: [discriminator loss: 0.7288036346435547, acc: 0.09375] [gan loss: 1.520110, acc: 0.000000]\n",
            "6466: [discriminator loss: 0.6889535188674927, acc: 0.3203125] [gan loss: 0.929248, acc: 0.125000]\n",
            "6467: [discriminator loss: 0.7358887195587158, acc: 0.078125] [gan loss: 1.525744, acc: 0.000000]\n",
            "6468: [discriminator loss: 0.6800377368927002, acc: 0.4296875] [gan loss: 0.700438, acc: 0.500000]\n",
            "6469: [discriminator loss: 0.76451575756073, acc: 0.0390625] [gan loss: 1.651378, acc: 0.000000]\n",
            "6470: [discriminator loss: 0.683904767036438, acc: 0.4609375] [gan loss: 0.671617, acc: 0.609375]\n",
            "6471: [discriminator loss: 0.7909567356109619, acc: 0.015625] [gan loss: 1.672973, acc: 0.000000]\n",
            "6472: [discriminator loss: 0.7071937918663025, acc: 0.4609375] [gan loss: 0.688792, acc: 0.578125]\n",
            "6473: [discriminator loss: 0.8169829249382019, acc: 0.015625] [gan loss: 1.714316, acc: 0.000000]\n",
            "6474: [discriminator loss: 0.6962299346923828, acc: 0.4921875] [gan loss: 0.612789, acc: 0.656250]\n",
            "6475: [discriminator loss: 0.7713233232498169, acc: 0.0234375] [gan loss: 1.454650, acc: 0.000000]\n",
            "6476: [discriminator loss: 0.6809622049331665, acc: 0.3515625] [gan loss: 0.989981, acc: 0.078125]\n",
            "6477: [discriminator loss: 0.7238199710845947, acc: 0.078125] [gan loss: 1.465044, acc: 0.000000]\n",
            "6478: [discriminator loss: 0.6623836755752563, acc: 0.4453125] [gan loss: 0.789759, acc: 0.312500]\n",
            "6479: [discriminator loss: 0.7329468131065369, acc: 0.0625] [gan loss: 1.595874, acc: 0.000000]\n",
            "6480: [discriminator loss: 0.6710005402565002, acc: 0.4453125] [gan loss: 0.834244, acc: 0.234375]\n",
            "6481: [discriminator loss: 0.7053768038749695, acc: 0.078125] [gan loss: 1.424062, acc: 0.000000]\n",
            "6482: [discriminator loss: 0.6638248562812805, acc: 0.3828125] [gan loss: 0.833402, acc: 0.359375]\n",
            "6483: [discriminator loss: 0.7396578788757324, acc: 0.0859375] [gan loss: 1.491876, acc: 0.000000]\n",
            "6484: [discriminator loss: 0.6924734115600586, acc: 0.3515625] [gan loss: 0.803903, acc: 0.250000]\n",
            "6485: [discriminator loss: 0.7389501333236694, acc: 0.0625] [gan loss: 1.535078, acc: 0.000000]\n",
            "6486: [discriminator loss: 0.6769183874130249, acc: 0.390625] [gan loss: 0.890189, acc: 0.218750]\n",
            "6487: [discriminator loss: 0.7685397863388062, acc: 0.1328125] [gan loss: 1.452869, acc: 0.000000]\n",
            "6488: [discriminator loss: 0.7013154625892639, acc: 0.3515625] [gan loss: 0.952479, acc: 0.109375]\n",
            "6489: [discriminator loss: 0.7049852609634399, acc: 0.1171875] [gan loss: 1.488752, acc: 0.000000]\n",
            "6490: [discriminator loss: 0.7146779298782349, acc: 0.3515625] [gan loss: 0.984894, acc: 0.078125]\n",
            "6491: [discriminator loss: 0.7145867347717285, acc: 0.09375] [gan loss: 1.586586, acc: 0.000000]\n",
            "6492: [discriminator loss: 0.6736562252044678, acc: 0.4375] [gan loss: 0.763265, acc: 0.375000]\n",
            "6493: [discriminator loss: 0.7674334645271301, acc: 0.015625] [gan loss: 1.978018, acc: 0.000000]\n",
            "6494: [discriminator loss: 0.6855429410934448, acc: 0.484375] [gan loss: 0.471010, acc: 0.937500]\n",
            "6495: [discriminator loss: 0.8551623821258545, acc: 0.0] [gan loss: 1.720033, acc: 0.000000]\n",
            "6496: [discriminator loss: 0.7254113554954529, acc: 0.4453125] [gan loss: 0.745814, acc: 0.375000]\n",
            "6497: [discriminator loss: 0.8030373454093933, acc: 0.015625] [gan loss: 1.552761, acc: 0.000000]\n",
            "6498: [discriminator loss: 0.6963232755661011, acc: 0.3828125] [gan loss: 0.863837, acc: 0.281250]\n",
            "6499: [discriminator loss: 0.7050617337226868, acc: 0.109375] [gan loss: 1.357963, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAIuCAYAAABdOBlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dZ4BUVbb3/12SG7rJuclIFAQktAqOCOpFRa6KIAN45eoog2K+JkQcxoAMM+oYGXS8hkGvM4iDMqIIKkFQcpKMSIYm52j/Xzz/+zyutTd1qror7vp+3v2KU7uO9OGwrLNYO1RQUGAAAAB8c06yTwAAACAeKHIAAICXKHIAAICXKHIAAICXKHIAAICXKHIAAICXiof7xVAoxL8v91RBQUEoUZ/FdeSvRF1HXEP+4l6EWDjbdcQ3OQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEth964CAAB+ysnJEfn48eMinz592nrPzz//HNdzijW+yQEAAF6iyAEAAF6iyAEAAF6iJweIQvHi8o9MXl6eyF988YX1nlKlSol88uRJkbdu3Spy+/btrTX2798f1XkiOcqWLWu9tmrVKpH37NkjcvPmzUU+cuSItcY558j/H61YsaLIBQUFUZ0n/Jefn2+9VrlyZZFDoVDU6+7bty/smql2LfJNDgAA8BJFDgAA8BJFDgAA8FIo3POzUCiUWg/XUox+nlmtWjXrGP3sfPXq1SIn6/llQUFB9A9jC8mn62jTpk0i16lTJ+o19M88kufiy5cvF7ldu3Yinzp1KurziIVEXUepeg3NnTtX5E6dOgW+58yZMyLrfptIroejR4+K7OoFShfci2JDXwPbt2+3jsnOzhZZ9weWLFky8HP0e26//XaR33777cA14uFs1xHf5AAAAC9R5AAAAC9R5AAAAC+l5ZycYsWKBR6Tm5sr8rZt20TWz8Uj2Y9DPyuvW7euyD/88IP1nr1794rcpEkTkY8dOxb4uUiezZs3i6yvK83VY3Xw4EGRX3jhBZGHDx8usu7RMMaYli1bijxr1iyRx48fL/JLL71krZFue86kovr164vcqlWrqNfQ96+1a9eK3KhRI+s9+pooXbq0yMuWLSvyeSG9jRgxQuSsrCzrmBMnTohcokSJqD9Hzwpr3bp11GskEt/kAAAAL1HkAAAAL1HkAAAAL1HkAAAAL6Vk47FuzNPNnOeee67Ia9assdbQDb+66VJn1wAu/bk6lytXLuyaxhhz/Phxke+8806Rx4wZY70HydGzZ0/rtdq1a4d9j27ke/TRR61jXnnllbBrXHLJJSK7mkarVKkism720w3sehM9Y4x55513wp4Hgj355JMi6+Fp+h80GGPfFyZOnCjy7NmzRR40aJC1RosWLcJ+rr4nVqhQwVqDTV79ov8xwm233Say6+80vWmnvibmz58v8i233GKtoZvg9fX6yCOPiJysQaX/i29yAACAlyhyAACAlyhyAACAl1KyJ0cPG9K9MLoHxzUc8PDhw2HX0AqzUebGjRtFPnTokHVMgwYNRF6wYEHUn4PY0NeVHp41bNgw6z36ufbp06dFvvjii0VeuHChtUbQtdWtWzeRGzZsaB3Tv39/kS+99FKRO3fuLHKXLl2sNZYuXSry4sWLw54XbLoXRvcnuPryVqxYIfIHH3wgsr4nuO4RetNDfY3o8zpy5Ii1Bvxy4MABkfXGmbov1RhjOnToILLu3Xr++edFdt279D2xTJkyIterV0/kdevWWWskEt/kAAAAL1HkAAAAL1HkAAAAL4XC9QuEQqHoG1ViQD/T05skli1bVmRXL0wyPPDAA9Zrzz77rMhPPfWUyCNHjozrOZ1NQUGBPUQhTpJ1HenN5/TzaL3JpWuzVH3ttW3bNvA98ZCdnS2ynpHx/vvvi6w3jzXGmBtuuEHkjz/+uMjnlajrKFnXkKY3PdR9D7o3xhh7NskVV1whst7Yt1SpUtYal19+ucgvvviiyLofQ/ckGmNMmzZtrNdSQSbci+JB96J27dpV5B49eljvGT16tMj6785OnTqJPGXKFGsNfV/dvXt32M9NVB/q2a4jvskBAABeosgBAABeosgBAABeSvqcnNKlS1uvffrppyLrvX0OHjwY13MqLNc8AN3z1KtXL5GffvppkV1736Bw9FyYpk2biqx7JUaNGmWt8cYbb4icqB4cTT8713vM6Pk9rn1rbrrpJpFj0ZOTafSfT92f4Pp91/uOPfbYYyLffPPNIufl5VlrtGvXLuzn6Lk5rn2qXn31VZGHDBliHYP0oa/FRYsWiaznghlj3/MqVaoksu4hdfWYaXqfPNessGTimxwAAOAlihwAAOAlihwAAOAlihwAAOClpDceV61a1XpNN4jeeuutIo8ZMyau51RYetCSMcEb59WvX1/k9evXx/y8MkGzZs0CX9ObJ+oN7nRjpjHGnDhxIgZnF3uvvfaayPo6cjXAVqtWTWTd9H/8+PHYnJzH9DWlN+h00cdMmzZN5EaNGom8ZMkSa41HHnlEZN0wqn/eFStWtNa4/fbbRX7ooYdEdg0QRPrQf5fq68wYeyDkgAEDRNZDRl30P6bRfz8XZrPreOKbHAAA4CWKHAAA4CWKHAAA4KWk9+R8/vnn1mt62N+HH36YqNOJin7W3rhx48D36E0i9+zZE9NzyhS6B2HcuHHWMXpAnt5IrnXr1iKnav+Ny9ixY0WO5Ln4mjVrRKYHJ3oDBw6M+j36Z3HllVeKrDdBdPUc6mGAZcqUifo89IaOemCg3hiUwaTp5c033xS5QoUK1jEfffSRyF26dBG5XLlygZ+zYsUKkefOnRvpKSYF3+QAAAAvUeQAAAAvUeQAAAAvJbwnp3v37iJv27bNOqZy5coiX3755SLrZ4/Jojfni2RmxtatW0V2baSHYJdddpnIur/GGGN27Nghcr9+/URO556UCRMmiKx7lFw9OcuWLYvrOWWCoUOHRv2eXbt2idynTx+R9UaKgwYNsta45557RL7++utFbt++vcj63mSM3ZOjc4MGDUR2bTiM1KFnsF1wwQVhf90YYx5//HGRdd9iJBv96lluqd67xTc5AADASxQ5AADASxQ5AADAS6Fw+0yEQqGYb0Kh91xx7dWUk5Mjsu5j+fWvfy3yjz/+aK2xc+dOkfWzxlg4dOiQyJHMGLjuuutE/vjjj2N6TpEqKCiwH7bGSTyuo02bNoms92Uyxph58+aJrK+bzZs3x/q04uaiiy4Sefbs2WGPP3nypPWanoMSC4m6juJxDUX4uSLr/c8016/rfcVicd3p87rrrrtEdv38H3vsMZHr1q0rsv67wNXXE4/+i3S/FyVLz549RZ40aVLMP8NVH+jPnTx5csw/tzDOdh3xTQ4AAPASRQ4AAPASRQ4AAPASRQ4AAPBSwocBPvjggyK7BhbpoXq5ubkiz5w5U2TXxop6Xd2oqje4cw0Y1BuD6s+NpNFYbwoZ1DCKyDz33HMiP/3009Yx559/vsizZs0SuW/fviIvWbLEWkNfW0GNp7HwwAMPWK/pTRt1Q6A+r+nTp8f+xDLQpZdeKrL+fdcNwK5/SOEaeFpU+jxeeumlwPfk5eWJfMstt4is/1suvPBCaw39ZwiJ869//Uvkf/u3fwt7vKtpOGhoqP511zDAlStXhv3cVMM3OQAAwEsUOQAAwEsUOQAAwEtxHwaoB5DNmTNHZNdwKT3YTQ+tikRQ74R+1njq1CnrGD0My/V8MsjLL78s8v333x/4uYmQ7gO4/v3f/13kt956yzomKytLZN3rpTesdA1p08+fb7jhBpH10DXdB2RM8CDKpUuXityyZUvrmKDNXydOnCiy3sAxXnwfBqh/34M2MHz++eetNUaMGCHy4cOHRQ53Dy4sPTzSGGMOHjwo8pdffily1apVwx5vjD3MNRbnnu73oniYMWOG9VqXLl2iWsPVO3Ps2DGR27VrF3YNfa0aY0z16tVFPnr0aFTnFS8MAwQAABmFIgcAAHiJIgcAAHgp7nNydM/ChAkTRK5Xr571nl69eoms+2v0c3LXHIrKlSuLvGfPHpFr1aolsmszuqAeHP082rUpnu4L0c+9t2/fHnZNuFWpUkVk/ftsjH2d6OuoZs2aIrdo0cJa45prrhE5aI7Ehg0brDXWrVsn8vDhw0U+77zzwq7p8vXXX4s8aNCgwPcgevqaGTZsmMhPPfWUyPfee6+1xm9/+1uR69SpI7KepRULixcvtl6LZAPOX9qxY0dMzwlnp3u9ihUrVuQ1a9eubb2Wn58v8saNG0XWfy+67iu6ryfV8U0OAADwEkUOAADwEkUOAADwUtzn5Oi+CD03xzUTRM+W0c+O9f4pW7dutdbQvRT63/JfeeWVZznjswuab+E6Dz3jZ8qUKSKvXr1aZD1Twxj799C1V1e00n02xf79+0XOyclxfa7Iur9CzwFxzWwqW7asyKVLlw57XpHsF6PPI5IenG+++Ubkxx9/XORk7Ynm+5ycIJMnTxb5qquuso7RP299H9FzR1x/voP2GNK/3qRJE2sN3Zeo9/P76aefRHbNjSrMfTNIut+LIqH3OTx06FDMP6Mw/Zz6WtP3mWuvvdZ6j6v3NBUwJwcAAGQUihwAAOAlihwAAOAlihwAAOCluDcea7rx2LVBZdeuXUWeOXOmyHrg1ttvv22t0ahRI5E7d+4scr9+/UT+7LPPrDVGjRol8pEjR0TOzs4W+auvvrLWaN26tci6uVU3EO7bt89a49133xX5scceE7kwjcjp3uy3d+9ekStWrGgdo69t/Z7ixeUsTN0EbowxHTp0EFlfAyVLlgw8V32MvgamT58u8rhx46w19JBBPQBTD5VMlExvPNZcg9J0s7q+5+lfD9pcOBJ6I01jjHnmmWdE7t+/v8iffPKJyHpopTHGrF+/vsjnpqX7vSgS+roI+gcMLkGNxYXZQFrfi3SD9PHjx6NeM1loPAYAABmFIgcAAHiJIgcAAHgp4T05QUOsInlPJM8edb+Ffs4dlAujffv21mtz5swJ+zmRbJqn+0B+//vfizxmzJiwa7qk23NwvWGd7kNyXRP6Nb05XYUKFUTWw9GMsXu79DPqNWvWiLxr1y5rDf3ce+jQoSLrIWx6sz5jUnfjVnpyJFdvVI0aNcK+Rw92fPHFF61j9ABBvZGiHkKoNw41xt4cWA9IffTRR0XWQ1njJd3uRZq+N+m+U2Pse7jelFX3ULn6O/WAR93Xo3++kVi4cKHIF1xwQdRrpAp6cgAAQEahyAEAAF6iyAEAAF5KeE9OptF9Hv/4xz9E/uMf/yjygQMHrDX0jAXXRpLRSvfn4D/++KPIrrkTeuPDtWvXitygQYOwaxpjTP369UXWvRF6fs3VV1/tPuFfcPXtpCt6coLpjTF1/4XeKFZvPmuMMQ8//LDIb775psi6D0Rv6muM3Q/Ys2dPkadOnSqyqzcsHtL9XrRgwQKR27VrF+uPMMbYfXl6o0xXL5D23XffiZyXl1f0E0sR9OQAAICMQpEDAAC8RJEDAAC8RE9OnOm9QMqUKSOyfv7u2ssrHtL9Obh2ySWXWK89/fTTIk+ePFlkvUfa7NmzA9e9++67Rb788stF1r0Sxth9PD6hJyeY3idPXyP6nuDqudOzWLSg+VvG2PN3HnjggbBrJkq634s2b94scm5ubqw/IiL6GnjyySetY/SMNZ/QkwMAADIKRQ4AAPASRQ4AAPASRQ4AAPASjccJ1q1bN5H1ICnXILB4SPdmP6QGGo+jF7TZaiSbseqhofq+MXjwYOs9c+fODbtGsqT7vUhvBv3II49Yx4wYMSLseyKhNwPW/whC/zz1z9t3NB4DAICMQpEDAAC8RJEDAAC8RE9OnJUsWVJkPTzus88+E1kPdIqXdH8OjtRAT0709GC/pUuXinzjjTda71m1apXIibpPJAL3IsQCPTkAACCjUOQAAAAvUeQAAAAv0ZOToXgOjligJwdFxb0IsUBPDgAAyCgUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEsUOQAAwEthN+gEAABIV3yTAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvFQ83C+GQqGCRJ0IEqugoCCUqM/iOvJXoq4jriF/cS9CLJztOuKbHAAA4CWKHAAA4CWKHAAA4CWKHAAA4CWKHAAA4CWKHAAA4KWw/4QcAADgf5UrV07kI0eOiFxQkFr/Sp9vcgAAgJcocgAAgJcocgAAgJcocgAAgJdoPAYAAOacc+T3HjNnzrSO0Y3Hl112mch79uyJ/YkVAd/kAAAAL1HkAAAAL1HkAAAAL9GTAwAAzKJFi0Ru1aqVdczGjRtF3r9/fzxPqcj4JgcAAHiJIgcAAHiJIgcAAHgpFG4zrVAolFo7bcEpFAqJ7PqZ6vkHZ86cCVkHxQnXkb8KCgoSch1xDfkrUdeQMVxHWm5ursi630b/3WKMMTk5OSLrDTqT5WzXEd/kAAAAL1HkAAAAL1HkAAAALzEnJ8H0M85atWqJ3Lp1a+s9em+Q7Oxske+66y6RT58+ba3x888/R3WeSC/nnXeeyH379rWOeeKJJ0QO14+H2NH9cMOGDRO5d+/eItetW9daQ+8HtHLlSpH/8z//U2TXn/dU21MIiVe8uPwrf926dSLrv5969eplrZEqPTiR4pscAADgJYocAADgJYocAADgJYocAADgJRqP/38lSpSwXjtz5ozI9evXF7lt27YiuwYnPfTQQyKfe+65ImdlZQWuoRu9vv76a5HLlCkj8qFDh6w1EBu6ibRkyZIid+rUyXrPiBEjRG7evLnIW7ZsEfmSSy4J/Nw6deqI/Oc//1nke++911ojkqGRKJq8vDzrtRkzZoismz9df+a1ChUqiNywYUORlyxZIrJr00T9jxz0vSg/Pz/wPJDeRo0aJXKxYsVEXrNmjchffPFF3M8p3vgmBwAAeIkiBwAAeIkiBwAAeCljN+h87bXXRB48eHCR13T9XgY9b9d9P5G48sorRZ42bVrUa7Apns31s9L9FHqg2gUXXCCy7o8yxphy5cqJrJ+D7969W+QNGzZYa+jP0VasWCHyRRddZB1z4sSJsGsURqZv0KmvGdfvsavf75eOHTsm8pgxY6xjJk2aJPLixYtF1n0+Tz31lLXG/fffL/Lo0aNFfuSRR8KeZ7xwL4qP8uXLW6/t3btXZD04Vg+jXb16dexPLE7YoBMAAGQUihwAAOAlihwAAOCljJmT8/zzz4t8xx13RL2G7rk5ePCgyHpmijHGnDp1SuQ5c+aIvGrVKpH1hmnGGHPfffeJvHXr1uCT9YzuYylML5Pul/nd734n8oMPPmi9J6inSm+E6Dpe91xs2rRJZD3XSM9fMsbuudi8ebPIDzzwgMjx6L+B7V//+pfI+udkjDGHDx8WuXr16iIfPXq0yOeheyv69etnHaOvzauvvlrkZPXkIDb0LK3ly5dbx+hrYNmyZSJv3Lgx5ueVbHyTAwAAvESRAwAAvESRAwAAvORtT87UqVNFvuyyy0SOZL+Y66+/XuT58+eLXKpUKZEbNWpkraH3ndJ7yuj+Gt3DY4wxCxcuFHn9+vVnOWN/FaYHR/+M9TPrypUrF+mcjLF7IU6ePGkd8/jjj4t8ww03hD0PfV0ZY//3ly1bVuTZs2cHnyyKTF9TVatWFXnnzp3We2688UaRY9GDo+lZWXqfKmPs/rFu3brF/DyQPC1atBA5Nzc38D21a9cW2cdePr7JAQAAXqLIAQAAXqLIAQAAXqLIAQAAXvKi8di1GWH37t3DvkcP9rv77rutYyZOnBh2Dd2EWKVKFesY/ZreWO/48eMi6+ZYY+yGZ93sCvfvW40aNUQeNmyYyNOnTxfZtQmmHtymBz7eddddIo8YMcJao2nTpiLrgXF6QGSzZs2sNfRAufbt24vsalhH7OmGXt24qTdjNcaY7777LubnMXnyZJH1P6xwGTdunMi7du2K6TkhsfQ/UHjiiSdEdm0YrZvPH3roodifWIrhmxwAAOAlihwAAOAlihwAAOClkOu53f/9xVDo7L+YRF27dhVZ91ZE4vvvvxe5S5cu1jGuwW6/dPPNN4v88MMPW8dkZ2eLrAcG6mekhRl6VxgFBQXB0xBjJFWvo1jQfVlZWVnWMfrZue4V0pvkufqLdM+NazPYZEjUdZQq15Dupzpw4IDIpUuXtt6zZs0akZs3bx72M1xr6CGiroGRv3T//fdbr7300ksip0pvH/eiyOifuR4IqvsBBw8ebK2hNzpu06aNyK5NPdPF2a4jvskBAABeosgBAABeosgBAABeSss5OR999FHU79G9R3ozM1dvkn7+rjc8e+utt0R2PeN+8cUXRWaeiV/0daM3ZHW9pp+la7pPyxh3vxcST/8Zr1evnsj5+fnWe/TcI913p9eMpN9KXyMbN24U+YUXXrDeE67/EqnFtYF0+fLlRdY9ozk5OSK7evv0dVOzZk2R07kn52z4JgcAAHiJIgcAAHiJIgcAAHgpLebk6NkjgwYNErlXr17We6ZNmyZy69atRZ41a5bIep6NMcZUq1ZN5DvvvFNk/Szd9Txez8VJlefizKZIHv28XfdpuXpy9J+BTJtxki7X0Lfffmu91rFjR5F1r4Sr/0LT942ZM2eKfMUVV4h84sSJwDVTBfcim55nY4x9X+jdu7fIzz33nMgNGjSw1ti+fbvI9957r8gffvhhVOeZSpiTAwAAMgpFDgAA8BJFDgAA8BJFDgAA8FJaNB4ngmtTPD1gq2rVqiLv3LlTZD1g0Bh7Y71UQbNf8qxevVrkJk2aiOxq/uvbt29cz6mwaDyWXE3EI0eOFLlMmTIiDxkyRGTXxsB6oOR9990n8j/+8Q+RXc3rqYp7UeGUKFFCZL0RbN26da336A1lBw4cKPKUKVNETtSG0bFA4zEAAMgoFDkAAMBLFDkAAMBLCd+gU29OePToUeuYZAzMcz3D1sMA9fN2PUgpVftvkFylSpUSWffg6GtP91cgfbjuXaNHjxZZ9/9deOGFIrs2Y3311VdFnj9/vsiRDBSEX/RA0Bo1agS+R1+fW7Zsiek5pSK+yQEAAF6iyAEAAF6iyAEAAF6Ke0+O3mjs2WefFVnPezAmOf82X/fXuPz6178WOZ03M0PiHD58WGT9XHz8+PEiT5o0Ke7nhMS5++67Rc7LyxO5TZs2Infv3t1aIycnR2S9YWuqbPyLxGnZsqXIJUuWFNnVp3Xo0CGRly9fLnI6zcWJFN/kAAAAL1HkAAAAL1HkAAAAL8W9J+ecc2Qd1bt3b5GffPJJ6z179+6N5ykZY4w5duyYyPo8jTFm+vTpIr///vtxPSekvx9++MF6TT8bP3HihMhjxowJ++tIH7Nnz7Ze03NwdI/WK6+8InKDBg2sNXQvRfHi8tadTntVITYuvfRSkfV9xtWnNXjwYJF97MHR+CYHAAB4iSIHAAB4iSIHAAB4iSIHAAB4Ke6Nx6dOnRJ5x44dIu/atct6z/bt20XWQ48OHjwY+LmNGzcW+e233xZZb5Knz9MYYwYOHBj4OchsEyZMELl58+bWMbrJvV27diKvWrUq9ieGhNAD2Dp16mQdk5+fL3KPHj1EXrx4scjXXnuttUaHDh1EjmQzRvhFD9bt27dv2ON1s7oxxsycOTOm55QO+CYHAAB4iSIHAAB4iSIHAAB4Ke49OZreEKxt27bWMbm5uSLv379f5M2bN4usn4sbE/zM+vTp0yKPGDHCOkb3BgEvvviiyNddd13ge0aOHCny6tWrY3pOSJ6OHTuKvHv3busY3ae1b98+kfUQtyNHjlhrRDLoDX7TAx91L1fnzp1FXr9+vbXG0aNHY39iKY5vcgAAgJcocgAAgJcocgAAgJdC4Z7thkKhuD/41c8RjTFm2rRpIutZOtWrVxfZNeNGz8HRfRA33XSTyEuXLg0+WY8UFBSEgo+KjURcR/EyZcoUkbt16yay3ijR1W/TqlUrkV3Xa7pK1HWUqtfQs88+K/LYsWOtYzZu3BjVml27drVe09fQSy+9JHI69+hwL4pM5cqVRd6yZYvI+u881ww63auazteNdrbriG9yAACAlyhyAACAlyhyAACAl5Lek6OfIxpjzHnnnSeynkWyd+/esNkYu69n27ZtIus5OZmG5+A2175Dc+fODfueM2fOiFyzZk3rGL13kU8yrSdHz+TSM2/mzZtnvUf3cWl9+vQR+cEHH7SOufHGG0XesGFD2DXTCfeiyNSrV0/koF4vV0+OnkGXCf2BfJMDAAC8RJEDAAC8RJEDAAC8RJEDAAC8lPANOrXjx49bry1YsCBs1gPYXE3EPg05QnzUrl1b5K+//jrwPXqTPD0MzucmY9ibA2dlZYn8q1/9ynqPvse9/vrrInfo0EHkcePGWWv41GiMYHpDVmOMGTBggMj6utL/iMf192K5cuVE1o3zPuKbHAAA4CWKHAAA4CWKHAAA4KWkDwNEcmTiAK4KFSqIrIdplS9fPnCNkydPilyqVKkin1c6y7RhgNWqVRNZb5JYokQJ6z36HqtzTk6OyEeOHCnKKaadTLwXIfYYBggAADIKRQ4AAPASRQ4AAPASPTkZiufgxsyYMUPkzp07W8dMmjRJZL2Zou7RyTSZ1pOjtWzZUuRNmzZZx+h5Jj5tihgL3IsQC/TkAACAjEKRAwAAvESRAwAAvERPTobiObjNtV9MmTJlRD569GiiTictZHpPDoqOexFigZ4cAACQUShyAACAlyhyAACAlyhyAACAl4on+wSAVOFqwqfRGADSF9/kAAAAL1HkAAAAL1HkAAAAL4UdBggAAJCu+CYHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iaSZkW0AACAASURBVCIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4iSIHAAB4qXi4XwyFQgWJOhEkVkFBQShRn8V15K9EXUdcQ/7iXoRYONt1xDc5AADASxQ5AADASxQ5AADASxQ5AADASxQ5AADASxQ5AADASxQ5AADASxQ5AADASxQ5AADASxQ5AADASxQ5AADAS2H3rsokHTt2tF6bN2+eyOecI2vCM2fOxPWcAABIJ6GQ3EKqoCC524XxTQ4AAPASRQ4AAPASRQ4AAPCStz05+rng999/L3L79u0Tch7fffedyF26dBH51KlTCTkPFI7uw1qyZInIo0aNEvm6666z1mjUqJHIpUuXFnnnzp0i9+/f31qjfv36Is+ePdt9wgBgjKlTp471Wu/evUUePny4yJ999pnIPXv2tNbIzs4WWfem1qtXT+Rt27ZZaySyT4dvcgAAgJcocgAAgJcocgAAgJdC4Z6NhUKh5P4D9yLIyckJ++uvv/66yH//+9+tY/Ly8kS+6qqrRG7atKnIJUqUsNY4ePCgyHfeeafI7733XtjzjJeCgoJQ8FGxkc7XUcOGDUW+6667RB46dKjIxYoVs9bQ/WGxoD/3lVdeETlRz7wTdR2l8zWE8LgXxYbuH+zTp491zK233iry1KlTRb7jjjtEbtCggbWGvp/t379f5NzcXJGPHDlyljOOrbNdR3yTAwAAvESRAwAAvESRAwAAvORFT46r50H3Rpw+fTrmn1uyZEmR9+zZYx2za9cukXv16iXy8uXLY35ekeA5uM11HS1dulTk8uXLi1yzZk2RixdPzOgpPZvip59+ElnP5okXenJQVNyLYkP35Fx++eXWMQsWLBD55MmTIrdr107kf/7zn9YaWVlZIq9Zs0bkli1bBp9sHNCTAwAAMgpFDgAA8BJFDgAA8BJFDgAA8FJabtCpG0RdzdPxaDTWdNPW9u3brWN0o2rlypXjek4oPFfjsd4Yc+PGjSLrRuOyZctaa/z888+Bx/ySa6CgbirUx+jN+HRTvDH29YrE0D+7SpUqiTxv3jyRXRsr7t69O+wa+/btC/uZxhhz3333iTx+/HiR9XWK1FaxYkWRDx8+LPLWrVut9+zdu1dkvVnwrFmzRNb3O2OMadWqlchz584NPNdk4pscAADgJYocAADgJYocAADgJS+GASbL6NGjRX7wwQetYxYvXizyBRdcIHKiNlLUGMBl9+C4+lh0v8TIkSNF1j+/a665xlrjnnvuETk/Pz/sGq5+Ctfmr+G0b9/eek0PAouFVB4GqPuWdM9JPP7s1atXz3pt/fr1IuvrzvXzjpb+b4mkT7F27doi676fROFeVDhB141rMGlQX57e+PeFF14I/Fy9iaerjycRGAYIAAAyCkUOAADwEkUOAADwEj05UXjxxRdF1s8vXc87mzRpIvKmTZtif2KFwHNwYy666CKRL7vsMusY3Wd17NgxkT/99FORmzVrZq1RtWpVkXXfxowZM0Ru3Lixtcbx48fDfo5+Tq5nZhhjTLVq1UTW/y2Fkco9Od27dxf54YcfFrlfv34iR9KTon+f9aa8FSpUCFwjqH/GNa9G9/Hoe43u2XLNWtLrPv300yKPGDHiLGccX9yL4sM190tfv3/6059Evuqqq0R23Yv09VqlShWR9SyeRKEnBwAAZBSKHAAA4CWKHAAA4KW03LsqUZYuXSpyy5YtRdbPPG+66SZrjVTpwYFNPzt+7LHHrGPKlCkjst53auDAgSKXKlXKWuPMmTMi6+fiP/zwg8h6/xhjjHn99ddF1v1hur/EtT/W0aNHRc7OzhbZ1ceTzm688UaRy5UrJ3LDhg1FdvXk6D/jLVq0CLumi+5hWLlypciDBg0SWc/VMcaY/fv3i9y2bVuR9TXjmpGi+3R0Txr84pqjc/fdd4t88803i6z3WnS57rrrRE5WD06k+CYHAAB4iSIHAAB4iSIHAAB4iSIHAAB4KWMaj3UDYdOmTUX++uuvrfdUr15dZD1MSw8bmzZtWhHOELGmm2/vvPNOkYcPHy6yq2lY0w2drgZPTTd86kbjZ599VmQ9YM4Yu3lZN7Tr5lbXIDBNf04k//3pRP/5nTBhgsh6GJ5uwjTGmGXLlolcqVKlsJ+5bt0667VHHnlE5EmTJol86tSpsGu6LF++XGQ9LDKSn6UeZIn0duWVV4qs7yvG2A3r2muvvSby+PHjrWNc/zAilfFNDgAA8BJFDgAA8BJFDgAA8JK3G3TqjcV0H4Te0M5F9+DMnDlTZN2Tc/r06WhOManSfVO8ihUriqx7VowxpkOHDiJPnTpVn1esT8tJX0fXX3+9yLpHw0Vfr3rTz7lz54qcm5sbuOaBAwdEjmRzSS2VN+jUww718L9atWqJvGjRImuN/v37i6z7HPT9c/78+dYal19+uch6KGNh6D6vwtx7evbsKXKyenTS/V6ULG3atBHZdf1GS/fp6fuMMfY1nyrYoBMAAGQUihwAAOAlihwAAOAlb+fkHDlyRGS9WZnu4fjkk0+sNfRmZvp5ZcmSJUV29XgUZgYGgh07dkzkoUOHWsfouSd6lojefDOSZ80LFy4U+cSJEyLv3LnTek+PHj1E1pstaq7r6OTJkyJv3bpVZL1xZH5+vrWGnp2i+zr056bqs/dIHTp0SOQlS5aIrGfguP57//KXv4jcuXNnkfWcJNfGqM2aNRNZ904U5vdZX7uR0J+zdu3aqNdAYrjmb+nZSHrWW2Hoa0LfI1xzdfQ9MNXxTQ4AAPASRQ4AAPASRQ4AAPCSt3NyND1nRPcj6N4KY4Kfles5Oa7ZO4sXLxZ5+/btYddMlHSfTTFo0CCRH330UeuYGjVqiFyuXDmRdd/W3r17rTV0H4fu89HPzl1r6OsoHr0uWVlZIut+FGPsvjR9HrrHLJLZK6k8Jyce9PyhSPYM07+Pul9Kz1FyraGP0TOADh48eJYz/n/27dsnctA+XImS7veieHj33Xet1wYMGBDVGroH0RhjGjRoILLuuZk4caLIrr8XGzVqJPLu3bujOq94YU4OAADIKBQ5AADASxQ5AADASxQ5AADASxnTeJwI1apVs17Tg8BmzZolsm4oTJR0b/bTDcE1a9a0jtGNtHow4/vvvy/y448/bq1x+PBhkRPRRBwJ3Tivh3jpJlNj7N8PvVFk+fLlRabxOFgkAxR1c7oeRFqYa2j8+PEi9+vXL/A9CxYsELl9+/ZRf248pPu9KB5czed16tQRWQ8e1QNDI7mu9OfoNVxDCceOHSvy4MGDAz8nEWg8BgAAGYUiBwAAeIkiBwAAeMnbDTqTwdUHcc0114ish3hNnjw5rufkC92Dct5554msnyUbY8yQIUNEnjBhQtj3uIZnJUNOTo71mh5cqDeX1AO6dP+Ny5NPPilyJD04kCLpe4jF76v+efbq1Svs8a5evzvuuKPI54HEcF1XmzZtivnn6AG2emCoi2tAYCrjmxwAAOAlihwAAOAlihwAAOAlenJiyPXsvV69eiKPHDlS5G+++UZkPZcF/0fQbBHXs+Q1a9aIHMkmhsmg+430vBpjjNmwYYPIum9HP1vXv1/GGPPll1+K/Oqrr0Z1nkieiy++WOSgnis9A8kYY3788ceYnhPS3549e0TW99Vt27ZZ73nmmWfiek6xxjc5AADASxQ5AADASxQ5AADAS/TkxFAkMzOqVKkicm5ursirVq2K6Tn5Svc/uXoUZsyYIbL+vd+/f7/IidqHSu8Xc/7554vsmp2k36P7eL799luRXdfRrbfeGtV5Ijn0dWqMMe+8847IQfth9e/f31pj7969MTg7aPpnkcrzprp27SpyVlaWyPrcO3bsaK2h98xKdXyTAwAAvESRAwAAvESRAwAAvESRAwAAvJQWjcdBm4a5GkYT1UT6S3/961+t1zp16iTyuHHjRF67dm1cz8lXeXl5Ii9atMg6Rjfr5ufni9y6dWuRDxw4YK2hN8bUAwX1Roj6M42xG0mrV68u8qeffiry7t27rTUaN24c9lx79uwpcqpsNoro6SGWxhhTsWJFkfX9TW8++9lnn8X+xGCMsf8srly5UuS5c+da7xk2bJjI+h9FxMPQoUOt1/785z+LrBuN9X3ENQww3fBNDgAA8BJFDgAA8BJFDgAA8FIoXO9KKBRKfGOLsYcr1apVS+Rq1aqJvG7dOmsN3bMQix6dli1bijxr1iyRXQPpVqxYIXLnzp1FPnnyZJHPqzAKCgrs5pE4icd1pK+RU6dORb2G7qdxbY6qn1nrjTL1ppd6uJYxxvTr109k3V+he870eRljP+fv27evyNu3bxc5UT1pibqOknUvSgTdo7Vjx47A9xw6dEjkhg0biuzq60pV6XYvKlu2rMj65+W6B+hevRYtWoi8evVqkQvz53ffvn0iV6hQIfA9P/30k8gNGjQo8nkky9muI77JAQAAXqLIAQAAXqLIAQAAXkrJnpx27dqJ/Le//U3kevXqiaw3JzTGmD59+ogcNN+kRo0a1hoLFy4UWfcCnThxQmTXXII33njDei0VpNtz8CC9e/e2Xvv73/8ust7EUG9yGQn950U/a3dtzhf0Obq/Qs/dMMaY66+/XuRUmV9BT07R6Z9lzZo1rWP0/Ur3/+lrO52k+72oR48eIn/yySfWMfoeoO8juofU1d+p+xBLlCghsmtGl/bUU0+JPHz48MD3pAt6cgAAQEahyAEAAF6iyAEAAF5Kyb2rVq1aJbLuc9DPN3UPjzHGrF+/XuTly5eLnJubK/LmzZutNbKzs0WePHmyyNdee63I6TRTwDf652uMMRs3bhRZz1sqTE9O0HNv16/rfgo912nAgAEiL1iwwFqDa8sfpUqVEtnVg6PpeVvp3IPjG71P2B//+EfrmN/85jci61lZkcy0CaJnhXXs2NE6ZvHixUX+nHTDNzkAAMBLFDkAAMBLFDkAAMBLFDkAAMBLKTkMUDdv6iFIH330kch6czpjjGnUqJHIe/bsEVk3nc6bN89aY8iQISK7mpPTVboP4IqE/hkPHjxY5Pvuu09kvVGiMe6hXOE+Qw+INMaY0aNHi/zyyy+LrK/NdMIwwOjpIaNt27YNfM/atWtFbtKkSUzPKZky4V6km81vv/12kfU/PnDdi2677TaR9d9ZeqBgpmEYIAAAyCgUOQAAwEsUOQAAwEsp2ZOD+MuE5+BBzjlH1vh6aJ8xdn+Y/vMSyRo+oycnerq3Tw8mddGbMbo2gk1X3IsQC/TkAACAjEKRAwAAvESRAwAAvJSSG3QCiRBJ/0zQxpiZ1oODoqtatWrYX58+fbr1mk89OEAi8U0OAADwEkUOAADwEkUOAADwEj05AJBAV1xxhcgtW7YUeezYsYk8HcBrfJMDAAC8RJEDAAC8RJEDAAC8RJEDAAC8xAadGYpN8RALbNCJouJehFhgg04AAJBRKHIAAICXKHIAAICXwvbkAAAApCu+yQEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF6iyAEAAF4qHu4XQ6FQQaJOBIlVUFAQStRncR35K1HXEdeQv7gXIRbOdh3xTQ4AAPASRQ4AAPASRQ4AAPASRQ4AAPASRQ4AAPASRQ4AAPASRQ4AAPASRQ4AAPASRQ4AAPASRQ4AAPASRQ4AAPBS2L2rfFaiRAmRBwwYYB0zffp0kXft2iXysWPHYn9iAAB4onLlytZre/bsSdjn800OAADwEkUOAADwEkUOAADwkrc9ORUrVhR5yZIlIteuXVvkc86Jvt77+eefRf7qq6+sY3r27CkyfTzpJRQKiVy6dGmRr732WpE//PDDwDX0dYP0oe8rx48fFzlRf77LlCkj8owZM0Ru3bq19R59j5s3b57IV1xxhciHDx8uyikiDehromPHjiJPmTLFek9OTo7I+v4WiUOHDol8zz33iPzWW29FvebZ8E0OAADwEkUOAADwEkUOAADwUlr25OhngK+++qp1zC233CLy1q1bRT5z5ozIU6dOtdYoW7asyFWqVBG5atWqIufm5lprVKpUKex5oHD0z+a///u/rWNOnTolcn5+vsh33HGHyKVKlSryeX3wwQdRv2fHjh0i6z4uY4yZP39+oc8JsVO9enWRR48eLfLw4cOt92zevFnkAwcOiKz7usaMGWOtUbduXZGLF5e37oKCApEj6ZO48MILRd67d6/ICxcutN6Tl5cXuC6il52dLfKRI0cC36P7sn73u9+JPHDgwMDP0T05en5cJL2qQdee/rvWGGM2bNgg8qJFiwI/p7D4JgcAAHiJIgcAAHiJIgcAAHgpLXtyVqxYIXLDhg2tY0qWLCnyuHHjRH7uueeKfB7NmjUTWfd4GJPYPTpSlX5Gq5/hRkL3IOh+qOuuu856z8mTJ0XWM25SRY0aNUT+7rvvrGN++9vfivyXv/wlrueE/0Nfd/o+onsc/vCHP1hr6PvVo48+KrKeGTJhwgRrjYsvvljkkSNHiqz7ulzzTf70pz+J/Nhjj4mclZUlcuPGja01YvFnORMVK1ZM5Dp16oisZxLpe5cxxnTq1Elk3Zd4/vnni+zaM0r//HTfov5cff0bY/duPfzww2E/47PPPgs8D70vZCzxTQ4AAPASRQ4AAPASRQ4AAPASRQ4AAPBSWjQeP/jggyI3b95cZNewoaVLl4oci0Zjbe3atSLrBkJj7GGA27Zti/l5pLpYNCfqBrigzQZdn6t/73v06CGyHoTl+pzt27eLrH/m//Vf/2WtMWTIEJG//PJLkcuVKyeyHmRpjDGvvPKKyG3atBH53nvvFdnVuIjo6Sbg8847L+zxX3zxhfXaO++8I/KJEydE1teDzoVRs2bNwGP00Ek9HHDWrFnWe2g0Lhx9HwkaCqsbgo0x5qeffhL5qaeeErlVq1ZhjzfGmKNHj4pcoUIFkfU9ctSoUdYaK1euFPn06dMib9myRWR9fzPGHnbo+js8VvgmBwAAeIkiBwAAeIkiBwAAeCkle3L04KRhw4aJrJ/fuZ5fdunSJfYnFuCNN96wXqtVq5bImdiTEwvHjx8XeePGjSLrfgJjggeX6WGOek1j7OfJu3fvDjpVy+TJk8P+ur7eK1asaB1z3333idynTx+Rv/32W5Hfe++9aE4Rxr2ppR5SpnvDXnjhBZE//vhja43ly5fH4OyKTg/D7NChg8j6OnT9eUDhuP6OipbusRk0aJDIv//970V+5plnrDX0PfCJJ54Q+dZbbxW5ffv21hr16tUT+W9/+5vIehBvYe6ZscQ3OQAAwEsUOQAAwEsUOQAAwEsp2ZPz888/i6z7WM4991yRf/zxR2sN/e/w40H3Bu3cudM65tJLLxV5/vz58TyljKGvEZegmR6rVq0S2dWTkZubK7LecDUWc0NKlSol8tChQwPfo+cvXXTRRSLr5+TGMOMkiJ5lYoy9EezMmTNF1r/P+ppKJV27dhXZNRfql6ZPnx7P00ER6fk0euNX18+3c+fOIt95550iz5kzR2TX7Lf3339fZH1PjOTenEh8kwMAALxEkQMAALxEkQMAALyUkj05el6Dnmein50fPHjQWkPPN3Edkwhly5ZNyucieq6eFX3d6GuzW7duIk+bNs1aQ++RdeDAAZG/+eabqM7TGHt+y0MPPSQy/TfRGzFihPWa/vnu2LFD5KZNm4qcKj05+jo1xpjRo0eHPUbPovr0009jf2JIGN2zY4w9c07PtNGzkWbMmGGtkZ+fL3Kq9eBofJMDAAC8RJEDAAC8RJEDAAC8RJEDAAC8lJKNx7qRSTc6VatWTeTmzZtba2RlZYmciMbjEydOWK/pQW+dOnUS+bvvvovrOaFo9u/fL7IePKmvMz2kL1b0UK68vDyRDx8+HJfPzST6z6ox9j9g0Jtc6p+/a6BkMprAjx49ar2mNxfVzet6sCHN6+lFN5LrTX2NMeZXv/qVyPo6ef3110X+4YcfrDVSvdFY45scAADgJYocAADgJYocAADgpbToyTl16lTY413P0vWz8kRwPY/Xz0X1QK7u3buL7Bq+hOQ5efKkyHoT1gsuuCDmn+nq7Xr55ZdF1kO7UHStW7e2XtP3ouzsbJE7duwo8sSJE601dG+L6+dbVHqQnx7y5rJ69WqRH3/8cZHpyUltDRs2FHnt2rUiuzacPXbsmMjz5s0Tec2aNSLH41pNNL7JAQAAXqLIAQAAXqLIAQAAXgqFe+4aCoVS4qHsf/zHf4j8/PPPi1y+fHnrPdWrVxd59+7dsT8xxfUMVM8h0M/Kv/rqK5H1hoDxUlBQYDcQxUmqXEexoPuu9DPsxo0bB66h+3z0NbF582brPU2aNBFZ92AkS6Kuo0RcQ+3bt7de03Osjhw5IrL+89u5c2drjc8//1zkiy++WGQ9e6dy5crWGnoGyvbt20WuWbOm9R5NXzN6c9FNmzYFrhEP3ItsjRo1sl7T11qdOnWiXlf/fa/70PT9zEXfv1LF2a4jvskBAABeosgBAABeosgBAABeSsk5OZreb6NChQoiu+bTtG3bVuSpU6dG/bl63aC5Ea7eIL33hz4vPWfD1deTbnuF+ExfA3379hXZ9fMbOXKkyO+++67I7733nsgHDhyw1kiVHhyf6TkjLnovqyuvvFJkvT+UMfY1ormumSBBPTiue9VLL70kcrJ6cGDTf9e4/r4qTA+ONm3aNJH1Na/7A3UPWjrimxwAAOAlihwAAOAlihwAAOAlihwAAOCltBgGqDfodDX3aZMmTRJZNwU3b95c5EqVKllrDB06VOS33npL5Eg2L2vTpo3I8+fPF3nbtm0i161bN3DNWGAAV/Lk5eWJPGfOHJEPHTpkvUdfv6myeaJPwwBd9J/PoIbfw4cPW6/pZs4SJUqI7PqHE9rp06dF1gPZ9D9O0BsvGmMPGs20a8iY9LkX1apVy3rtf/7nf0TWwyv1RtVbt2611hg+fHjYz9UDB11r6GsxVTAMEAAAZBSKHAAA4CWKHAAA4KWU7MnRw7HOnDkT9njXf8OuXbtE1pve6Q3vXM8Z9bP0wtD/LfoZ/4IFC0S++uqri/yZkeA5ePL06tVL5I8//lhk1+C/rKwskTOtnyJZ15C+B+iNMfWf7+nTp1trdOjQQeQtW7aI3KlTJ5H18EhjjGnXrl3Yz9VDCa+55hprjS+++MJ6LRVwL7J/nmXKlLGOueKKK0TWfVcDBgwQ+Q9/+IO1RpUqVUTWgyovvPBCkX/zm99Ya7j6zlIBPTkAACCjUOQAAAAvUeQAAAAvpeQGndH2G7g2sNyzZ4/IkydPFln37Dz66KNRfWak9LmVLVtW5NatW8flc5G6CjMLKZIeMsSe/n2uX7++yLoPQm++aowxTzzxhMjPPPOMyLpPYt26ddYaH3zwgchLliwR+aqrrhL56NGj1hpIDFcvp+4r1Vn/PeHaGFP37um/J0eNGhV4bvrvPX39rl+/XuQbb7zRWkNf4/o8dH+RnnOXaHyTAwAAvESRAwAAvESRAwAAvJSSc3I0/Vxc9yfs3LnTes+IESNEHjt2bOxPLAI5OTki5+fnizxw4ECRP/zww7ifkzHMpkimAwcOiJydnS3ywYMHrffUqFFDZNcsnWTwfU6OpvsNdI9d6dKlrffs3r1b5MLMONL3vPHjx4vcp08fkW+77TZrjTfffDPqz02EdL8X9e7dW+R77rnHOkbPV9I/r0TRc9j0zK6ffvpJZFePmf477e677w77GXqfSGPc97iiYk4OAADIKBQ5AADASxQ5AADASxQ5AADAS2nReDxo0CCRx40bJ3IoZPcbTZw4UWS9EVnQpp+FoRuyjDHmn//8p8iNGzcWuVmzZiK7hkDFQ7o3+8WCbvh1DVDTfz5cgyeD6IZPvXliuXLlAs+jZs2aIqfKMMBMazwuXlzOT9XXRzzuKy66YVTf7/TGv8bYG4WminS/F+lhd7pJ3Bj7OtEbYX7//fexPi3rvmKMMStXrhS5RIkSIuuhk61atbLW0P8IQt+b9N+Dc+bMsdbo3r27yCdOnLCOiRaNxwAAIKNQ5AAAAC9R5AAAAC+lRU+Opgfq6Q3ujLGfgd5www0ir1ixQuQff/zRWkM/a9U9HPPnzxe5Xr161hq6x+aWW24R+ZNPPrHekwjp/hw8EvrZuO6n0D/zFi1aWGvowX36uurWrZvIn376qbWGa0DcL+nBfhs2bLCOadOmjcjJ3vTuf2VaT06qqFSpksjbtm0T2dWnqN+TqP6/IOl+L9L3kdzcXOsYfe/5/PPPRda9m67htQ0aNBB57969IuuhuK6/2/XmofoYfV9x9SDq60YPxNSf4Rpcqjcp1htqFwY9OQAAIKNQ5AAAAC9R5AAAAC+lZU9OmTJlRHY9W9bPpE+ePCmy3ojM1dejnxPWr18/7Hm5ZmR88cUXIvfv31/kQ4cOhV0zXtL9ObimN040xn42/vLLL4vctm1bkVu2bGmtoa8bPedo2bJlUZ2nMfZz8P3794use72MMaZHjx4iJ2oeSxB6cpJD93joXgrXfX369Oki61klyZLu96KPPvpI5J49e1rH6J+X7nWJZNaOfo+ecePqw4qW/vtIXzPGGPPtt9+K/Ne//lVkPcNL/7e7jtH3wMKgJwcAAGQUihwAAOAlihwAAOAl+2FZGjh27JjI1apVs45Zs2aNyIsWLRK5SZMmIuteGWOMGTlypMgzZswI+x7Xs0c9v6Iwex8hWIUKFazXGjVqJHLFihVFrlq1qsjDhw+31ujTp4/IderU8RdZaQAAAgJJREFUifrc9PPndevWiTxw4ECRFy5caK3BdYNf0n0Qkbj44ovjcCZ45ZVXRD7//POtY3Q/p+6fiWS2jO5F1X15uo/HNUtr/fr1Infu3FnkWMyrSTV8kwMAALxEkQMAALxEkQMAALxEkQMAALyUlsMAC8M1LO6XXMOXdMOoFu73LtWl+wCuSIwZM0bkBx54IOzxrp+nbhDUx+hf1xt6GmM3923evDnwPemCYYCxpzc8NMaY22+/XWR9bQfd34yxr93y5cuLzGDS2NBD+owxpmnTpiI3b95cZP0PHHSDsDHGzJo1S+QtW7aIrDcC1vcZY4zZunWr44z9wDBAAACQUShyAACAlyhyAACAlzKmJweSb8/BXXSf1YIFC0TWG3IuWbLEWkMP3NJr3H///SK7hnj5jJ6c2JsyZYr1WqlSpUTevXu3yJdeeqnIX331lbVG+/btRdabzSZr4GQm3IsQf/TkAACAjEKRAwAAvESRAwAAvERPTobiOThigZ6c9KVn69CTg3RGTw4AAMgoFDkAAMBLFDkAAMBLxZN9AgCAxEtWDw6QSHyTAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvESRAwAAvBR2g04AAIB0xTc5AADASxQ5AADASxQ5AADASxQ5AADASxQ5AADASxQ5AADAS/8fIDZNiaS2q9sAAAAASUVORK5CYII=\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "6500: [discriminator loss: 0.6631754636764526, acc: 0.390625] [gan loss: 0.924964, acc: 0.140625]\n",
            "6501: [discriminator loss: 0.7313941121101379, acc: 0.078125] [gan loss: 1.388981, acc: 0.000000]\n",
            "6502: [discriminator loss: 0.690470278263092, acc: 0.390625] [gan loss: 0.954747, acc: 0.078125]\n",
            "6503: [discriminator loss: 0.6941646337509155, acc: 0.1640625] [gan loss: 1.457036, acc: 0.031250]\n",
            "6504: [discriminator loss: 0.7080556154251099, acc: 0.296875] [gan loss: 0.932403, acc: 0.156250]\n",
            "6505: [discriminator loss: 0.7059544324874878, acc: 0.140625] [gan loss: 1.361231, acc: 0.015625]\n",
            "6506: [discriminator loss: 0.7063052654266357, acc: 0.3828125] [gan loss: 0.794111, acc: 0.265625]\n",
            "6507: [discriminator loss: 0.7471217513084412, acc: 0.0703125] [gan loss: 1.627130, acc: 0.000000]\n",
            "6508: [discriminator loss: 0.6930875778198242, acc: 0.4140625] [gan loss: 0.656324, acc: 0.593750]\n",
            "6509: [discriminator loss: 0.7904318571090698, acc: 0.0078125] [gan loss: 1.783141, acc: 0.000000]\n",
            "6510: [discriminator loss: 0.696002721786499, acc: 0.5] [gan loss: 0.529410, acc: 0.812500]\n",
            "6511: [discriminator loss: 0.8301606774330139, acc: 0.015625] [gan loss: 1.579191, acc: 0.000000]\n",
            "6512: [discriminator loss: 0.6889688968658447, acc: 0.40625] [gan loss: 0.794625, acc: 0.343750]\n",
            "6513: [discriminator loss: 0.7727219462394714, acc: 0.046875] [gan loss: 1.562001, acc: 0.000000]\n",
            "6514: [discriminator loss: 0.706833004951477, acc: 0.3984375] [gan loss: 0.918366, acc: 0.125000]\n",
            "6515: [discriminator loss: 0.7519402503967285, acc: 0.046875] [gan loss: 1.557051, acc: 0.000000]\n",
            "6516: [discriminator loss: 0.688339352607727, acc: 0.3828125] [gan loss: 0.859754, acc: 0.187500]\n",
            "6517: [discriminator loss: 0.7406144738197327, acc: 0.0703125] [gan loss: 1.659281, acc: 0.000000]\n",
            "6518: [discriminator loss: 0.6891874074935913, acc: 0.4140625] [gan loss: 0.800869, acc: 0.343750]\n",
            "6519: [discriminator loss: 0.7611546516418457, acc: 0.03125] [gan loss: 1.549264, acc: 0.000000]\n",
            "6520: [discriminator loss: 0.6943137645721436, acc: 0.4453125] [gan loss: 0.667857, acc: 0.640625]\n",
            "6521: [discriminator loss: 0.7665160298347473, acc: 0.03125] [gan loss: 1.538532, acc: 0.000000]\n",
            "6522: [discriminator loss: 0.6792173981666565, acc: 0.453125] [gan loss: 0.820623, acc: 0.328125]\n",
            "6523: [discriminator loss: 0.7708247900009155, acc: 0.0625] [gan loss: 1.701152, acc: 0.000000]\n",
            "6524: [discriminator loss: 0.6801493167877197, acc: 0.46875] [gan loss: 0.693400, acc: 0.531250]\n",
            "6525: [discriminator loss: 0.7418969869613647, acc: 0.0390625] [gan loss: 1.676382, acc: 0.000000]\n",
            "6526: [discriminator loss: 0.6915468573570251, acc: 0.4453125] [gan loss: 0.745809, acc: 0.375000]\n",
            "6527: [discriminator loss: 0.7674173712730408, acc: 0.03125] [gan loss: 1.715575, acc: 0.000000]\n",
            "6528: [discriminator loss: 0.6620794534683228, acc: 0.484375] [gan loss: 0.691832, acc: 0.500000]\n",
            "6529: [discriminator loss: 0.7575761675834656, acc: 0.0546875] [gan loss: 1.426549, acc: 0.015625]\n",
            "6530: [discriminator loss: 0.6573025584220886, acc: 0.453125] [gan loss: 0.743691, acc: 0.390625]\n",
            "6531: [discriminator loss: 0.7787787914276123, acc: 0.0390625] [gan loss: 1.562173, acc: 0.015625]\n",
            "6532: [discriminator loss: 0.7041724920272827, acc: 0.453125] [gan loss: 0.733691, acc: 0.484375]\n",
            "6533: [discriminator loss: 0.7674328088760376, acc: 0.0234375] [gan loss: 1.634609, acc: 0.000000]\n",
            "6534: [discriminator loss: 0.7160494327545166, acc: 0.4296875] [gan loss: 0.751246, acc: 0.406250]\n",
            "6535: [discriminator loss: 0.7973330020904541, acc: 0.015625] [gan loss: 1.770878, acc: 0.000000]\n",
            "6536: [discriminator loss: 0.7090534567832947, acc: 0.453125] [gan loss: 0.608236, acc: 0.718750]\n",
            "6537: [discriminator loss: 0.7562776803970337, acc: 0.0546875] [gan loss: 1.409925, acc: 0.000000]\n",
            "6538: [discriminator loss: 0.6889104843139648, acc: 0.3828125] [gan loss: 0.843262, acc: 0.187500]\n",
            "6539: [discriminator loss: 0.7335982322692871, acc: 0.09375] [gan loss: 1.266117, acc: 0.015625]\n",
            "6540: [discriminator loss: 0.6981462836265564, acc: 0.296875] [gan loss: 1.112423, acc: 0.015625]\n",
            "6541: [discriminator loss: 0.706531286239624, acc: 0.1875] [gan loss: 1.181524, acc: 0.031250]\n",
            "6542: [discriminator loss: 0.701382040977478, acc: 0.3046875] [gan loss: 0.985517, acc: 0.093750]\n",
            "6543: [discriminator loss: 0.724595844745636, acc: 0.109375] [gan loss: 1.492497, acc: 0.000000]\n",
            "6544: [discriminator loss: 0.6774513721466064, acc: 0.3828125] [gan loss: 0.821099, acc: 0.328125]\n",
            "6545: [discriminator loss: 0.7494425773620605, acc: 0.0703125] [gan loss: 1.667190, acc: 0.000000]\n",
            "6546: [discriminator loss: 0.6769841909408569, acc: 0.4765625] [gan loss: 0.640214, acc: 0.625000]\n",
            "6547: [discriminator loss: 0.7891631722450256, acc: 0.0] [gan loss: 1.762510, acc: 0.000000]\n",
            "6548: [discriminator loss: 0.6975024938583374, acc: 0.4765625] [gan loss: 0.691981, acc: 0.531250]\n",
            "6549: [discriminator loss: 0.7755348682403564, acc: 0.0390625] [gan loss: 1.496940, acc: 0.000000]\n",
            "6550: [discriminator loss: 0.6897763609886169, acc: 0.421875] [gan loss: 0.748299, acc: 0.437500]\n",
            "6551: [discriminator loss: 0.7561821937561035, acc: 0.0390625] [gan loss: 1.477360, acc: 0.000000]\n",
            "6552: [discriminator loss: 0.6663016080856323, acc: 0.4140625] [gan loss: 0.788339, acc: 0.375000]\n",
            "6553: [discriminator loss: 0.721872091293335, acc: 0.0859375] [gan loss: 1.378830, acc: 0.000000]\n",
            "6554: [discriminator loss: 0.6935461759567261, acc: 0.296875] [gan loss: 0.984377, acc: 0.109375]\n",
            "6555: [discriminator loss: 0.7203710079193115, acc: 0.1484375] [gan loss: 1.391073, acc: 0.000000]\n",
            "6556: [discriminator loss: 0.669916033744812, acc: 0.421875] [gan loss: 0.843213, acc: 0.343750]\n",
            "6557: [discriminator loss: 0.7775042653083801, acc: 0.0703125] [gan loss: 1.781660, acc: 0.000000]\n",
            "6558: [discriminator loss: 0.6952843070030212, acc: 0.46875] [gan loss: 0.633978, acc: 0.671875]\n",
            "6559: [discriminator loss: 0.7996848821640015, acc: 0.0390625] [gan loss: 1.668627, acc: 0.015625]\n",
            "6560: [discriminator loss: 0.69940584897995, acc: 0.4609375] [gan loss: 0.616224, acc: 0.734375]\n",
            "6561: [discriminator loss: 0.8049113750457764, acc: 0.0234375] [gan loss: 1.621254, acc: 0.000000]\n",
            "6562: [discriminator loss: 0.6812098622322083, acc: 0.453125] [gan loss: 0.720255, acc: 0.421875]\n",
            "6563: [discriminator loss: 0.7835432291030884, acc: 0.046875] [gan loss: 1.486760, acc: 0.000000]\n",
            "6564: [discriminator loss: 0.6665826439857483, acc: 0.4609375] [gan loss: 0.713769, acc: 0.484375]\n",
            "6565: [discriminator loss: 0.7874712944030762, acc: 0.03125] [gan loss: 1.558263, acc: 0.000000]\n",
            "6566: [discriminator loss: 0.6909862756729126, acc: 0.4375] [gan loss: 0.794241, acc: 0.312500]\n",
            "6567: [discriminator loss: 0.7654022574424744, acc: 0.046875] [gan loss: 1.526327, acc: 0.000000]\n",
            "6568: [discriminator loss: 0.7072591781616211, acc: 0.4375] [gan loss: 0.695445, acc: 0.546875]\n",
            "6569: [discriminator loss: 0.7838568687438965, acc: 0.0234375] [gan loss: 1.651522, acc: 0.000000]\n",
            "6570: [discriminator loss: 0.6909980177879333, acc: 0.4765625] [gan loss: 0.683170, acc: 0.546875]\n",
            "6571: [discriminator loss: 0.7511109709739685, acc: 0.0625] [gan loss: 1.616977, acc: 0.000000]\n",
            "6572: [discriminator loss: 0.6906594634056091, acc: 0.4375] [gan loss: 0.774863, acc: 0.343750]\n",
            "6573: [discriminator loss: 0.762602686882019, acc: 0.0234375] [gan loss: 1.567332, acc: 0.000000]\n",
            "6574: [discriminator loss: 0.6896892786026001, acc: 0.4453125] [gan loss: 0.730548, acc: 0.437500]\n",
            "6575: [discriminator loss: 0.7388907670974731, acc: 0.03125] [gan loss: 1.479312, acc: 0.000000]\n",
            "6576: [discriminator loss: 0.6924373507499695, acc: 0.421875] [gan loss: 0.816989, acc: 0.281250]\n",
            "6577: [discriminator loss: 0.7779500484466553, acc: 0.015625] [gan loss: 1.648169, acc: 0.000000]\n",
            "6578: [discriminator loss: 0.6944547891616821, acc: 0.4453125] [gan loss: 0.646678, acc: 0.640625]\n",
            "6579: [discriminator loss: 0.7827800512313843, acc: 0.0] [gan loss: 1.691512, acc: 0.000000]\n",
            "6580: [discriminator loss: 0.6837685108184814, acc: 0.484375] [gan loss: 0.679582, acc: 0.593750]\n",
            "6581: [discriminator loss: 0.7954730987548828, acc: 0.0] [gan loss: 1.554688, acc: 0.000000]\n",
            "6582: [discriminator loss: 0.6854745149612427, acc: 0.4375] [gan loss: 0.664937, acc: 0.593750]\n",
            "6583: [discriminator loss: 0.75108802318573, acc: 0.0234375] [gan loss: 1.379269, acc: 0.000000]\n",
            "6584: [discriminator loss: 0.6802669763565063, acc: 0.4140625] [gan loss: 0.726214, acc: 0.421875]\n",
            "6585: [discriminator loss: 0.7725361585617065, acc: 0.0234375] [gan loss: 1.409922, acc: 0.000000]\n",
            "6586: [discriminator loss: 0.6789925694465637, acc: 0.375] [gan loss: 0.916371, acc: 0.218750]\n",
            "6587: [discriminator loss: 0.7019113302230835, acc: 0.1640625] [gan loss: 1.287804, acc: 0.000000]\n",
            "6588: [discriminator loss: 0.7148821353912354, acc: 0.2578125] [gan loss: 1.082137, acc: 0.000000]\n",
            "6589: [discriminator loss: 0.699783444404602, acc: 0.171875] [gan loss: 1.253284, acc: 0.000000]\n",
            "6590: [discriminator loss: 0.7169413566589355, acc: 0.234375] [gan loss: 1.230492, acc: 0.000000]\n",
            "6591: [discriminator loss: 0.6945990324020386, acc: 0.28125] [gan loss: 1.115710, acc: 0.031250]\n",
            "6592: [discriminator loss: 0.6730815172195435, acc: 0.234375] [gan loss: 1.296434, acc: 0.000000]\n",
            "6593: [discriminator loss: 0.68962562084198, acc: 0.2734375] [gan loss: 1.023429, acc: 0.078125]\n",
            "6594: [discriminator loss: 0.7055190801620483, acc: 0.1796875] [gan loss: 1.299278, acc: 0.000000]\n",
            "6595: [discriminator loss: 0.6902188062667847, acc: 0.3203125] [gan loss: 1.116087, acc: 0.031250]\n",
            "6596: [discriminator loss: 0.7163543701171875, acc: 0.1796875] [gan loss: 1.340555, acc: 0.000000]\n",
            "6597: [discriminator loss: 0.6557036638259888, acc: 0.375] [gan loss: 0.934461, acc: 0.125000]\n",
            "6598: [discriminator loss: 0.7244943380355835, acc: 0.109375] [gan loss: 1.828127, acc: 0.000000]\n",
            "6599: [discriminator loss: 0.6844702959060669, acc: 0.4765625] [gan loss: 0.566525, acc: 0.750000]\n",
            "6600: [discriminator loss: 0.8547915816307068, acc: 0.0] [gan loss: 2.163957, acc: 0.000000]\n",
            "6601: [discriminator loss: 0.7959277629852295, acc: 0.484375] [gan loss: 0.414504, acc: 0.984375]\n",
            "6602: [discriminator loss: 0.881912887096405, acc: 0.0] [gan loss: 1.577553, acc: 0.015625]\n",
            "6603: [discriminator loss: 0.7117513418197632, acc: 0.4140625] [gan loss: 0.766105, acc: 0.375000]\n",
            "6604: [discriminator loss: 0.7492367029190063, acc: 0.0390625] [gan loss: 1.471066, acc: 0.000000]\n",
            "6605: [discriminator loss: 0.6869032382965088, acc: 0.3984375] [gan loss: 0.883913, acc: 0.250000]\n",
            "6606: [discriminator loss: 0.7566121220588684, acc: 0.0859375] [gan loss: 1.364621, acc: 0.015625]\n",
            "6607: [discriminator loss: 0.6796252727508545, acc: 0.265625] [gan loss: 1.113146, acc: 0.000000]\n",
            "6608: [discriminator loss: 0.6729729771614075, acc: 0.2265625] [gan loss: 1.162449, acc: 0.000000]\n",
            "6609: [discriminator loss: 0.6666437983512878, acc: 0.265625] [gan loss: 1.011975, acc: 0.125000]\n",
            "6610: [discriminator loss: 0.7365919351577759, acc: 0.15625] [gan loss: 1.359064, acc: 0.015625]\n",
            "6611: [discriminator loss: 0.6829913854598999, acc: 0.328125] [gan loss: 0.976965, acc: 0.046875]\n",
            "6612: [discriminator loss: 0.6767311096191406, acc: 0.1640625] [gan loss: 1.250415, acc: 0.015625]\n",
            "6613: [discriminator loss: 0.6639658212661743, acc: 0.3125] [gan loss: 1.123480, acc: 0.062500]\n",
            "6614: [discriminator loss: 0.6835590600967407, acc: 0.25] [gan loss: 1.330928, acc: 0.015625]\n",
            "6615: [discriminator loss: 0.7217692136764526, acc: 0.2578125] [gan loss: 1.109788, acc: 0.015625]\n",
            "6616: [discriminator loss: 0.7082170844078064, acc: 0.25] [gan loss: 1.209681, acc: 0.015625]\n",
            "6617: [discriminator loss: 0.6820505261421204, acc: 0.25] [gan loss: 1.338691, acc: 0.000000]\n",
            "6618: [discriminator loss: 0.6872560977935791, acc: 0.328125] [gan loss: 0.903507, acc: 0.171875]\n",
            "6619: [discriminator loss: 0.7285217046737671, acc: 0.09375] [gan loss: 1.729814, acc: 0.000000]\n",
            "6620: [discriminator loss: 0.6965070962905884, acc: 0.4609375] [gan loss: 0.485974, acc: 0.921875]\n",
            "6621: [discriminator loss: 0.8734877705574036, acc: 0.0078125] [gan loss: 2.110647, acc: 0.000000]\n",
            "6622: [discriminator loss: 0.7744730710983276, acc: 0.5] [gan loss: 0.455701, acc: 0.906250]\n",
            "6623: [discriminator loss: 0.8850610256195068, acc: 0.0] [gan loss: 1.840320, acc: 0.000000]\n",
            "6624: [discriminator loss: 0.6779161095619202, acc: 0.46875] [gan loss: 0.680192, acc: 0.578125]\n",
            "6625: [discriminator loss: 0.7767095565795898, acc: 0.0078125] [gan loss: 1.417016, acc: 0.031250]\n",
            "6626: [discriminator loss: 0.6774113774299622, acc: 0.4296875] [gan loss: 0.838314, acc: 0.234375]\n",
            "6627: [discriminator loss: 0.7176764011383057, acc: 0.1171875] [gan loss: 1.280890, acc: 0.000000]\n",
            "6628: [discriminator loss: 0.6669152975082397, acc: 0.359375] [gan loss: 0.902110, acc: 0.171875]\n",
            "6629: [discriminator loss: 0.7193242311477661, acc: 0.078125] [gan loss: 1.457742, acc: 0.000000]\n",
            "6630: [discriminator loss: 0.6879515051841736, acc: 0.40625] [gan loss: 0.792914, acc: 0.312500]\n",
            "6631: [discriminator loss: 0.7473582625389099, acc: 0.0234375] [gan loss: 1.667182, acc: 0.000000]\n",
            "6632: [discriminator loss: 0.6864750385284424, acc: 0.4453125] [gan loss: 0.715712, acc: 0.453125]\n",
            "6633: [discriminator loss: 0.7596533894538879, acc: 0.03125] [gan loss: 1.495584, acc: 0.000000]\n",
            "6634: [discriminator loss: 0.6734538674354553, acc: 0.3984375] [gan loss: 0.808537, acc: 0.312500]\n",
            "6635: [discriminator loss: 0.7556852102279663, acc: 0.078125] [gan loss: 1.364664, acc: 0.000000]\n",
            "6636: [discriminator loss: 0.6873744130134583, acc: 0.3515625] [gan loss: 0.950817, acc: 0.093750]\n",
            "6637: [discriminator loss: 0.7292200326919556, acc: 0.125] [gan loss: 1.519390, acc: 0.000000]\n",
            "6638: [discriminator loss: 0.6642137765884399, acc: 0.3984375] [gan loss: 0.871661, acc: 0.140625]\n",
            "6639: [discriminator loss: 0.731410026550293, acc: 0.078125] [gan loss: 1.620083, acc: 0.000000]\n",
            "6640: [discriminator loss: 0.6749767065048218, acc: 0.4296875] [gan loss: 0.704395, acc: 0.500000]\n",
            "6641: [discriminator loss: 0.7535226345062256, acc: 0.0390625] [gan loss: 1.758336, acc: 0.000000]\n",
            "6642: [discriminator loss: 0.6600962281227112, acc: 0.484375] [gan loss: 0.581783, acc: 0.671875]\n",
            "6643: [discriminator loss: 0.8115626573562622, acc: 0.0078125] [gan loss: 1.635126, acc: 0.000000]\n",
            "6644: [discriminator loss: 0.7181724905967712, acc: 0.4375] [gan loss: 0.738392, acc: 0.484375]\n",
            "6645: [discriminator loss: 0.8187797665596008, acc: 0.0078125] [gan loss: 1.890276, acc: 0.000000]\n",
            "6646: [discriminator loss: 0.6694685220718384, acc: 0.4921875] [gan loss: 0.750892, acc: 0.437500]\n",
            "6647: [discriminator loss: 0.7441487312316895, acc: 0.0390625] [gan loss: 1.524601, acc: 0.015625]\n",
            "6648: [discriminator loss: 0.6928226351737976, acc: 0.453125] [gan loss: 0.744217, acc: 0.375000]\n",
            "6649: [discriminator loss: 0.811367392539978, acc: 0.015625] [gan loss: 1.658233, acc: 0.000000]\n",
            "6650: [discriminator loss: 0.6583704352378845, acc: 0.4765625] [gan loss: 0.662666, acc: 0.625000]\n",
            "6651: [discriminator loss: 0.7420421838760376, acc: 0.03125] [gan loss: 1.376667, acc: 0.000000]\n",
            "6652: [discriminator loss: 0.6808744668960571, acc: 0.4296875] [gan loss: 0.784838, acc: 0.296875]\n",
            "6653: [discriminator loss: 0.7416645884513855, acc: 0.0546875] [gan loss: 1.426442, acc: 0.000000]\n",
            "6654: [discriminator loss: 0.7149061560630798, acc: 0.3828125] [gan loss: 0.857658, acc: 0.125000]\n",
            "6655: [discriminator loss: 0.719194769859314, acc: 0.078125] [gan loss: 1.487298, acc: 0.000000]\n",
            "6656: [discriminator loss: 0.7028993368148804, acc: 0.3828125] [gan loss: 0.901727, acc: 0.171875]\n",
            "6657: [discriminator loss: 0.6951850652694702, acc: 0.078125] [gan loss: 1.348461, acc: 0.000000]\n",
            "6658: [discriminator loss: 0.6808426976203918, acc: 0.3203125] [gan loss: 0.871527, acc: 0.171875]\n",
            "6659: [discriminator loss: 0.7340828776359558, acc: 0.1015625] [gan loss: 1.428183, acc: 0.000000]\n",
            "6660: [discriminator loss: 0.695979118347168, acc: 0.40625] [gan loss: 0.843931, acc: 0.203125]\n",
            "6661: [discriminator loss: 0.7455819845199585, acc: 0.0625] [gan loss: 1.757285, acc: 0.000000]\n",
            "6662: [discriminator loss: 0.6674548387527466, acc: 0.453125] [gan loss: 0.729840, acc: 0.531250]\n",
            "6663: [discriminator loss: 0.7829071879386902, acc: 0.0078125] [gan loss: 1.745262, acc: 0.000000]\n",
            "6664: [discriminator loss: 0.7136112451553345, acc: 0.5] [gan loss: 0.525883, acc: 0.812500]\n",
            "6665: [discriminator loss: 0.8493168354034424, acc: 0.0078125] [gan loss: 1.639168, acc: 0.000000]\n",
            "6666: [discriminator loss: 0.6841275691986084, acc: 0.4609375] [gan loss: 0.720353, acc: 0.500000]\n",
            "6667: [discriminator loss: 0.7765011191368103, acc: 0.0703125] [gan loss: 1.439647, acc: 0.000000]\n",
            "6668: [discriminator loss: 0.6831378936767578, acc: 0.4140625] [gan loss: 0.816909, acc: 0.296875]\n",
            "6669: [discriminator loss: 0.7320540547370911, acc: 0.0625] [gan loss: 1.556037, acc: 0.000000]\n",
            "6670: [discriminator loss: 0.697770893573761, acc: 0.3984375] [gan loss: 0.687173, acc: 0.546875]\n",
            "6671: [discriminator loss: 0.7953312397003174, acc: 0.0234375] [gan loss: 1.554463, acc: 0.000000]\n",
            "6672: [discriminator loss: 0.68345707654953, acc: 0.4296875] [gan loss: 0.778671, acc: 0.359375]\n",
            "6673: [discriminator loss: 0.7220956087112427, acc: 0.0703125] [gan loss: 1.456689, acc: 0.000000]\n",
            "6674: [discriminator loss: 0.6690480709075928, acc: 0.4140625] [gan loss: 0.789687, acc: 0.390625]\n",
            "6675: [discriminator loss: 0.7616680860519409, acc: 0.046875] [gan loss: 1.750345, acc: 0.000000]\n",
            "6676: [discriminator loss: 0.6907215118408203, acc: 0.4609375] [gan loss: 0.613398, acc: 0.703125]\n",
            "6677: [discriminator loss: 0.7948296666145325, acc: 0.0234375] [gan loss: 1.677131, acc: 0.000000]\n",
            "6678: [discriminator loss: 0.6427002549171448, acc: 0.484375] [gan loss: 0.755192, acc: 0.375000]\n",
            "6679: [discriminator loss: 0.7609630823135376, acc: 0.0390625] [gan loss: 1.381612, acc: 0.015625]\n",
            "6680: [discriminator loss: 0.7027707099914551, acc: 0.3515625] [gan loss: 0.922738, acc: 0.156250]\n",
            "6681: [discriminator loss: 0.7176568508148193, acc: 0.0859375] [gan loss: 1.427771, acc: 0.000000]\n",
            "6682: [discriminator loss: 0.657254695892334, acc: 0.3828125] [gan loss: 0.913250, acc: 0.109375]\n",
            "6683: [discriminator loss: 0.7391726970672607, acc: 0.078125] [gan loss: 1.482256, acc: 0.000000]\n",
            "6684: [discriminator loss: 0.6686782836914062, acc: 0.4140625] [gan loss: 0.831288, acc: 0.250000]\n",
            "6685: [discriminator loss: 0.740179717540741, acc: 0.015625] [gan loss: 1.612548, acc: 0.000000]\n",
            "6686: [discriminator loss: 0.69622802734375, acc: 0.3984375] [gan loss: 0.710767, acc: 0.531250]\n",
            "6687: [discriminator loss: 0.775373637676239, acc: 0.046875] [gan loss: 1.618763, acc: 0.000000]\n",
            "6688: [discriminator loss: 0.6741849184036255, acc: 0.4765625] [gan loss: 0.761905, acc: 0.421875]\n",
            "6689: [discriminator loss: 0.7543163299560547, acc: 0.0390625] [gan loss: 1.700802, acc: 0.000000]\n",
            "6690: [discriminator loss: 0.6613041162490845, acc: 0.46875] [gan loss: 0.644236, acc: 0.609375]\n",
            "6691: [discriminator loss: 0.815382719039917, acc: 0.0078125] [gan loss: 1.667633, acc: 0.000000]\n",
            "6692: [discriminator loss: 0.7046504020690918, acc: 0.4453125] [gan loss: 0.635924, acc: 0.625000]\n",
            "6693: [discriminator loss: 0.7955009341239929, acc: 0.0390625] [gan loss: 1.644053, acc: 0.000000]\n",
            "6694: [discriminator loss: 0.6731909513473511, acc: 0.4375] [gan loss: 0.779774, acc: 0.328125]\n",
            "6695: [discriminator loss: 0.7640100717544556, acc: 0.046875] [gan loss: 1.490078, acc: 0.000000]\n",
            "6696: [discriminator loss: 0.6982876658439636, acc: 0.421875] [gan loss: 0.699080, acc: 0.500000]\n",
            "6697: [discriminator loss: 0.7480821013450623, acc: 0.0390625] [gan loss: 1.569397, acc: 0.000000]\n",
            "6698: [discriminator loss: 0.6873116493225098, acc: 0.4453125] [gan loss: 0.760155, acc: 0.343750]\n",
            "6699: [discriminator loss: 0.7538307309150696, acc: 0.0390625] [gan loss: 1.425225, acc: 0.000000]\n",
            "6700: [discriminator loss: 0.6955260038375854, acc: 0.3828125] [gan loss: 0.878710, acc: 0.187500]\n",
            "6701: [discriminator loss: 0.739519476890564, acc: 0.0703125] [gan loss: 1.438236, acc: 0.000000]\n",
            "6702: [discriminator loss: 0.6861571073532104, acc: 0.4140625] [gan loss: 0.834435, acc: 0.156250]\n",
            "6703: [discriminator loss: 0.7302414774894714, acc: 0.046875] [gan loss: 1.525123, acc: 0.000000]\n",
            "6704: [discriminator loss: 0.6859043836593628, acc: 0.4609375] [gan loss: 0.679847, acc: 0.562500]\n",
            "6705: [discriminator loss: 0.7339447140693665, acc: 0.015625] [gan loss: 1.559541, acc: 0.000000]\n",
            "6706: [discriminator loss: 0.6810046434402466, acc: 0.3984375] [gan loss: 0.774319, acc: 0.343750]\n",
            "6707: [discriminator loss: 0.7305964231491089, acc: 0.0859375] [gan loss: 1.616614, acc: 0.000000]\n",
            "6708: [discriminator loss: 0.7268925905227661, acc: 0.4140625] [gan loss: 0.767215, acc: 0.343750]\n",
            "6709: [discriminator loss: 0.7818830013275146, acc: 0.0390625] [gan loss: 1.759969, acc: 0.000000]\n",
            "6710: [discriminator loss: 0.6806066632270813, acc: 0.4609375] [gan loss: 0.667441, acc: 0.578125]\n",
            "6711: [discriminator loss: 0.7839325666427612, acc: 0.0234375] [gan loss: 1.577227, acc: 0.000000]\n",
            "6712: [discriminator loss: 0.6713849902153015, acc: 0.484375] [gan loss: 0.702692, acc: 0.468750]\n",
            "6713: [discriminator loss: 0.7895214557647705, acc: 0.015625] [gan loss: 1.588347, acc: 0.000000]\n",
            "6714: [discriminator loss: 0.6820842027664185, acc: 0.4296875] [gan loss: 0.827642, acc: 0.265625]\n",
            "6715: [discriminator loss: 0.7281199097633362, acc: 0.0234375] [gan loss: 1.511358, acc: 0.000000]\n",
            "6716: [discriminator loss: 0.6865264177322388, acc: 0.4140625] [gan loss: 0.793408, acc: 0.343750]\n",
            "6717: [discriminator loss: 0.7335650324821472, acc: 0.0859375] [gan loss: 1.438733, acc: 0.000000]\n",
            "6718: [discriminator loss: 0.6919227242469788, acc: 0.4296875] [gan loss: 0.846169, acc: 0.296875]\n",
            "6719: [discriminator loss: 0.7357965707778931, acc: 0.0390625] [gan loss: 1.583104, acc: 0.000000]\n",
            "6720: [discriminator loss: 0.6795114874839783, acc: 0.4296875] [gan loss: 0.742455, acc: 0.390625]\n",
            "6721: [discriminator loss: 0.7723686695098877, acc: 0.0625] [gan loss: 1.580410, acc: 0.000000]\n",
            "6722: [discriminator loss: 0.7276521921157837, acc: 0.4765625] [gan loss: 0.635295, acc: 0.718750]\n",
            "6723: [discriminator loss: 0.7962542176246643, acc: 0.0] [gan loss: 1.571439, acc: 0.000000]\n",
            "6724: [discriminator loss: 0.690554141998291, acc: 0.4453125] [gan loss: 0.727468, acc: 0.484375]\n",
            "6725: [discriminator loss: 0.7478955388069153, acc: 0.0390625] [gan loss: 1.392069, acc: 0.000000]\n",
            "6726: [discriminator loss: 0.6850807070732117, acc: 0.4296875] [gan loss: 0.749587, acc: 0.406250]\n",
            "6727: [discriminator loss: 0.7689526677131653, acc: 0.015625] [gan loss: 1.559410, acc: 0.000000]\n",
            "6728: [discriminator loss: 0.6857807636260986, acc: 0.40625] [gan loss: 0.848859, acc: 0.250000]\n",
            "6729: [discriminator loss: 0.7185847759246826, acc: 0.09375] [gan loss: 1.457908, acc: 0.000000]\n",
            "6730: [discriminator loss: 0.6590110063552856, acc: 0.4140625] [gan loss: 0.854607, acc: 0.125000]\n",
            "6731: [discriminator loss: 0.7446783781051636, acc: 0.0859375] [gan loss: 1.534764, acc: 0.000000]\n",
            "6732: [discriminator loss: 0.698538064956665, acc: 0.4140625] [gan loss: 0.751482, acc: 0.390625]\n",
            "6733: [discriminator loss: 0.7622547149658203, acc: 0.03125] [gan loss: 1.620723, acc: 0.000000]\n",
            "6734: [discriminator loss: 0.6750299334526062, acc: 0.4375] [gan loss: 0.809117, acc: 0.296875]\n",
            "6735: [discriminator loss: 0.7301244139671326, acc: 0.0625] [gan loss: 1.529854, acc: 0.000000]\n",
            "6736: [discriminator loss: 0.6820846199989319, acc: 0.4375] [gan loss: 0.747216, acc: 0.453125]\n",
            "6737: [discriminator loss: 0.7528996467590332, acc: 0.0625] [gan loss: 1.557623, acc: 0.000000]\n",
            "6738: [discriminator loss: 0.7057302594184875, acc: 0.3828125] [gan loss: 0.730638, acc: 0.546875]\n",
            "6739: [discriminator loss: 0.7796601057052612, acc: 0.03125] [gan loss: 1.525205, acc: 0.000000]\n",
            "6740: [discriminator loss: 0.7180045247077942, acc: 0.4453125] [gan loss: 0.683684, acc: 0.562500]\n",
            "6741: [discriminator loss: 0.7931081652641296, acc: 0.015625] [gan loss: 1.620328, acc: 0.000000]\n",
            "6742: [discriminator loss: 0.689641535282135, acc: 0.4921875] [gan loss: 0.697964, acc: 0.500000]\n",
            "6743: [discriminator loss: 0.7996097207069397, acc: 0.03125] [gan loss: 1.446031, acc: 0.000000]\n",
            "6744: [discriminator loss: 0.6980705261230469, acc: 0.4296875] [gan loss: 0.757152, acc: 0.359375]\n",
            "6745: [discriminator loss: 0.7402232885360718, acc: 0.046875] [gan loss: 1.268899, acc: 0.000000]\n",
            "6746: [discriminator loss: 0.6830933094024658, acc: 0.3359375] [gan loss: 1.020343, acc: 0.000000]\n",
            "6747: [discriminator loss: 0.6857551336288452, acc: 0.1796875] [gan loss: 1.324528, acc: 0.000000]\n",
            "6748: [discriminator loss: 0.718974232673645, acc: 0.3046875] [gan loss: 0.966168, acc: 0.046875]\n",
            "6749: [discriminator loss: 0.6998084783554077, acc: 0.1015625] [gan loss: 1.451145, acc: 0.000000]\n",
            "6750: [discriminator loss: 0.6946295499801636, acc: 0.3828125] [gan loss: 0.849898, acc: 0.234375]\n",
            "6751: [discriminator loss: 0.7333368062973022, acc: 0.09375] [gan loss: 1.401095, acc: 0.000000]\n",
            "6752: [discriminator loss: 0.6843950748443604, acc: 0.40625] [gan loss: 0.776365, acc: 0.406250]\n",
            "6753: [discriminator loss: 0.7549455761909485, acc: 0.0390625] [gan loss: 1.761830, acc: 0.000000]\n",
            "6754: [discriminator loss: 0.7031722664833069, acc: 0.453125] [gan loss: 0.677872, acc: 0.546875]\n",
            "6755: [discriminator loss: 0.794928789138794, acc: 0.0234375] [gan loss: 1.934822, acc: 0.000000]\n",
            "6756: [discriminator loss: 0.7452061176300049, acc: 0.484375] [gan loss: 0.578060, acc: 0.828125]\n",
            "6757: [discriminator loss: 0.8410049676895142, acc: 0.015625] [gan loss: 1.616839, acc: 0.000000]\n",
            "6758: [discriminator loss: 0.6935845613479614, acc: 0.4921875] [gan loss: 0.717840, acc: 0.453125]\n",
            "6759: [discriminator loss: 0.7545921802520752, acc: 0.0234375] [gan loss: 1.396098, acc: 0.000000]\n",
            "6760: [discriminator loss: 0.6838964223861694, acc: 0.40625] [gan loss: 0.787572, acc: 0.312500]\n",
            "6761: [discriminator loss: 0.7407869100570679, acc: 0.0625] [gan loss: 1.316937, acc: 0.000000]\n",
            "6762: [discriminator loss: 0.6780729293823242, acc: 0.40625] [gan loss: 0.790482, acc: 0.328125]\n",
            "6763: [discriminator loss: 0.7105435132980347, acc: 0.0703125] [gan loss: 1.368733, acc: 0.015625]\n",
            "6764: [discriminator loss: 0.6992005109786987, acc: 0.390625] [gan loss: 0.952150, acc: 0.140625]\n",
            "6765: [discriminator loss: 0.7153601050376892, acc: 0.09375] [gan loss: 1.340968, acc: 0.000000]\n",
            "6766: [discriminator loss: 0.6931997537612915, acc: 0.3203125] [gan loss: 0.890824, acc: 0.203125]\n",
            "6767: [discriminator loss: 0.733512282371521, acc: 0.1328125] [gan loss: 1.416514, acc: 0.000000]\n",
            "6768: [discriminator loss: 0.7131479978561401, acc: 0.34375] [gan loss: 0.781805, acc: 0.375000]\n",
            "6769: [discriminator loss: 0.7127476334571838, acc: 0.1171875] [gan loss: 1.423742, acc: 0.000000]\n",
            "6770: [discriminator loss: 0.6988091468811035, acc: 0.4140625] [gan loss: 0.773536, acc: 0.359375]\n",
            "6771: [discriminator loss: 0.7951509952545166, acc: 0.0546875] [gan loss: 1.764241, acc: 0.000000]\n",
            "6772: [discriminator loss: 0.699302613735199, acc: 0.5] [gan loss: 0.548644, acc: 0.812500]\n",
            "6773: [discriminator loss: 0.8625365495681763, acc: 0.0] [gan loss: 1.797253, acc: 0.000000]\n",
            "6774: [discriminator loss: 0.7045033574104309, acc: 0.5] [gan loss: 0.605733, acc: 0.703125]\n",
            "6775: [discriminator loss: 0.7960841655731201, acc: 0.0078125] [gan loss: 1.500523, acc: 0.000000]\n",
            "6776: [discriminator loss: 0.6837598085403442, acc: 0.4609375] [gan loss: 0.713875, acc: 0.546875]\n",
            "6777: [discriminator loss: 0.7419770359992981, acc: 0.0234375] [gan loss: 1.361938, acc: 0.031250]\n",
            "6778: [discriminator loss: 0.6859297752380371, acc: 0.3515625] [gan loss: 0.972661, acc: 0.093750]\n",
            "6779: [discriminator loss: 0.7232621312141418, acc: 0.0703125] [gan loss: 1.333216, acc: 0.000000]\n",
            "6780: [discriminator loss: 0.6970229744911194, acc: 0.328125] [gan loss: 1.044802, acc: 0.093750]\n",
            "6781: [discriminator loss: 0.7212720513343811, acc: 0.09375] [gan loss: 1.265933, acc: 0.015625]\n",
            "6782: [discriminator loss: 0.6973214149475098, acc: 0.2890625] [gan loss: 1.043864, acc: 0.046875]\n",
            "6783: [discriminator loss: 0.7120686769485474, acc: 0.1796875] [gan loss: 1.376886, acc: 0.000000]\n",
            "6784: [discriminator loss: 0.6908842325210571, acc: 0.3125] [gan loss: 0.980799, acc: 0.078125]\n",
            "6785: [discriminator loss: 0.6880021095275879, acc: 0.1875] [gan loss: 1.337493, acc: 0.000000]\n",
            "6786: [discriminator loss: 0.6558669209480286, acc: 0.34375] [gan loss: 1.077734, acc: 0.078125]\n",
            "6787: [discriminator loss: 0.7354335784912109, acc: 0.15625] [gan loss: 1.360781, acc: 0.000000]\n",
            "6788: [discriminator loss: 0.6688013076782227, acc: 0.375] [gan loss: 0.894443, acc: 0.218750]\n",
            "6789: [discriminator loss: 0.6929150819778442, acc: 0.15625] [gan loss: 1.583142, acc: 0.000000]\n",
            "6790: [discriminator loss: 0.6973434686660767, acc: 0.4296875] [gan loss: 0.776481, acc: 0.375000]\n",
            "6791: [discriminator loss: 0.7640856504440308, acc: 0.078125] [gan loss: 1.777584, acc: 0.000000]\n",
            "6792: [discriminator loss: 0.709929347038269, acc: 0.484375] [gan loss: 0.507280, acc: 0.890625]\n",
            "6793: [discriminator loss: 0.8558943271636963, acc: 0.0] [gan loss: 1.954285, acc: 0.000000]\n",
            "6794: [discriminator loss: 0.7282267212867737, acc: 0.5] [gan loss: 0.563144, acc: 0.796875]\n",
            "6795: [discriminator loss: 0.8483237028121948, acc: 0.0078125] [gan loss: 1.681718, acc: 0.000000]\n",
            "6796: [discriminator loss: 0.7076461315155029, acc: 0.46875] [gan loss: 0.750830, acc: 0.437500]\n",
            "6797: [discriminator loss: 0.7790124416351318, acc: 0.046875] [gan loss: 1.508877, acc: 0.000000]\n",
            "6798: [discriminator loss: 0.6878083348274231, acc: 0.3671875] [gan loss: 0.767149, acc: 0.406250]\n",
            "6799: [discriminator loss: 0.7463696599006653, acc: 0.03125] [gan loss: 1.392406, acc: 0.000000]\n",
            "6800: [discriminator loss: 0.7089820504188538, acc: 0.390625] [gan loss: 0.768161, acc: 0.343750]\n",
            "6801: [discriminator loss: 0.7604515552520752, acc: 0.0625] [gan loss: 1.419456, acc: 0.000000]\n",
            "6802: [discriminator loss: 0.6815701723098755, acc: 0.4609375] [gan loss: 0.812664, acc: 0.234375]\n",
            "6803: [discriminator loss: 0.7162983417510986, acc: 0.09375] [gan loss: 1.355396, acc: 0.000000]\n",
            "6804: [discriminator loss: 0.675721287727356, acc: 0.375] [gan loss: 0.893154, acc: 0.171875]\n",
            "6805: [discriminator loss: 0.7356170415878296, acc: 0.078125] [gan loss: 1.373250, acc: 0.000000]\n",
            "6806: [discriminator loss: 0.6978961229324341, acc: 0.3203125] [gan loss: 0.980887, acc: 0.078125]\n",
            "6807: [discriminator loss: 0.6999963521957397, acc: 0.1171875] [gan loss: 1.365671, acc: 0.000000]\n",
            "6808: [discriminator loss: 0.6936445236206055, acc: 0.40625] [gan loss: 0.779647, acc: 0.296875]\n",
            "6809: [discriminator loss: 0.7496646642684937, acc: 0.0234375] [gan loss: 1.634083, acc: 0.000000]\n",
            "6810: [discriminator loss: 0.6858429312705994, acc: 0.4921875] [gan loss: 0.591405, acc: 0.750000]\n",
            "6811: [discriminator loss: 0.8241987228393555, acc: 0.015625] [gan loss: 1.835447, acc: 0.000000]\n",
            "6812: [discriminator loss: 0.711817741394043, acc: 0.4921875] [gan loss: 0.632696, acc: 0.671875]\n",
            "6813: [discriminator loss: 0.801655650138855, acc: 0.0078125] [gan loss: 1.565022, acc: 0.000000]\n",
            "6814: [discriminator loss: 0.7108805179595947, acc: 0.4140625] [gan loss: 0.756013, acc: 0.359375]\n",
            "6815: [discriminator loss: 0.7633938789367676, acc: 0.0390625] [gan loss: 1.419343, acc: 0.000000]\n",
            "6816: [discriminator loss: 0.6820668578147888, acc: 0.4375] [gan loss: 0.828014, acc: 0.296875]\n",
            "6817: [discriminator loss: 0.7482433319091797, acc: 0.0703125] [gan loss: 1.343213, acc: 0.000000]\n",
            "6818: [discriminator loss: 0.6608431339263916, acc: 0.4296875] [gan loss: 0.817972, acc: 0.343750]\n",
            "6819: [discriminator loss: 0.708911657333374, acc: 0.1484375] [gan loss: 1.250480, acc: 0.062500]\n",
            "6820: [discriminator loss: 0.6601646542549133, acc: 0.390625] [gan loss: 0.954195, acc: 0.156250]\n",
            "6821: [discriminator loss: 0.7168258428573608, acc: 0.1328125] [gan loss: 1.396866, acc: 0.015625]\n",
            "6822: [discriminator loss: 0.7079077959060669, acc: 0.3671875] [gan loss: 0.816810, acc: 0.281250]\n",
            "6823: [discriminator loss: 0.751067042350769, acc: 0.078125] [gan loss: 1.611667, acc: 0.015625]\n",
            "6824: [discriminator loss: 0.6899300217628479, acc: 0.4453125] [gan loss: 0.708035, acc: 0.500000]\n",
            "6825: [discriminator loss: 0.7722141742706299, acc: 0.0390625] [gan loss: 1.723620, acc: 0.000000]\n",
            "6826: [discriminator loss: 0.6984572410583496, acc: 0.4765625] [gan loss: 0.619382, acc: 0.703125]\n",
            "6827: [discriminator loss: 0.7782032489776611, acc: 0.03125] [gan loss: 1.666550, acc: 0.000000]\n",
            "6828: [discriminator loss: 0.6447062492370605, acc: 0.4921875] [gan loss: 0.622299, acc: 0.718750]\n",
            "6829: [discriminator loss: 0.7825539708137512, acc: 0.0390625] [gan loss: 1.524396, acc: 0.000000]\n",
            "6830: [discriminator loss: 0.7047710418701172, acc: 0.421875] [gan loss: 0.775020, acc: 0.359375]\n",
            "6831: [discriminator loss: 0.7495940923690796, acc: 0.0546875] [gan loss: 1.547412, acc: 0.000000]\n",
            "6832: [discriminator loss: 0.6790899634361267, acc: 0.46875] [gan loss: 0.713072, acc: 0.515625]\n",
            "6833: [discriminator loss: 0.7298738360404968, acc: 0.046875] [gan loss: 1.262185, acc: 0.000000]\n",
            "6834: [discriminator loss: 0.6852974891662598, acc: 0.34375] [gan loss: 0.923566, acc: 0.093750]\n",
            "6835: [discriminator loss: 0.7280153632164001, acc: 0.15625] [gan loss: 1.310738, acc: 0.000000]\n",
            "6836: [discriminator loss: 0.6866543292999268, acc: 0.390625] [gan loss: 0.948052, acc: 0.171875]\n",
            "6837: [discriminator loss: 0.707846462726593, acc: 0.203125] [gan loss: 1.325459, acc: 0.000000]\n",
            "6838: [discriminator loss: 0.6831976175308228, acc: 0.3046875] [gan loss: 1.050791, acc: 0.031250]\n",
            "6839: [discriminator loss: 0.7149406671524048, acc: 0.09375] [gan loss: 1.325523, acc: 0.000000]\n",
            "6840: [discriminator loss: 0.6687474250793457, acc: 0.4296875] [gan loss: 0.809851, acc: 0.328125]\n",
            "6841: [discriminator loss: 0.7632229328155518, acc: 0.0625] [gan loss: 1.622025, acc: 0.000000]\n",
            "6842: [discriminator loss: 0.6913298964500427, acc: 0.5] [gan loss: 0.571340, acc: 0.703125]\n",
            "6843: [discriminator loss: 0.8292482495307922, acc: 0.0390625] [gan loss: 1.746507, acc: 0.000000]\n",
            "6844: [discriminator loss: 0.7059833407402039, acc: 0.484375] [gan loss: 0.617480, acc: 0.750000]\n",
            "6845: [discriminator loss: 0.8180559873580933, acc: 0.0] [gan loss: 1.563545, acc: 0.000000]\n",
            "6846: [discriminator loss: 0.7059600353240967, acc: 0.484375] [gan loss: 0.630649, acc: 0.609375]\n",
            "6847: [discriminator loss: 0.7671929597854614, acc: 0.046875] [gan loss: 1.382453, acc: 0.000000]\n",
            "6848: [discriminator loss: 0.6809375286102295, acc: 0.390625] [gan loss: 0.895222, acc: 0.187500]\n",
            "6849: [discriminator loss: 0.6914564371109009, acc: 0.1328125] [gan loss: 1.245657, acc: 0.000000]\n",
            "6850: [discriminator loss: 0.693129301071167, acc: 0.234375] [gan loss: 1.044542, acc: 0.078125]\n",
            "6851: [discriminator loss: 0.7361898422241211, acc: 0.171875] [gan loss: 1.419667, acc: 0.000000]\n",
            "6852: [discriminator loss: 0.7029169201850891, acc: 0.328125] [gan loss: 0.913560, acc: 0.156250]\n",
            "6853: [discriminator loss: 0.7161115407943726, acc: 0.0859375] [gan loss: 1.456159, acc: 0.000000]\n",
            "6854: [discriminator loss: 0.6844035387039185, acc: 0.3984375] [gan loss: 0.825466, acc: 0.281250]\n",
            "6855: [discriminator loss: 0.7315008044242859, acc: 0.0390625] [gan loss: 1.434444, acc: 0.000000]\n",
            "6856: [discriminator loss: 0.7025502920150757, acc: 0.40625] [gan loss: 0.780694, acc: 0.328125]\n",
            "6857: [discriminator loss: 0.7601765394210815, acc: 0.0390625] [gan loss: 1.612240, acc: 0.000000]\n",
            "6858: [discriminator loss: 0.6788851022720337, acc: 0.46875] [gan loss: 0.715212, acc: 0.531250]\n",
            "6859: [discriminator loss: 0.7971120476722717, acc: 0.0] [gan loss: 1.837335, acc: 0.000000]\n",
            "6860: [discriminator loss: 0.7187744975090027, acc: 0.5] [gan loss: 0.592277, acc: 0.828125]\n",
            "6861: [discriminator loss: 0.8251288533210754, acc: 0.0] [gan loss: 1.580441, acc: 0.000000]\n",
            "6862: [discriminator loss: 0.6811890602111816, acc: 0.46875] [gan loss: 0.754339, acc: 0.406250]\n",
            "6863: [discriminator loss: 0.7384349703788757, acc: 0.0859375] [gan loss: 1.418999, acc: 0.015625]\n",
            "6864: [discriminator loss: 0.6801711916923523, acc: 0.390625] [gan loss: 0.849220, acc: 0.250000]\n",
            "6865: [discriminator loss: 0.7612829804420471, acc: 0.0703125] [gan loss: 1.466730, acc: 0.000000]\n",
            "6866: [discriminator loss: 0.7098673582077026, acc: 0.40625] [gan loss: 0.845625, acc: 0.156250]\n",
            "6867: [discriminator loss: 0.7599009275436401, acc: 0.0234375] [gan loss: 1.605677, acc: 0.000000]\n",
            "6868: [discriminator loss: 0.6788656115531921, acc: 0.46875] [gan loss: 0.817001, acc: 0.281250]\n",
            "6869: [discriminator loss: 0.7515360713005066, acc: 0.0078125] [gan loss: 1.589907, acc: 0.000000]\n",
            "6870: [discriminator loss: 0.6866487264633179, acc: 0.46875] [gan loss: 0.650988, acc: 0.625000]\n",
            "6871: [discriminator loss: 0.7696607112884521, acc: 0.015625] [gan loss: 1.497792, acc: 0.000000]\n",
            "6872: [discriminator loss: 0.6897655725479126, acc: 0.4375] [gan loss: 0.753549, acc: 0.453125]\n",
            "6873: [discriminator loss: 0.7372217774391174, acc: 0.0625] [gan loss: 1.348189, acc: 0.000000]\n",
            "6874: [discriminator loss: 0.6838518381118774, acc: 0.4140625] [gan loss: 0.823152, acc: 0.250000]\n",
            "6875: [discriminator loss: 0.7142428159713745, acc: 0.09375] [gan loss: 1.336905, acc: 0.000000]\n",
            "6876: [discriminator loss: 0.716893196105957, acc: 0.3125] [gan loss: 0.984040, acc: 0.078125]\n",
            "6877: [discriminator loss: 0.6903027296066284, acc: 0.15625] [gan loss: 1.190560, acc: 0.031250]\n",
            "6878: [discriminator loss: 0.6778956651687622, acc: 0.3203125] [gan loss: 0.991969, acc: 0.109375]\n",
            "6879: [discriminator loss: 0.7223509550094604, acc: 0.171875] [gan loss: 1.379537, acc: 0.000000]\n",
            "6880: [discriminator loss: 0.6902974247932434, acc: 0.3359375] [gan loss: 1.057029, acc: 0.046875]\n",
            "6881: [discriminator loss: 0.6983466148376465, acc: 0.1171875] [gan loss: 1.511777, acc: 0.000000]\n",
            "6882: [discriminator loss: 0.6728044748306274, acc: 0.421875] [gan loss: 0.624589, acc: 0.687500]\n",
            "6883: [discriminator loss: 0.757533848285675, acc: 0.03125] [gan loss: 1.905089, acc: 0.000000]\n",
            "6884: [discriminator loss: 0.7575704455375671, acc: 0.4921875] [gan loss: 0.456643, acc: 0.953125]\n",
            "6885: [discriminator loss: 0.8449076414108276, acc: 0.0] [gan loss: 1.761058, acc: 0.000000]\n",
            "6886: [discriminator loss: 0.6921994090080261, acc: 0.4921875] [gan loss: 0.628354, acc: 0.734375]\n",
            "6887: [discriminator loss: 0.7643221616744995, acc: 0.03125] [gan loss: 1.377748, acc: 0.000000]\n",
            "6888: [discriminator loss: 0.713951826095581, acc: 0.3828125] [gan loss: 0.845796, acc: 0.187500]\n",
            "6889: [discriminator loss: 0.7600352764129639, acc: 0.0625] [gan loss: 1.549056, acc: 0.000000]\n",
            "6890: [discriminator loss: 0.6952687501907349, acc: 0.40625] [gan loss: 0.817711, acc: 0.203125]\n",
            "6891: [discriminator loss: 0.7179332971572876, acc: 0.09375] [gan loss: 1.258824, acc: 0.015625]\n",
            "6892: [discriminator loss: 0.6730090379714966, acc: 0.3359375] [gan loss: 1.023699, acc: 0.031250]\n",
            "6893: [discriminator loss: 0.7007669806480408, acc: 0.171875] [gan loss: 1.165414, acc: 0.000000]\n",
            "6894: [discriminator loss: 0.6881982684135437, acc: 0.265625] [gan loss: 1.047752, acc: 0.015625]\n",
            "6895: [discriminator loss: 0.7202110886573792, acc: 0.171875] [gan loss: 1.316378, acc: 0.000000]\n",
            "6896: [discriminator loss: 0.6635256409645081, acc: 0.3515625] [gan loss: 0.979524, acc: 0.078125]\n",
            "6897: [discriminator loss: 0.7217808961868286, acc: 0.09375] [gan loss: 1.564988, acc: 0.015625]\n",
            "6898: [discriminator loss: 0.6793535947799683, acc: 0.4296875] [gan loss: 0.792214, acc: 0.312500]\n",
            "6899: [discriminator loss: 0.728183388710022, acc: 0.0546875] [gan loss: 1.427209, acc: 0.000000]\n",
            "6900: [discriminator loss: 0.7085565328598022, acc: 0.3671875] [gan loss: 0.812824, acc: 0.296875]\n",
            "6901: [discriminator loss: 0.7280476689338684, acc: 0.0234375] [gan loss: 1.678632, acc: 0.000000]\n",
            "6902: [discriminator loss: 0.7231737375259399, acc: 0.4375] [gan loss: 0.677368, acc: 0.593750]\n",
            "6903: [discriminator loss: 0.7686251401901245, acc: 0.0234375] [gan loss: 1.812404, acc: 0.000000]\n",
            "6904: [discriminator loss: 0.7016972899436951, acc: 0.484375] [gan loss: 0.620035, acc: 0.718750]\n",
            "6905: [discriminator loss: 0.8100215196609497, acc: 0.03125] [gan loss: 1.657800, acc: 0.000000]\n",
            "6906: [discriminator loss: 0.6666510701179504, acc: 0.4921875] [gan loss: 0.709679, acc: 0.546875]\n",
            "6907: [discriminator loss: 0.7682185173034668, acc: 0.0234375] [gan loss: 1.469497, acc: 0.000000]\n",
            "6908: [discriminator loss: 0.7144144773483276, acc: 0.3984375] [gan loss: 0.727499, acc: 0.421875]\n",
            "6909: [discriminator loss: 0.7334994673728943, acc: 0.0390625] [gan loss: 1.349675, acc: 0.015625]\n",
            "6910: [discriminator loss: 0.6764678955078125, acc: 0.40625] [gan loss: 0.813404, acc: 0.218750]\n",
            "6911: [discriminator loss: 0.7476361989974976, acc: 0.0625] [gan loss: 1.432205, acc: 0.000000]\n",
            "6912: [discriminator loss: 0.688754677772522, acc: 0.4375] [gan loss: 0.743957, acc: 0.406250]\n",
            "6913: [discriminator loss: 0.7639135122299194, acc: 0.0234375] [gan loss: 1.627585, acc: 0.000000]\n",
            "6914: [discriminator loss: 0.696976363658905, acc: 0.4375] [gan loss: 0.723196, acc: 0.500000]\n",
            "6915: [discriminator loss: 0.7651424407958984, acc: 0.03125] [gan loss: 1.506668, acc: 0.000000]\n",
            "6916: [discriminator loss: 0.7149242758750916, acc: 0.4296875] [gan loss: 0.733338, acc: 0.500000]\n",
            "6917: [discriminator loss: 0.7833325862884521, acc: 0.0] [gan loss: 1.634105, acc: 0.000000]\n",
            "6918: [discriminator loss: 0.6771541237831116, acc: 0.46875] [gan loss: 0.637056, acc: 0.609375]\n",
            "6919: [discriminator loss: 0.7669960856437683, acc: 0.03125] [gan loss: 1.584910, acc: 0.000000]\n",
            "6920: [discriminator loss: 0.6998206377029419, acc: 0.421875] [gan loss: 0.763974, acc: 0.296875]\n",
            "6921: [discriminator loss: 0.7963372468948364, acc: 0.0390625] [gan loss: 1.479108, acc: 0.000000]\n",
            "6922: [discriminator loss: 0.6898384094238281, acc: 0.4140625] [gan loss: 0.803801, acc: 0.312500]\n",
            "6923: [discriminator loss: 0.7377831935882568, acc: 0.0234375] [gan loss: 1.439322, acc: 0.000000]\n",
            "6924: [discriminator loss: 0.6733963489532471, acc: 0.4375] [gan loss: 0.797207, acc: 0.265625]\n",
            "6925: [discriminator loss: 0.7287912368774414, acc: 0.0703125] [gan loss: 1.401381, acc: 0.000000]\n",
            "6926: [discriminator loss: 0.6764371395111084, acc: 0.4140625] [gan loss: 0.809434, acc: 0.296875]\n",
            "6927: [discriminator loss: 0.7303460836410522, acc: 0.1015625] [gan loss: 1.594269, acc: 0.000000]\n",
            "6928: [discriminator loss: 0.666002631187439, acc: 0.453125] [gan loss: 0.724441, acc: 0.515625]\n",
            "6929: [discriminator loss: 0.7713620066642761, acc: 0.078125] [gan loss: 1.481319, acc: 0.000000]\n",
            "6930: [discriminator loss: 0.6701357960700989, acc: 0.4609375] [gan loss: 0.694163, acc: 0.562500]\n",
            "6931: [discriminator loss: 0.7756991386413574, acc: 0.046875] [gan loss: 1.610063, acc: 0.000000]\n",
            "6932: [discriminator loss: 0.7302834987640381, acc: 0.421875] [gan loss: 0.698766, acc: 0.531250]\n",
            "6933: [discriminator loss: 0.7750121355056763, acc: 0.03125] [gan loss: 1.439282, acc: 0.000000]\n",
            "6934: [discriminator loss: 0.6831203699111938, acc: 0.421875] [gan loss: 0.822964, acc: 0.203125]\n",
            "6935: [discriminator loss: 0.7352399826049805, acc: 0.0546875] [gan loss: 1.449169, acc: 0.000000]\n",
            "6936: [discriminator loss: 0.6934478282928467, acc: 0.4140625] [gan loss: 0.889165, acc: 0.093750]\n",
            "6937: [discriminator loss: 0.7189462184906006, acc: 0.1328125] [gan loss: 1.379110, acc: 0.000000]\n",
            "6938: [discriminator loss: 0.6980704069137573, acc: 0.3671875] [gan loss: 0.872285, acc: 0.171875]\n",
            "6939: [discriminator loss: 0.7154828310012817, acc: 0.1015625] [gan loss: 1.526355, acc: 0.000000]\n",
            "6940: [discriminator loss: 0.6782399415969849, acc: 0.453125] [gan loss: 0.705669, acc: 0.593750]\n",
            "6941: [discriminator loss: 0.7842015027999878, acc: 0.0625] [gan loss: 1.736379, acc: 0.000000]\n",
            "6942: [discriminator loss: 0.7066612243652344, acc: 0.4609375] [gan loss: 0.624516, acc: 0.671875]\n",
            "6943: [discriminator loss: 0.8301501274108887, acc: 0.0078125] [gan loss: 1.671065, acc: 0.000000]\n",
            "6944: [discriminator loss: 0.705442488193512, acc: 0.4609375] [gan loss: 0.684528, acc: 0.546875]\n",
            "6945: [discriminator loss: 0.7611732482910156, acc: 0.0625] [gan loss: 1.500188, acc: 0.000000]\n",
            "6946: [discriminator loss: 0.648091733455658, acc: 0.4453125] [gan loss: 0.824168, acc: 0.328125]\n",
            "6947: [discriminator loss: 0.7415724396705627, acc: 0.078125] [gan loss: 1.408128, acc: 0.015625]\n",
            "6948: [discriminator loss: 0.6998259425163269, acc: 0.40625] [gan loss: 0.875976, acc: 0.203125]\n",
            "6949: [discriminator loss: 0.7372671961784363, acc: 0.03125] [gan loss: 1.501018, acc: 0.015625]\n",
            "6950: [discriminator loss: 0.6910933256149292, acc: 0.4609375] [gan loss: 0.698277, acc: 0.531250]\n",
            "6951: [discriminator loss: 0.7845261096954346, acc: 0.0390625] [gan loss: 1.581667, acc: 0.000000]\n",
            "6952: [discriminator loss: 0.6853137612342834, acc: 0.421875] [gan loss: 0.755047, acc: 0.437500]\n",
            "6953: [discriminator loss: 0.7201690673828125, acc: 0.0859375] [gan loss: 1.332961, acc: 0.015625]\n",
            "6954: [discriminator loss: 0.696832537651062, acc: 0.3671875] [gan loss: 0.813398, acc: 0.312500]\n",
            "6955: [discriminator loss: 0.7503439784049988, acc: 0.0625] [gan loss: 1.536468, acc: 0.000000]\n",
            "6956: [discriminator loss: 0.6948496103286743, acc: 0.4296875] [gan loss: 0.675141, acc: 0.515625]\n",
            "6957: [discriminator loss: 0.7795317769050598, acc: 0.0078125] [gan loss: 1.451266, acc: 0.000000]\n",
            "6958: [discriminator loss: 0.7073550820350647, acc: 0.4765625] [gan loss: 0.617007, acc: 0.703125]\n",
            "6959: [discriminator loss: 0.8362531065940857, acc: 0.0] [gan loss: 1.644412, acc: 0.000000]\n",
            "6960: [discriminator loss: 0.6940593719482422, acc: 0.4765625] [gan loss: 0.683678, acc: 0.515625]\n",
            "6961: [discriminator loss: 0.757516622543335, acc: 0.03125] [gan loss: 1.469532, acc: 0.000000]\n",
            "6962: [discriminator loss: 0.6883758306503296, acc: 0.390625] [gan loss: 0.844822, acc: 0.171875]\n",
            "6963: [discriminator loss: 0.7338213920593262, acc: 0.078125] [gan loss: 1.348565, acc: 0.000000]\n",
            "6964: [discriminator loss: 0.6824097633361816, acc: 0.3671875] [gan loss: 0.858511, acc: 0.234375]\n",
            "6965: [discriminator loss: 0.7355426549911499, acc: 0.0703125] [gan loss: 1.409731, acc: 0.000000]\n",
            "6966: [discriminator loss: 0.6872467994689941, acc: 0.3984375] [gan loss: 0.846142, acc: 0.171875]\n",
            "6967: [discriminator loss: 0.7500815987586975, acc: 0.03125] [gan loss: 1.414501, acc: 0.000000]\n",
            "6968: [discriminator loss: 0.6851394176483154, acc: 0.421875] [gan loss: 0.665542, acc: 0.531250]\n",
            "6969: [discriminator loss: 0.7399755716323853, acc: 0.046875] [gan loss: 1.362720, acc: 0.000000]\n",
            "6970: [discriminator loss: 0.7005727291107178, acc: 0.3984375] [gan loss: 0.903066, acc: 0.203125]\n",
            "6971: [discriminator loss: 0.7506686449050903, acc: 0.0546875] [gan loss: 1.518693, acc: 0.000000]\n",
            "6972: [discriminator loss: 0.6932111978530884, acc: 0.421875] [gan loss: 0.838444, acc: 0.218750]\n",
            "6973: [discriminator loss: 0.7413015961647034, acc: 0.0625] [gan loss: 1.464246, acc: 0.000000]\n",
            "6974: [discriminator loss: 0.6628258228302002, acc: 0.4140625] [gan loss: 0.799272, acc: 0.296875]\n",
            "6975: [discriminator loss: 0.7536811828613281, acc: 0.0703125] [gan loss: 1.452658, acc: 0.000000]\n",
            "6976: [discriminator loss: 0.6871981620788574, acc: 0.421875] [gan loss: 0.720944, acc: 0.468750]\n",
            "6977: [discriminator loss: 0.7429229021072388, acc: 0.078125] [gan loss: 1.430948, acc: 0.015625]\n",
            "6978: [discriminator loss: 0.6752979755401611, acc: 0.4296875] [gan loss: 0.860763, acc: 0.265625]\n",
            "6979: [discriminator loss: 0.7239044904708862, acc: 0.1015625] [gan loss: 1.522896, acc: 0.000000]\n",
            "6980: [discriminator loss: 0.6693854331970215, acc: 0.4453125] [gan loss: 0.801772, acc: 0.328125]\n",
            "6981: [discriminator loss: 0.7511400580406189, acc: 0.046875] [gan loss: 1.704158, acc: 0.000000]\n",
            "6982: [discriminator loss: 0.6801446676254272, acc: 0.484375] [gan loss: 0.705522, acc: 0.500000]\n",
            "6983: [discriminator loss: 0.7777807712554932, acc: 0.0390625] [gan loss: 1.780905, acc: 0.000000]\n",
            "6984: [discriminator loss: 0.7144808173179626, acc: 0.4609375] [gan loss: 0.624902, acc: 0.687500]\n",
            "6985: [discriminator loss: 0.78126460313797, acc: 0.015625] [gan loss: 1.708076, acc: 0.000000]\n",
            "6986: [discriminator loss: 0.7111592888832092, acc: 0.4609375] [gan loss: 0.678659, acc: 0.593750]\n",
            "6987: [discriminator loss: 0.7800424695014954, acc: 0.03125] [gan loss: 1.694996, acc: 0.000000]\n",
            "6988: [discriminator loss: 0.6887536644935608, acc: 0.46875] [gan loss: 0.643422, acc: 0.625000]\n",
            "6989: [discriminator loss: 0.786479651927948, acc: 0.046875] [gan loss: 1.434637, acc: 0.000000]\n",
            "6990: [discriminator loss: 0.6950841546058655, acc: 0.4375] [gan loss: 0.765398, acc: 0.390625]\n",
            "6991: [discriminator loss: 0.7413667440414429, acc: 0.0703125] [gan loss: 1.470239, acc: 0.000000]\n",
            "6992: [discriminator loss: 0.7009607553482056, acc: 0.4453125] [gan loss: 0.784275, acc: 0.328125]\n",
            "6993: [discriminator loss: 0.7512508034706116, acc: 0.0078125] [gan loss: 1.538662, acc: 0.000000]\n",
            "6994: [discriminator loss: 0.676730215549469, acc: 0.4296875] [gan loss: 0.803228, acc: 0.265625]\n",
            "6995: [discriminator loss: 0.713683009147644, acc: 0.0703125] [gan loss: 1.464443, acc: 0.000000]\n",
            "6996: [discriminator loss: 0.691684365272522, acc: 0.34375] [gan loss: 0.760879, acc: 0.296875]\n",
            "6997: [discriminator loss: 0.7221977114677429, acc: 0.0625] [gan loss: 1.415127, acc: 0.000000]\n",
            "6998: [discriminator loss: 0.6872183084487915, acc: 0.421875] [gan loss: 0.791771, acc: 0.359375]\n",
            "6999: [discriminator loss: 0.7245228886604309, acc: 0.1171875] [gan loss: 1.419871, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "7000: [discriminator loss: 0.6737174987792969, acc: 0.390625] [gan loss: 0.859915, acc: 0.218750]\n",
            "7001: [discriminator loss: 0.751034140586853, acc: 0.0546875] [gan loss: 1.638504, acc: 0.000000]\n",
            "7002: [discriminator loss: 0.6853212118148804, acc: 0.4765625] [gan loss: 0.639280, acc: 0.687500]\n",
            "7003: [discriminator loss: 0.7931267023086548, acc: 0.0] [gan loss: 1.676692, acc: 0.000000]\n",
            "7004: [discriminator loss: 0.724615752696991, acc: 0.4375] [gan loss: 0.587533, acc: 0.765625]\n",
            "7005: [discriminator loss: 0.7842437624931335, acc: 0.0390625] [gan loss: 1.504017, acc: 0.000000]\n",
            "7006: [discriminator loss: 0.7037351727485657, acc: 0.421875] [gan loss: 0.765385, acc: 0.437500]\n",
            "7007: [discriminator loss: 0.7712810635566711, acc: 0.0390625] [gan loss: 1.277498, acc: 0.015625]\n",
            "7008: [discriminator loss: 0.7299380302429199, acc: 0.375] [gan loss: 0.841131, acc: 0.187500]\n",
            "7009: [discriminator loss: 0.7577559947967529, acc: 0.0390625] [gan loss: 1.403839, acc: 0.000000]\n",
            "7010: [discriminator loss: 0.6817623376846313, acc: 0.4296875] [gan loss: 0.673494, acc: 0.578125]\n",
            "7011: [discriminator loss: 0.7827285528182983, acc: 0.0390625] [gan loss: 1.459288, acc: 0.000000]\n",
            "7012: [discriminator loss: 0.7098971605300903, acc: 0.4140625] [gan loss: 0.838422, acc: 0.187500]\n",
            "7013: [discriminator loss: 0.7416590452194214, acc: 0.0546875] [gan loss: 1.526587, acc: 0.000000]\n",
            "7014: [discriminator loss: 0.6747786998748779, acc: 0.421875] [gan loss: 0.811276, acc: 0.359375]\n",
            "7015: [discriminator loss: 0.724697470664978, acc: 0.0859375] [gan loss: 1.529066, acc: 0.000000]\n",
            "7016: [discriminator loss: 0.6745033860206604, acc: 0.4296875] [gan loss: 0.837983, acc: 0.187500]\n",
            "7017: [discriminator loss: 0.7441635131835938, acc: 0.046875] [gan loss: 1.575891, acc: 0.000000]\n",
            "7018: [discriminator loss: 0.6789785027503967, acc: 0.4765625] [gan loss: 0.680122, acc: 0.625000]\n",
            "7019: [discriminator loss: 0.751022458076477, acc: 0.0078125] [gan loss: 1.515474, acc: 0.000000]\n",
            "7020: [discriminator loss: 0.7089319229125977, acc: 0.4609375] [gan loss: 0.601509, acc: 0.750000]\n",
            "7021: [discriminator loss: 0.7901172637939453, acc: 0.0078125] [gan loss: 1.611463, acc: 0.000000]\n",
            "7022: [discriminator loss: 0.7056590914726257, acc: 0.46875] [gan loss: 0.619517, acc: 0.687500]\n",
            "7023: [discriminator loss: 0.780543863773346, acc: 0.03125] [gan loss: 1.446105, acc: 0.000000]\n",
            "7024: [discriminator loss: 0.6551329493522644, acc: 0.4296875] [gan loss: 0.799612, acc: 0.312500]\n",
            "7025: [discriminator loss: 0.7445819973945618, acc: 0.0546875] [gan loss: 1.413163, acc: 0.000000]\n",
            "7026: [discriminator loss: 0.6832998991012573, acc: 0.4140625] [gan loss: 0.898519, acc: 0.218750]\n",
            "7027: [discriminator loss: 0.7556628584861755, acc: 0.0859375] [gan loss: 1.459874, acc: 0.000000]\n",
            "7028: [discriminator loss: 0.6850452423095703, acc: 0.3671875] [gan loss: 0.877171, acc: 0.140625]\n",
            "7029: [discriminator loss: 0.7305698394775391, acc: 0.0703125] [gan loss: 1.361267, acc: 0.000000]\n",
            "7030: [discriminator loss: 0.6898912191390991, acc: 0.375] [gan loss: 0.793249, acc: 0.296875]\n",
            "7031: [discriminator loss: 0.7528688311576843, acc: 0.078125] [gan loss: 1.622817, acc: 0.000000]\n",
            "7032: [discriminator loss: 0.6814382672309875, acc: 0.4375] [gan loss: 0.725414, acc: 0.421875]\n",
            "7033: [discriminator loss: 0.7760171890258789, acc: 0.03125] [gan loss: 1.647228, acc: 0.000000]\n",
            "7034: [discriminator loss: 0.6882608532905579, acc: 0.484375] [gan loss: 0.680758, acc: 0.578125]\n",
            "7035: [discriminator loss: 0.7577698230743408, acc: 0.0390625] [gan loss: 1.527054, acc: 0.000000]\n",
            "7036: [discriminator loss: 0.697051465511322, acc: 0.375] [gan loss: 0.837404, acc: 0.203125]\n",
            "7037: [discriminator loss: 0.7224381566047668, acc: 0.078125] [gan loss: 1.353701, acc: 0.000000]\n",
            "7038: [discriminator loss: 0.6986465454101562, acc: 0.375] [gan loss: 0.803901, acc: 0.343750]\n",
            "7039: [discriminator loss: 0.738106906414032, acc: 0.078125] [gan loss: 1.473380, acc: 0.000000]\n",
            "7040: [discriminator loss: 0.6707826256752014, acc: 0.40625] [gan loss: 0.747792, acc: 0.406250]\n",
            "7041: [discriminator loss: 0.7398571372032166, acc: 0.0703125] [gan loss: 1.578057, acc: 0.000000]\n",
            "7042: [discriminator loss: 0.680695116519928, acc: 0.46875] [gan loss: 0.695874, acc: 0.437500]\n",
            "7043: [discriminator loss: 0.7630281448364258, acc: 0.0] [gan loss: 1.574696, acc: 0.000000]\n",
            "7044: [discriminator loss: 0.6862941980361938, acc: 0.453125] [gan loss: 0.695426, acc: 0.453125]\n",
            "7045: [discriminator loss: 0.7681592106819153, acc: 0.0234375] [gan loss: 1.506241, acc: 0.000000]\n",
            "7046: [discriminator loss: 0.6997673511505127, acc: 0.4375] [gan loss: 0.713822, acc: 0.421875]\n",
            "7047: [discriminator loss: 0.740572452545166, acc: 0.0703125] [gan loss: 1.415839, acc: 0.015625]\n",
            "7048: [discriminator loss: 0.7137208580970764, acc: 0.421875] [gan loss: 0.811158, acc: 0.234375]\n",
            "7049: [discriminator loss: 0.7388600707054138, acc: 0.046875] [gan loss: 1.408468, acc: 0.000000]\n",
            "7050: [discriminator loss: 0.6987468004226685, acc: 0.4140625] [gan loss: 0.690387, acc: 0.484375]\n",
            "7051: [discriminator loss: 0.7829282879829407, acc: 0.03125] [gan loss: 1.593461, acc: 0.000000]\n",
            "7052: [discriminator loss: 0.6728370189666748, acc: 0.4765625] [gan loss: 0.701308, acc: 0.390625]\n",
            "7053: [discriminator loss: 0.7659702301025391, acc: 0.0390625] [gan loss: 1.531290, acc: 0.000000]\n",
            "7054: [discriminator loss: 0.703445315361023, acc: 0.40625] [gan loss: 0.837253, acc: 0.296875]\n",
            "7055: [discriminator loss: 0.7422361373901367, acc: 0.078125] [gan loss: 1.480759, acc: 0.000000]\n",
            "7056: [discriminator loss: 0.6737311482429504, acc: 0.421875] [gan loss: 0.847180, acc: 0.265625]\n",
            "7057: [discriminator loss: 0.7138116359710693, acc: 0.0859375] [gan loss: 1.368794, acc: 0.000000]\n",
            "7058: [discriminator loss: 0.7009613513946533, acc: 0.359375] [gan loss: 0.822754, acc: 0.218750]\n",
            "7059: [discriminator loss: 0.7764953374862671, acc: 0.0625] [gan loss: 1.492735, acc: 0.000000]\n",
            "7060: [discriminator loss: 0.7198529243469238, acc: 0.3984375] [gan loss: 0.796783, acc: 0.343750]\n",
            "7061: [discriminator loss: 0.7308502793312073, acc: 0.0859375] [gan loss: 1.620624, acc: 0.000000]\n",
            "7062: [discriminator loss: 0.6891202330589294, acc: 0.4453125] [gan loss: 0.619483, acc: 0.640625]\n",
            "7063: [discriminator loss: 0.7768218517303467, acc: 0.0078125] [gan loss: 1.785099, acc: 0.000000]\n",
            "7064: [discriminator loss: 0.7174917459487915, acc: 0.46875] [gan loss: 0.676071, acc: 0.578125]\n",
            "7065: [discriminator loss: 0.7745208740234375, acc: 0.015625] [gan loss: 1.672932, acc: 0.000000]\n",
            "7066: [discriminator loss: 0.7121713161468506, acc: 0.4609375] [gan loss: 0.767742, acc: 0.406250]\n",
            "7067: [discriminator loss: 0.7653274536132812, acc: 0.0234375] [gan loss: 1.499131, acc: 0.000000]\n",
            "7068: [discriminator loss: 0.6887757778167725, acc: 0.4765625] [gan loss: 0.736804, acc: 0.468750]\n",
            "7069: [discriminator loss: 0.7641748189926147, acc: 0.0078125] [gan loss: 1.423480, acc: 0.000000]\n",
            "7070: [discriminator loss: 0.6806091666221619, acc: 0.4375] [gan loss: 0.832206, acc: 0.281250]\n",
            "7071: [discriminator loss: 0.7268179655075073, acc: 0.0546875] [gan loss: 1.398029, acc: 0.015625]\n",
            "7072: [discriminator loss: 0.7023343443870544, acc: 0.3828125] [gan loss: 0.882862, acc: 0.109375]\n",
            "7073: [discriminator loss: 0.7571520805358887, acc: 0.0234375] [gan loss: 1.515282, acc: 0.000000]\n",
            "7074: [discriminator loss: 0.6968858242034912, acc: 0.421875] [gan loss: 0.684634, acc: 0.562500]\n",
            "7075: [discriminator loss: 0.7785242795944214, acc: 0.03125] [gan loss: 1.568349, acc: 0.000000]\n",
            "7076: [discriminator loss: 0.6799620389938354, acc: 0.484375] [gan loss: 0.704881, acc: 0.562500]\n",
            "7077: [discriminator loss: 0.7574905753135681, acc: 0.0234375] [gan loss: 1.449408, acc: 0.000000]\n",
            "7078: [discriminator loss: 0.6996626853942871, acc: 0.3828125] [gan loss: 0.939596, acc: 0.125000]\n",
            "7079: [discriminator loss: 0.7527132034301758, acc: 0.0703125] [gan loss: 1.283420, acc: 0.015625]\n",
            "7080: [discriminator loss: 0.6867959499359131, acc: 0.3515625] [gan loss: 0.925570, acc: 0.031250]\n",
            "7081: [discriminator loss: 0.7252678871154785, acc: 0.0546875] [gan loss: 1.477597, acc: 0.000000]\n",
            "7082: [discriminator loss: 0.6937023997306824, acc: 0.4375] [gan loss: 0.822244, acc: 0.250000]\n",
            "7083: [discriminator loss: 0.7259688377380371, acc: 0.0625] [gan loss: 1.446042, acc: 0.000000]\n",
            "7084: [discriminator loss: 0.6903799176216125, acc: 0.3671875] [gan loss: 0.812617, acc: 0.156250]\n",
            "7085: [discriminator loss: 0.7374841570854187, acc: 0.03125] [gan loss: 1.624665, acc: 0.000000]\n",
            "7086: [discriminator loss: 0.7000828981399536, acc: 0.4375] [gan loss: 0.622679, acc: 0.718750]\n",
            "7087: [discriminator loss: 0.808087944984436, acc: 0.0] [gan loss: 1.829400, acc: 0.000000]\n",
            "7088: [discriminator loss: 0.7127028107643127, acc: 0.5] [gan loss: 0.579467, acc: 0.781250]\n",
            "7089: [discriminator loss: 0.8011869192123413, acc: 0.0078125] [gan loss: 1.536631, acc: 0.000000]\n",
            "7090: [discriminator loss: 0.6836565732955933, acc: 0.453125] [gan loss: 0.720258, acc: 0.421875]\n",
            "7091: [discriminator loss: 0.7257513999938965, acc: 0.0546875] [gan loss: 1.345048, acc: 0.000000]\n",
            "7092: [discriminator loss: 0.6907537579536438, acc: 0.3203125] [gan loss: 0.869544, acc: 0.203125]\n",
            "7093: [discriminator loss: 0.7229761481285095, acc: 0.078125] [gan loss: 1.359212, acc: 0.000000]\n",
            "7094: [discriminator loss: 0.6924833059310913, acc: 0.3671875] [gan loss: 0.939253, acc: 0.078125]\n",
            "7095: [discriminator loss: 0.7377828359603882, acc: 0.078125] [gan loss: 1.337827, acc: 0.000000]\n",
            "7096: [discriminator loss: 0.705435037612915, acc: 0.328125] [gan loss: 1.026034, acc: 0.031250]\n",
            "7097: [discriminator loss: 0.7214247584342957, acc: 0.1015625] [gan loss: 1.448655, acc: 0.000000]\n",
            "7098: [discriminator loss: 0.6904011964797974, acc: 0.328125] [gan loss: 0.941303, acc: 0.109375]\n",
            "7099: [discriminator loss: 0.7301433086395264, acc: 0.1015625] [gan loss: 1.434431, acc: 0.000000]\n",
            "7100: [discriminator loss: 0.6819823384284973, acc: 0.3984375] [gan loss: 0.744329, acc: 0.484375]\n",
            "7101: [discriminator loss: 0.7422677874565125, acc: 0.0703125] [gan loss: 1.507146, acc: 0.000000]\n",
            "7102: [discriminator loss: 0.6883562803268433, acc: 0.3984375] [gan loss: 0.896278, acc: 0.140625]\n",
            "7103: [discriminator loss: 0.7163504362106323, acc: 0.09375] [gan loss: 1.543773, acc: 0.000000]\n",
            "7104: [discriminator loss: 0.6968376040458679, acc: 0.421875] [gan loss: 0.736268, acc: 0.406250]\n",
            "7105: [discriminator loss: 0.802513599395752, acc: 0.015625] [gan loss: 1.680938, acc: 0.000000]\n",
            "7106: [discriminator loss: 0.6946682333946228, acc: 0.46875] [gan loss: 0.600868, acc: 0.734375]\n",
            "7107: [discriminator loss: 0.809070348739624, acc: 0.015625] [gan loss: 1.781806, acc: 0.000000]\n",
            "7108: [discriminator loss: 0.715877115726471, acc: 0.4921875] [gan loss: 0.596688, acc: 0.765625]\n",
            "7109: [discriminator loss: 0.7985751032829285, acc: 0.0078125] [gan loss: 1.563535, acc: 0.000000]\n",
            "7110: [discriminator loss: 0.6852434873580933, acc: 0.4765625] [gan loss: 0.749779, acc: 0.375000]\n",
            "7111: [discriminator loss: 0.7529129981994629, acc: 0.0703125] [gan loss: 1.288576, acc: 0.000000]\n",
            "7112: [discriminator loss: 0.6987423896789551, acc: 0.3828125] [gan loss: 0.797855, acc: 0.312500]\n",
            "7113: [discriminator loss: 0.6982930898666382, acc: 0.125] [gan loss: 1.311837, acc: 0.000000]\n",
            "7114: [discriminator loss: 0.6995435953140259, acc: 0.234375] [gan loss: 1.110548, acc: 0.015625]\n",
            "7115: [discriminator loss: 0.6872968673706055, acc: 0.2421875] [gan loss: 0.950414, acc: 0.078125]\n",
            "7116: [discriminator loss: 0.7202932834625244, acc: 0.1640625] [gan loss: 1.329489, acc: 0.000000]\n",
            "7117: [discriminator loss: 0.7270232439041138, acc: 0.28125] [gan loss: 1.103752, acc: 0.078125]\n",
            "7118: [discriminator loss: 0.7012001276016235, acc: 0.21875] [gan loss: 1.229620, acc: 0.000000]\n",
            "7119: [discriminator loss: 0.7176188230514526, acc: 0.25] [gan loss: 1.055557, acc: 0.062500]\n",
            "7120: [discriminator loss: 0.7032119035720825, acc: 0.2109375] [gan loss: 1.259891, acc: 0.000000]\n",
            "7121: [discriminator loss: 0.700735867023468, acc: 0.3203125] [gan loss: 1.212644, acc: 0.015625]\n",
            "7122: [discriminator loss: 0.693047285079956, acc: 0.28125] [gan loss: 1.121000, acc: 0.000000]\n",
            "7123: [discriminator loss: 0.7213090062141418, acc: 0.1640625] [gan loss: 1.387707, acc: 0.000000]\n",
            "7124: [discriminator loss: 0.7018049359321594, acc: 0.3671875] [gan loss: 0.792502, acc: 0.359375]\n",
            "7125: [discriminator loss: 0.7415387630462646, acc: 0.078125] [gan loss: 1.883499, acc: 0.000000]\n",
            "7126: [discriminator loss: 0.7030476331710815, acc: 0.4921875] [gan loss: 0.419623, acc: 0.953125]\n",
            "7127: [discriminator loss: 0.8372064828872681, acc: 0.0078125] [gan loss: 1.959566, acc: 0.000000]\n",
            "7128: [discriminator loss: 0.7246754765510559, acc: 0.4765625] [gan loss: 0.587511, acc: 0.687500]\n",
            "7129: [discriminator loss: 0.8422773480415344, acc: 0.03125] [gan loss: 1.459583, acc: 0.015625]\n",
            "7130: [discriminator loss: 0.679681658744812, acc: 0.453125] [gan loss: 0.642581, acc: 0.671875]\n",
            "7131: [discriminator loss: 0.7999480962753296, acc: 0.0078125] [gan loss: 1.432631, acc: 0.000000]\n",
            "7132: [discriminator loss: 0.6966361999511719, acc: 0.390625] [gan loss: 0.836050, acc: 0.171875]\n",
            "7133: [discriminator loss: 0.7316243052482605, acc: 0.0625] [gan loss: 1.471321, acc: 0.000000]\n",
            "7134: [discriminator loss: 0.6634657382965088, acc: 0.40625] [gan loss: 0.886023, acc: 0.171875]\n",
            "7135: [discriminator loss: 0.7313375473022461, acc: 0.0859375] [gan loss: 1.455594, acc: 0.000000]\n",
            "7136: [discriminator loss: 0.6639363765716553, acc: 0.4140625] [gan loss: 0.791525, acc: 0.265625]\n",
            "7137: [discriminator loss: 0.7715091705322266, acc: 0.0390625] [gan loss: 1.444330, acc: 0.000000]\n",
            "7138: [discriminator loss: 0.7075796723365784, acc: 0.4140625] [gan loss: 0.799603, acc: 0.312500]\n",
            "7139: [discriminator loss: 0.7471752762794495, acc: 0.046875] [gan loss: 1.548402, acc: 0.000000]\n",
            "7140: [discriminator loss: 0.7042237520217896, acc: 0.4453125] [gan loss: 0.677982, acc: 0.609375]\n",
            "7141: [discriminator loss: 0.7826186418533325, acc: 0.0] [gan loss: 1.452922, acc: 0.000000]\n",
            "7142: [discriminator loss: 0.6860393285751343, acc: 0.4453125] [gan loss: 0.748824, acc: 0.406250]\n",
            "7143: [discriminator loss: 0.7759589552879333, acc: 0.015625] [gan loss: 1.593804, acc: 0.000000]\n",
            "7144: [discriminator loss: 0.7071325778961182, acc: 0.4609375] [gan loss: 0.647404, acc: 0.578125]\n",
            "7145: [discriminator loss: 0.8309657573699951, acc: 0.0] [gan loss: 1.569837, acc: 0.000000]\n",
            "7146: [discriminator loss: 0.6947541832923889, acc: 0.484375] [gan loss: 0.673568, acc: 0.640625]\n",
            "7147: [discriminator loss: 0.7738574743270874, acc: 0.015625] [gan loss: 1.464247, acc: 0.000000]\n",
            "7148: [discriminator loss: 0.6690604090690613, acc: 0.46875] [gan loss: 0.756382, acc: 0.453125]\n",
            "7149: [discriminator loss: 0.7711833715438843, acc: 0.03125] [gan loss: 1.327743, acc: 0.000000]\n",
            "7150: [discriminator loss: 0.6666725277900696, acc: 0.4375] [gan loss: 0.838687, acc: 0.312500]\n",
            "7151: [discriminator loss: 0.7481949329376221, acc: 0.078125] [gan loss: 1.339020, acc: 0.000000]\n",
            "7152: [discriminator loss: 0.6944055557250977, acc: 0.328125] [gan loss: 0.978106, acc: 0.046875]\n",
            "7153: [discriminator loss: 0.7180273532867432, acc: 0.09375] [gan loss: 1.363043, acc: 0.000000]\n",
            "7154: [discriminator loss: 0.6974233388900757, acc: 0.3125] [gan loss: 0.936841, acc: 0.109375]\n",
            "7155: [discriminator loss: 0.7070614099502563, acc: 0.09375] [gan loss: 1.490537, acc: 0.000000]\n",
            "7156: [discriminator loss: 0.6971429586410522, acc: 0.359375] [gan loss: 0.794955, acc: 0.281250]\n",
            "7157: [discriminator loss: 0.7437396049499512, acc: 0.0234375] [gan loss: 1.653877, acc: 0.000000]\n",
            "7158: [discriminator loss: 0.6885127425193787, acc: 0.4765625] [gan loss: 0.649804, acc: 0.609375]\n",
            "7159: [discriminator loss: 0.8188118934631348, acc: 0.0078125] [gan loss: 1.859370, acc: 0.000000]\n",
            "7160: [discriminator loss: 0.7149664163589478, acc: 0.5] [gan loss: 0.511884, acc: 0.875000]\n",
            "7161: [discriminator loss: 0.8372818231582642, acc: 0.0] [gan loss: 1.556227, acc: 0.000000]\n",
            "7162: [discriminator loss: 0.6940456628799438, acc: 0.4609375] [gan loss: 0.741611, acc: 0.421875]\n",
            "7163: [discriminator loss: 0.7521123886108398, acc: 0.0546875] [gan loss: 1.472796, acc: 0.000000]\n",
            "7164: [discriminator loss: 0.6923466324806213, acc: 0.40625] [gan loss: 0.766258, acc: 0.453125]\n",
            "7165: [discriminator loss: 0.7546262145042419, acc: 0.015625] [gan loss: 1.359494, acc: 0.000000]\n",
            "7166: [discriminator loss: 0.6938214302062988, acc: 0.390625] [gan loss: 0.858289, acc: 0.218750]\n",
            "7167: [discriminator loss: 0.7260417938232422, acc: 0.0859375] [gan loss: 1.355507, acc: 0.000000]\n",
            "7168: [discriminator loss: 0.6588932275772095, acc: 0.40625] [gan loss: 0.845081, acc: 0.234375]\n",
            "7169: [discriminator loss: 0.7487511038780212, acc: 0.09375] [gan loss: 1.477677, acc: 0.000000]\n",
            "7170: [discriminator loss: 0.6657662391662598, acc: 0.34375] [gan loss: 0.905764, acc: 0.140625]\n",
            "7171: [discriminator loss: 0.708057701587677, acc: 0.1171875] [gan loss: 1.429631, acc: 0.015625]\n",
            "7172: [discriminator loss: 0.6811591386795044, acc: 0.390625] [gan loss: 0.877457, acc: 0.187500]\n",
            "7173: [discriminator loss: 0.7242965698242188, acc: 0.0625] [gan loss: 1.419421, acc: 0.000000]\n",
            "7174: [discriminator loss: 0.7018274068832397, acc: 0.390625] [gan loss: 0.899396, acc: 0.171875]\n",
            "7175: [discriminator loss: 0.736467719078064, acc: 0.0625] [gan loss: 1.642606, acc: 0.000000]\n",
            "7176: [discriminator loss: 0.6565580368041992, acc: 0.4921875] [gan loss: 0.592194, acc: 0.718750]\n",
            "7177: [discriminator loss: 0.8114719986915588, acc: 0.015625] [gan loss: 1.702930, acc: 0.000000]\n",
            "7178: [discriminator loss: 0.7200565338134766, acc: 0.484375] [gan loss: 0.699595, acc: 0.531250]\n",
            "7179: [discriminator loss: 0.7667935490608215, acc: 0.015625] [gan loss: 1.557067, acc: 0.000000]\n",
            "7180: [discriminator loss: 0.7169584035873413, acc: 0.390625] [gan loss: 0.723570, acc: 0.515625]\n",
            "7181: [discriminator loss: 0.7633908987045288, acc: 0.0546875] [gan loss: 1.547878, acc: 0.000000]\n",
            "7182: [discriminator loss: 0.6852728724479675, acc: 0.46875] [gan loss: 0.669752, acc: 0.578125]\n",
            "7183: [discriminator loss: 0.7789275050163269, acc: 0.0703125] [gan loss: 1.558970, acc: 0.000000]\n",
            "7184: [discriminator loss: 0.7059856057167053, acc: 0.4609375] [gan loss: 0.754942, acc: 0.328125]\n",
            "7185: [discriminator loss: 0.8048747181892395, acc: 0.0] [gan loss: 1.648182, acc: 0.000000]\n",
            "7186: [discriminator loss: 0.698164165019989, acc: 0.484375] [gan loss: 0.630278, acc: 0.671875]\n",
            "7187: [discriminator loss: 0.7657702565193176, acc: 0.03125] [gan loss: 1.411508, acc: 0.000000]\n",
            "7188: [discriminator loss: 0.6922326683998108, acc: 0.40625] [gan loss: 0.805125, acc: 0.296875]\n",
            "7189: [discriminator loss: 0.7271960377693176, acc: 0.0625] [gan loss: 1.347121, acc: 0.000000]\n",
            "7190: [discriminator loss: 0.6826995611190796, acc: 0.375] [gan loss: 0.888283, acc: 0.156250]\n",
            "7191: [discriminator loss: 0.7076494693756104, acc: 0.1015625] [gan loss: 1.478562, acc: 0.000000]\n",
            "7192: [discriminator loss: 0.6839860677719116, acc: 0.4375] [gan loss: 0.698148, acc: 0.500000]\n",
            "7193: [discriminator loss: 0.7857942581176758, acc: 0.0234375] [gan loss: 1.745699, acc: 0.000000]\n",
            "7194: [discriminator loss: 0.7113321423530579, acc: 0.46875] [gan loss: 0.608878, acc: 0.718750]\n",
            "7195: [discriminator loss: 0.7797687649726868, acc: 0.0078125] [gan loss: 1.470868, acc: 0.000000]\n",
            "7196: [discriminator loss: 0.6722351312637329, acc: 0.4609375] [gan loss: 0.715413, acc: 0.453125]\n",
            "7197: [discriminator loss: 0.7547391653060913, acc: 0.0625] [gan loss: 1.401720, acc: 0.000000]\n",
            "7198: [discriminator loss: 0.6771957874298096, acc: 0.4140625] [gan loss: 0.770959, acc: 0.359375]\n",
            "7199: [discriminator loss: 0.7544598579406738, acc: 0.0546875] [gan loss: 1.282413, acc: 0.000000]\n",
            "7200: [discriminator loss: 0.6906054615974426, acc: 0.34375] [gan loss: 0.982132, acc: 0.093750]\n",
            "7201: [discriminator loss: 0.7336487174034119, acc: 0.09375] [gan loss: 1.268408, acc: 0.031250]\n",
            "7202: [discriminator loss: 0.7130905389785767, acc: 0.3203125] [gan loss: 0.930990, acc: 0.156250]\n",
            "7203: [discriminator loss: 0.7036499977111816, acc: 0.125] [gan loss: 1.236495, acc: 0.000000]\n",
            "7204: [discriminator loss: 0.6701081991195679, acc: 0.3203125] [gan loss: 0.980825, acc: 0.078125]\n",
            "7205: [discriminator loss: 0.705807089805603, acc: 0.140625] [gan loss: 1.249248, acc: 0.000000]\n",
            "7206: [discriminator loss: 0.7093896865844727, acc: 0.25] [gan loss: 1.192844, acc: 0.000000]\n",
            "7207: [discriminator loss: 0.6984115242958069, acc: 0.2421875] [gan loss: 1.193881, acc: 0.031250]\n",
            "7208: [discriminator loss: 0.6818239688873291, acc: 0.3203125] [gan loss: 0.915529, acc: 0.187500]\n",
            "7209: [discriminator loss: 0.7538461685180664, acc: 0.0859375] [gan loss: 1.761619, acc: 0.000000]\n",
            "7210: [discriminator loss: 0.6894459128379822, acc: 0.4765625] [gan loss: 0.502525, acc: 0.859375]\n",
            "7211: [discriminator loss: 0.8649376034736633, acc: 0.0234375] [gan loss: 1.876657, acc: 0.000000]\n",
            "7212: [discriminator loss: 0.7064197063446045, acc: 0.5] [gan loss: 0.529921, acc: 0.859375]\n",
            "7213: [discriminator loss: 0.8791654706001282, acc: 0.0] [gan loss: 1.685303, acc: 0.000000]\n",
            "7214: [discriminator loss: 0.6703727841377258, acc: 0.484375] [gan loss: 0.659270, acc: 0.546875]\n",
            "7215: [discriminator loss: 0.8260884284973145, acc: 0.0546875] [gan loss: 1.443093, acc: 0.000000]\n",
            "7216: [discriminator loss: 0.7026038765907288, acc: 0.3984375] [gan loss: 0.843547, acc: 0.203125]\n",
            "7217: [discriminator loss: 0.753258466720581, acc: 0.0625] [gan loss: 1.375874, acc: 0.000000]\n",
            "7218: [discriminator loss: 0.6951565146446228, acc: 0.390625] [gan loss: 0.828867, acc: 0.203125]\n",
            "7219: [discriminator loss: 0.7388225793838501, acc: 0.0390625] [gan loss: 1.571636, acc: 0.000000]\n",
            "7220: [discriminator loss: 0.6809781789779663, acc: 0.4453125] [gan loss: 0.696403, acc: 0.453125]\n",
            "7221: [discriminator loss: 0.7449881434440613, acc: 0.0234375] [gan loss: 1.650077, acc: 0.000000]\n",
            "7222: [discriminator loss: 0.7139223217964172, acc: 0.4296875] [gan loss: 0.790721, acc: 0.328125]\n",
            "7223: [discriminator loss: 0.7675286531448364, acc: 0.0390625] [gan loss: 1.355150, acc: 0.000000]\n",
            "7224: [discriminator loss: 0.6962403059005737, acc: 0.3671875] [gan loss: 0.931290, acc: 0.171875]\n",
            "7225: [discriminator loss: 0.7259870767593384, acc: 0.1328125] [gan loss: 1.347207, acc: 0.000000]\n",
            "7226: [discriminator loss: 0.6893596649169922, acc: 0.3671875] [gan loss: 0.923620, acc: 0.140625]\n",
            "7227: [discriminator loss: 0.7394278645515442, acc: 0.1015625] [gan loss: 1.440305, acc: 0.000000]\n",
            "7228: [discriminator loss: 0.6906419992446899, acc: 0.40625] [gan loss: 0.804987, acc: 0.312500]\n",
            "7229: [discriminator loss: 0.7568708658218384, acc: 0.0703125] [gan loss: 1.485854, acc: 0.000000]\n",
            "7230: [discriminator loss: 0.6849803924560547, acc: 0.4375] [gan loss: 0.666536, acc: 0.640625]\n",
            "7231: [discriminator loss: 0.762189507484436, acc: 0.015625] [gan loss: 1.660117, acc: 0.000000]\n",
            "7232: [discriminator loss: 0.6959511637687683, acc: 0.4453125] [gan loss: 0.625699, acc: 0.703125]\n",
            "7233: [discriminator loss: 0.8258577585220337, acc: 0.03125] [gan loss: 1.721084, acc: 0.000000]\n",
            "7234: [discriminator loss: 0.6887264251708984, acc: 0.4765625] [gan loss: 0.742409, acc: 0.375000]\n",
            "7235: [discriminator loss: 0.765362560749054, acc: 0.0390625] [gan loss: 1.454122, acc: 0.000000]\n",
            "7236: [discriminator loss: 0.6785126328468323, acc: 0.4296875] [gan loss: 0.731594, acc: 0.468750]\n",
            "7237: [discriminator loss: 0.7239577770233154, acc: 0.1015625] [gan loss: 1.480423, acc: 0.000000]\n",
            "7238: [discriminator loss: 0.6788520216941833, acc: 0.3671875] [gan loss: 0.874615, acc: 0.171875]\n",
            "7239: [discriminator loss: 0.7564007043838501, acc: 0.0625] [gan loss: 1.450698, acc: 0.000000]\n",
            "7240: [discriminator loss: 0.6931169033050537, acc: 0.375] [gan loss: 0.891810, acc: 0.125000]\n",
            "7241: [discriminator loss: 0.7189494371414185, acc: 0.1015625] [gan loss: 1.290806, acc: 0.000000]\n",
            "7242: [discriminator loss: 0.6989374756813049, acc: 0.34375] [gan loss: 0.862424, acc: 0.171875]\n",
            "7243: [discriminator loss: 0.7477219700813293, acc: 0.0546875] [gan loss: 1.567855, acc: 0.000000]\n",
            "7244: [discriminator loss: 0.693009614944458, acc: 0.453125] [gan loss: 0.816248, acc: 0.234375]\n",
            "7245: [discriminator loss: 0.7838829755783081, acc: 0.0] [gan loss: 1.761026, acc: 0.000000]\n",
            "7246: [discriminator loss: 0.6965304613113403, acc: 0.4921875] [gan loss: 0.608996, acc: 0.671875]\n",
            "7247: [discriminator loss: 0.7975103855133057, acc: 0.0234375] [gan loss: 1.536556, acc: 0.000000]\n",
            "7248: [discriminator loss: 0.6925776600837708, acc: 0.4453125] [gan loss: 0.717306, acc: 0.562500]\n",
            "7249: [discriminator loss: 0.7829304933547974, acc: 0.0859375] [gan loss: 1.471846, acc: 0.015625]\n",
            "7250: [discriminator loss: 0.6967123746871948, acc: 0.4296875] [gan loss: 0.765731, acc: 0.406250]\n",
            "7251: [discriminator loss: 0.7509526014328003, acc: 0.03125] [gan loss: 1.250658, acc: 0.015625]\n",
            "7252: [discriminator loss: 0.6541062593460083, acc: 0.3828125] [gan loss: 0.875805, acc: 0.218750]\n",
            "7253: [discriminator loss: 0.734092652797699, acc: 0.09375] [gan loss: 1.402889, acc: 0.000000]\n",
            "7254: [discriminator loss: 0.6948727965354919, acc: 0.3984375] [gan loss: 0.831481, acc: 0.281250]\n",
            "7255: [discriminator loss: 0.7172077894210815, acc: 0.1015625] [gan loss: 1.449741, acc: 0.000000]\n",
            "7256: [discriminator loss: 0.6822713613510132, acc: 0.3984375] [gan loss: 0.831959, acc: 0.265625]\n",
            "7257: [discriminator loss: 0.7653161287307739, acc: 0.03125] [gan loss: 1.499824, acc: 0.000000]\n",
            "7258: [discriminator loss: 0.6817382574081421, acc: 0.4453125] [gan loss: 0.746486, acc: 0.437500]\n",
            "7259: [discriminator loss: 0.7625914812088013, acc: 0.0390625] [gan loss: 1.637699, acc: 0.000000]\n",
            "7260: [discriminator loss: 0.6903049349784851, acc: 0.46875] [gan loss: 0.633319, acc: 0.593750]\n",
            "7261: [discriminator loss: 0.7760063409805298, acc: 0.03125] [gan loss: 1.577774, acc: 0.000000]\n",
            "7262: [discriminator loss: 0.6788163185119629, acc: 0.46875] [gan loss: 0.608619, acc: 0.703125]\n",
            "7263: [discriminator loss: 0.8084710836410522, acc: 0.03125] [gan loss: 1.597148, acc: 0.000000]\n",
            "7264: [discriminator loss: 0.6830224394798279, acc: 0.4765625] [gan loss: 0.619427, acc: 0.640625]\n",
            "7265: [discriminator loss: 0.8754585981369019, acc: 0.0078125] [gan loss: 1.684023, acc: 0.000000]\n",
            "7266: [discriminator loss: 0.6754076480865479, acc: 0.4765625] [gan loss: 0.667578, acc: 0.578125]\n",
            "7267: [discriminator loss: 0.8211972117424011, acc: 0.0234375] [gan loss: 1.483556, acc: 0.000000]\n",
            "7268: [discriminator loss: 0.6715123653411865, acc: 0.4765625] [gan loss: 0.724157, acc: 0.468750]\n",
            "7269: [discriminator loss: 0.7732133865356445, acc: 0.0234375] [gan loss: 1.441865, acc: 0.000000]\n",
            "7270: [discriminator loss: 0.6889744400978088, acc: 0.390625] [gan loss: 0.840790, acc: 0.156250]\n",
            "7271: [discriminator loss: 0.7326096892356873, acc: 0.0625] [gan loss: 1.470480, acc: 0.000000]\n",
            "7272: [discriminator loss: 0.673147439956665, acc: 0.40625] [gan loss: 0.770397, acc: 0.312500]\n",
            "7273: [discriminator loss: 0.7509427070617676, acc: 0.0625] [gan loss: 1.409256, acc: 0.000000]\n",
            "7274: [discriminator loss: 0.7137228846549988, acc: 0.359375] [gan loss: 0.883866, acc: 0.109375]\n",
            "7275: [discriminator loss: 0.7223626375198364, acc: 0.0703125] [gan loss: 1.295597, acc: 0.000000]\n",
            "7276: [discriminator loss: 0.690732479095459, acc: 0.34375] [gan loss: 0.967443, acc: 0.062500]\n",
            "7277: [discriminator loss: 0.7240061163902283, acc: 0.1328125] [gan loss: 1.386330, acc: 0.000000]\n",
            "7278: [discriminator loss: 0.6819891929626465, acc: 0.328125] [gan loss: 1.014753, acc: 0.031250]\n",
            "7279: [discriminator loss: 0.7032600045204163, acc: 0.1875] [gan loss: 1.343503, acc: 0.000000]\n",
            "7280: [discriminator loss: 0.6824061870574951, acc: 0.3515625] [gan loss: 0.964674, acc: 0.046875]\n",
            "7281: [discriminator loss: 0.731775164604187, acc: 0.109375] [gan loss: 1.468025, acc: 0.015625]\n",
            "7282: [discriminator loss: 0.6745576858520508, acc: 0.4140625] [gan loss: 0.821677, acc: 0.218750]\n",
            "7283: [discriminator loss: 0.7538290023803711, acc: 0.046875] [gan loss: 1.746509, acc: 0.000000]\n",
            "7284: [discriminator loss: 0.7041088342666626, acc: 0.4765625] [gan loss: 0.531121, acc: 0.875000]\n",
            "7285: [discriminator loss: 0.8580212593078613, acc: 0.0] [gan loss: 1.854568, acc: 0.000000]\n",
            "7286: [discriminator loss: 0.715366542339325, acc: 0.4921875] [gan loss: 0.555310, acc: 0.828125]\n",
            "7287: [discriminator loss: 0.8089833855628967, acc: 0.0] [gan loss: 1.431562, acc: 0.000000]\n",
            "7288: [discriminator loss: 0.6833025813102722, acc: 0.40625] [gan loss: 0.882468, acc: 0.140625]\n",
            "7289: [discriminator loss: 0.7641485333442688, acc: 0.0703125] [gan loss: 1.460712, acc: 0.000000]\n",
            "7290: [discriminator loss: 0.6962499618530273, acc: 0.3984375] [gan loss: 0.854527, acc: 0.234375]\n",
            "7291: [discriminator loss: 0.7780843377113342, acc: 0.0703125] [gan loss: 1.475715, acc: 0.000000]\n",
            "7292: [discriminator loss: 0.7003430724143982, acc: 0.4765625] [gan loss: 0.658815, acc: 0.578125]\n",
            "7293: [discriminator loss: 0.79460209608078, acc: 0.015625] [gan loss: 1.607831, acc: 0.000000]\n",
            "7294: [discriminator loss: 0.6807575225830078, acc: 0.4765625] [gan loss: 0.664321, acc: 0.562500]\n",
            "7295: [discriminator loss: 0.7725480198860168, acc: 0.0390625] [gan loss: 1.481525, acc: 0.015625]\n",
            "7296: [discriminator loss: 0.7086385488510132, acc: 0.40625] [gan loss: 0.795829, acc: 0.281250]\n",
            "7297: [discriminator loss: 0.7230989933013916, acc: 0.0703125] [gan loss: 1.439381, acc: 0.000000]\n",
            "7298: [discriminator loss: 0.6855524182319641, acc: 0.34375] [gan loss: 0.859759, acc: 0.140625]\n",
            "7299: [discriminator loss: 0.7286918759346008, acc: 0.1015625] [gan loss: 1.333008, acc: 0.000000]\n",
            "7300: [discriminator loss: 0.6970322132110596, acc: 0.3828125] [gan loss: 0.743647, acc: 0.421875]\n",
            "7301: [discriminator loss: 0.7326720952987671, acc: 0.0703125] [gan loss: 1.580174, acc: 0.000000]\n",
            "7302: [discriminator loss: 0.7228308916091919, acc: 0.359375] [gan loss: 0.848540, acc: 0.171875]\n",
            "7303: [discriminator loss: 0.7947592735290527, acc: 0.015625] [gan loss: 1.678229, acc: 0.000000]\n",
            "7304: [discriminator loss: 0.6927643418312073, acc: 0.46875] [gan loss: 0.655021, acc: 0.640625]\n",
            "7305: [discriminator loss: 0.8436106443405151, acc: 0.0078125] [gan loss: 1.615132, acc: 0.000000]\n",
            "7306: [discriminator loss: 0.7436944246292114, acc: 0.4453125] [gan loss: 0.526471, acc: 0.890625]\n",
            "7307: [discriminator loss: 0.8312761783599854, acc: 0.0] [gan loss: 1.477580, acc: 0.000000]\n",
            "7308: [discriminator loss: 0.7053331136703491, acc: 0.46875] [gan loss: 0.713959, acc: 0.484375]\n",
            "7309: [discriminator loss: 0.75777268409729, acc: 0.015625] [gan loss: 1.487509, acc: 0.000000]\n",
            "7310: [discriminator loss: 0.7124367952346802, acc: 0.46875] [gan loss: 0.745422, acc: 0.406250]\n",
            "7311: [discriminator loss: 0.795124351978302, acc: 0.0234375] [gan loss: 1.562846, acc: 0.000000]\n",
            "7312: [discriminator loss: 0.6926391124725342, acc: 0.4375] [gan loss: 0.737024, acc: 0.390625]\n",
            "7313: [discriminator loss: 0.7531198263168335, acc: 0.0546875] [gan loss: 1.337826, acc: 0.000000]\n",
            "7314: [discriminator loss: 0.6832951307296753, acc: 0.359375] [gan loss: 0.880635, acc: 0.187500]\n",
            "7315: [discriminator loss: 0.7283929586410522, acc: 0.03125] [gan loss: 1.499574, acc: 0.000000]\n",
            "7316: [discriminator loss: 0.6803005337715149, acc: 0.4140625] [gan loss: 0.778933, acc: 0.312500]\n",
            "7317: [discriminator loss: 0.7459813356399536, acc: 0.0390625] [gan loss: 1.442838, acc: 0.000000]\n",
            "7318: [discriminator loss: 0.6862853765487671, acc: 0.4375] [gan loss: 0.727474, acc: 0.453125]\n",
            "7319: [discriminator loss: 0.7490066885948181, acc: 0.078125] [gan loss: 1.402037, acc: 0.000000]\n",
            "7320: [discriminator loss: 0.6811845898628235, acc: 0.4296875] [gan loss: 0.801725, acc: 0.281250]\n",
            "7321: [discriminator loss: 0.7293624877929688, acc: 0.0625] [gan loss: 1.536421, acc: 0.000000]\n",
            "7322: [discriminator loss: 0.6756396293640137, acc: 0.421875] [gan loss: 0.837358, acc: 0.281250]\n",
            "7323: [discriminator loss: 0.7368401885032654, acc: 0.078125] [gan loss: 1.415971, acc: 0.000000]\n",
            "7324: [discriminator loss: 0.6634150147438049, acc: 0.421875] [gan loss: 0.831900, acc: 0.265625]\n",
            "7325: [discriminator loss: 0.7448977828025818, acc: 0.03125] [gan loss: 1.737757, acc: 0.000000]\n",
            "7326: [discriminator loss: 0.6915181875228882, acc: 0.46875] [gan loss: 0.606660, acc: 0.718750]\n",
            "7327: [discriminator loss: 0.8020712733268738, acc: 0.0390625] [gan loss: 1.646577, acc: 0.015625]\n",
            "7328: [discriminator loss: 0.6999107003211975, acc: 0.453125] [gan loss: 0.661793, acc: 0.546875]\n",
            "7329: [discriminator loss: 0.7945693731307983, acc: 0.046875] [gan loss: 1.629110, acc: 0.000000]\n",
            "7330: [discriminator loss: 0.7226935625076294, acc: 0.4453125] [gan loss: 0.665192, acc: 0.562500]\n",
            "7331: [discriminator loss: 0.7767922878265381, acc: 0.0234375] [gan loss: 1.559739, acc: 0.000000]\n",
            "7332: [discriminator loss: 0.6992016434669495, acc: 0.453125] [gan loss: 0.621223, acc: 0.718750]\n",
            "7333: [discriminator loss: 0.8032459020614624, acc: 0.015625] [gan loss: 1.448709, acc: 0.000000]\n",
            "7334: [discriminator loss: 0.6926550269126892, acc: 0.3984375] [gan loss: 0.740509, acc: 0.390625]\n",
            "7335: [discriminator loss: 0.763979971408844, acc: 0.0859375] [gan loss: 1.391542, acc: 0.000000]\n",
            "7336: [discriminator loss: 0.6762181520462036, acc: 0.390625] [gan loss: 0.914622, acc: 0.156250]\n",
            "7337: [discriminator loss: 0.7368270754814148, acc: 0.1015625] [gan loss: 1.358126, acc: 0.000000]\n",
            "7338: [discriminator loss: 0.6782649755477905, acc: 0.359375] [gan loss: 1.010070, acc: 0.062500]\n",
            "7339: [discriminator loss: 0.744898796081543, acc: 0.09375] [gan loss: 1.461572, acc: 0.000000]\n",
            "7340: [discriminator loss: 0.6765950918197632, acc: 0.4453125] [gan loss: 0.752340, acc: 0.421875]\n",
            "7341: [discriminator loss: 0.7616468071937561, acc: 0.0546875] [gan loss: 1.664068, acc: 0.000000]\n",
            "7342: [discriminator loss: 0.6809481382369995, acc: 0.4375] [gan loss: 0.646534, acc: 0.656250]\n",
            "7343: [discriminator loss: 0.7920279502868652, acc: 0.0] [gan loss: 1.752896, acc: 0.000000]\n",
            "7344: [discriminator loss: 0.7100212574005127, acc: 0.4609375] [gan loss: 0.749037, acc: 0.437500]\n",
            "7345: [discriminator loss: 0.771148681640625, acc: 0.0078125] [gan loss: 1.544545, acc: 0.000000]\n",
            "7346: [discriminator loss: 0.6674113273620605, acc: 0.4453125] [gan loss: 0.737181, acc: 0.500000]\n",
            "7347: [discriminator loss: 0.750930905342102, acc: 0.0390625] [gan loss: 1.587065, acc: 0.000000]\n",
            "7348: [discriminator loss: 0.6932041049003601, acc: 0.421875] [gan loss: 0.671432, acc: 0.546875]\n",
            "7349: [discriminator loss: 0.7818763256072998, acc: 0.0390625] [gan loss: 1.644422, acc: 0.015625]\n",
            "7350: [discriminator loss: 0.7306973338127136, acc: 0.4296875] [gan loss: 0.705471, acc: 0.484375]\n",
            "7351: [discriminator loss: 0.7962022423744202, acc: 0.03125] [gan loss: 1.614255, acc: 0.000000]\n",
            "7352: [discriminator loss: 0.7071382999420166, acc: 0.4375] [gan loss: 0.677641, acc: 0.531250]\n",
            "7353: [discriminator loss: 0.7861509323120117, acc: 0.0078125] [gan loss: 1.647357, acc: 0.000000]\n",
            "7354: [discriminator loss: 0.6980588436126709, acc: 0.4765625] [gan loss: 0.677278, acc: 0.609375]\n",
            "7355: [discriminator loss: 0.7881112098693848, acc: 0.0078125] [gan loss: 1.510920, acc: 0.000000]\n",
            "7356: [discriminator loss: 0.7049468159675598, acc: 0.4453125] [gan loss: 0.749516, acc: 0.390625]\n",
            "7357: [discriminator loss: 0.7455357313156128, acc: 0.0390625] [gan loss: 1.382572, acc: 0.000000]\n",
            "7358: [discriminator loss: 0.6993947625160217, acc: 0.3671875] [gan loss: 0.864678, acc: 0.156250]\n",
            "7359: [discriminator loss: 0.7216640114784241, acc: 0.1484375] [gan loss: 1.159553, acc: 0.000000]\n",
            "7360: [discriminator loss: 0.7141321897506714, acc: 0.1875] [gan loss: 1.048466, acc: 0.062500]\n",
            "7361: [discriminator loss: 0.6836681962013245, acc: 0.2421875] [gan loss: 1.152224, acc: 0.015625]\n",
            "7362: [discriminator loss: 0.6944898962974548, acc: 0.1953125] [gan loss: 1.131664, acc: 0.000000]\n",
            "7363: [discriminator loss: 0.6846699118614197, acc: 0.2734375] [gan loss: 1.077208, acc: 0.062500]\n",
            "7364: [discriminator loss: 0.7248302102088928, acc: 0.1484375] [gan loss: 1.162229, acc: 0.000000]\n",
            "7365: [discriminator loss: 0.7057144045829773, acc: 0.2421875] [gan loss: 1.052785, acc: 0.031250]\n",
            "7366: [discriminator loss: 0.7024036049842834, acc: 0.1953125] [gan loss: 1.274708, acc: 0.000000]\n",
            "7367: [discriminator loss: 0.6649924516677856, acc: 0.375] [gan loss: 1.002870, acc: 0.093750]\n",
            "7368: [discriminator loss: 0.7069393992424011, acc: 0.125] [gan loss: 1.573717, acc: 0.000000]\n",
            "7369: [discriminator loss: 0.6799231767654419, acc: 0.4375] [gan loss: 0.614044, acc: 0.750000]\n",
            "7370: [discriminator loss: 0.8075826168060303, acc: 0.015625] [gan loss: 1.998647, acc: 0.000000]\n",
            "7371: [discriminator loss: 0.7210052013397217, acc: 0.5] [gan loss: 0.456859, acc: 0.906250]\n",
            "7372: [discriminator loss: 0.8966070413589478, acc: 0.0078125] [gan loss: 1.441407, acc: 0.000000]\n",
            "7373: [discriminator loss: 0.6772209405899048, acc: 0.46875] [gan loss: 0.694065, acc: 0.546875]\n",
            "7374: [discriminator loss: 0.8211065530776978, acc: 0.0078125] [gan loss: 1.570569, acc: 0.000000]\n",
            "7375: [discriminator loss: 0.6924114227294922, acc: 0.453125] [gan loss: 0.754282, acc: 0.453125]\n",
            "7376: [discriminator loss: 0.7508127689361572, acc: 0.0703125] [gan loss: 1.402436, acc: 0.000000]\n",
            "7377: [discriminator loss: 0.7020941972732544, acc: 0.3984375] [gan loss: 0.808304, acc: 0.234375]\n",
            "7378: [discriminator loss: 0.7469615340232849, acc: 0.0390625] [gan loss: 1.410710, acc: 0.000000]\n",
            "7379: [discriminator loss: 0.6867700815200806, acc: 0.3984375] [gan loss: 0.749837, acc: 0.406250]\n",
            "7380: [discriminator loss: 0.7456458806991577, acc: 0.046875] [gan loss: 1.447803, acc: 0.000000]\n",
            "7381: [discriminator loss: 0.7070713639259338, acc: 0.375] [gan loss: 0.849024, acc: 0.187500]\n",
            "7382: [discriminator loss: 0.7158583402633667, acc: 0.109375] [gan loss: 1.246103, acc: 0.031250]\n",
            "7383: [discriminator loss: 0.6844969391822815, acc: 0.375] [gan loss: 0.850260, acc: 0.281250]\n",
            "7384: [discriminator loss: 0.7275077104568481, acc: 0.0625] [gan loss: 1.385557, acc: 0.000000]\n",
            "7385: [discriminator loss: 0.7692906260490417, acc: 0.2734375] [gan loss: 1.085245, acc: 0.031250]\n",
            "7386: [discriminator loss: 0.7155561447143555, acc: 0.15625] [gan loss: 1.259812, acc: 0.000000]\n",
            "7387: [discriminator loss: 0.7097089290618896, acc: 0.2734375] [gan loss: 0.975580, acc: 0.078125]\n",
            "7388: [discriminator loss: 0.7065632343292236, acc: 0.1796875] [gan loss: 1.291585, acc: 0.000000]\n",
            "7389: [discriminator loss: 0.6928346157073975, acc: 0.2578125] [gan loss: 1.097226, acc: 0.031250]\n",
            "7390: [discriminator loss: 0.6807507872581482, acc: 0.2734375] [gan loss: 1.118592, acc: 0.046875]\n",
            "7391: [discriminator loss: 0.6823370456695557, acc: 0.203125] [gan loss: 1.356602, acc: 0.000000]\n",
            "7392: [discriminator loss: 0.6853374242782593, acc: 0.375] [gan loss: 1.063344, acc: 0.015625]\n",
            "7393: [discriminator loss: 0.703291654586792, acc: 0.1875] [gan loss: 1.342179, acc: 0.000000]\n",
            "7394: [discriminator loss: 0.6997531652450562, acc: 0.34375] [gan loss: 0.938723, acc: 0.187500]\n",
            "7395: [discriminator loss: 0.737001359462738, acc: 0.125] [gan loss: 1.651058, acc: 0.000000]\n",
            "7396: [discriminator loss: 0.7052935361862183, acc: 0.453125] [gan loss: 0.572128, acc: 0.750000]\n",
            "7397: [discriminator loss: 0.8011503219604492, acc: 0.0234375] [gan loss: 2.050782, acc: 0.000000]\n",
            "7398: [discriminator loss: 0.7389095425605774, acc: 0.4921875] [gan loss: 0.432936, acc: 0.953125]\n",
            "7399: [discriminator loss: 0.9405373930931091, acc: 0.0078125] [gan loss: 1.929136, acc: 0.000000]\n",
            "7400: [discriminator loss: 0.708723783493042, acc: 0.4921875] [gan loss: 0.614573, acc: 0.703125]\n",
            "7401: [discriminator loss: 0.7748894691467285, acc: 0.046875] [gan loss: 1.438082, acc: 0.000000]\n",
            "7402: [discriminator loss: 0.682470977306366, acc: 0.390625] [gan loss: 0.742855, acc: 0.437500]\n",
            "7403: [discriminator loss: 0.7738876938819885, acc: 0.046875] [gan loss: 1.451386, acc: 0.000000]\n",
            "7404: [discriminator loss: 0.6758350133895874, acc: 0.4453125] [gan loss: 0.877752, acc: 0.203125]\n",
            "7405: [discriminator loss: 0.7443162202835083, acc: 0.0703125] [gan loss: 1.436816, acc: 0.000000]\n",
            "7406: [discriminator loss: 0.6916792392730713, acc: 0.34375] [gan loss: 0.949448, acc: 0.125000]\n",
            "7407: [discriminator loss: 0.709213376045227, acc: 0.09375] [gan loss: 1.212133, acc: 0.000000]\n",
            "7408: [discriminator loss: 0.7110092639923096, acc: 0.2421875] [gan loss: 1.083681, acc: 0.015625]\n",
            "7409: [discriminator loss: 0.7456021904945374, acc: 0.140625] [gan loss: 1.281319, acc: 0.000000]\n",
            "7410: [discriminator loss: 0.6934097409248352, acc: 0.359375] [gan loss: 0.903315, acc: 0.187500]\n",
            "7411: [discriminator loss: 0.7090743780136108, acc: 0.0625] [gan loss: 1.486521, acc: 0.000000]\n",
            "7412: [discriminator loss: 0.6950226426124573, acc: 0.40625] [gan loss: 0.800090, acc: 0.296875]\n",
            "7413: [discriminator loss: 0.7351391911506653, acc: 0.0546875] [gan loss: 1.466302, acc: 0.000000]\n",
            "7414: [discriminator loss: 0.6871861219406128, acc: 0.4296875] [gan loss: 0.698156, acc: 0.484375]\n",
            "7415: [discriminator loss: 0.7746800780296326, acc: 0.0390625] [gan loss: 1.600996, acc: 0.000000]\n",
            "7416: [discriminator loss: 0.6988565921783447, acc: 0.4375] [gan loss: 0.748297, acc: 0.375000]\n",
            "7417: [discriminator loss: 0.7432320713996887, acc: 0.0390625] [gan loss: 1.559066, acc: 0.000000]\n",
            "7418: [discriminator loss: 0.6998509764671326, acc: 0.46875] [gan loss: 0.641388, acc: 0.593750]\n",
            "7419: [discriminator loss: 0.776451051235199, acc: 0.0234375] [gan loss: 1.636652, acc: 0.015625]\n",
            "7420: [discriminator loss: 0.691605269908905, acc: 0.4765625] [gan loss: 0.571092, acc: 0.843750]\n",
            "7421: [discriminator loss: 0.8061721324920654, acc: 0.0078125] [gan loss: 1.654517, acc: 0.000000]\n",
            "7422: [discriminator loss: 0.6978195309638977, acc: 0.5] [gan loss: 0.669145, acc: 0.656250]\n",
            "7423: [discriminator loss: 0.7805406451225281, acc: 0.0078125] [gan loss: 1.528522, acc: 0.000000]\n",
            "7424: [discriminator loss: 0.6963764429092407, acc: 0.4375] [gan loss: 0.721997, acc: 0.453125]\n",
            "7425: [discriminator loss: 0.7565855979919434, acc: 0.0546875] [gan loss: 1.403407, acc: 0.000000]\n",
            "7426: [discriminator loss: 0.6930930614471436, acc: 0.3203125] [gan loss: 0.926299, acc: 0.109375]\n",
            "7427: [discriminator loss: 0.6963551044464111, acc: 0.109375] [gan loss: 1.262057, acc: 0.000000]\n",
            "7428: [discriminator loss: 0.6663459539413452, acc: 0.3984375] [gan loss: 0.844998, acc: 0.250000]\n",
            "7429: [discriminator loss: 0.7298558950424194, acc: 0.0859375] [gan loss: 1.390642, acc: 0.015625]\n",
            "7430: [discriminator loss: 0.6890184879302979, acc: 0.359375] [gan loss: 0.887869, acc: 0.250000]\n",
            "7431: [discriminator loss: 0.7551993727684021, acc: 0.109375] [gan loss: 1.597147, acc: 0.000000]\n",
            "7432: [discriminator loss: 0.713175892829895, acc: 0.4140625] [gan loss: 0.713195, acc: 0.515625]\n",
            "7433: [discriminator loss: 0.8049271106719971, acc: 0.0] [gan loss: 1.681953, acc: 0.000000]\n",
            "7434: [discriminator loss: 0.6977684497833252, acc: 0.4609375] [gan loss: 0.608848, acc: 0.718750]\n",
            "7435: [discriminator loss: 0.808612048625946, acc: 0.0078125] [gan loss: 1.627018, acc: 0.000000]\n",
            "7436: [discriminator loss: 0.7071133852005005, acc: 0.4453125] [gan loss: 0.696048, acc: 0.562500]\n",
            "7437: [discriminator loss: 0.8122217655181885, acc: 0.015625] [gan loss: 1.602819, acc: 0.000000]\n",
            "7438: [discriminator loss: 0.6938843727111816, acc: 0.4296875] [gan loss: 0.786155, acc: 0.390625]\n",
            "7439: [discriminator loss: 0.7616214752197266, acc: 0.03125] [gan loss: 1.443585, acc: 0.000000]\n",
            "7440: [discriminator loss: 0.6860324144363403, acc: 0.4609375] [gan loss: 0.853059, acc: 0.218750]\n",
            "7441: [discriminator loss: 0.7274322509765625, acc: 0.0703125] [gan loss: 1.412302, acc: 0.000000]\n",
            "7442: [discriminator loss: 0.6621878147125244, acc: 0.421875] [gan loss: 0.789478, acc: 0.343750]\n",
            "7443: [discriminator loss: 0.7176300883293152, acc: 0.0859375] [gan loss: 1.434065, acc: 0.000000]\n",
            "7444: [discriminator loss: 0.6896061897277832, acc: 0.4140625] [gan loss: 0.826356, acc: 0.265625]\n",
            "7445: [discriminator loss: 0.7386322021484375, acc: 0.0703125] [gan loss: 1.438674, acc: 0.000000]\n",
            "7446: [discriminator loss: 0.7020512819290161, acc: 0.4375] [gan loss: 0.766971, acc: 0.375000]\n",
            "7447: [discriminator loss: 0.7451801896095276, acc: 0.0625] [gan loss: 1.604460, acc: 0.000000]\n",
            "7448: [discriminator loss: 0.6880167722702026, acc: 0.4375] [gan loss: 0.793680, acc: 0.250000]\n",
            "7449: [discriminator loss: 0.7807155847549438, acc: 0.0234375] [gan loss: 1.605400, acc: 0.000000]\n",
            "7450: [discriminator loss: 0.6926180124282837, acc: 0.4765625] [gan loss: 0.676383, acc: 0.562500]\n",
            "7451: [discriminator loss: 0.7481992840766907, acc: 0.03125] [gan loss: 1.660695, acc: 0.000000]\n",
            "7452: [discriminator loss: 0.6923031210899353, acc: 0.46875] [gan loss: 0.639962, acc: 0.671875]\n",
            "7453: [discriminator loss: 0.8007528781890869, acc: 0.015625] [gan loss: 1.537443, acc: 0.000000]\n",
            "7454: [discriminator loss: 0.7186629176139832, acc: 0.3828125] [gan loss: 0.836599, acc: 0.203125]\n",
            "7455: [discriminator loss: 0.7328283786773682, acc: 0.046875] [gan loss: 1.461790, acc: 0.000000]\n",
            "7456: [discriminator loss: 0.6911522746086121, acc: 0.453125] [gan loss: 0.659425, acc: 0.625000]\n",
            "7457: [discriminator loss: 0.7690184116363525, acc: 0.015625] [gan loss: 1.570803, acc: 0.000000]\n",
            "7458: [discriminator loss: 0.6887641549110413, acc: 0.4609375] [gan loss: 0.668519, acc: 0.515625]\n",
            "7459: [discriminator loss: 0.8063304424285889, acc: 0.015625] [gan loss: 1.613353, acc: 0.000000]\n",
            "7460: [discriminator loss: 0.7042891383171082, acc: 0.46875] [gan loss: 0.670217, acc: 0.578125]\n",
            "7461: [discriminator loss: 0.8078910112380981, acc: 0.015625] [gan loss: 1.658986, acc: 0.000000]\n",
            "7462: [discriminator loss: 0.6547346711158752, acc: 0.4921875] [gan loss: 0.758028, acc: 0.390625]\n",
            "7463: [discriminator loss: 0.7795546650886536, acc: 0.0] [gan loss: 1.466700, acc: 0.000000]\n",
            "7464: [discriminator loss: 0.6827160716056824, acc: 0.4296875] [gan loss: 0.783048, acc: 0.359375]\n",
            "7465: [discriminator loss: 0.7358449697494507, acc: 0.0625] [gan loss: 1.414143, acc: 0.000000]\n",
            "7466: [discriminator loss: 0.6720612049102783, acc: 0.4296875] [gan loss: 0.784861, acc: 0.343750]\n",
            "7467: [discriminator loss: 0.7432762384414673, acc: 0.0859375] [gan loss: 1.331205, acc: 0.015625]\n",
            "7468: [discriminator loss: 0.668491542339325, acc: 0.40625] [gan loss: 0.780352, acc: 0.390625]\n",
            "7469: [discriminator loss: 0.7786736488342285, acc: 0.09375] [gan loss: 1.478785, acc: 0.000000]\n",
            "7470: [discriminator loss: 0.6963173151016235, acc: 0.4140625] [gan loss: 0.762872, acc: 0.343750]\n",
            "7471: [discriminator loss: 0.7482375502586365, acc: 0.046875] [gan loss: 1.394933, acc: 0.000000]\n",
            "7472: [discriminator loss: 0.6982191801071167, acc: 0.4140625] [gan loss: 0.775616, acc: 0.296875]\n",
            "7473: [discriminator loss: 0.7876065969467163, acc: 0.03125] [gan loss: 1.623966, acc: 0.000000]\n",
            "7474: [discriminator loss: 0.6904147863388062, acc: 0.453125] [gan loss: 0.718148, acc: 0.390625]\n",
            "7475: [discriminator loss: 0.7375591397285461, acc: 0.046875] [gan loss: 1.595401, acc: 0.000000]\n",
            "7476: [discriminator loss: 0.6910616755485535, acc: 0.4453125] [gan loss: 0.664108, acc: 0.593750]\n",
            "7477: [discriminator loss: 0.7287078499794006, acc: 0.0390625] [gan loss: 1.466793, acc: 0.000000]\n",
            "7478: [discriminator loss: 0.6729803085327148, acc: 0.4609375] [gan loss: 0.801959, acc: 0.265625]\n",
            "7479: [discriminator loss: 0.7770198583602905, acc: 0.0390625] [gan loss: 1.561624, acc: 0.000000]\n",
            "7480: [discriminator loss: 0.6942024827003479, acc: 0.4375] [gan loss: 0.655592, acc: 0.593750]\n",
            "7481: [discriminator loss: 0.7941208481788635, acc: 0.015625] [gan loss: 1.596089, acc: 0.000000]\n",
            "7482: [discriminator loss: 0.6808542609214783, acc: 0.4765625] [gan loss: 0.754342, acc: 0.390625]\n",
            "7483: [discriminator loss: 0.7970611453056335, acc: 0.0] [gan loss: 1.459445, acc: 0.000000]\n",
            "7484: [discriminator loss: 0.7141717076301575, acc: 0.390625] [gan loss: 0.890000, acc: 0.234375]\n",
            "7485: [discriminator loss: 0.697210431098938, acc: 0.125] [gan loss: 1.272521, acc: 0.000000]\n",
            "7486: [discriminator loss: 0.679348349571228, acc: 0.3359375] [gan loss: 1.007690, acc: 0.093750]\n",
            "7487: [discriminator loss: 0.7133257985115051, acc: 0.1484375] [gan loss: 1.345157, acc: 0.000000]\n",
            "7488: [discriminator loss: 0.6890994310379028, acc: 0.390625] [gan loss: 0.841136, acc: 0.265625]\n",
            "7489: [discriminator loss: 0.757036030292511, acc: 0.0625] [gan loss: 1.417544, acc: 0.000000]\n",
            "7490: [discriminator loss: 0.6733034253120422, acc: 0.421875] [gan loss: 0.726753, acc: 0.484375]\n",
            "7491: [discriminator loss: 0.8075964450836182, acc: 0.015625] [gan loss: 1.776639, acc: 0.000000]\n",
            "7492: [discriminator loss: 0.7015715837478638, acc: 0.4921875] [gan loss: 0.593130, acc: 0.734375]\n",
            "7493: [discriminator loss: 0.832004189491272, acc: 0.0078125] [gan loss: 1.643500, acc: 0.000000]\n",
            "7494: [discriminator loss: 0.7237550020217896, acc: 0.4921875] [gan loss: 0.598166, acc: 0.781250]\n",
            "7495: [discriminator loss: 0.8267520666122437, acc: 0.015625] [gan loss: 1.589175, acc: 0.000000]\n",
            "7496: [discriminator loss: 0.6900177001953125, acc: 0.4609375] [gan loss: 0.805650, acc: 0.265625]\n",
            "7497: [discriminator loss: 0.7278177738189697, acc: 0.078125] [gan loss: 1.321337, acc: 0.000000]\n",
            "7498: [discriminator loss: 0.6851589679718018, acc: 0.34375] [gan loss: 0.800968, acc: 0.296875]\n",
            "7499: [discriminator loss: 0.7101423740386963, acc: 0.1328125] [gan loss: 1.247288, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "7500: [discriminator loss: 0.7073647975921631, acc: 0.3125] [gan loss: 0.969413, acc: 0.093750]\n",
            "7501: [discriminator loss: 0.7524145841598511, acc: 0.09375] [gan loss: 1.451369, acc: 0.000000]\n",
            "7502: [discriminator loss: 0.677486002445221, acc: 0.421875] [gan loss: 0.802337, acc: 0.265625]\n",
            "7503: [discriminator loss: 0.7610841989517212, acc: 0.015625] [gan loss: 1.477080, acc: 0.000000]\n",
            "7504: [discriminator loss: 0.7003047466278076, acc: 0.453125] [gan loss: 0.662753, acc: 0.625000]\n",
            "7505: [discriminator loss: 0.8086111545562744, acc: 0.0] [gan loss: 1.716491, acc: 0.000000]\n",
            "7506: [discriminator loss: 0.67244952917099, acc: 0.5] [gan loss: 0.548031, acc: 0.781250]\n",
            "7507: [discriminator loss: 0.815220832824707, acc: 0.0390625] [gan loss: 1.618231, acc: 0.000000]\n",
            "7508: [discriminator loss: 0.6971756219863892, acc: 0.4921875] [gan loss: 0.730766, acc: 0.406250]\n",
            "7509: [discriminator loss: 0.8237055540084839, acc: 0.0078125] [gan loss: 1.532673, acc: 0.000000]\n",
            "7510: [discriminator loss: 0.6752477884292603, acc: 0.453125] [gan loss: 0.760529, acc: 0.328125]\n",
            "7511: [discriminator loss: 0.7595627307891846, acc: 0.046875] [gan loss: 1.367797, acc: 0.000000]\n",
            "7512: [discriminator loss: 0.6981204152107239, acc: 0.3515625] [gan loss: 0.909195, acc: 0.125000]\n",
            "7513: [discriminator loss: 0.7379368543624878, acc: 0.0703125] [gan loss: 1.403767, acc: 0.000000]\n",
            "7514: [discriminator loss: 0.686857283115387, acc: 0.421875] [gan loss: 0.700988, acc: 0.500000]\n",
            "7515: [discriminator loss: 0.7747926712036133, acc: 0.03125] [gan loss: 1.474409, acc: 0.000000]\n",
            "7516: [discriminator loss: 0.687001645565033, acc: 0.4609375] [gan loss: 0.751338, acc: 0.328125]\n",
            "7517: [discriminator loss: 0.7640410661697388, acc: 0.0] [gan loss: 1.616230, acc: 0.000000]\n",
            "7518: [discriminator loss: 0.6991592049598694, acc: 0.484375] [gan loss: 0.643499, acc: 0.750000]\n",
            "7519: [discriminator loss: 0.78019779920578, acc: 0.0390625] [gan loss: 1.544655, acc: 0.000000]\n",
            "7520: [discriminator loss: 0.6802821159362793, acc: 0.4921875] [gan loss: 0.667052, acc: 0.609375]\n",
            "7521: [discriminator loss: 0.7541735172271729, acc: 0.0390625] [gan loss: 1.307773, acc: 0.000000]\n",
            "7522: [discriminator loss: 0.7148966193199158, acc: 0.34375] [gan loss: 0.812417, acc: 0.203125]\n",
            "7523: [discriminator loss: 0.7771335244178772, acc: 0.046875] [gan loss: 1.518480, acc: 0.000000]\n",
            "7524: [discriminator loss: 0.6813600063323975, acc: 0.40625] [gan loss: 0.876963, acc: 0.109375]\n",
            "7525: [discriminator loss: 0.7616992592811584, acc: 0.0390625] [gan loss: 1.433824, acc: 0.000000]\n",
            "7526: [discriminator loss: 0.6768704056739807, acc: 0.4765625] [gan loss: 0.640347, acc: 0.671875]\n",
            "7527: [discriminator loss: 0.7904678583145142, acc: 0.0703125] [gan loss: 1.557158, acc: 0.000000]\n",
            "7528: [discriminator loss: 0.6795737147331238, acc: 0.4296875] [gan loss: 0.765418, acc: 0.484375]\n",
            "7529: [discriminator loss: 0.782654881477356, acc: 0.0546875] [gan loss: 1.444240, acc: 0.000000]\n",
            "7530: [discriminator loss: 0.6792192459106445, acc: 0.421875] [gan loss: 0.790555, acc: 0.312500]\n",
            "7531: [discriminator loss: 0.7438395023345947, acc: 0.0546875] [gan loss: 1.379528, acc: 0.000000]\n",
            "7532: [discriminator loss: 0.7074885964393616, acc: 0.328125] [gan loss: 0.913145, acc: 0.171875]\n",
            "7533: [discriminator loss: 0.7407708168029785, acc: 0.1171875] [gan loss: 1.452023, acc: 0.015625]\n",
            "7534: [discriminator loss: 0.6974128484725952, acc: 0.390625] [gan loss: 0.846009, acc: 0.234375]\n",
            "7535: [discriminator loss: 0.7513872981071472, acc: 0.0625] [gan loss: 1.662928, acc: 0.000000]\n",
            "7536: [discriminator loss: 0.6793490648269653, acc: 0.4921875] [gan loss: 0.640823, acc: 0.625000]\n",
            "7537: [discriminator loss: 0.810198187828064, acc: 0.0234375] [gan loss: 1.762812, acc: 0.000000]\n",
            "7538: [discriminator loss: 0.6835522055625916, acc: 0.484375] [gan loss: 0.644386, acc: 0.656250]\n",
            "7539: [discriminator loss: 0.8247160315513611, acc: 0.0078125] [gan loss: 1.762114, acc: 0.000000]\n",
            "7540: [discriminator loss: 0.721773087978363, acc: 0.4609375] [gan loss: 0.655878, acc: 0.609375]\n",
            "7541: [discriminator loss: 0.7897030115127563, acc: 0.03125] [gan loss: 1.453190, acc: 0.000000]\n",
            "7542: [discriminator loss: 0.6978129148483276, acc: 0.3984375] [gan loss: 0.872641, acc: 0.140625]\n",
            "7543: [discriminator loss: 0.740119993686676, acc: 0.0703125] [gan loss: 1.388404, acc: 0.015625]\n",
            "7544: [discriminator loss: 0.6977097988128662, acc: 0.34375] [gan loss: 0.864877, acc: 0.265625]\n",
            "7545: [discriminator loss: 0.6908113956451416, acc: 0.1640625] [gan loss: 1.199234, acc: 0.000000]\n",
            "7546: [discriminator loss: 0.6945908069610596, acc: 0.3203125] [gan loss: 0.995705, acc: 0.046875]\n",
            "7547: [discriminator loss: 0.7018370628356934, acc: 0.1796875] [gan loss: 1.211449, acc: 0.015625]\n",
            "7548: [discriminator loss: 0.7013582587242126, acc: 0.296875] [gan loss: 1.085599, acc: 0.046875]\n",
            "7549: [discriminator loss: 0.7284376621246338, acc: 0.125] [gan loss: 1.362563, acc: 0.000000]\n",
            "7550: [discriminator loss: 0.7000804543495178, acc: 0.390625] [gan loss: 0.819857, acc: 0.265625]\n",
            "7551: [discriminator loss: 0.7299923300743103, acc: 0.0703125] [gan loss: 1.520124, acc: 0.000000]\n",
            "7552: [discriminator loss: 0.686834454536438, acc: 0.40625] [gan loss: 0.805117, acc: 0.359375]\n",
            "7553: [discriminator loss: 0.7220214605331421, acc: 0.109375] [gan loss: 1.484109, acc: 0.000000]\n",
            "7554: [discriminator loss: 0.7033894658088684, acc: 0.375] [gan loss: 0.756216, acc: 0.468750]\n",
            "7555: [discriminator loss: 0.7831225991249084, acc: 0.0546875] [gan loss: 1.666109, acc: 0.000000]\n",
            "7556: [discriminator loss: 0.693979799747467, acc: 0.484375] [gan loss: 0.600757, acc: 0.734375]\n",
            "7557: [discriminator loss: 0.878591775894165, acc: 0.0078125] [gan loss: 1.894482, acc: 0.000000]\n",
            "7558: [discriminator loss: 0.7362635731697083, acc: 0.4921875] [gan loss: 0.556627, acc: 0.875000]\n",
            "7559: [discriminator loss: 0.8379541039466858, acc: 0.0] [gan loss: 1.577382, acc: 0.000000]\n",
            "7560: [discriminator loss: 0.691409707069397, acc: 0.46875] [gan loss: 0.737129, acc: 0.437500]\n",
            "7561: [discriminator loss: 0.7752610445022583, acc: 0.03125] [gan loss: 1.533133, acc: 0.000000]\n",
            "7562: [discriminator loss: 0.6738765835762024, acc: 0.4609375] [gan loss: 0.690864, acc: 0.546875]\n",
            "7563: [discriminator loss: 0.7738859057426453, acc: 0.015625] [gan loss: 1.565174, acc: 0.000000]\n",
            "7564: [discriminator loss: 0.6654922962188721, acc: 0.4765625] [gan loss: 0.771536, acc: 0.375000]\n",
            "7565: [discriminator loss: 0.7325904369354248, acc: 0.0390625] [gan loss: 1.316686, acc: 0.015625]\n",
            "7566: [discriminator loss: 0.718812882900238, acc: 0.3359375] [gan loss: 0.916701, acc: 0.125000]\n",
            "7567: [discriminator loss: 0.7456004023551941, acc: 0.109375] [gan loss: 1.174622, acc: 0.015625]\n",
            "7568: [discriminator loss: 0.7297893166542053, acc: 0.25] [gan loss: 1.017698, acc: 0.015625]\n",
            "7569: [discriminator loss: 0.739849328994751, acc: 0.09375] [gan loss: 1.351491, acc: 0.000000]\n",
            "7570: [discriminator loss: 0.7107480764389038, acc: 0.2890625] [gan loss: 0.865976, acc: 0.140625]\n",
            "7571: [discriminator loss: 0.7345894575119019, acc: 0.0703125] [gan loss: 1.476919, acc: 0.000000]\n",
            "7572: [discriminator loss: 0.6826115846633911, acc: 0.4296875] [gan loss: 0.809983, acc: 0.375000]\n",
            "7573: [discriminator loss: 0.7588449716567993, acc: 0.046875] [gan loss: 1.622483, acc: 0.000000]\n",
            "7574: [discriminator loss: 0.6782529950141907, acc: 0.4765625] [gan loss: 0.590195, acc: 0.671875]\n",
            "7575: [discriminator loss: 0.8709314465522766, acc: 0.03125] [gan loss: 1.694709, acc: 0.000000]\n",
            "7576: [discriminator loss: 0.7325783967971802, acc: 0.4375] [gan loss: 0.631459, acc: 0.656250]\n",
            "7577: [discriminator loss: 0.8043177127838135, acc: 0.015625] [gan loss: 1.576210, acc: 0.000000]\n",
            "7578: [discriminator loss: 0.6907219886779785, acc: 0.4296875] [gan loss: 0.816959, acc: 0.390625]\n",
            "7579: [discriminator loss: 0.7576772570610046, acc: 0.03125] [gan loss: 1.365083, acc: 0.000000]\n",
            "7580: [discriminator loss: 0.6847705245018005, acc: 0.3828125] [gan loss: 0.800550, acc: 0.312500]\n",
            "7581: [discriminator loss: 0.7664183974266052, acc: 0.0390625] [gan loss: 1.308993, acc: 0.000000]\n",
            "7582: [discriminator loss: 0.6891560554504395, acc: 0.328125] [gan loss: 0.857129, acc: 0.250000]\n",
            "7583: [discriminator loss: 0.7378816604614258, acc: 0.0625] [gan loss: 1.368816, acc: 0.000000]\n",
            "7584: [discriminator loss: 0.686701238155365, acc: 0.390625] [gan loss: 0.896492, acc: 0.140625]\n",
            "7585: [discriminator loss: 0.6870191693305969, acc: 0.140625] [gan loss: 1.253962, acc: 0.031250]\n",
            "7586: [discriminator loss: 0.7064950466156006, acc: 0.2890625] [gan loss: 1.054238, acc: 0.031250]\n",
            "7587: [discriminator loss: 0.7671420574188232, acc: 0.1328125] [gan loss: 1.245642, acc: 0.000000]\n",
            "7588: [discriminator loss: 0.7204053401947021, acc: 0.2734375] [gan loss: 1.090681, acc: 0.062500]\n",
            "7589: [discriminator loss: 0.692952036857605, acc: 0.1640625] [gan loss: 1.245889, acc: 0.015625]\n",
            "7590: [discriminator loss: 0.6882675886154175, acc: 0.328125] [gan loss: 1.024356, acc: 0.031250]\n",
            "7591: [discriminator loss: 0.6995368003845215, acc: 0.1796875] [gan loss: 1.342688, acc: 0.000000]\n",
            "7592: [discriminator loss: 0.6851935386657715, acc: 0.3515625] [gan loss: 0.922146, acc: 0.093750]\n",
            "7593: [discriminator loss: 0.7229022979736328, acc: 0.09375] [gan loss: 1.452571, acc: 0.000000]\n",
            "7594: [discriminator loss: 0.7151416540145874, acc: 0.375] [gan loss: 0.793201, acc: 0.312500]\n",
            "7595: [discriminator loss: 0.7369850873947144, acc: 0.03125] [gan loss: 1.538563, acc: 0.000000]\n",
            "7596: [discriminator loss: 0.7203052043914795, acc: 0.4375] [gan loss: 0.667482, acc: 0.625000]\n",
            "7597: [discriminator loss: 0.8006631135940552, acc: 0.0390625] [gan loss: 1.872783, acc: 0.000000]\n",
            "7598: [discriminator loss: 0.7397750616073608, acc: 0.4921875] [gan loss: 0.534353, acc: 0.859375]\n",
            "7599: [discriminator loss: 0.8277662396430969, acc: 0.0078125] [gan loss: 1.693716, acc: 0.000000]\n",
            "7600: [discriminator loss: 0.7139525413513184, acc: 0.4921875] [gan loss: 0.586734, acc: 0.828125]\n",
            "7601: [discriminator loss: 0.8269618153572083, acc: 0.0234375] [gan loss: 1.465383, acc: 0.000000]\n",
            "7602: [discriminator loss: 0.7131186723709106, acc: 0.421875] [gan loss: 0.772215, acc: 0.359375]\n",
            "7603: [discriminator loss: 0.7383270263671875, acc: 0.0234375] [gan loss: 1.421035, acc: 0.000000]\n",
            "7604: [discriminator loss: 0.6835567355155945, acc: 0.359375] [gan loss: 0.900708, acc: 0.203125]\n",
            "7605: [discriminator loss: 0.7000582814216614, acc: 0.140625] [gan loss: 1.219265, acc: 0.046875]\n",
            "7606: [discriminator loss: 0.6845840215682983, acc: 0.328125] [gan loss: 0.893229, acc: 0.203125]\n",
            "7607: [discriminator loss: 0.6924293041229248, acc: 0.203125] [gan loss: 1.116249, acc: 0.046875]\n",
            "7608: [discriminator loss: 0.6799535155296326, acc: 0.2421875] [gan loss: 1.146966, acc: 0.000000]\n",
            "7609: [discriminator loss: 0.7189339399337769, acc: 0.21875] [gan loss: 1.154735, acc: 0.000000]\n",
            "7610: [discriminator loss: 0.7299351096153259, acc: 0.1875] [gan loss: 1.328243, acc: 0.015625]\n",
            "7611: [discriminator loss: 0.6880613565444946, acc: 0.3125] [gan loss: 1.031409, acc: 0.031250]\n",
            "7612: [discriminator loss: 0.717732310295105, acc: 0.1171875] [gan loss: 1.523946, acc: 0.000000]\n",
            "7613: [discriminator loss: 0.6719743013381958, acc: 0.4140625] [gan loss: 0.869687, acc: 0.234375]\n",
            "7614: [discriminator loss: 0.7282443642616272, acc: 0.046875] [gan loss: 1.859202, acc: 0.000000]\n",
            "7615: [discriminator loss: 0.7106825709342957, acc: 0.484375] [gan loss: 0.478449, acc: 0.953125]\n",
            "7616: [discriminator loss: 0.8427683711051941, acc: 0.0] [gan loss: 1.839589, acc: 0.000000]\n",
            "7617: [discriminator loss: 0.7322129607200623, acc: 0.484375] [gan loss: 0.662705, acc: 0.625000]\n",
            "7618: [discriminator loss: 0.7850771546363831, acc: 0.03125] [gan loss: 1.474916, acc: 0.000000]\n",
            "7619: [discriminator loss: 0.6870136260986328, acc: 0.4765625] [gan loss: 0.683717, acc: 0.593750]\n",
            "7620: [discriminator loss: 0.8147798180580139, acc: 0.03125] [gan loss: 1.512521, acc: 0.000000]\n",
            "7621: [discriminator loss: 0.7017977833747864, acc: 0.390625] [gan loss: 0.845359, acc: 0.171875]\n",
            "7622: [discriminator loss: 0.7241962552070618, acc: 0.078125] [gan loss: 1.387295, acc: 0.000000]\n",
            "7623: [discriminator loss: 0.6912357211112976, acc: 0.3828125] [gan loss: 0.884167, acc: 0.203125]\n",
            "7624: [discriminator loss: 0.7462172508239746, acc: 0.0859375] [gan loss: 1.399518, acc: 0.000000]\n",
            "7625: [discriminator loss: 0.7028340101242065, acc: 0.40625] [gan loss: 0.774864, acc: 0.265625]\n",
            "7626: [discriminator loss: 0.734470009803772, acc: 0.046875] [gan loss: 1.473518, acc: 0.000000]\n",
            "7627: [discriminator loss: 0.6976314783096313, acc: 0.421875] [gan loss: 0.683089, acc: 0.593750]\n",
            "7628: [discriminator loss: 0.834364116191864, acc: 0.0234375] [gan loss: 1.698813, acc: 0.000000]\n",
            "7629: [discriminator loss: 0.6884120106697083, acc: 0.4921875] [gan loss: 0.639777, acc: 0.625000]\n",
            "7630: [discriminator loss: 0.7805761694908142, acc: 0.0078125] [gan loss: 1.655466, acc: 0.000000]\n",
            "7631: [discriminator loss: 0.6992325186729431, acc: 0.4375] [gan loss: 0.729302, acc: 0.421875]\n",
            "7632: [discriminator loss: 0.7738557457923889, acc: 0.0078125] [gan loss: 1.496467, acc: 0.000000]\n",
            "7633: [discriminator loss: 0.6896092891693115, acc: 0.4375] [gan loss: 0.770935, acc: 0.250000]\n",
            "7634: [discriminator loss: 0.741534948348999, acc: 0.0546875] [gan loss: 1.488218, acc: 0.000000]\n",
            "7635: [discriminator loss: 0.6678217649459839, acc: 0.390625] [gan loss: 0.787709, acc: 0.453125]\n",
            "7636: [discriminator loss: 0.7982578277587891, acc: 0.0859375] [gan loss: 1.535284, acc: 0.000000]\n",
            "7637: [discriminator loss: 0.6901757121086121, acc: 0.4453125] [gan loss: 0.664466, acc: 0.578125]\n",
            "7638: [discriminator loss: 0.781707763671875, acc: 0.0703125] [gan loss: 1.646027, acc: 0.000000]\n",
            "7639: [discriminator loss: 0.6986550688743591, acc: 0.4609375] [gan loss: 0.689458, acc: 0.484375]\n",
            "7640: [discriminator loss: 0.7535595297813416, acc: 0.015625] [gan loss: 1.509397, acc: 0.000000]\n",
            "7641: [discriminator loss: 0.7029573917388916, acc: 0.453125] [gan loss: 0.683963, acc: 0.562500]\n",
            "7642: [discriminator loss: 0.8012164831161499, acc: 0.0] [gan loss: 1.664223, acc: 0.000000]\n",
            "7643: [discriminator loss: 0.6952062845230103, acc: 0.5] [gan loss: 0.641581, acc: 0.640625]\n",
            "7644: [discriminator loss: 0.7870830297470093, acc: 0.0234375] [gan loss: 1.504797, acc: 0.000000]\n",
            "7645: [discriminator loss: 0.685093343257904, acc: 0.46875] [gan loss: 0.754499, acc: 0.500000]\n",
            "7646: [discriminator loss: 0.7694402933120728, acc: 0.046875] [gan loss: 1.488462, acc: 0.000000]\n",
            "7647: [discriminator loss: 0.7116909027099609, acc: 0.359375] [gan loss: 0.894973, acc: 0.156250]\n",
            "7648: [discriminator loss: 0.7299078702926636, acc: 0.0703125] [gan loss: 1.375193, acc: 0.000000]\n",
            "7649: [discriminator loss: 0.7150182723999023, acc: 0.34375] [gan loss: 0.809785, acc: 0.250000]\n",
            "7650: [discriminator loss: 0.7289038896560669, acc: 0.078125] [gan loss: 1.419067, acc: 0.015625]\n",
            "7651: [discriminator loss: 0.6803430914878845, acc: 0.4140625] [gan loss: 0.731197, acc: 0.484375]\n",
            "7652: [discriminator loss: 0.7785249948501587, acc: 0.03125] [gan loss: 1.514510, acc: 0.000000]\n",
            "7653: [discriminator loss: 0.7210915684700012, acc: 0.4609375] [gan loss: 0.660430, acc: 0.640625]\n",
            "7654: [discriminator loss: 0.7362076044082642, acc: 0.0390625] [gan loss: 1.426380, acc: 0.000000]\n",
            "7655: [discriminator loss: 0.6903655529022217, acc: 0.4375] [gan loss: 0.735369, acc: 0.453125]\n",
            "7656: [discriminator loss: 0.7239149212837219, acc: 0.0859375] [gan loss: 1.352384, acc: 0.000000]\n",
            "7657: [discriminator loss: 0.7086045742034912, acc: 0.4140625] [gan loss: 0.845658, acc: 0.203125]\n",
            "7658: [discriminator loss: 0.759986937046051, acc: 0.03125] [gan loss: 1.547940, acc: 0.000000]\n",
            "7659: [discriminator loss: 0.6889876127243042, acc: 0.4140625] [gan loss: 0.715053, acc: 0.421875]\n",
            "7660: [discriminator loss: 0.7940698266029358, acc: 0.015625] [gan loss: 1.578688, acc: 0.000000]\n",
            "7661: [discriminator loss: 0.7071251273155212, acc: 0.453125] [gan loss: 0.674090, acc: 0.593750]\n",
            "7662: [discriminator loss: 0.7968125343322754, acc: 0.0078125] [gan loss: 1.659557, acc: 0.000000]\n",
            "7663: [discriminator loss: 0.6917061805725098, acc: 0.4609375] [gan loss: 0.677389, acc: 0.593750]\n",
            "7664: [discriminator loss: 0.7565484046936035, acc: 0.0234375] [gan loss: 1.562253, acc: 0.000000]\n",
            "7665: [discriminator loss: 0.6846573948860168, acc: 0.46875] [gan loss: 0.647322, acc: 0.578125]\n",
            "7666: [discriminator loss: 0.8104792833328247, acc: 0.0078125] [gan loss: 1.463710, acc: 0.000000]\n",
            "7667: [discriminator loss: 0.6761364340782166, acc: 0.4296875] [gan loss: 0.780208, acc: 0.375000]\n",
            "7668: [discriminator loss: 0.743537187576294, acc: 0.0390625] [gan loss: 1.527662, acc: 0.000000]\n",
            "7669: [discriminator loss: 0.7184478640556335, acc: 0.359375] [gan loss: 0.809300, acc: 0.281250]\n",
            "7670: [discriminator loss: 0.7477151155471802, acc: 0.0234375] [gan loss: 1.553774, acc: 0.000000]\n",
            "7671: [discriminator loss: 0.7005383968353271, acc: 0.4453125] [gan loss: 0.815782, acc: 0.234375]\n",
            "7672: [discriminator loss: 0.7348971366882324, acc: 0.0546875] [gan loss: 1.486389, acc: 0.000000]\n",
            "7673: [discriminator loss: 0.6788859367370605, acc: 0.4375] [gan loss: 0.707831, acc: 0.484375]\n",
            "7674: [discriminator loss: 0.7364174127578735, acc: 0.0390625] [gan loss: 1.475268, acc: 0.000000]\n",
            "7675: [discriminator loss: 0.6786839962005615, acc: 0.4609375] [gan loss: 0.744826, acc: 0.406250]\n",
            "7676: [discriminator loss: 0.7192627191543579, acc: 0.0546875] [gan loss: 1.307948, acc: 0.031250]\n",
            "7677: [discriminator loss: 0.6977810263633728, acc: 0.3671875] [gan loss: 0.889816, acc: 0.140625]\n",
            "7678: [discriminator loss: 0.7600647211074829, acc: 0.0546875] [gan loss: 1.474563, acc: 0.000000]\n",
            "7679: [discriminator loss: 0.6776715517044067, acc: 0.4296875] [gan loss: 0.773922, acc: 0.406250]\n",
            "7680: [discriminator loss: 0.782444953918457, acc: 0.015625] [gan loss: 1.575436, acc: 0.000000]\n",
            "7681: [discriminator loss: 0.7053593993186951, acc: 0.484375] [gan loss: 0.634072, acc: 0.625000]\n",
            "7682: [discriminator loss: 0.7943407297134399, acc: 0.015625] [gan loss: 1.493617, acc: 0.000000]\n",
            "7683: [discriminator loss: 0.7067331671714783, acc: 0.421875] [gan loss: 0.777054, acc: 0.390625]\n",
            "7684: [discriminator loss: 0.7651622295379639, acc: 0.0546875] [gan loss: 1.508291, acc: 0.000000]\n",
            "7685: [discriminator loss: 0.6824895143508911, acc: 0.4609375] [gan loss: 0.724447, acc: 0.453125]\n",
            "7686: [discriminator loss: 0.7688504457473755, acc: 0.0078125] [gan loss: 1.583495, acc: 0.000000]\n",
            "7687: [discriminator loss: 0.676762580871582, acc: 0.421875] [gan loss: 0.762366, acc: 0.375000]\n",
            "7688: [discriminator loss: 0.786777138710022, acc: 0.046875] [gan loss: 1.425260, acc: 0.000000]\n",
            "7689: [discriminator loss: 0.7297573089599609, acc: 0.421875] [gan loss: 0.737612, acc: 0.468750]\n",
            "7690: [discriminator loss: 0.7661011219024658, acc: 0.03125] [gan loss: 1.432474, acc: 0.000000]\n",
            "7691: [discriminator loss: 0.7000694274902344, acc: 0.390625] [gan loss: 0.835082, acc: 0.312500]\n",
            "7692: [discriminator loss: 0.7463490962982178, acc: 0.1015625] [gan loss: 1.372021, acc: 0.000000]\n",
            "7693: [discriminator loss: 0.685154378414154, acc: 0.3984375] [gan loss: 0.868574, acc: 0.140625]\n",
            "7694: [discriminator loss: 0.7258930802345276, acc: 0.0859375] [gan loss: 1.454283, acc: 0.000000]\n",
            "7695: [discriminator loss: 0.7151947021484375, acc: 0.4375] [gan loss: 0.660583, acc: 0.609375]\n",
            "7696: [discriminator loss: 0.778060793876648, acc: 0.046875] [gan loss: 1.525567, acc: 0.000000]\n",
            "7697: [discriminator loss: 0.679545521736145, acc: 0.4375] [gan loss: 0.763816, acc: 0.343750]\n",
            "7698: [discriminator loss: 0.7753058671951294, acc: 0.0390625] [gan loss: 1.450945, acc: 0.000000]\n",
            "7699: [discriminator loss: 0.668447732925415, acc: 0.4609375] [gan loss: 0.694295, acc: 0.640625]\n",
            "7700: [discriminator loss: 0.8036057949066162, acc: 0.0078125] [gan loss: 1.566197, acc: 0.000000]\n",
            "7701: [discriminator loss: 0.7008353471755981, acc: 0.4765625] [gan loss: 0.627442, acc: 0.687500]\n",
            "7702: [discriminator loss: 0.7772246599197388, acc: 0.015625] [gan loss: 1.537148, acc: 0.000000]\n",
            "7703: [discriminator loss: 0.717187762260437, acc: 0.4296875] [gan loss: 0.724856, acc: 0.453125]\n",
            "7704: [discriminator loss: 0.7584744691848755, acc: 0.03125] [gan loss: 1.499339, acc: 0.000000]\n",
            "7705: [discriminator loss: 0.6794989109039307, acc: 0.4296875] [gan loss: 0.756735, acc: 0.343750]\n",
            "7706: [discriminator loss: 0.7783302068710327, acc: 0.0390625] [gan loss: 1.490348, acc: 0.000000]\n",
            "7707: [discriminator loss: 0.6912619471549988, acc: 0.4375] [gan loss: 0.750107, acc: 0.359375]\n",
            "7708: [discriminator loss: 0.7691570520401001, acc: 0.0390625] [gan loss: 1.414132, acc: 0.000000]\n",
            "7709: [discriminator loss: 0.7171159982681274, acc: 0.3984375] [gan loss: 0.747921, acc: 0.406250]\n",
            "7710: [discriminator loss: 0.7519943714141846, acc: 0.0625] [gan loss: 1.546500, acc: 0.000000]\n",
            "7711: [discriminator loss: 0.6898711919784546, acc: 0.4765625] [gan loss: 0.739443, acc: 0.453125]\n",
            "7712: [discriminator loss: 0.7672525644302368, acc: 0.0390625] [gan loss: 1.397123, acc: 0.000000]\n",
            "7713: [discriminator loss: 0.6869920492172241, acc: 0.4140625] [gan loss: 0.795998, acc: 0.328125]\n",
            "7714: [discriminator loss: 0.7684555053710938, acc: 0.03125] [gan loss: 1.488907, acc: 0.000000]\n",
            "7715: [discriminator loss: 0.6944860219955444, acc: 0.421875] [gan loss: 0.722914, acc: 0.484375]\n",
            "7716: [discriminator loss: 0.7503951787948608, acc: 0.0625] [gan loss: 1.495213, acc: 0.000000]\n",
            "7717: [discriminator loss: 0.6967343091964722, acc: 0.4453125] [gan loss: 0.696941, acc: 0.531250]\n",
            "7718: [discriminator loss: 0.7706138491630554, acc: 0.0234375] [gan loss: 1.429335, acc: 0.000000]\n",
            "7719: [discriminator loss: 0.6912704110145569, acc: 0.4140625] [gan loss: 0.865767, acc: 0.171875]\n",
            "7720: [discriminator loss: 0.7682315111160278, acc: 0.0546875] [gan loss: 1.450724, acc: 0.000000]\n",
            "7721: [discriminator loss: 0.7088921070098877, acc: 0.40625] [gan loss: 0.781829, acc: 0.234375]\n",
            "7722: [discriminator loss: 0.7481281757354736, acc: 0.015625] [gan loss: 1.528324, acc: 0.000000]\n",
            "7723: [discriminator loss: 0.6876243352890015, acc: 0.46875] [gan loss: 0.727942, acc: 0.453125]\n",
            "7724: [discriminator loss: 0.7426648736000061, acc: 0.015625] [gan loss: 1.383537, acc: 0.000000]\n",
            "7725: [discriminator loss: 0.6752198338508606, acc: 0.4375] [gan loss: 0.765761, acc: 0.390625]\n",
            "7726: [discriminator loss: 0.7662760019302368, acc: 0.0234375] [gan loss: 1.399958, acc: 0.000000]\n",
            "7727: [discriminator loss: 0.7101073265075684, acc: 0.375] [gan loss: 0.919183, acc: 0.109375]\n",
            "7728: [discriminator loss: 0.7697404623031616, acc: 0.078125] [gan loss: 1.496227, acc: 0.000000]\n",
            "7729: [discriminator loss: 0.6804485321044922, acc: 0.453125] [gan loss: 0.823982, acc: 0.250000]\n",
            "7730: [discriminator loss: 0.7655351161956787, acc: 0.0234375] [gan loss: 1.592999, acc: 0.000000]\n",
            "7731: [discriminator loss: 0.6938838958740234, acc: 0.453125] [gan loss: 0.711792, acc: 0.468750]\n",
            "7732: [discriminator loss: 0.7713323831558228, acc: 0.0234375] [gan loss: 1.556967, acc: 0.000000]\n",
            "7733: [discriminator loss: 0.7008812427520752, acc: 0.484375] [gan loss: 0.667034, acc: 0.562500]\n",
            "7734: [discriminator loss: 0.7723262310028076, acc: 0.03125] [gan loss: 1.552915, acc: 0.000000]\n",
            "7735: [discriminator loss: 0.69749915599823, acc: 0.4453125] [gan loss: 0.731139, acc: 0.515625]\n",
            "7736: [discriminator loss: 0.7638660669326782, acc: 0.0546875] [gan loss: 1.539106, acc: 0.000000]\n",
            "7737: [discriminator loss: 0.6812842488288879, acc: 0.421875] [gan loss: 0.862735, acc: 0.218750]\n",
            "7738: [discriminator loss: 0.7282651662826538, acc: 0.1328125] [gan loss: 1.337812, acc: 0.000000]\n",
            "7739: [discriminator loss: 0.6937887668609619, acc: 0.40625] [gan loss: 0.704789, acc: 0.437500]\n",
            "7740: [discriminator loss: 0.7557852268218994, acc: 0.0078125] [gan loss: 1.494984, acc: 0.000000]\n",
            "7741: [discriminator loss: 0.6782185435295105, acc: 0.46875] [gan loss: 0.731200, acc: 0.500000]\n",
            "7742: [discriminator loss: 0.796515941619873, acc: 0.0078125] [gan loss: 1.599351, acc: 0.000000]\n",
            "7743: [discriminator loss: 0.7486052513122559, acc: 0.4453125] [gan loss: 0.589189, acc: 0.750000]\n",
            "7744: [discriminator loss: 0.8105206489562988, acc: 0.015625] [gan loss: 1.487938, acc: 0.000000]\n",
            "7745: [discriminator loss: 0.688108503818512, acc: 0.421875] [gan loss: 0.786461, acc: 0.281250]\n",
            "7746: [discriminator loss: 0.7694684863090515, acc: 0.0546875] [gan loss: 1.371338, acc: 0.000000]\n",
            "7747: [discriminator loss: 0.6937291026115417, acc: 0.40625] [gan loss: 0.831680, acc: 0.187500]\n",
            "7748: [discriminator loss: 0.7353031635284424, acc: 0.0546875] [gan loss: 1.245493, acc: 0.000000]\n",
            "7749: [discriminator loss: 0.683035135269165, acc: 0.421875] [gan loss: 0.829015, acc: 0.312500]\n",
            "7750: [discriminator loss: 0.7419224381446838, acc: 0.1015625] [gan loss: 1.448823, acc: 0.000000]\n",
            "7751: [discriminator loss: 0.7107428312301636, acc: 0.3828125] [gan loss: 0.845958, acc: 0.250000]\n",
            "7752: [discriminator loss: 0.7246935963630676, acc: 0.078125] [gan loss: 1.467645, acc: 0.000000]\n",
            "7753: [discriminator loss: 0.6995551586151123, acc: 0.453125] [gan loss: 0.744530, acc: 0.343750]\n",
            "7754: [discriminator loss: 0.7861424684524536, acc: 0.015625] [gan loss: 1.628222, acc: 0.000000]\n",
            "7755: [discriminator loss: 0.6979392766952515, acc: 0.5] [gan loss: 0.609750, acc: 0.812500]\n",
            "7756: [discriminator loss: 0.7819162607192993, acc: 0.015625] [gan loss: 1.500613, acc: 0.000000]\n",
            "7757: [discriminator loss: 0.7033788561820984, acc: 0.46875] [gan loss: 0.642068, acc: 0.625000]\n",
            "7758: [discriminator loss: 0.7852466106414795, acc: 0.015625] [gan loss: 1.487124, acc: 0.000000]\n",
            "7759: [discriminator loss: 0.7251147031784058, acc: 0.4375] [gan loss: 0.798666, acc: 0.234375]\n",
            "7760: [discriminator loss: 0.7488611340522766, acc: 0.015625] [gan loss: 1.415698, acc: 0.000000]\n",
            "7761: [discriminator loss: 0.7024738788604736, acc: 0.421875] [gan loss: 0.741317, acc: 0.375000]\n",
            "7762: [discriminator loss: 0.7576744556427002, acc: 0.0078125] [gan loss: 1.438152, acc: 0.000000]\n",
            "7763: [discriminator loss: 0.6949703693389893, acc: 0.4375] [gan loss: 0.721115, acc: 0.484375]\n",
            "7764: [discriminator loss: 0.7579812407493591, acc: 0.03125] [gan loss: 1.451802, acc: 0.000000]\n",
            "7765: [discriminator loss: 0.7000623345375061, acc: 0.4453125] [gan loss: 0.782133, acc: 0.296875]\n",
            "7766: [discriminator loss: 0.7278578877449036, acc: 0.0546875] [gan loss: 1.408377, acc: 0.000000]\n",
            "7767: [discriminator loss: 0.6825557947158813, acc: 0.4296875] [gan loss: 0.710799, acc: 0.453125]\n",
            "7768: [discriminator loss: 0.734697699546814, acc: 0.046875] [gan loss: 1.267789, acc: 0.015625]\n",
            "7769: [discriminator loss: 0.6605552434921265, acc: 0.453125] [gan loss: 0.845512, acc: 0.218750]\n",
            "7770: [discriminator loss: 0.7544597387313843, acc: 0.078125] [gan loss: 1.285977, acc: 0.000000]\n",
            "7771: [discriminator loss: 0.688132643699646, acc: 0.3984375] [gan loss: 0.810450, acc: 0.312500]\n",
            "7772: [discriminator loss: 0.7435296177864075, acc: 0.03125] [gan loss: 1.531582, acc: 0.015625]\n",
            "7773: [discriminator loss: 0.7213974595069885, acc: 0.4453125] [gan loss: 0.797083, acc: 0.250000]\n",
            "7774: [discriminator loss: 0.7536352872848511, acc: 0.0546875] [gan loss: 1.341342, acc: 0.000000]\n",
            "7775: [discriminator loss: 0.7193057537078857, acc: 0.421875] [gan loss: 0.718666, acc: 0.515625]\n",
            "7776: [discriminator loss: 0.7847874760627747, acc: 0.0234375] [gan loss: 1.669985, acc: 0.000000]\n",
            "7777: [discriminator loss: 0.7117134928703308, acc: 0.46875] [gan loss: 0.663679, acc: 0.578125]\n",
            "7778: [discriminator loss: 0.7976487874984741, acc: 0.0234375] [gan loss: 1.469496, acc: 0.000000]\n",
            "7779: [discriminator loss: 0.688572883605957, acc: 0.4609375] [gan loss: 0.733333, acc: 0.484375]\n",
            "7780: [discriminator loss: 0.7498002052307129, acc: 0.0625] [gan loss: 1.486151, acc: 0.000000]\n",
            "7781: [discriminator loss: 0.6944186091423035, acc: 0.4453125] [gan loss: 0.783529, acc: 0.375000]\n",
            "7782: [discriminator loss: 0.7482192516326904, acc: 0.046875] [gan loss: 1.454779, acc: 0.000000]\n",
            "7783: [discriminator loss: 0.6985628604888916, acc: 0.3984375] [gan loss: 0.804718, acc: 0.343750]\n",
            "7784: [discriminator loss: 0.7226215600967407, acc: 0.109375] [gan loss: 1.295128, acc: 0.000000]\n",
            "7785: [discriminator loss: 0.682570219039917, acc: 0.375] [gan loss: 0.924975, acc: 0.078125]\n",
            "7786: [discriminator loss: 0.7471029162406921, acc: 0.0703125] [gan loss: 1.452655, acc: 0.000000]\n",
            "7787: [discriminator loss: 0.7148489356040955, acc: 0.3671875] [gan loss: 0.944612, acc: 0.109375]\n",
            "7788: [discriminator loss: 0.7079067230224609, acc: 0.09375] [gan loss: 1.363979, acc: 0.000000]\n",
            "7789: [discriminator loss: 0.6629676818847656, acc: 0.4609375] [gan loss: 0.781052, acc: 0.296875]\n",
            "7790: [discriminator loss: 0.7359920144081116, acc: 0.03125] [gan loss: 1.644675, acc: 0.000000]\n",
            "7791: [discriminator loss: 0.7236372828483582, acc: 0.4140625] [gan loss: 0.648021, acc: 0.640625]\n",
            "7792: [discriminator loss: 0.8025146722793579, acc: 0.0] [gan loss: 1.692732, acc: 0.000000]\n",
            "7793: [discriminator loss: 0.6903793811798096, acc: 0.4921875] [gan loss: 0.612199, acc: 0.765625]\n",
            "7794: [discriminator loss: 0.8069773316383362, acc: 0.03125] [gan loss: 1.648097, acc: 0.000000]\n",
            "7795: [discriminator loss: 0.6939566135406494, acc: 0.484375] [gan loss: 0.726024, acc: 0.484375]\n",
            "7796: [discriminator loss: 0.8024957776069641, acc: 0.0234375] [gan loss: 1.574612, acc: 0.000000]\n",
            "7797: [discriminator loss: 0.6885312795639038, acc: 0.4453125] [gan loss: 0.718174, acc: 0.421875]\n",
            "7798: [discriminator loss: 0.7846294641494751, acc: 0.0234375] [gan loss: 1.325278, acc: 0.000000]\n",
            "7799: [discriminator loss: 0.6963783502578735, acc: 0.40625] [gan loss: 0.767980, acc: 0.375000]\n",
            "7800: [discriminator loss: 0.7464340925216675, acc: 0.046875] [gan loss: 1.320869, acc: 0.000000]\n",
            "7801: [discriminator loss: 0.7014679908752441, acc: 0.3828125] [gan loss: 0.871836, acc: 0.218750]\n",
            "7802: [discriminator loss: 0.7211719155311584, acc: 0.0625] [gan loss: 1.302022, acc: 0.000000]\n",
            "7803: [discriminator loss: 0.6953589916229248, acc: 0.3359375] [gan loss: 0.940214, acc: 0.046875]\n",
            "7804: [discriminator loss: 0.7211935520172119, acc: 0.109375] [gan loss: 1.347428, acc: 0.000000]\n",
            "7805: [discriminator loss: 0.6920311450958252, acc: 0.3828125] [gan loss: 0.842950, acc: 0.296875]\n",
            "7806: [discriminator loss: 0.7183511257171631, acc: 0.0859375] [gan loss: 1.372916, acc: 0.000000]\n",
            "7807: [discriminator loss: 0.6998962163925171, acc: 0.3359375] [gan loss: 0.977281, acc: 0.046875]\n",
            "7808: [discriminator loss: 0.6954852342605591, acc: 0.1640625] [gan loss: 1.340229, acc: 0.000000]\n",
            "7809: [discriminator loss: 0.6956318616867065, acc: 0.359375] [gan loss: 0.939804, acc: 0.093750]\n",
            "7810: [discriminator loss: 0.71190345287323, acc: 0.140625] [gan loss: 1.351363, acc: 0.000000]\n",
            "7811: [discriminator loss: 0.7054787278175354, acc: 0.3828125] [gan loss: 0.868715, acc: 0.218750]\n",
            "7812: [discriminator loss: 0.7329381704330444, acc: 0.0546875] [gan loss: 1.587662, acc: 0.000000]\n",
            "7813: [discriminator loss: 0.7069755792617798, acc: 0.484375] [gan loss: 0.537499, acc: 0.875000]\n",
            "7814: [discriminator loss: 0.8276258707046509, acc: 0.0078125] [gan loss: 1.806396, acc: 0.000000]\n",
            "7815: [discriminator loss: 0.7123085856437683, acc: 0.4921875] [gan loss: 0.540155, acc: 0.890625]\n",
            "7816: [discriminator loss: 0.8055776357650757, acc: 0.015625] [gan loss: 1.476676, acc: 0.000000]\n",
            "7817: [discriminator loss: 0.6958043575286865, acc: 0.4609375] [gan loss: 0.680451, acc: 0.546875]\n",
            "7818: [discriminator loss: 0.783980131149292, acc: 0.0234375] [gan loss: 1.486872, acc: 0.000000]\n",
            "7819: [discriminator loss: 0.6992555856704712, acc: 0.46875] [gan loss: 0.717965, acc: 0.390625]\n",
            "7820: [discriminator loss: 0.7627235651016235, acc: 0.0625] [gan loss: 1.410103, acc: 0.000000]\n",
            "7821: [discriminator loss: 0.6959185600280762, acc: 0.3828125] [gan loss: 0.820397, acc: 0.296875]\n",
            "7822: [discriminator loss: 0.7321151494979858, acc: 0.0625] [gan loss: 1.304879, acc: 0.000000]\n",
            "7823: [discriminator loss: 0.6932786703109741, acc: 0.3359375] [gan loss: 0.918428, acc: 0.109375]\n",
            "7824: [discriminator loss: 0.7257974743843079, acc: 0.09375] [gan loss: 1.160279, acc: 0.000000]\n",
            "7825: [discriminator loss: 0.6981222629547119, acc: 0.296875] [gan loss: 0.946854, acc: 0.140625]\n",
            "7826: [discriminator loss: 0.7133495807647705, acc: 0.09375] [gan loss: 1.356471, acc: 0.000000]\n",
            "7827: [discriminator loss: 0.6879925727844238, acc: 0.390625] [gan loss: 0.913624, acc: 0.078125]\n",
            "7828: [discriminator loss: 0.7032445073127747, acc: 0.109375] [gan loss: 1.538515, acc: 0.000000]\n",
            "7829: [discriminator loss: 0.6903252601623535, acc: 0.4375] [gan loss: 0.744896, acc: 0.421875]\n",
            "7830: [discriminator loss: 0.7551088929176331, acc: 0.0390625] [gan loss: 1.759981, acc: 0.000000]\n",
            "7831: [discriminator loss: 0.711370587348938, acc: 0.4765625] [gan loss: 0.612747, acc: 0.718750]\n",
            "7832: [discriminator loss: 0.8010895252227783, acc: 0.015625] [gan loss: 1.669741, acc: 0.000000]\n",
            "7833: [discriminator loss: 0.6994376182556152, acc: 0.4765625] [gan loss: 0.652045, acc: 0.593750]\n",
            "7834: [discriminator loss: 0.7938783168792725, acc: 0.0] [gan loss: 1.675187, acc: 0.000000]\n",
            "7835: [discriminator loss: 0.7142646312713623, acc: 0.46875] [gan loss: 0.654766, acc: 0.640625]\n",
            "7836: [discriminator loss: 0.799312174320221, acc: 0.03125] [gan loss: 1.282559, acc: 0.000000]\n",
            "7837: [discriminator loss: 0.6945204734802246, acc: 0.4296875] [gan loss: 0.786890, acc: 0.312500]\n",
            "7838: [discriminator loss: 0.7400405406951904, acc: 0.09375] [gan loss: 1.343885, acc: 0.000000]\n",
            "7839: [discriminator loss: 0.6909067034721375, acc: 0.375] [gan loss: 0.789012, acc: 0.390625]\n",
            "7840: [discriminator loss: 0.7329785823822021, acc: 0.0859375] [gan loss: 1.264133, acc: 0.000000]\n",
            "7841: [discriminator loss: 0.6899499893188477, acc: 0.3828125] [gan loss: 0.903655, acc: 0.125000]\n",
            "7842: [discriminator loss: 0.7099238038063049, acc: 0.2109375] [gan loss: 1.260612, acc: 0.000000]\n",
            "7843: [discriminator loss: 0.6797464489936829, acc: 0.34375] [gan loss: 0.918858, acc: 0.171875]\n",
            "7844: [discriminator loss: 0.7336149215698242, acc: 0.109375] [gan loss: 1.516885, acc: 0.000000]\n",
            "7845: [discriminator loss: 0.6811806559562683, acc: 0.4296875] [gan loss: 0.693170, acc: 0.515625]\n",
            "7846: [discriminator loss: 0.7903280258178711, acc: 0.03125] [gan loss: 1.582689, acc: 0.000000]\n",
            "7847: [discriminator loss: 0.7078654766082764, acc: 0.46875] [gan loss: 0.616434, acc: 0.640625]\n",
            "7848: [discriminator loss: 0.8254081010818481, acc: 0.0078125] [gan loss: 1.539137, acc: 0.000000]\n",
            "7849: [discriminator loss: 0.7001017928123474, acc: 0.4609375] [gan loss: 0.625080, acc: 0.687500]\n",
            "7850: [discriminator loss: 0.7672615051269531, acc: 0.0390625] [gan loss: 1.533162, acc: 0.000000]\n",
            "7851: [discriminator loss: 0.6654554009437561, acc: 0.453125] [gan loss: 0.712744, acc: 0.468750]\n",
            "7852: [discriminator loss: 0.8107240200042725, acc: 0.0234375] [gan loss: 1.622681, acc: 0.000000]\n",
            "7853: [discriminator loss: 0.6880286931991577, acc: 0.4375] [gan loss: 0.792321, acc: 0.312500]\n",
            "7854: [discriminator loss: 0.7515527009963989, acc: 0.0390625] [gan loss: 1.460559, acc: 0.000000]\n",
            "7855: [discriminator loss: 0.6815716028213501, acc: 0.4375] [gan loss: 0.791476, acc: 0.375000]\n",
            "7856: [discriminator loss: 0.7441089749336243, acc: 0.0234375] [gan loss: 1.458189, acc: 0.000000]\n",
            "7857: [discriminator loss: 0.7039473652839661, acc: 0.4375] [gan loss: 0.766445, acc: 0.390625]\n",
            "7858: [discriminator loss: 0.7652134895324707, acc: 0.0390625] [gan loss: 1.458190, acc: 0.000000]\n",
            "7859: [discriminator loss: 0.6800107955932617, acc: 0.4453125] [gan loss: 0.857645, acc: 0.187500]\n",
            "7860: [discriminator loss: 0.7410084009170532, acc: 0.046875] [gan loss: 1.478197, acc: 0.000000]\n",
            "7861: [discriminator loss: 0.6649369597434998, acc: 0.4375] [gan loss: 0.746803, acc: 0.453125]\n",
            "7862: [discriminator loss: 0.7566097378730774, acc: 0.0546875] [gan loss: 1.408556, acc: 0.000000]\n",
            "7863: [discriminator loss: 0.6720523834228516, acc: 0.390625] [gan loss: 0.771695, acc: 0.328125]\n",
            "7864: [discriminator loss: 0.7549886703491211, acc: 0.0859375] [gan loss: 1.459871, acc: 0.031250]\n",
            "7865: [discriminator loss: 0.6985307335853577, acc: 0.4140625] [gan loss: 0.818924, acc: 0.343750]\n",
            "7866: [discriminator loss: 0.7653408646583557, acc: 0.046875] [gan loss: 1.627912, acc: 0.000000]\n",
            "7867: [discriminator loss: 0.6925082206726074, acc: 0.46875] [gan loss: 0.629878, acc: 0.703125]\n",
            "7868: [discriminator loss: 0.770492434501648, acc: 0.015625] [gan loss: 1.548539, acc: 0.000000]\n",
            "7869: [discriminator loss: 0.7145867943763733, acc: 0.3984375] [gan loss: 0.725130, acc: 0.468750]\n",
            "7870: [discriminator loss: 0.7502517700195312, acc: 0.03125] [gan loss: 1.477320, acc: 0.000000]\n",
            "7871: [discriminator loss: 0.7030885815620422, acc: 0.359375] [gan loss: 0.803170, acc: 0.265625]\n",
            "7872: [discriminator loss: 0.7670978307723999, acc: 0.0703125] [gan loss: 1.517956, acc: 0.000000]\n",
            "7873: [discriminator loss: 0.7224743962287903, acc: 0.4765625] [gan loss: 0.665227, acc: 0.625000]\n",
            "7874: [discriminator loss: 0.78360915184021, acc: 0.0] [gan loss: 1.633545, acc: 0.000000]\n",
            "7875: [discriminator loss: 0.6884186267852783, acc: 0.4921875] [gan loss: 0.701943, acc: 0.468750]\n",
            "7876: [discriminator loss: 0.7617961168289185, acc: 0.0078125] [gan loss: 1.388536, acc: 0.000000]\n",
            "7877: [discriminator loss: 0.6857694387435913, acc: 0.453125] [gan loss: 0.740313, acc: 0.453125]\n",
            "7878: [discriminator loss: 0.7880385518074036, acc: 0.0546875] [gan loss: 1.320363, acc: 0.000000]\n",
            "7879: [discriminator loss: 0.7228072881698608, acc: 0.375] [gan loss: 0.900179, acc: 0.125000]\n",
            "7880: [discriminator loss: 0.7084856629371643, acc: 0.0859375] [gan loss: 1.338091, acc: 0.000000]\n",
            "7881: [discriminator loss: 0.6928390860557556, acc: 0.3125] [gan loss: 0.900808, acc: 0.062500]\n",
            "7882: [discriminator loss: 0.7303401231765747, acc: 0.1328125] [gan loss: 1.328210, acc: 0.000000]\n",
            "7883: [discriminator loss: 0.6844138503074646, acc: 0.359375] [gan loss: 0.972152, acc: 0.125000]\n",
            "7884: [discriminator loss: 0.7427144646644592, acc: 0.0859375] [gan loss: 1.432870, acc: 0.000000]\n",
            "7885: [discriminator loss: 0.6801064014434814, acc: 0.4609375] [gan loss: 0.767411, acc: 0.375000]\n",
            "7886: [discriminator loss: 0.7754772901535034, acc: 0.03125] [gan loss: 1.596020, acc: 0.000000]\n",
            "7887: [discriminator loss: 0.6950748562812805, acc: 0.484375] [gan loss: 0.631599, acc: 0.718750]\n",
            "7888: [discriminator loss: 0.7937144637107849, acc: 0.0] [gan loss: 1.716799, acc: 0.015625]\n",
            "7889: [discriminator loss: 0.7155644297599792, acc: 0.46875] [gan loss: 0.625378, acc: 0.687500]\n",
            "7890: [discriminator loss: 0.7803220152854919, acc: 0.0234375] [gan loss: 1.503957, acc: 0.000000]\n",
            "7891: [discriminator loss: 0.6944843530654907, acc: 0.4921875] [gan loss: 0.708042, acc: 0.453125]\n",
            "7892: [discriminator loss: 0.7282636165618896, acc: 0.0625] [gan loss: 1.404434, acc: 0.000000]\n",
            "7893: [discriminator loss: 0.7050914168357849, acc: 0.40625] [gan loss: 0.825202, acc: 0.234375]\n",
            "7894: [discriminator loss: 0.7451552748680115, acc: 0.0546875] [gan loss: 1.343693, acc: 0.000000]\n",
            "7895: [discriminator loss: 0.6715073585510254, acc: 0.421875] [gan loss: 0.797402, acc: 0.265625]\n",
            "7896: [discriminator loss: 0.7305391430854797, acc: 0.1015625] [gan loss: 1.362011, acc: 0.031250]\n",
            "7897: [discriminator loss: 0.6924179792404175, acc: 0.34375] [gan loss: 0.987174, acc: 0.078125]\n",
            "7898: [discriminator loss: 0.7117801904678345, acc: 0.171875] [gan loss: 1.308245, acc: 0.000000]\n",
            "7899: [discriminator loss: 0.6803545951843262, acc: 0.34375] [gan loss: 0.886781, acc: 0.171875]\n",
            "7900: [discriminator loss: 0.6985520720481873, acc: 0.140625] [gan loss: 1.210485, acc: 0.000000]\n",
            "7901: [discriminator loss: 0.6822745203971863, acc: 0.34375] [gan loss: 1.044106, acc: 0.000000]\n",
            "7902: [discriminator loss: 0.7166455388069153, acc: 0.140625] [gan loss: 1.317191, acc: 0.000000]\n",
            "7903: [discriminator loss: 0.6800630688667297, acc: 0.40625] [gan loss: 0.857333, acc: 0.234375]\n",
            "7904: [discriminator loss: 0.730626106262207, acc: 0.078125] [gan loss: 1.527706, acc: 0.000000]\n",
            "7905: [discriminator loss: 0.6857950091362, acc: 0.4453125] [gan loss: 0.626849, acc: 0.656250]\n",
            "7906: [discriminator loss: 0.7945943474769592, acc: 0.0234375] [gan loss: 1.739379, acc: 0.000000]\n",
            "7907: [discriminator loss: 0.7320087552070618, acc: 0.484375] [gan loss: 0.578342, acc: 0.812500]\n",
            "7908: [discriminator loss: 0.8124722242355347, acc: 0.0078125] [gan loss: 1.686117, acc: 0.000000]\n",
            "7909: [discriminator loss: 0.7283294796943665, acc: 0.4609375] [gan loss: 0.709518, acc: 0.500000]\n",
            "7910: [discriminator loss: 0.7551519870758057, acc: 0.0234375] [gan loss: 1.503450, acc: 0.000000]\n",
            "7911: [discriminator loss: 0.6928294897079468, acc: 0.4453125] [gan loss: 0.737461, acc: 0.468750]\n",
            "7912: [discriminator loss: 0.7678958773612976, acc: 0.0625] [gan loss: 1.414355, acc: 0.000000]\n",
            "7913: [discriminator loss: 0.6932817101478577, acc: 0.4453125] [gan loss: 0.778958, acc: 0.281250]\n",
            "7914: [discriminator loss: 0.745690107345581, acc: 0.0625] [gan loss: 1.515898, acc: 0.000000]\n",
            "7915: [discriminator loss: 0.7063360810279846, acc: 0.3671875] [gan loss: 0.870300, acc: 0.125000]\n",
            "7916: [discriminator loss: 0.7234656810760498, acc: 0.0859375] [gan loss: 1.310041, acc: 0.000000]\n",
            "7917: [discriminator loss: 0.6904580593109131, acc: 0.328125] [gan loss: 0.928675, acc: 0.140625]\n",
            "7918: [discriminator loss: 0.7288858294487, acc: 0.1015625] [gan loss: 1.405071, acc: 0.000000]\n",
            "7919: [discriminator loss: 0.6973872184753418, acc: 0.46875] [gan loss: 0.699965, acc: 0.562500]\n",
            "7920: [discriminator loss: 0.7662941217422485, acc: 0.0234375] [gan loss: 1.657755, acc: 0.000000]\n",
            "7921: [discriminator loss: 0.6777019500732422, acc: 0.46875] [gan loss: 0.657977, acc: 0.593750]\n",
            "7922: [discriminator loss: 0.7993804216384888, acc: 0.0859375] [gan loss: 1.493869, acc: 0.000000]\n",
            "7923: [discriminator loss: 0.7086618542671204, acc: 0.4375] [gan loss: 0.685898, acc: 0.578125]\n",
            "7924: [discriminator loss: 0.7947542071342468, acc: 0.03125] [gan loss: 1.458455, acc: 0.000000]\n",
            "7925: [discriminator loss: 0.6779087781906128, acc: 0.4453125] [gan loss: 0.780048, acc: 0.328125]\n",
            "7926: [discriminator loss: 0.7380648851394653, acc: 0.0703125] [gan loss: 1.358919, acc: 0.000000]\n",
            "7927: [discriminator loss: 0.7176515460014343, acc: 0.3671875] [gan loss: 0.808971, acc: 0.218750]\n",
            "7928: [discriminator loss: 0.7435534596443176, acc: 0.1171875] [gan loss: 1.189430, acc: 0.015625]\n",
            "7929: [discriminator loss: 0.6770334243774414, acc: 0.3203125] [gan loss: 0.908895, acc: 0.140625]\n",
            "7930: [discriminator loss: 0.7280569076538086, acc: 0.1640625] [gan loss: 1.200160, acc: 0.000000]\n",
            "7931: [discriminator loss: 0.7032464146614075, acc: 0.2890625] [gan loss: 1.007807, acc: 0.046875]\n",
            "7932: [discriminator loss: 0.7074336409568787, acc: 0.203125] [gan loss: 1.241688, acc: 0.000000]\n",
            "7933: [discriminator loss: 0.7079374194145203, acc: 0.234375] [gan loss: 1.257431, acc: 0.000000]\n",
            "7934: [discriminator loss: 0.6806957125663757, acc: 0.2890625] [gan loss: 1.000327, acc: 0.062500]\n",
            "7935: [discriminator loss: 0.7349929809570312, acc: 0.09375] [gan loss: 1.335902, acc: 0.000000]\n",
            "7936: [discriminator loss: 0.679034411907196, acc: 0.390625] [gan loss: 0.779594, acc: 0.406250]\n",
            "7937: [discriminator loss: 0.7447812557220459, acc: 0.078125] [gan loss: 1.849972, acc: 0.000000]\n",
            "7938: [discriminator loss: 0.6906898617744446, acc: 0.484375] [gan loss: 0.628284, acc: 0.609375]\n",
            "7939: [discriminator loss: 0.8607582449913025, acc: 0.0] [gan loss: 1.789394, acc: 0.000000]\n",
            "7940: [discriminator loss: 0.7263491749763489, acc: 0.453125] [gan loss: 0.565833, acc: 0.765625]\n",
            "7941: [discriminator loss: 0.8436740040779114, acc: 0.0078125] [gan loss: 1.486857, acc: 0.000000]\n",
            "7942: [discriminator loss: 0.6936618089675903, acc: 0.484375] [gan loss: 0.698285, acc: 0.531250]\n",
            "7943: [discriminator loss: 0.7746085524559021, acc: 0.03125] [gan loss: 1.274993, acc: 0.000000]\n",
            "7944: [discriminator loss: 0.7083728909492493, acc: 0.3359375] [gan loss: 0.911337, acc: 0.078125]\n",
            "7945: [discriminator loss: 0.7259316444396973, acc: 0.0859375] [gan loss: 1.241975, acc: 0.000000]\n",
            "7946: [discriminator loss: 0.6957383155822754, acc: 0.3125] [gan loss: 0.877690, acc: 0.140625]\n",
            "7947: [discriminator loss: 0.7426121830940247, acc: 0.0625] [gan loss: 1.365548, acc: 0.000000]\n",
            "7948: [discriminator loss: 0.6785765886306763, acc: 0.4375] [gan loss: 0.755228, acc: 0.375000]\n",
            "7949: [discriminator loss: 0.783844530582428, acc: 0.03125] [gan loss: 1.563859, acc: 0.000000]\n",
            "7950: [discriminator loss: 0.6792526245117188, acc: 0.4609375] [gan loss: 0.773312, acc: 0.328125]\n",
            "7951: [discriminator loss: 0.7736291885375977, acc: 0.0234375] [gan loss: 1.552208, acc: 0.000000]\n",
            "7952: [discriminator loss: 0.6849151849746704, acc: 0.4453125] [gan loss: 0.645963, acc: 0.593750]\n",
            "7953: [discriminator loss: 0.8047527074813843, acc: 0.015625] [gan loss: 1.610379, acc: 0.000000]\n",
            "7954: [discriminator loss: 0.737592875957489, acc: 0.4609375] [gan loss: 0.694461, acc: 0.546875]\n",
            "7955: [discriminator loss: 0.7655125856399536, acc: 0.046875] [gan loss: 1.382555, acc: 0.000000]\n",
            "7956: [discriminator loss: 0.6749918460845947, acc: 0.421875] [gan loss: 0.878020, acc: 0.156250]\n",
            "7957: [discriminator loss: 0.7176401615142822, acc: 0.0859375] [gan loss: 1.259361, acc: 0.000000]\n",
            "7958: [discriminator loss: 0.691169023513794, acc: 0.2578125] [gan loss: 1.065548, acc: 0.031250]\n",
            "7959: [discriminator loss: 0.6937589049339294, acc: 0.1796875] [gan loss: 1.197379, acc: 0.015625]\n",
            "7960: [discriminator loss: 0.6893402338027954, acc: 0.3203125] [gan loss: 0.940638, acc: 0.109375]\n",
            "7961: [discriminator loss: 0.7139661312103271, acc: 0.1015625] [gan loss: 1.482654, acc: 0.000000]\n",
            "7962: [discriminator loss: 0.7009565234184265, acc: 0.40625] [gan loss: 0.678176, acc: 0.546875]\n",
            "7963: [discriminator loss: 0.7943025231361389, acc: 0.0078125] [gan loss: 1.614630, acc: 0.000000]\n",
            "7964: [discriminator loss: 0.7011370658874512, acc: 0.484375] [gan loss: 0.633181, acc: 0.703125]\n",
            "7965: [discriminator loss: 0.7761648297309875, acc: 0.0078125] [gan loss: 1.532347, acc: 0.000000]\n",
            "7966: [discriminator loss: 0.6858460903167725, acc: 0.4609375] [gan loss: 0.724302, acc: 0.484375]\n",
            "7967: [discriminator loss: 0.7549500465393066, acc: 0.046875] [gan loss: 1.461597, acc: 0.000000]\n",
            "7968: [discriminator loss: 0.7254205942153931, acc: 0.421875] [gan loss: 0.748472, acc: 0.453125]\n",
            "7969: [discriminator loss: 0.7693113088607788, acc: 0.015625] [gan loss: 1.396971, acc: 0.000000]\n",
            "7970: [discriminator loss: 0.7091492414474487, acc: 0.390625] [gan loss: 0.797624, acc: 0.296875]\n",
            "7971: [discriminator loss: 0.7363770008087158, acc: 0.0859375] [gan loss: 1.308923, acc: 0.000000]\n",
            "7972: [discriminator loss: 0.6792136430740356, acc: 0.40625] [gan loss: 0.838242, acc: 0.250000]\n",
            "7973: [discriminator loss: 0.7182374000549316, acc: 0.0859375] [gan loss: 1.420969, acc: 0.000000]\n",
            "7974: [discriminator loss: 0.6734707355499268, acc: 0.4609375] [gan loss: 0.708069, acc: 0.500000]\n",
            "7975: [discriminator loss: 0.7995431423187256, acc: 0.03125] [gan loss: 1.641685, acc: 0.000000]\n",
            "7976: [discriminator loss: 0.7373222708702087, acc: 0.46875] [gan loss: 0.666557, acc: 0.625000]\n",
            "7977: [discriminator loss: 0.7782799601554871, acc: 0.0234375] [gan loss: 1.484696, acc: 0.000000]\n",
            "7978: [discriminator loss: 0.6935850977897644, acc: 0.421875] [gan loss: 0.754123, acc: 0.312500]\n",
            "7979: [discriminator loss: 0.769473135471344, acc: 0.0234375] [gan loss: 1.462282, acc: 0.000000]\n",
            "7980: [discriminator loss: 0.6807588338851929, acc: 0.4296875] [gan loss: 0.809970, acc: 0.296875]\n",
            "7981: [discriminator loss: 0.7389342784881592, acc: 0.0625] [gan loss: 1.284051, acc: 0.015625]\n",
            "7982: [discriminator loss: 0.6769545078277588, acc: 0.3671875] [gan loss: 0.820547, acc: 0.281250]\n",
            "7983: [discriminator loss: 0.7188377380371094, acc: 0.09375] [gan loss: 1.379117, acc: 0.015625]\n",
            "7984: [discriminator loss: 0.703889787197113, acc: 0.390625] [gan loss: 0.744481, acc: 0.375000]\n",
            "7985: [discriminator loss: 0.7824770212173462, acc: 0.03125] [gan loss: 1.528053, acc: 0.000000]\n",
            "7986: [discriminator loss: 0.7056227326393127, acc: 0.4609375] [gan loss: 0.735699, acc: 0.468750]\n",
            "7987: [discriminator loss: 0.7521663904190063, acc: 0.0390625] [gan loss: 1.501166, acc: 0.015625]\n",
            "7988: [discriminator loss: 0.6820042729377747, acc: 0.4609375] [gan loss: 0.637572, acc: 0.671875]\n",
            "7989: [discriminator loss: 0.7970713376998901, acc: 0.0234375] [gan loss: 1.436091, acc: 0.000000]\n",
            "7990: [discriminator loss: 0.690561056137085, acc: 0.453125] [gan loss: 0.719411, acc: 0.406250]\n",
            "7991: [discriminator loss: 0.8280155062675476, acc: 0.0078125] [gan loss: 1.410079, acc: 0.000000]\n",
            "7992: [discriminator loss: 0.6883989572525024, acc: 0.421875] [gan loss: 0.826366, acc: 0.171875]\n",
            "7993: [discriminator loss: 0.7304540872573853, acc: 0.046875] [gan loss: 1.393881, acc: 0.000000]\n",
            "7994: [discriminator loss: 0.6961449980735779, acc: 0.4375] [gan loss: 0.730096, acc: 0.421875]\n",
            "7995: [discriminator loss: 0.7459336519241333, acc: 0.046875] [gan loss: 1.392563, acc: 0.000000]\n",
            "7996: [discriminator loss: 0.6937181949615479, acc: 0.421875] [gan loss: 0.837945, acc: 0.156250]\n",
            "7997: [discriminator loss: 0.7229387164115906, acc: 0.046875] [gan loss: 1.407663, acc: 0.000000]\n",
            "7998: [discriminator loss: 0.6833153367042542, acc: 0.4375] [gan loss: 0.766018, acc: 0.453125]\n",
            "7999: [discriminator loss: 0.7467583417892456, acc: 0.0546875] [gan loss: 1.597444, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "8000: [discriminator loss: 0.7050265073776245, acc: 0.453125] [gan loss: 0.680450, acc: 0.593750]\n",
            "8001: [discriminator loss: 0.8042909502983093, acc: 0.0234375] [gan loss: 1.605113, acc: 0.000000]\n",
            "8002: [discriminator loss: 0.7167547941207886, acc: 0.4140625] [gan loss: 0.681381, acc: 0.468750]\n",
            "8003: [discriminator loss: 0.7863041162490845, acc: 0.0078125] [gan loss: 1.532173, acc: 0.000000]\n",
            "8004: [discriminator loss: 0.7155612707138062, acc: 0.4375] [gan loss: 0.805398, acc: 0.218750]\n",
            "8005: [discriminator loss: 0.7380611896514893, acc: 0.0546875] [gan loss: 1.404602, acc: 0.000000]\n",
            "8006: [discriminator loss: 0.6967679262161255, acc: 0.3671875] [gan loss: 0.837027, acc: 0.203125]\n",
            "8007: [discriminator loss: 0.7651946544647217, acc: 0.0546875] [gan loss: 1.397375, acc: 0.000000]\n",
            "8008: [discriminator loss: 0.6897104382514954, acc: 0.4453125] [gan loss: 0.812395, acc: 0.218750]\n",
            "8009: [discriminator loss: 0.7508217096328735, acc: 0.03125] [gan loss: 1.512832, acc: 0.000000]\n",
            "8010: [discriminator loss: 0.712712287902832, acc: 0.4296875] [gan loss: 0.672923, acc: 0.546875]\n",
            "8011: [discriminator loss: 0.7943775653839111, acc: 0.0] [gan loss: 1.503854, acc: 0.000000]\n",
            "8012: [discriminator loss: 0.6931252479553223, acc: 0.46875] [gan loss: 0.729789, acc: 0.437500]\n",
            "8013: [discriminator loss: 0.7488921284675598, acc: 0.0390625] [gan loss: 1.415872, acc: 0.000000]\n",
            "8014: [discriminator loss: 0.6971807479858398, acc: 0.4453125] [gan loss: 0.722138, acc: 0.515625]\n",
            "8015: [discriminator loss: 0.7473556995391846, acc: 0.03125] [gan loss: 1.426186, acc: 0.000000]\n",
            "8016: [discriminator loss: 0.6800332069396973, acc: 0.4296875] [gan loss: 0.807104, acc: 0.234375]\n",
            "8017: [discriminator loss: 0.7416765689849854, acc: 0.078125] [gan loss: 1.385636, acc: 0.000000]\n",
            "8018: [discriminator loss: 0.7033640146255493, acc: 0.4140625] [gan loss: 0.760203, acc: 0.375000]\n",
            "8019: [discriminator loss: 0.7548686265945435, acc: 0.0625] [gan loss: 1.500908, acc: 0.000000]\n",
            "8020: [discriminator loss: 0.7036677598953247, acc: 0.375] [gan loss: 0.861706, acc: 0.234375]\n",
            "8021: [discriminator loss: 0.7831839919090271, acc: 0.0390625] [gan loss: 1.504685, acc: 0.000000]\n",
            "8022: [discriminator loss: 0.7022477388381958, acc: 0.453125] [gan loss: 0.708403, acc: 0.437500]\n",
            "8023: [discriminator loss: 0.7633872032165527, acc: 0.09375] [gan loss: 1.400428, acc: 0.000000]\n",
            "8024: [discriminator loss: 0.6738092303276062, acc: 0.5] [gan loss: 0.656183, acc: 0.625000]\n",
            "8025: [discriminator loss: 0.7607239484786987, acc: 0.0546875] [gan loss: 1.354712, acc: 0.000000]\n",
            "8026: [discriminator loss: 0.7389230728149414, acc: 0.3359375] [gan loss: 0.836717, acc: 0.125000]\n",
            "8027: [discriminator loss: 0.738426685333252, acc: 0.0625] [gan loss: 1.497406, acc: 0.000000]\n",
            "8028: [discriminator loss: 0.6940371990203857, acc: 0.453125] [gan loss: 0.734213, acc: 0.359375]\n",
            "8029: [discriminator loss: 0.7620357275009155, acc: 0.0703125] [gan loss: 1.602888, acc: 0.000000]\n",
            "8030: [discriminator loss: 0.702021598815918, acc: 0.4765625] [gan loss: 0.706045, acc: 0.531250]\n",
            "8031: [discriminator loss: 0.7964286208152771, acc: 0.0078125] [gan loss: 1.760864, acc: 0.000000]\n",
            "8032: [discriminator loss: 0.7098663449287415, acc: 0.46875] [gan loss: 0.696975, acc: 0.562500]\n",
            "8033: [discriminator loss: 0.7776524424552917, acc: 0.03125] [gan loss: 1.515547, acc: 0.000000]\n",
            "8034: [discriminator loss: 0.7146352529525757, acc: 0.46875] [gan loss: 0.698635, acc: 0.515625]\n",
            "8035: [discriminator loss: 0.7475262880325317, acc: 0.078125] [gan loss: 1.346139, acc: 0.015625]\n",
            "8036: [discriminator loss: 0.7010078430175781, acc: 0.4140625] [gan loss: 0.840686, acc: 0.171875]\n",
            "8037: [discriminator loss: 0.7403712868690491, acc: 0.078125] [gan loss: 1.349761, acc: 0.000000]\n",
            "8038: [discriminator loss: 0.6798703670501709, acc: 0.3828125] [gan loss: 0.762661, acc: 0.390625]\n",
            "8039: [discriminator loss: 0.7527785301208496, acc: 0.0703125] [gan loss: 1.449668, acc: 0.000000]\n",
            "8040: [discriminator loss: 0.7019490003585815, acc: 0.4296875] [gan loss: 0.754360, acc: 0.421875]\n",
            "8041: [discriminator loss: 0.7456719875335693, acc: 0.09375] [gan loss: 1.437868, acc: 0.000000]\n",
            "8042: [discriminator loss: 0.6862983703613281, acc: 0.3984375] [gan loss: 0.838680, acc: 0.250000]\n",
            "8043: [discriminator loss: 0.6972396373748779, acc: 0.125] [gan loss: 1.313331, acc: 0.000000]\n",
            "8044: [discriminator loss: 0.7205526828765869, acc: 0.3046875] [gan loss: 1.008915, acc: 0.031250]\n",
            "8045: [discriminator loss: 0.6935733556747437, acc: 0.1875] [gan loss: 1.117387, acc: 0.031250]\n",
            "8046: [discriminator loss: 0.7379150390625, acc: 0.234375] [gan loss: 1.000948, acc: 0.031250]\n",
            "8047: [discriminator loss: 0.7392758727073669, acc: 0.15625] [gan loss: 1.152005, acc: 0.000000]\n",
            "8048: [discriminator loss: 0.6992030143737793, acc: 0.296875] [gan loss: 1.172833, acc: 0.000000]\n",
            "8049: [discriminator loss: 0.7151550054550171, acc: 0.203125] [gan loss: 1.182400, acc: 0.000000]\n",
            "8050: [discriminator loss: 0.6908975839614868, acc: 0.28125] [gan loss: 1.034242, acc: 0.015625]\n",
            "8051: [discriminator loss: 0.7036682367324829, acc: 0.25] [gan loss: 1.200773, acc: 0.000000]\n",
            "8052: [discriminator loss: 0.7013201713562012, acc: 0.28125] [gan loss: 1.212468, acc: 0.015625]\n",
            "8053: [discriminator loss: 0.7080883979797363, acc: 0.21875] [gan loss: 1.396220, acc: 0.000000]\n",
            "8054: [discriminator loss: 0.6904211044311523, acc: 0.34375] [gan loss: 0.819544, acc: 0.171875]\n",
            "8055: [discriminator loss: 0.714299201965332, acc: 0.1015625] [gan loss: 1.572448, acc: 0.000000]\n",
            "8056: [discriminator loss: 0.7310482263565063, acc: 0.484375] [gan loss: 0.556974, acc: 0.796875]\n",
            "8057: [discriminator loss: 0.8666175603866577, acc: 0.0] [gan loss: 2.042084, acc: 0.000000]\n",
            "8058: [discriminator loss: 0.7538304328918457, acc: 0.5] [gan loss: 0.442546, acc: 0.937500]\n",
            "8059: [discriminator loss: 0.8960936069488525, acc: 0.0] [gan loss: 1.564383, acc: 0.000000]\n",
            "8060: [discriminator loss: 0.7237626910209656, acc: 0.4765625] [gan loss: 0.637441, acc: 0.640625]\n",
            "8061: [discriminator loss: 0.8084979057312012, acc: 0.0234375] [gan loss: 1.355174, acc: 0.000000]\n",
            "8062: [discriminator loss: 0.7045997381210327, acc: 0.40625] [gan loss: 0.785018, acc: 0.296875]\n",
            "8063: [discriminator loss: 0.7447630167007446, acc: 0.09375] [gan loss: 1.133591, acc: 0.015625]\n",
            "8064: [discriminator loss: 0.6994542479515076, acc: 0.3515625] [gan loss: 0.883259, acc: 0.218750]\n",
            "8065: [discriminator loss: 0.7122390866279602, acc: 0.171875] [gan loss: 1.065000, acc: 0.000000]\n",
            "8066: [discriminator loss: 0.7133216261863708, acc: 0.234375] [gan loss: 1.101563, acc: 0.015625]\n",
            "8067: [discriminator loss: 0.7116782665252686, acc: 0.1875] [gan loss: 1.241600, acc: 0.015625]\n",
            "8068: [discriminator loss: 0.6857879757881165, acc: 0.3125] [gan loss: 0.906576, acc: 0.109375]\n",
            "8069: [discriminator loss: 0.7348822355270386, acc: 0.078125] [gan loss: 1.297559, acc: 0.000000]\n",
            "8070: [discriminator loss: 0.6933317184448242, acc: 0.3671875] [gan loss: 0.863009, acc: 0.125000]\n",
            "8071: [discriminator loss: 0.7269032597541809, acc: 0.1171875] [gan loss: 1.302159, acc: 0.000000]\n",
            "8072: [discriminator loss: 0.7130190134048462, acc: 0.34375] [gan loss: 0.879015, acc: 0.093750]\n",
            "8073: [discriminator loss: 0.7516878247261047, acc: 0.09375] [gan loss: 1.488127, acc: 0.000000]\n",
            "8074: [discriminator loss: 0.6775742769241333, acc: 0.46875] [gan loss: 0.724347, acc: 0.468750]\n",
            "8075: [discriminator loss: 0.7537965774536133, acc: 0.0234375] [gan loss: 1.511089, acc: 0.000000]\n",
            "8076: [discriminator loss: 0.6849766969680786, acc: 0.4765625] [gan loss: 0.626816, acc: 0.671875]\n",
            "8077: [discriminator loss: 0.8165369033813477, acc: 0.0] [gan loss: 1.680497, acc: 0.000000]\n",
            "8078: [discriminator loss: 0.7122146487236023, acc: 0.484375] [gan loss: 0.648117, acc: 0.671875]\n",
            "8079: [discriminator loss: 0.7979950904846191, acc: 0.015625] [gan loss: 1.463879, acc: 0.000000]\n",
            "8080: [discriminator loss: 0.6986326575279236, acc: 0.4765625] [gan loss: 0.738022, acc: 0.500000]\n",
            "8081: [discriminator loss: 0.7663940191268921, acc: 0.046875] [gan loss: 1.415714, acc: 0.000000]\n",
            "8082: [discriminator loss: 0.7029166221618652, acc: 0.4296875] [gan loss: 0.798160, acc: 0.218750]\n",
            "8083: [discriminator loss: 0.7328040599822998, acc: 0.046875] [gan loss: 1.334778, acc: 0.000000]\n",
            "8084: [discriminator loss: 0.6778406500816345, acc: 0.3984375] [gan loss: 0.815791, acc: 0.281250]\n",
            "8085: [discriminator loss: 0.7500944137573242, acc: 0.0859375] [gan loss: 1.425350, acc: 0.000000]\n",
            "8086: [discriminator loss: 0.6788309812545776, acc: 0.4296875] [gan loss: 0.843941, acc: 0.265625]\n",
            "8087: [discriminator loss: 0.7339774370193481, acc: 0.0859375] [gan loss: 1.397280, acc: 0.000000]\n",
            "8088: [discriminator loss: 0.6891080141067505, acc: 0.4375] [gan loss: 0.674592, acc: 0.593750]\n",
            "8089: [discriminator loss: 0.7675767540931702, acc: 0.046875] [gan loss: 1.540974, acc: 0.000000]\n",
            "8090: [discriminator loss: 0.6804856061935425, acc: 0.4765625] [gan loss: 0.692505, acc: 0.546875]\n",
            "8091: [discriminator loss: 0.8106876015663147, acc: 0.0] [gan loss: 1.577037, acc: 0.000000]\n",
            "8092: [discriminator loss: 0.7011957764625549, acc: 0.4609375] [gan loss: 0.701673, acc: 0.500000]\n",
            "8093: [discriminator loss: 0.7936056852340698, acc: 0.0078125] [gan loss: 1.444459, acc: 0.000000]\n",
            "8094: [discriminator loss: 0.6999003887176514, acc: 0.4453125] [gan loss: 0.728017, acc: 0.421875]\n",
            "8095: [discriminator loss: 0.7618951797485352, acc: 0.03125] [gan loss: 1.306420, acc: 0.000000]\n",
            "8096: [discriminator loss: 0.7346892356872559, acc: 0.390625] [gan loss: 0.818703, acc: 0.203125]\n",
            "8097: [discriminator loss: 0.7364346385002136, acc: 0.0703125] [gan loss: 1.326088, acc: 0.000000]\n",
            "8098: [discriminator loss: 0.6980410814285278, acc: 0.375] [gan loss: 0.860328, acc: 0.156250]\n",
            "8099: [discriminator loss: 0.7412599325180054, acc: 0.0703125] [gan loss: 1.319373, acc: 0.046875]\n",
            "8100: [discriminator loss: 0.70636385679245, acc: 0.3984375] [gan loss: 0.834827, acc: 0.250000]\n",
            "8101: [discriminator loss: 0.7461588978767395, acc: 0.09375] [gan loss: 1.374348, acc: 0.000000]\n",
            "8102: [discriminator loss: 0.6885969042778015, acc: 0.4453125] [gan loss: 0.803115, acc: 0.265625]\n",
            "8103: [discriminator loss: 0.7407913208007812, acc: 0.0625] [gan loss: 1.361845, acc: 0.015625]\n",
            "8104: [discriminator loss: 0.7131712436676025, acc: 0.3828125] [gan loss: 0.792583, acc: 0.281250]\n",
            "8105: [discriminator loss: 0.7180005311965942, acc: 0.0703125] [gan loss: 1.385931, acc: 0.000000]\n",
            "8106: [discriminator loss: 0.7036830186843872, acc: 0.421875] [gan loss: 0.780115, acc: 0.234375]\n",
            "8107: [discriminator loss: 0.7337615489959717, acc: 0.0390625] [gan loss: 1.329557, acc: 0.000000]\n",
            "8108: [discriminator loss: 0.7177841663360596, acc: 0.390625] [gan loss: 0.775054, acc: 0.343750]\n",
            "8109: [discriminator loss: 0.7477715015411377, acc: 0.078125] [gan loss: 1.258737, acc: 0.000000]\n",
            "8110: [discriminator loss: 0.6926111578941345, acc: 0.40625] [gan loss: 0.773535, acc: 0.359375]\n",
            "8111: [discriminator loss: 0.7607915997505188, acc: 0.0703125] [gan loss: 1.530868, acc: 0.000000]\n",
            "8112: [discriminator loss: 0.7104064226150513, acc: 0.3984375] [gan loss: 0.813177, acc: 0.343750]\n",
            "8113: [discriminator loss: 0.7567627429962158, acc: 0.046875] [gan loss: 1.411301, acc: 0.000000]\n",
            "8114: [discriminator loss: 0.7224774360656738, acc: 0.3828125] [gan loss: 0.769301, acc: 0.343750]\n",
            "8115: [discriminator loss: 0.7380934357643127, acc: 0.0703125] [gan loss: 1.452875, acc: 0.015625]\n",
            "8116: [discriminator loss: 0.7252660989761353, acc: 0.4140625] [gan loss: 0.677535, acc: 0.546875]\n",
            "8117: [discriminator loss: 0.7808241844177246, acc: 0.015625] [gan loss: 1.510138, acc: 0.000000]\n",
            "8118: [discriminator loss: 0.6901338696479797, acc: 0.46875] [gan loss: 0.671399, acc: 0.578125]\n",
            "8119: [discriminator loss: 0.7795494198799133, acc: 0.0390625] [gan loss: 1.432399, acc: 0.000000]\n",
            "8120: [discriminator loss: 0.7076182961463928, acc: 0.4296875] [gan loss: 0.812946, acc: 0.296875]\n",
            "8121: [discriminator loss: 0.7690883278846741, acc: 0.0234375] [gan loss: 1.390530, acc: 0.000000]\n",
            "8122: [discriminator loss: 0.7020895481109619, acc: 0.4296875] [gan loss: 0.770939, acc: 0.265625]\n",
            "8123: [discriminator loss: 0.736841082572937, acc: 0.0390625] [gan loss: 1.410387, acc: 0.000000]\n",
            "8124: [discriminator loss: 0.7052690982818604, acc: 0.390625] [gan loss: 0.716197, acc: 0.500000]\n",
            "8125: [discriminator loss: 0.7468875646591187, acc: 0.078125] [gan loss: 1.317292, acc: 0.000000]\n",
            "8126: [discriminator loss: 0.7054554224014282, acc: 0.40625] [gan loss: 0.836154, acc: 0.265625]\n",
            "8127: [discriminator loss: 0.7584827542304993, acc: 0.0234375] [gan loss: 1.430843, acc: 0.000000]\n",
            "8128: [discriminator loss: 0.6855472326278687, acc: 0.4609375] [gan loss: 0.798541, acc: 0.250000]\n",
            "8129: [discriminator loss: 0.7550004720687866, acc: 0.0625] [gan loss: 1.332654, acc: 0.000000]\n",
            "8130: [discriminator loss: 0.7113659381866455, acc: 0.3984375] [gan loss: 0.826725, acc: 0.187500]\n",
            "8131: [discriminator loss: 0.7338816523551941, acc: 0.046875] [gan loss: 1.291464, acc: 0.000000]\n",
            "8132: [discriminator loss: 0.6671756505966187, acc: 0.4609375] [gan loss: 0.799223, acc: 0.312500]\n",
            "8133: [discriminator loss: 0.7442355751991272, acc: 0.0703125] [gan loss: 1.405937, acc: 0.000000]\n",
            "8134: [discriminator loss: 0.6868960857391357, acc: 0.4375] [gan loss: 0.792295, acc: 0.250000]\n",
            "8135: [discriminator loss: 0.7448471784591675, acc: 0.0625] [gan loss: 1.416686, acc: 0.000000]\n",
            "8136: [discriminator loss: 0.6911409497261047, acc: 0.3828125] [gan loss: 0.818334, acc: 0.218750]\n",
            "8137: [discriminator loss: 0.7279861569404602, acc: 0.078125] [gan loss: 1.341047, acc: 0.000000]\n",
            "8138: [discriminator loss: 0.710777759552002, acc: 0.375] [gan loss: 0.836334, acc: 0.234375]\n",
            "8139: [discriminator loss: 0.7612452507019043, acc: 0.0625] [gan loss: 1.554198, acc: 0.000000]\n",
            "8140: [discriminator loss: 0.6816438436508179, acc: 0.46875] [gan loss: 0.673439, acc: 0.609375]\n",
            "8141: [discriminator loss: 0.7629839777946472, acc: 0.0390625] [gan loss: 1.411710, acc: 0.000000]\n",
            "8142: [discriminator loss: 0.694097638130188, acc: 0.4765625] [gan loss: 0.672007, acc: 0.531250]\n",
            "8143: [discriminator loss: 0.7829604148864746, acc: 0.015625] [gan loss: 1.597638, acc: 0.000000]\n",
            "8144: [discriminator loss: 0.7148755192756653, acc: 0.453125] [gan loss: 0.690365, acc: 0.546875]\n",
            "8145: [discriminator loss: 0.7787457704544067, acc: 0.0078125] [gan loss: 1.348432, acc: 0.000000]\n",
            "8146: [discriminator loss: 0.6881954669952393, acc: 0.4609375] [gan loss: 0.739532, acc: 0.468750]\n",
            "8147: [discriminator loss: 0.7189807295799255, acc: 0.0546875] [gan loss: 1.320536, acc: 0.015625]\n",
            "8148: [discriminator loss: 0.7100220918655396, acc: 0.34375] [gan loss: 0.932208, acc: 0.062500]\n",
            "8149: [discriminator loss: 0.7438005208969116, acc: 0.09375] [gan loss: 1.149861, acc: 0.015625]\n",
            "8150: [discriminator loss: 0.7156652212142944, acc: 0.25] [gan loss: 1.043974, acc: 0.000000]\n",
            "8151: [discriminator loss: 0.6891790628433228, acc: 0.2265625] [gan loss: 1.014761, acc: 0.015625]\n",
            "8152: [discriminator loss: 0.7136282920837402, acc: 0.171875] [gan loss: 1.211199, acc: 0.000000]\n",
            "8153: [discriminator loss: 0.693564772605896, acc: 0.390625] [gan loss: 0.933541, acc: 0.109375]\n",
            "8154: [discriminator loss: 0.710220456123352, acc: 0.109375] [gan loss: 1.317924, acc: 0.000000]\n",
            "8155: [discriminator loss: 0.7122299671173096, acc: 0.34375] [gan loss: 0.813868, acc: 0.265625]\n",
            "8156: [discriminator loss: 0.7647014856338501, acc: 0.0546875] [gan loss: 1.638153, acc: 0.000000]\n",
            "8157: [discriminator loss: 0.7211765050888062, acc: 0.484375] [gan loss: 0.577442, acc: 0.843750]\n",
            "8158: [discriminator loss: 0.8141415119171143, acc: 0.015625] [gan loss: 1.527691, acc: 0.000000]\n",
            "8159: [discriminator loss: 0.7126633524894714, acc: 0.4765625] [gan loss: 0.626570, acc: 0.671875]\n",
            "8160: [discriminator loss: 0.7683997750282288, acc: 0.0] [gan loss: 1.413658, acc: 0.000000]\n",
            "8161: [discriminator loss: 0.7044856548309326, acc: 0.4453125] [gan loss: 0.750946, acc: 0.375000]\n",
            "8162: [discriminator loss: 0.7487558722496033, acc: 0.015625] [gan loss: 1.416537, acc: 0.000000]\n",
            "8163: [discriminator loss: 0.7062712907791138, acc: 0.4375] [gan loss: 0.705096, acc: 0.390625]\n",
            "8164: [discriminator loss: 0.7579820156097412, acc: 0.015625] [gan loss: 1.341839, acc: 0.000000]\n",
            "8165: [discriminator loss: 0.6901061534881592, acc: 0.3984375] [gan loss: 0.819922, acc: 0.203125]\n",
            "8166: [discriminator loss: 0.7659288644790649, acc: 0.0703125] [gan loss: 1.318515, acc: 0.000000]\n",
            "8167: [discriminator loss: 0.7032968997955322, acc: 0.375] [gan loss: 0.839987, acc: 0.203125]\n",
            "8168: [discriminator loss: 0.7162612676620483, acc: 0.0703125] [gan loss: 1.181113, acc: 0.000000]\n",
            "8169: [discriminator loss: 0.6907693147659302, acc: 0.3203125] [gan loss: 0.943203, acc: 0.203125]\n",
            "8170: [discriminator loss: 0.7236420512199402, acc: 0.2109375] [gan loss: 1.130457, acc: 0.000000]\n",
            "8171: [discriminator loss: 0.7197027802467346, acc: 0.2265625] [gan loss: 1.115411, acc: 0.015625]\n",
            "8172: [discriminator loss: 0.7160188555717468, acc: 0.2109375] [gan loss: 1.077765, acc: 0.015625]\n",
            "8173: [discriminator loss: 0.684147298336029, acc: 0.3046875] [gan loss: 0.900685, acc: 0.109375]\n",
            "8174: [discriminator loss: 0.730749249458313, acc: 0.0546875] [gan loss: 1.395288, acc: 0.015625]\n",
            "8175: [discriminator loss: 0.6878235340118408, acc: 0.3984375] [gan loss: 0.908431, acc: 0.062500]\n",
            "8176: [discriminator loss: 0.7291290760040283, acc: 0.0703125] [gan loss: 1.458533, acc: 0.000000]\n",
            "8177: [discriminator loss: 0.7049667835235596, acc: 0.3984375] [gan loss: 0.880845, acc: 0.140625]\n",
            "8178: [discriminator loss: 0.7446399927139282, acc: 0.0546875] [gan loss: 1.491217, acc: 0.000000]\n",
            "8179: [discriminator loss: 0.6796044707298279, acc: 0.4765625] [gan loss: 0.638889, acc: 0.656250]\n",
            "8180: [discriminator loss: 0.8015831112861633, acc: 0.0234375] [gan loss: 1.637748, acc: 0.000000]\n",
            "8181: [discriminator loss: 0.7228796482086182, acc: 0.4921875] [gan loss: 0.522608, acc: 0.906250]\n",
            "8182: [discriminator loss: 0.8414022922515869, acc: 0.0078125] [gan loss: 1.628664, acc: 0.000000]\n",
            "8183: [discriminator loss: 0.7228788137435913, acc: 0.4765625] [gan loss: 0.698508, acc: 0.515625]\n",
            "8184: [discriminator loss: 0.778536319732666, acc: 0.0] [gan loss: 1.315344, acc: 0.000000]\n",
            "8185: [discriminator loss: 0.7033634185791016, acc: 0.4140625] [gan loss: 0.854183, acc: 0.203125]\n",
            "8186: [discriminator loss: 0.7297146320343018, acc: 0.0859375] [gan loss: 1.232236, acc: 0.000000]\n",
            "8187: [discriminator loss: 0.7159778475761414, acc: 0.3046875] [gan loss: 1.017698, acc: 0.015625]\n",
            "8188: [discriminator loss: 0.6842384934425354, acc: 0.171875] [gan loss: 1.122874, acc: 0.000000]\n",
            "8189: [discriminator loss: 0.7153091430664062, acc: 0.265625] [gan loss: 0.979464, acc: 0.031250]\n",
            "8190: [discriminator loss: 0.7121503949165344, acc: 0.1484375] [gan loss: 1.146985, acc: 0.000000]\n",
            "8191: [discriminator loss: 0.7075644731521606, acc: 0.21875] [gan loss: 1.008578, acc: 0.046875]\n",
            "8192: [discriminator loss: 0.7335833311080933, acc: 0.1484375] [gan loss: 1.129521, acc: 0.000000]\n",
            "8193: [discriminator loss: 0.6818373203277588, acc: 0.34375] [gan loss: 0.894542, acc: 0.109375]\n",
            "8194: [discriminator loss: 0.6949440836906433, acc: 0.1953125] [gan loss: 1.288508, acc: 0.000000]\n",
            "8195: [discriminator loss: 0.7030302286148071, acc: 0.3203125] [gan loss: 0.894749, acc: 0.140625]\n",
            "8196: [discriminator loss: 0.7451508045196533, acc: 0.109375] [gan loss: 1.425215, acc: 0.000000]\n",
            "8197: [discriminator loss: 0.7035671472549438, acc: 0.4375] [gan loss: 0.680727, acc: 0.578125]\n",
            "8198: [discriminator loss: 0.780802845954895, acc: 0.0234375] [gan loss: 1.759637, acc: 0.000000]\n",
            "8199: [discriminator loss: 0.7233473658561707, acc: 0.5] [gan loss: 0.591523, acc: 0.734375]\n",
            "8200: [discriminator loss: 0.8259379863739014, acc: 0.015625] [gan loss: 1.741654, acc: 0.000000]\n",
            "8201: [discriminator loss: 0.7123545408248901, acc: 0.4765625] [gan loss: 0.595809, acc: 0.812500]\n",
            "8202: [discriminator loss: 0.7936441898345947, acc: 0.0] [gan loss: 1.531340, acc: 0.000000]\n",
            "8203: [discriminator loss: 0.6911606788635254, acc: 0.4765625] [gan loss: 0.735215, acc: 0.421875]\n",
            "8204: [discriminator loss: 0.7666537761688232, acc: 0.0390625] [gan loss: 1.307136, acc: 0.000000]\n",
            "8205: [discriminator loss: 0.6704843044281006, acc: 0.4140625] [gan loss: 0.877104, acc: 0.140625]\n",
            "8206: [discriminator loss: 0.7209883332252502, acc: 0.125] [gan loss: 1.193182, acc: 0.031250]\n",
            "8207: [discriminator loss: 0.6896061897277832, acc: 0.34375] [gan loss: 0.870266, acc: 0.140625]\n",
            "8208: [discriminator loss: 0.7378660440444946, acc: 0.1328125] [gan loss: 1.161692, acc: 0.015625]\n",
            "8209: [discriminator loss: 0.7219338417053223, acc: 0.2421875] [gan loss: 1.073798, acc: 0.015625]\n",
            "8210: [discriminator loss: 0.676344633102417, acc: 0.2734375] [gan loss: 1.046287, acc: 0.031250]\n",
            "8211: [discriminator loss: 0.7153851985931396, acc: 0.171875] [gan loss: 1.299641, acc: 0.000000]\n",
            "8212: [discriminator loss: 0.6979564428329468, acc: 0.34375] [gan loss: 0.929236, acc: 0.109375]\n",
            "8213: [discriminator loss: 0.7115805149078369, acc: 0.1484375] [gan loss: 1.205359, acc: 0.000000]\n",
            "8214: [discriminator loss: 0.7127517461776733, acc: 0.28125] [gan loss: 0.970166, acc: 0.109375]\n",
            "8215: [discriminator loss: 0.7181926965713501, acc: 0.1328125] [gan loss: 1.441405, acc: 0.000000]\n",
            "8216: [discriminator loss: 0.6843199729919434, acc: 0.4375] [gan loss: 0.658125, acc: 0.640625]\n",
            "8217: [discriminator loss: 0.7599100470542908, acc: 0.0546875] [gan loss: 1.662321, acc: 0.000000]\n",
            "8218: [discriminator loss: 0.7134141325950623, acc: 0.4921875] [gan loss: 0.592952, acc: 0.812500]\n",
            "8219: [discriminator loss: 0.7798480987548828, acc: 0.015625] [gan loss: 1.452712, acc: 0.000000]\n",
            "8220: [discriminator loss: 0.6901329755783081, acc: 0.4765625] [gan loss: 0.652059, acc: 0.640625]\n",
            "8221: [discriminator loss: 0.8034564852714539, acc: 0.0546875] [gan loss: 1.421040, acc: 0.000000]\n",
            "8222: [discriminator loss: 0.6806668639183044, acc: 0.453125] [gan loss: 0.744688, acc: 0.406250]\n",
            "8223: [discriminator loss: 0.7226263284683228, acc: 0.0625] [gan loss: 1.310907, acc: 0.000000]\n",
            "8224: [discriminator loss: 0.6750260591506958, acc: 0.40625] [gan loss: 0.864496, acc: 0.218750]\n",
            "8225: [discriminator loss: 0.7283616065979004, acc: 0.109375] [gan loss: 1.294790, acc: 0.015625]\n",
            "8226: [discriminator loss: 0.6855542063713074, acc: 0.3671875] [gan loss: 0.928036, acc: 0.156250]\n",
            "8227: [discriminator loss: 0.7349923849105835, acc: 0.140625] [gan loss: 1.357035, acc: 0.000000]\n",
            "8228: [discriminator loss: 0.6816864013671875, acc: 0.375] [gan loss: 0.916240, acc: 0.109375]\n",
            "8229: [discriminator loss: 0.7062256336212158, acc: 0.109375] [gan loss: 1.240509, acc: 0.000000]\n",
            "8230: [discriminator loss: 0.6733661890029907, acc: 0.3828125] [gan loss: 0.903027, acc: 0.140625]\n",
            "8231: [discriminator loss: 0.7570400238037109, acc: 0.125] [gan loss: 1.252428, acc: 0.000000]\n",
            "8232: [discriminator loss: 0.6997956037521362, acc: 0.375] [gan loss: 0.779364, acc: 0.312500]\n",
            "8233: [discriminator loss: 0.7305914163589478, acc: 0.078125] [gan loss: 1.411709, acc: 0.000000]\n",
            "8234: [discriminator loss: 0.6708164811134338, acc: 0.46875] [gan loss: 0.735204, acc: 0.437500]\n",
            "8235: [discriminator loss: 0.7703045606613159, acc: 0.015625] [gan loss: 1.495898, acc: 0.000000]\n",
            "8236: [discriminator loss: 0.718338668346405, acc: 0.46875] [gan loss: 0.653467, acc: 0.609375]\n",
            "8237: [discriminator loss: 0.7566447257995605, acc: 0.0078125] [gan loss: 1.521637, acc: 0.000000]\n",
            "8238: [discriminator loss: 0.7135251760482788, acc: 0.4453125] [gan loss: 0.726340, acc: 0.500000]\n",
            "8239: [discriminator loss: 0.7483340501785278, acc: 0.015625] [gan loss: 1.419784, acc: 0.000000]\n",
            "8240: [discriminator loss: 0.7133711576461792, acc: 0.4296875] [gan loss: 0.755456, acc: 0.375000]\n",
            "8241: [discriminator loss: 0.7835009694099426, acc: 0.03125] [gan loss: 1.416960, acc: 0.000000]\n",
            "8242: [discriminator loss: 0.7179264426231384, acc: 0.4609375] [gan loss: 0.755769, acc: 0.406250]\n",
            "8243: [discriminator loss: 0.7476953268051147, acc: 0.0390625] [gan loss: 1.334438, acc: 0.000000]\n",
            "8244: [discriminator loss: 0.7044819593429565, acc: 0.40625] [gan loss: 0.759224, acc: 0.328125]\n",
            "8245: [discriminator loss: 0.769290566444397, acc: 0.03125] [gan loss: 1.346770, acc: 0.015625]\n",
            "8246: [discriminator loss: 0.6668299436569214, acc: 0.453125] [gan loss: 0.763261, acc: 0.343750]\n",
            "8247: [discriminator loss: 0.7129335403442383, acc: 0.1015625] [gan loss: 1.417315, acc: 0.015625]\n",
            "8248: [discriminator loss: 0.6909655332565308, acc: 0.3828125] [gan loss: 0.868704, acc: 0.156250]\n",
            "8249: [discriminator loss: 0.7408478856086731, acc: 0.1328125] [gan loss: 1.285828, acc: 0.015625]\n",
            "8250: [discriminator loss: 0.6985781192779541, acc: 0.375] [gan loss: 0.937459, acc: 0.125000]\n",
            "8251: [discriminator loss: 0.6891640424728394, acc: 0.15625] [gan loss: 1.323203, acc: 0.000000]\n",
            "8252: [discriminator loss: 0.7373533844947815, acc: 0.3046875] [gan loss: 1.030444, acc: 0.015625]\n",
            "8253: [discriminator loss: 0.6890681385993958, acc: 0.234375] [gan loss: 1.159059, acc: 0.000000]\n",
            "8254: [discriminator loss: 0.7045601606369019, acc: 0.2421875] [gan loss: 1.075355, acc: 0.015625]\n",
            "8255: [discriminator loss: 0.7308551073074341, acc: 0.171875] [gan loss: 1.176437, acc: 0.015625]\n",
            "8256: [discriminator loss: 0.6843360066413879, acc: 0.3359375] [gan loss: 1.074332, acc: 0.046875]\n",
            "8257: [discriminator loss: 0.7214246988296509, acc: 0.1875] [gan loss: 1.113527, acc: 0.000000]\n",
            "8258: [discriminator loss: 0.6996521949768066, acc: 0.25] [gan loss: 1.025766, acc: 0.093750]\n",
            "8259: [discriminator loss: 0.7177377343177795, acc: 0.1796875] [gan loss: 1.362414, acc: 0.000000]\n",
            "8260: [discriminator loss: 0.6945245265960693, acc: 0.421875] [gan loss: 0.641226, acc: 0.593750]\n",
            "8261: [discriminator loss: 0.8003402948379517, acc: 0.03125] [gan loss: 1.792834, acc: 0.000000]\n",
            "8262: [discriminator loss: 0.7286545038223267, acc: 0.5] [gan loss: 0.433285, acc: 0.937500]\n",
            "8263: [discriminator loss: 0.867510974407196, acc: 0.0] [gan loss: 1.708876, acc: 0.000000]\n",
            "8264: [discriminator loss: 0.7171589136123657, acc: 0.484375] [gan loss: 0.602682, acc: 0.781250]\n",
            "8265: [discriminator loss: 0.8046709299087524, acc: 0.015625] [gan loss: 1.298795, acc: 0.000000]\n",
            "8266: [discriminator loss: 0.6964571475982666, acc: 0.421875] [gan loss: 0.820706, acc: 0.203125]\n",
            "8267: [discriminator loss: 0.7348721027374268, acc: 0.1015625] [gan loss: 1.118017, acc: 0.015625]\n",
            "8268: [discriminator loss: 0.7196703553199768, acc: 0.2265625] [gan loss: 0.947538, acc: 0.109375]\n",
            "8269: [discriminator loss: 0.7282456159591675, acc: 0.109375] [gan loss: 1.202491, acc: 0.015625]\n",
            "8270: [discriminator loss: 0.7015529870986938, acc: 0.328125] [gan loss: 1.013517, acc: 0.000000]\n",
            "8271: [discriminator loss: 0.6943154335021973, acc: 0.171875] [gan loss: 1.041631, acc: 0.000000]\n",
            "8272: [discriminator loss: 0.6964161396026611, acc: 0.2578125] [gan loss: 1.085873, acc: 0.031250]\n",
            "8273: [discriminator loss: 0.7153842449188232, acc: 0.1484375] [gan loss: 1.153836, acc: 0.000000]\n",
            "8274: [discriminator loss: 0.7079954147338867, acc: 0.3046875] [gan loss: 0.920736, acc: 0.156250]\n",
            "8275: [discriminator loss: 0.7444367408752441, acc: 0.109375] [gan loss: 1.358427, acc: 0.000000]\n",
            "8276: [discriminator loss: 0.7086237668991089, acc: 0.453125] [gan loss: 0.732416, acc: 0.390625]\n",
            "8277: [discriminator loss: 0.7728204727172852, acc: 0.015625] [gan loss: 1.657409, acc: 0.000000]\n",
            "8278: [discriminator loss: 0.6976321339607239, acc: 0.4765625] [gan loss: 0.640831, acc: 0.671875]\n",
            "8279: [discriminator loss: 0.7943860292434692, acc: 0.0] [gan loss: 1.454882, acc: 0.000000]\n",
            "8280: [discriminator loss: 0.691777765750885, acc: 0.4609375] [gan loss: 0.591800, acc: 0.812500]\n",
            "8281: [discriminator loss: 0.792609453201294, acc: 0.0] [gan loss: 1.403042, acc: 0.000000]\n",
            "8282: [discriminator loss: 0.7134664058685303, acc: 0.375] [gan loss: 0.793370, acc: 0.265625]\n",
            "8283: [discriminator loss: 0.7385956048965454, acc: 0.0703125] [gan loss: 1.267583, acc: 0.000000]\n",
            "8284: [discriminator loss: 0.6849572062492371, acc: 0.4140625] [gan loss: 0.735059, acc: 0.453125]\n",
            "8285: [discriminator loss: 0.7480860948562622, acc: 0.046875] [gan loss: 1.382490, acc: 0.000000]\n",
            "8286: [discriminator loss: 0.715304970741272, acc: 0.421875] [gan loss: 0.755490, acc: 0.421875]\n",
            "8287: [discriminator loss: 0.7443084716796875, acc: 0.078125] [gan loss: 1.247383, acc: 0.000000]\n",
            "8288: [discriminator loss: 0.7114443778991699, acc: 0.3515625] [gan loss: 0.800732, acc: 0.218750]\n",
            "8289: [discriminator loss: 0.7447434663772583, acc: 0.09375] [gan loss: 1.443614, acc: 0.000000]\n",
            "8290: [discriminator loss: 0.7256476283073425, acc: 0.4375] [gan loss: 0.835985, acc: 0.234375]\n",
            "8291: [discriminator loss: 0.7434483766555786, acc: 0.0390625] [gan loss: 1.479147, acc: 0.000000]\n",
            "8292: [discriminator loss: 0.7208499908447266, acc: 0.3984375] [gan loss: 0.737045, acc: 0.437500]\n",
            "8293: [discriminator loss: 0.7553767561912537, acc: 0.046875] [gan loss: 1.366578, acc: 0.015625]\n",
            "8294: [discriminator loss: 0.715564489364624, acc: 0.3828125] [gan loss: 0.801655, acc: 0.281250]\n",
            "8295: [discriminator loss: 0.7602790594100952, acc: 0.0625] [gan loss: 1.459852, acc: 0.000000]\n",
            "8296: [discriminator loss: 0.692756175994873, acc: 0.46875] [gan loss: 0.791314, acc: 0.281250]\n",
            "8297: [discriminator loss: 0.7627520561218262, acc: 0.015625] [gan loss: 1.505821, acc: 0.000000]\n",
            "8298: [discriminator loss: 0.6873920559883118, acc: 0.4609375] [gan loss: 0.793636, acc: 0.250000]\n",
            "8299: [discriminator loss: 0.7806214690208435, acc: 0.0234375] [gan loss: 1.469093, acc: 0.000000]\n",
            "8300: [discriminator loss: 0.6827196478843689, acc: 0.4765625] [gan loss: 0.686089, acc: 0.515625]\n",
            "8301: [discriminator loss: 0.7719106674194336, acc: 0.0078125] [gan loss: 1.361539, acc: 0.000000]\n",
            "8302: [discriminator loss: 0.6810721158981323, acc: 0.4375] [gan loss: 0.774050, acc: 0.343750]\n",
            "8303: [discriminator loss: 0.7634499073028564, acc: 0.078125] [gan loss: 1.405834, acc: 0.000000]\n",
            "8304: [discriminator loss: 0.7173991203308105, acc: 0.3984375] [gan loss: 0.699548, acc: 0.484375]\n",
            "8305: [discriminator loss: 0.7876569032669067, acc: 0.03125] [gan loss: 1.505009, acc: 0.000000]\n",
            "8306: [discriminator loss: 0.6935142278671265, acc: 0.4765625] [gan loss: 0.675242, acc: 0.562500]\n",
            "8307: [discriminator loss: 0.7689327001571655, acc: 0.0078125] [gan loss: 1.480252, acc: 0.000000]\n",
            "8308: [discriminator loss: 0.693766176700592, acc: 0.453125] [gan loss: 0.800888, acc: 0.296875]\n",
            "8309: [discriminator loss: 0.7552621364593506, acc: 0.046875] [gan loss: 1.255638, acc: 0.000000]\n",
            "8310: [discriminator loss: 0.6827969551086426, acc: 0.3984375] [gan loss: 0.909328, acc: 0.109375]\n",
            "8311: [discriminator loss: 0.7510212063789368, acc: 0.109375] [gan loss: 1.152660, acc: 0.000000]\n",
            "8312: [discriminator loss: 0.706276535987854, acc: 0.3515625] [gan loss: 0.868092, acc: 0.109375]\n",
            "8313: [discriminator loss: 0.7237672805786133, acc: 0.0546875] [gan loss: 1.235532, acc: 0.000000]\n",
            "8314: [discriminator loss: 0.697690486907959, acc: 0.390625] [gan loss: 0.850355, acc: 0.203125]\n",
            "8315: [discriminator loss: 0.732083797454834, acc: 0.0703125] [gan loss: 1.215222, acc: 0.000000]\n",
            "8316: [discriminator loss: 0.7016157507896423, acc: 0.375] [gan loss: 0.754198, acc: 0.359375]\n",
            "8317: [discriminator loss: 0.7385386228561401, acc: 0.0546875] [gan loss: 1.375219, acc: 0.000000]\n",
            "8318: [discriminator loss: 0.6832064986228943, acc: 0.421875] [gan loss: 0.804623, acc: 0.343750]\n",
            "8319: [discriminator loss: 0.7473641633987427, acc: 0.0546875] [gan loss: 1.350660, acc: 0.000000]\n",
            "8320: [discriminator loss: 0.6941976547241211, acc: 0.390625] [gan loss: 0.772762, acc: 0.328125]\n",
            "8321: [discriminator loss: 0.7315518260002136, acc: 0.0859375] [gan loss: 1.276049, acc: 0.000000]\n",
            "8322: [discriminator loss: 0.6932157278060913, acc: 0.4140625] [gan loss: 0.752352, acc: 0.453125]\n",
            "8323: [discriminator loss: 0.7279735207557678, acc: 0.1171875] [gan loss: 1.395600, acc: 0.000000]\n",
            "8324: [discriminator loss: 0.687238335609436, acc: 0.453125] [gan loss: 0.745276, acc: 0.437500]\n",
            "8325: [discriminator loss: 0.7859840393066406, acc: 0.03125] [gan loss: 1.707696, acc: 0.000000]\n",
            "8326: [discriminator loss: 0.6847033500671387, acc: 0.4765625] [gan loss: 0.640370, acc: 0.625000]\n",
            "8327: [discriminator loss: 0.7630465626716614, acc: 0.0234375] [gan loss: 1.466814, acc: 0.000000]\n",
            "8328: [discriminator loss: 0.6950601935386658, acc: 0.46875] [gan loss: 0.667739, acc: 0.546875]\n",
            "8329: [discriminator loss: 0.7691702842712402, acc: 0.0234375] [gan loss: 1.485293, acc: 0.000000]\n",
            "8330: [discriminator loss: 0.6927297711372375, acc: 0.4375] [gan loss: 0.731417, acc: 0.421875]\n",
            "8331: [discriminator loss: 0.7327476739883423, acc: 0.0703125] [gan loss: 1.336098, acc: 0.000000]\n",
            "8332: [discriminator loss: 0.6703733205795288, acc: 0.4296875] [gan loss: 0.771038, acc: 0.437500]\n",
            "8333: [discriminator loss: 0.7838600873947144, acc: 0.0859375] [gan loss: 1.291626, acc: 0.000000]\n",
            "8334: [discriminator loss: 0.7064458131790161, acc: 0.3359375] [gan loss: 0.909761, acc: 0.156250]\n",
            "8335: [discriminator loss: 0.7107710242271423, acc: 0.125] [gan loss: 1.231255, acc: 0.000000]\n",
            "8336: [discriminator loss: 0.6777979731559753, acc: 0.3515625] [gan loss: 0.886225, acc: 0.125000]\n",
            "8337: [discriminator loss: 0.7464202642440796, acc: 0.125] [gan loss: 1.311804, acc: 0.000000]\n",
            "8338: [discriminator loss: 0.6890568733215332, acc: 0.421875] [gan loss: 0.727699, acc: 0.468750]\n",
            "8339: [discriminator loss: 0.7812409996986389, acc: 0.046875] [gan loss: 1.535254, acc: 0.000000]\n",
            "8340: [discriminator loss: 0.7236489653587341, acc: 0.4609375] [gan loss: 0.690859, acc: 0.500000]\n",
            "8341: [discriminator loss: 0.7864376902580261, acc: 0.0078125] [gan loss: 1.533740, acc: 0.000000]\n",
            "8342: [discriminator loss: 0.6963530778884888, acc: 0.4453125] [gan loss: 0.729592, acc: 0.421875]\n",
            "8343: [discriminator loss: 0.7628923654556274, acc: 0.0078125] [gan loss: 1.431086, acc: 0.000000]\n",
            "8344: [discriminator loss: 0.6987991333007812, acc: 0.421875] [gan loss: 0.798274, acc: 0.406250]\n",
            "8345: [discriminator loss: 0.754708468914032, acc: 0.0625] [gan loss: 1.265948, acc: 0.000000]\n",
            "8346: [discriminator loss: 0.7275996208190918, acc: 0.3515625] [gan loss: 0.786083, acc: 0.312500]\n",
            "8347: [discriminator loss: 0.7499541640281677, acc: 0.078125] [gan loss: 1.294998, acc: 0.000000]\n",
            "8348: [discriminator loss: 0.6954220533370972, acc: 0.3671875] [gan loss: 0.830468, acc: 0.218750]\n",
            "8349: [discriminator loss: 0.737822413444519, acc: 0.09375] [gan loss: 1.537813, acc: 0.000000]\n",
            "8350: [discriminator loss: 0.6892719268798828, acc: 0.4375] [gan loss: 0.760947, acc: 0.312500]\n",
            "8351: [discriminator loss: 0.7463937401771545, acc: 0.046875] [gan loss: 1.435205, acc: 0.000000]\n",
            "8352: [discriminator loss: 0.6845754384994507, acc: 0.453125] [gan loss: 0.653470, acc: 0.640625]\n",
            "8353: [discriminator loss: 0.8059722781181335, acc: 0.046875] [gan loss: 1.706449, acc: 0.000000]\n",
            "8354: [discriminator loss: 0.7180537581443787, acc: 0.484375] [gan loss: 0.607359, acc: 0.781250]\n",
            "8355: [discriminator loss: 0.7981852889060974, acc: 0.015625] [gan loss: 1.410169, acc: 0.000000]\n",
            "8356: [discriminator loss: 0.7173540592193604, acc: 0.4296875] [gan loss: 0.796834, acc: 0.171875]\n",
            "8357: [discriminator loss: 0.7435474991798401, acc: 0.046875] [gan loss: 1.287169, acc: 0.000000]\n",
            "8358: [discriminator loss: 0.7039345502853394, acc: 0.3828125] [gan loss: 0.767180, acc: 0.328125]\n",
            "8359: [discriminator loss: 0.7514470815658569, acc: 0.0546875] [gan loss: 1.241541, acc: 0.000000]\n",
            "8360: [discriminator loss: 0.6923835873603821, acc: 0.3671875] [gan loss: 0.852746, acc: 0.156250]\n",
            "8361: [discriminator loss: 0.7216587066650391, acc: 0.1484375] [gan loss: 1.265630, acc: 0.000000]\n",
            "8362: [discriminator loss: 0.6859557032585144, acc: 0.3671875] [gan loss: 0.907783, acc: 0.109375]\n",
            "8363: [discriminator loss: 0.7151007652282715, acc: 0.1328125] [gan loss: 1.209089, acc: 0.000000]\n",
            "8364: [discriminator loss: 0.6657874584197998, acc: 0.359375] [gan loss: 0.944830, acc: 0.125000]\n",
            "8365: [discriminator loss: 0.7331609725952148, acc: 0.1484375] [gan loss: 1.256098, acc: 0.000000]\n",
            "8366: [discriminator loss: 0.7126122713088989, acc: 0.3359375] [gan loss: 0.906201, acc: 0.109375]\n",
            "8367: [discriminator loss: 0.7226608991622925, acc: 0.0859375] [gan loss: 1.454047, acc: 0.000000]\n",
            "8368: [discriminator loss: 0.7116061449050903, acc: 0.453125] [gan loss: 0.736395, acc: 0.375000]\n",
            "8369: [discriminator loss: 0.7894493341445923, acc: 0.0234375] [gan loss: 1.679617, acc: 0.000000]\n",
            "8370: [discriminator loss: 0.7207425236701965, acc: 0.4921875] [gan loss: 0.585806, acc: 0.750000]\n",
            "8371: [discriminator loss: 0.8250595331192017, acc: 0.0078125] [gan loss: 1.620348, acc: 0.000000]\n",
            "8372: [discriminator loss: 0.7156278491020203, acc: 0.4921875] [gan loss: 0.651980, acc: 0.593750]\n",
            "8373: [discriminator loss: 0.791283130645752, acc: 0.0078125] [gan loss: 1.512632, acc: 0.000000]\n",
            "8374: [discriminator loss: 0.7137171626091003, acc: 0.4765625] [gan loss: 0.677084, acc: 0.609375]\n",
            "8375: [discriminator loss: 0.7511081695556641, acc: 0.0390625] [gan loss: 1.267954, acc: 0.000000]\n",
            "8376: [discriminator loss: 0.698104202747345, acc: 0.359375] [gan loss: 0.858767, acc: 0.156250]\n",
            "8377: [discriminator loss: 0.7459730505943298, acc: 0.0859375] [gan loss: 1.306587, acc: 0.000000]\n",
            "8378: [discriminator loss: 0.6901808977127075, acc: 0.4140625] [gan loss: 0.828333, acc: 0.187500]\n",
            "8379: [discriminator loss: 0.7298323512077332, acc: 0.078125] [gan loss: 1.148589, acc: 0.000000]\n",
            "8380: [discriminator loss: 0.707312285900116, acc: 0.328125] [gan loss: 0.896162, acc: 0.093750]\n",
            "8381: [discriminator loss: 0.7256625890731812, acc: 0.078125] [gan loss: 1.178857, acc: 0.000000]\n",
            "8382: [discriminator loss: 0.7086551189422607, acc: 0.3125] [gan loss: 0.927943, acc: 0.031250]\n",
            "8383: [discriminator loss: 0.7120280861854553, acc: 0.078125] [gan loss: 1.267100, acc: 0.000000]\n",
            "8384: [discriminator loss: 0.7149227857589722, acc: 0.34375] [gan loss: 0.864641, acc: 0.140625]\n",
            "8385: [discriminator loss: 0.7119475603103638, acc: 0.1015625] [gan loss: 1.367815, acc: 0.000000]\n",
            "8386: [discriminator loss: 0.7000085115432739, acc: 0.4296875] [gan loss: 0.764784, acc: 0.359375]\n",
            "8387: [discriminator loss: 0.7533133029937744, acc: 0.0390625] [gan loss: 1.471112, acc: 0.000000]\n",
            "8388: [discriminator loss: 0.6715079545974731, acc: 0.4609375] [gan loss: 0.740479, acc: 0.359375]\n",
            "8389: [discriminator loss: 0.7653833031654358, acc: 0.015625] [gan loss: 1.535401, acc: 0.000000]\n",
            "8390: [discriminator loss: 0.7200427055358887, acc: 0.484375] [gan loss: 0.619283, acc: 0.750000]\n",
            "8391: [discriminator loss: 0.7835833430290222, acc: 0.015625] [gan loss: 1.495999, acc: 0.000000]\n",
            "8392: [discriminator loss: 0.7109026908874512, acc: 0.46875] [gan loss: 0.660260, acc: 0.640625]\n",
            "8393: [discriminator loss: 0.7496805787086487, acc: 0.0390625] [gan loss: 1.399243, acc: 0.000000]\n",
            "8394: [discriminator loss: 0.709721565246582, acc: 0.453125] [gan loss: 0.709436, acc: 0.500000]\n",
            "8395: [discriminator loss: 0.749580442905426, acc: 0.03125] [gan loss: 1.318972, acc: 0.000000]\n",
            "8396: [discriminator loss: 0.6810486316680908, acc: 0.4609375] [gan loss: 0.760554, acc: 0.390625]\n",
            "8397: [discriminator loss: 0.7127755880355835, acc: 0.09375] [gan loss: 1.113574, acc: 0.046875]\n",
            "8398: [discriminator loss: 0.7280856370925903, acc: 0.2578125] [gan loss: 0.863725, acc: 0.156250]\n",
            "8399: [discriminator loss: 0.7366490364074707, acc: 0.109375] [gan loss: 1.367248, acc: 0.000000]\n",
            "8400: [discriminator loss: 0.7118041515350342, acc: 0.4296875] [gan loss: 0.786421, acc: 0.234375]\n",
            "8401: [discriminator loss: 0.7361605167388916, acc: 0.0703125] [gan loss: 1.333768, acc: 0.000000]\n",
            "8402: [discriminator loss: 0.6895180940628052, acc: 0.4375] [gan loss: 0.758521, acc: 0.375000]\n",
            "8403: [discriminator loss: 0.7788292765617371, acc: 0.0234375] [gan loss: 1.418722, acc: 0.000000]\n",
            "8404: [discriminator loss: 0.7066086530685425, acc: 0.4375] [gan loss: 0.724248, acc: 0.468750]\n",
            "8405: [discriminator loss: 0.7460443377494812, acc: 0.0546875] [gan loss: 1.352833, acc: 0.000000]\n",
            "8406: [discriminator loss: 0.6921051740646362, acc: 0.4375] [gan loss: 0.805538, acc: 0.343750]\n",
            "8407: [discriminator loss: 0.7158931493759155, acc: 0.1484375] [gan loss: 1.250793, acc: 0.000000]\n",
            "8408: [discriminator loss: 0.7037714719772339, acc: 0.3515625] [gan loss: 0.785159, acc: 0.296875]\n",
            "8409: [discriminator loss: 0.7502843141555786, acc: 0.0859375] [gan loss: 1.430803, acc: 0.000000]\n",
            "8410: [discriminator loss: 0.6958448886871338, acc: 0.4609375] [gan loss: 0.785031, acc: 0.296875]\n",
            "8411: [discriminator loss: 0.7352873086929321, acc: 0.0234375] [gan loss: 1.445810, acc: 0.000000]\n",
            "8412: [discriminator loss: 0.6877524852752686, acc: 0.4609375] [gan loss: 0.727112, acc: 0.390625]\n",
            "8413: [discriminator loss: 0.7540675401687622, acc: 0.0390625] [gan loss: 1.519342, acc: 0.000000]\n",
            "8414: [discriminator loss: 0.6770921945571899, acc: 0.484375] [gan loss: 0.654662, acc: 0.609375]\n",
            "8415: [discriminator loss: 0.7575096487998962, acc: 0.0078125] [gan loss: 1.361769, acc: 0.000000]\n",
            "8416: [discriminator loss: 0.728542149066925, acc: 0.4296875] [gan loss: 0.708102, acc: 0.406250]\n",
            "8417: [discriminator loss: 0.7367843389511108, acc: 0.0703125] [gan loss: 1.307441, acc: 0.000000]\n",
            "8418: [discriminator loss: 0.6794300079345703, acc: 0.4140625] [gan loss: 0.792611, acc: 0.296875]\n",
            "8419: [discriminator loss: 0.7587831616401672, acc: 0.0625] [gan loss: 1.342284, acc: 0.000000]\n",
            "8420: [discriminator loss: 0.6996008157730103, acc: 0.4296875] [gan loss: 0.788332, acc: 0.328125]\n",
            "8421: [discriminator loss: 0.7579923868179321, acc: 0.046875] [gan loss: 1.354557, acc: 0.000000]\n",
            "8422: [discriminator loss: 0.7043695449829102, acc: 0.4296875] [gan loss: 0.727999, acc: 0.421875]\n",
            "8423: [discriminator loss: 0.7639394998550415, acc: 0.015625] [gan loss: 1.436059, acc: 0.000000]\n",
            "8424: [discriminator loss: 0.6852366328239441, acc: 0.453125] [gan loss: 0.788762, acc: 0.375000]\n",
            "8425: [discriminator loss: 0.767512321472168, acc: 0.015625] [gan loss: 1.319618, acc: 0.000000]\n",
            "8426: [discriminator loss: 0.7398667931556702, acc: 0.3359375] [gan loss: 0.871516, acc: 0.125000]\n",
            "8427: [discriminator loss: 0.7406906485557556, acc: 0.1015625] [gan loss: 1.276671, acc: 0.000000]\n",
            "8428: [discriminator loss: 0.7312554717063904, acc: 0.3515625] [gan loss: 0.764974, acc: 0.359375]\n",
            "8429: [discriminator loss: 0.7212056517601013, acc: 0.03125] [gan loss: 1.299607, acc: 0.000000]\n",
            "8430: [discriminator loss: 0.6845446825027466, acc: 0.40625] [gan loss: 0.808815, acc: 0.171875]\n",
            "8431: [discriminator loss: 0.7373444437980652, acc: 0.0625] [gan loss: 1.256765, acc: 0.000000]\n",
            "8432: [discriminator loss: 0.6866804361343384, acc: 0.40625] [gan loss: 0.852158, acc: 0.140625]\n",
            "8433: [discriminator loss: 0.7402645349502563, acc: 0.140625] [gan loss: 1.365687, acc: 0.000000]\n",
            "8434: [discriminator loss: 0.7052364349365234, acc: 0.3359375] [gan loss: 0.928616, acc: 0.187500]\n",
            "8435: [discriminator loss: 0.7105608582496643, acc: 0.171875] [gan loss: 1.300307, acc: 0.000000]\n",
            "8436: [discriminator loss: 0.6928668022155762, acc: 0.3828125] [gan loss: 0.897763, acc: 0.203125]\n",
            "8437: [discriminator loss: 0.7582454681396484, acc: 0.046875] [gan loss: 1.481064, acc: 0.000000]\n",
            "8438: [discriminator loss: 0.68570876121521, acc: 0.484375] [gan loss: 0.710984, acc: 0.406250]\n",
            "8439: [discriminator loss: 0.8115885853767395, acc: 0.0078125] [gan loss: 1.563136, acc: 0.000000]\n",
            "8440: [discriminator loss: 0.7052292823791504, acc: 0.484375] [gan loss: 0.625473, acc: 0.671875]\n",
            "8441: [discriminator loss: 0.7531033754348755, acc: 0.0234375] [gan loss: 1.343277, acc: 0.000000]\n",
            "8442: [discriminator loss: 0.7116343975067139, acc: 0.4375] [gan loss: 0.754308, acc: 0.343750]\n",
            "8443: [discriminator loss: 0.7554815411567688, acc: 0.03125] [gan loss: 1.392209, acc: 0.000000]\n",
            "8444: [discriminator loss: 0.71772301197052, acc: 0.375] [gan loss: 0.852110, acc: 0.203125]\n",
            "8445: [discriminator loss: 0.7365849018096924, acc: 0.1015625] [gan loss: 1.295310, acc: 0.000000]\n",
            "8446: [discriminator loss: 0.7055001258850098, acc: 0.375] [gan loss: 0.908720, acc: 0.093750]\n",
            "8447: [discriminator loss: 0.7330532073974609, acc: 0.1328125] [gan loss: 1.355469, acc: 0.000000]\n",
            "8448: [discriminator loss: 0.7013373970985413, acc: 0.359375] [gan loss: 0.896065, acc: 0.078125]\n",
            "8449: [discriminator loss: 0.7054637670516968, acc: 0.1171875] [gan loss: 1.257657, acc: 0.000000]\n",
            "8450: [discriminator loss: 0.7039722204208374, acc: 0.359375] [gan loss: 0.907540, acc: 0.093750]\n",
            "8451: [discriminator loss: 0.7228014469146729, acc: 0.1015625] [gan loss: 1.371596, acc: 0.000000]\n",
            "8452: [discriminator loss: 0.6948935389518738, acc: 0.4453125] [gan loss: 0.732643, acc: 0.406250]\n",
            "8453: [discriminator loss: 0.7632994055747986, acc: 0.015625] [gan loss: 1.465361, acc: 0.000000]\n",
            "8454: [discriminator loss: 0.7131175994873047, acc: 0.4765625] [gan loss: 0.670716, acc: 0.593750]\n",
            "8455: [discriminator loss: 0.7811784148216248, acc: 0.03125] [gan loss: 1.426515, acc: 0.000000]\n",
            "8456: [discriminator loss: 0.6935027837753296, acc: 0.4609375] [gan loss: 0.649141, acc: 0.687500]\n",
            "8457: [discriminator loss: 0.7608672380447388, acc: 0.03125] [gan loss: 1.370764, acc: 0.000000]\n",
            "8458: [discriminator loss: 0.6954729557037354, acc: 0.40625] [gan loss: 0.801658, acc: 0.343750]\n",
            "8459: [discriminator loss: 0.7205859422683716, acc: 0.109375] [gan loss: 1.338752, acc: 0.000000]\n",
            "8460: [discriminator loss: 0.6981542110443115, acc: 0.3828125] [gan loss: 0.750780, acc: 0.328125]\n",
            "8461: [discriminator loss: 0.7468945980072021, acc: 0.046875] [gan loss: 1.332140, acc: 0.000000]\n",
            "8462: [discriminator loss: 0.6730009317398071, acc: 0.4296875] [gan loss: 0.806963, acc: 0.265625]\n",
            "8463: [discriminator loss: 0.7362222671508789, acc: 0.078125] [gan loss: 1.374765, acc: 0.000000]\n",
            "8464: [discriminator loss: 0.6893938779830933, acc: 0.4609375] [gan loss: 0.717499, acc: 0.562500]\n",
            "8465: [discriminator loss: 0.7320274710655212, acc: 0.0625] [gan loss: 1.426056, acc: 0.000000]\n",
            "8466: [discriminator loss: 0.702537477016449, acc: 0.4140625] [gan loss: 0.758040, acc: 0.421875]\n",
            "8467: [discriminator loss: 0.7636374831199646, acc: 0.0546875] [gan loss: 1.351484, acc: 0.000000]\n",
            "8468: [discriminator loss: 0.6928813457489014, acc: 0.4140625] [gan loss: 0.790989, acc: 0.328125]\n",
            "8469: [discriminator loss: 0.7267395853996277, acc: 0.078125] [gan loss: 1.375111, acc: 0.000000]\n",
            "8470: [discriminator loss: 0.6831587553024292, acc: 0.40625] [gan loss: 0.823679, acc: 0.234375]\n",
            "8471: [discriminator loss: 0.7600023746490479, acc: 0.03125] [gan loss: 1.300179, acc: 0.000000]\n",
            "8472: [discriminator loss: 0.7160928845405579, acc: 0.4140625] [gan loss: 0.649293, acc: 0.687500]\n",
            "8473: [discriminator loss: 0.7678436636924744, acc: 0.0234375] [gan loss: 1.373137, acc: 0.000000]\n",
            "8474: [discriminator loss: 0.7003146409988403, acc: 0.4453125] [gan loss: 0.716444, acc: 0.531250]\n",
            "8475: [discriminator loss: 0.7420657277107239, acc: 0.0390625] [gan loss: 1.446440, acc: 0.000000]\n",
            "8476: [discriminator loss: 0.7188628911972046, acc: 0.4296875] [gan loss: 0.808014, acc: 0.234375]\n",
            "8477: [discriminator loss: 0.7218981981277466, acc: 0.078125] [gan loss: 1.359140, acc: 0.015625]\n",
            "8478: [discriminator loss: 0.7304494976997375, acc: 0.359375] [gan loss: 0.735077, acc: 0.375000]\n",
            "8479: [discriminator loss: 0.7544139623641968, acc: 0.046875] [gan loss: 1.153741, acc: 0.000000]\n",
            "8480: [discriminator loss: 0.7186775207519531, acc: 0.28125] [gan loss: 0.936783, acc: 0.015625]\n",
            "8481: [discriminator loss: 0.7115142345428467, acc: 0.140625] [gan loss: 1.294312, acc: 0.000000]\n",
            "8482: [discriminator loss: 0.7311611771583557, acc: 0.2890625] [gan loss: 0.934326, acc: 0.046875]\n",
            "8483: [discriminator loss: 0.7053030729293823, acc: 0.140625] [gan loss: 1.114054, acc: 0.000000]\n",
            "8484: [discriminator loss: 0.6951788663864136, acc: 0.3125] [gan loss: 1.029982, acc: 0.031250]\n",
            "8485: [discriminator loss: 0.7102088928222656, acc: 0.1640625] [gan loss: 1.258928, acc: 0.000000]\n",
            "8486: [discriminator loss: 0.7100245952606201, acc: 0.28125] [gan loss: 1.181602, acc: 0.000000]\n",
            "8487: [discriminator loss: 0.6858518719673157, acc: 0.2734375] [gan loss: 1.015470, acc: 0.046875]\n",
            "8488: [discriminator loss: 0.7297569513320923, acc: 0.1484375] [gan loss: 1.220890, acc: 0.000000]\n",
            "8489: [discriminator loss: 0.661278486251831, acc: 0.4453125] [gan loss: 0.827349, acc: 0.234375]\n",
            "8490: [discriminator loss: 0.7278341054916382, acc: 0.0546875] [gan loss: 1.516766, acc: 0.000000]\n",
            "8491: [discriminator loss: 0.7167490720748901, acc: 0.4765625] [gan loss: 0.563847, acc: 0.750000]\n",
            "8492: [discriminator loss: 0.7944572567939758, acc: 0.03125] [gan loss: 1.656474, acc: 0.000000]\n",
            "8493: [discriminator loss: 0.7230656743049622, acc: 0.4921875] [gan loss: 0.568806, acc: 0.781250]\n",
            "8494: [discriminator loss: 0.8451504707336426, acc: 0.0] [gan loss: 1.603034, acc: 0.000000]\n",
            "8495: [discriminator loss: 0.7263249158859253, acc: 0.453125] [gan loss: 0.711775, acc: 0.484375]\n",
            "8496: [discriminator loss: 0.7659682035446167, acc: 0.046875] [gan loss: 1.351137, acc: 0.000000]\n",
            "8497: [discriminator loss: 0.6973412036895752, acc: 0.3984375] [gan loss: 0.893956, acc: 0.156250]\n",
            "8498: [discriminator loss: 0.7161785364151001, acc: 0.109375] [gan loss: 1.223585, acc: 0.000000]\n",
            "8499: [discriminator loss: 0.7119697332382202, acc: 0.3359375] [gan loss: 0.901331, acc: 0.125000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "8500: [discriminator loss: 0.7324008345603943, acc: 0.078125] [gan loss: 1.333454, acc: 0.000000]\n",
            "8501: [discriminator loss: 0.6844260096549988, acc: 0.40625] [gan loss: 0.879660, acc: 0.171875]\n",
            "8502: [discriminator loss: 0.7354347705841064, acc: 0.1171875] [gan loss: 1.186435, acc: 0.046875]\n",
            "8503: [discriminator loss: 0.7014634013175964, acc: 0.3203125] [gan loss: 0.856726, acc: 0.218750]\n",
            "8504: [discriminator loss: 0.7493562698364258, acc: 0.078125] [gan loss: 1.317387, acc: 0.000000]\n",
            "8505: [discriminator loss: 0.6873564720153809, acc: 0.390625] [gan loss: 0.859358, acc: 0.281250]\n",
            "8506: [discriminator loss: 0.758186936378479, acc: 0.0859375] [gan loss: 1.395214, acc: 0.000000]\n",
            "8507: [discriminator loss: 0.7251521944999695, acc: 0.375] [gan loss: 0.850920, acc: 0.171875]\n",
            "8508: [discriminator loss: 0.7427476644515991, acc: 0.1015625] [gan loss: 1.379878, acc: 0.000000]\n",
            "8509: [discriminator loss: 0.711654007434845, acc: 0.4296875] [gan loss: 0.726443, acc: 0.437500]\n",
            "8510: [discriminator loss: 0.7565521001815796, acc: 0.0234375] [gan loss: 1.488150, acc: 0.000000]\n",
            "8511: [discriminator loss: 0.6933570504188538, acc: 0.4296875] [gan loss: 0.776756, acc: 0.281250]\n",
            "8512: [discriminator loss: 0.7581907510757446, acc: 0.03125] [gan loss: 1.466714, acc: 0.000000]\n",
            "8513: [discriminator loss: 0.7002661228179932, acc: 0.484375] [gan loss: 0.705002, acc: 0.531250]\n",
            "8514: [discriminator loss: 0.7709866762161255, acc: 0.0078125] [gan loss: 1.388832, acc: 0.000000]\n",
            "8515: [discriminator loss: 0.6856027841567993, acc: 0.4609375] [gan loss: 0.655084, acc: 0.671875]\n",
            "8516: [discriminator loss: 0.7671263813972473, acc: 0.0390625] [gan loss: 1.434524, acc: 0.000000]\n",
            "8517: [discriminator loss: 0.6932746171951294, acc: 0.46875] [gan loss: 0.758092, acc: 0.375000]\n",
            "8518: [discriminator loss: 0.7522454857826233, acc: 0.0390625] [gan loss: 1.346579, acc: 0.000000]\n",
            "8519: [discriminator loss: 0.67680424451828, acc: 0.4609375] [gan loss: 0.823826, acc: 0.250000]\n",
            "8520: [discriminator loss: 0.7828363180160522, acc: 0.0546875] [gan loss: 1.494124, acc: 0.000000]\n",
            "8521: [discriminator loss: 0.685962438583374, acc: 0.453125] [gan loss: 0.730610, acc: 0.468750]\n",
            "8522: [discriminator loss: 0.7611896395683289, acc: 0.0546875] [gan loss: 1.418916, acc: 0.000000]\n",
            "8523: [discriminator loss: 0.6999951004981995, acc: 0.46875] [gan loss: 0.732999, acc: 0.406250]\n",
            "8524: [discriminator loss: 0.7954806685447693, acc: 0.0390625] [gan loss: 1.418520, acc: 0.000000]\n",
            "8525: [discriminator loss: 0.6944851875305176, acc: 0.453125] [gan loss: 0.790285, acc: 0.328125]\n",
            "8526: [discriminator loss: 0.7567852139472961, acc: 0.0546875] [gan loss: 1.240006, acc: 0.000000]\n",
            "8527: [discriminator loss: 0.7030433416366577, acc: 0.359375] [gan loss: 0.820622, acc: 0.281250]\n",
            "8528: [discriminator loss: 0.750781774520874, acc: 0.0546875] [gan loss: 1.321010, acc: 0.000000]\n",
            "8529: [discriminator loss: 0.6871918439865112, acc: 0.375] [gan loss: 0.903623, acc: 0.171875]\n",
            "8530: [discriminator loss: 0.733431339263916, acc: 0.125] [gan loss: 1.258790, acc: 0.000000]\n",
            "8531: [discriminator loss: 0.7138196229934692, acc: 0.3828125] [gan loss: 0.804666, acc: 0.203125]\n",
            "8532: [discriminator loss: 0.7520643472671509, acc: 0.0703125] [gan loss: 1.413317, acc: 0.000000]\n",
            "8533: [discriminator loss: 0.6967751979827881, acc: 0.4375] [gan loss: 0.721578, acc: 0.500000]\n",
            "8534: [discriminator loss: 0.7493602633476257, acc: 0.0234375] [gan loss: 1.447520, acc: 0.000000]\n",
            "8535: [discriminator loss: 0.699760377407074, acc: 0.421875] [gan loss: 0.722166, acc: 0.515625]\n",
            "8536: [discriminator loss: 0.7875078916549683, acc: 0.015625] [gan loss: 1.388618, acc: 0.000000]\n",
            "8537: [discriminator loss: 0.695010781288147, acc: 0.3671875] [gan loss: 0.817019, acc: 0.312500]\n",
            "8538: [discriminator loss: 0.7643157839775085, acc: 0.0859375] [gan loss: 1.302483, acc: 0.000000]\n",
            "8539: [discriminator loss: 0.7112681865692139, acc: 0.34375] [gan loss: 0.854050, acc: 0.171875]\n",
            "8540: [discriminator loss: 0.7254267930984497, acc: 0.078125] [gan loss: 1.370856, acc: 0.000000]\n",
            "8541: [discriminator loss: 0.7021127939224243, acc: 0.421875] [gan loss: 0.786441, acc: 0.281250]\n",
            "8542: [discriminator loss: 0.7167069315910339, acc: 0.0625] [gan loss: 1.272738, acc: 0.000000]\n",
            "8543: [discriminator loss: 0.6808555722236633, acc: 0.4296875] [gan loss: 0.792081, acc: 0.343750]\n",
            "8544: [discriminator loss: 0.7426608204841614, acc: 0.046875] [gan loss: 1.499482, acc: 0.000000]\n",
            "8545: [discriminator loss: 0.6835637092590332, acc: 0.484375] [gan loss: 0.658078, acc: 0.671875]\n",
            "8546: [discriminator loss: 0.8056542873382568, acc: 0.0] [gan loss: 1.538979, acc: 0.000000]\n",
            "8547: [discriminator loss: 0.6834524273872375, acc: 0.4765625] [gan loss: 0.751029, acc: 0.421875]\n",
            "8548: [discriminator loss: 0.7587267160415649, acc: 0.0390625] [gan loss: 1.398865, acc: 0.000000]\n",
            "8549: [discriminator loss: 0.7179173231124878, acc: 0.4453125] [gan loss: 0.674726, acc: 0.500000]\n",
            "8550: [discriminator loss: 0.7384880781173706, acc: 0.0703125] [gan loss: 1.401451, acc: 0.000000]\n",
            "8551: [discriminator loss: 0.6975642442703247, acc: 0.453125] [gan loss: 0.697011, acc: 0.453125]\n",
            "8552: [discriminator loss: 0.7692856788635254, acc: 0.0546875] [gan loss: 1.248391, acc: 0.000000]\n",
            "8553: [discriminator loss: 0.7122390270233154, acc: 0.3984375] [gan loss: 0.828297, acc: 0.281250]\n",
            "8554: [discriminator loss: 0.7304112911224365, acc: 0.1015625] [gan loss: 1.150329, acc: 0.015625]\n",
            "8555: [discriminator loss: 0.6955893039703369, acc: 0.3203125] [gan loss: 0.945003, acc: 0.140625]\n",
            "8556: [discriminator loss: 0.7002670168876648, acc: 0.1484375] [gan loss: 1.094726, acc: 0.031250]\n",
            "8557: [discriminator loss: 0.7374017238616943, acc: 0.2578125] [gan loss: 1.144707, acc: 0.000000]\n",
            "8558: [discriminator loss: 0.736286997795105, acc: 0.25] [gan loss: 1.173661, acc: 0.000000]\n",
            "8559: [discriminator loss: 0.7003724575042725, acc: 0.25] [gan loss: 1.243150, acc: 0.000000]\n",
            "8560: [discriminator loss: 0.689704418182373, acc: 0.3046875] [gan loss: 0.952168, acc: 0.171875]\n",
            "8561: [discriminator loss: 0.7388230562210083, acc: 0.15625] [gan loss: 1.311578, acc: 0.000000]\n",
            "8562: [discriminator loss: 0.6925615072250366, acc: 0.3515625] [gan loss: 0.776701, acc: 0.265625]\n",
            "8563: [discriminator loss: 0.7474543452262878, acc: 0.0390625] [gan loss: 1.425621, acc: 0.000000]\n",
            "8564: [discriminator loss: 0.6979426145553589, acc: 0.453125] [gan loss: 0.683554, acc: 0.546875]\n",
            "8565: [discriminator loss: 0.7678650617599487, acc: 0.0703125] [gan loss: 1.462048, acc: 0.000000]\n",
            "8566: [discriminator loss: 0.6944363713264465, acc: 0.484375] [gan loss: 0.625025, acc: 0.656250]\n",
            "8567: [discriminator loss: 0.7844324707984924, acc: 0.0390625] [gan loss: 1.565170, acc: 0.000000]\n",
            "8568: [discriminator loss: 0.72203528881073, acc: 0.4609375] [gan loss: 0.675640, acc: 0.546875]\n",
            "8569: [discriminator loss: 0.7498776912689209, acc: 0.03125] [gan loss: 1.263814, acc: 0.000000]\n",
            "8570: [discriminator loss: 0.6927264332771301, acc: 0.4609375] [gan loss: 0.799121, acc: 0.203125]\n",
            "8571: [discriminator loss: 0.7539341449737549, acc: 0.0703125] [gan loss: 1.199451, acc: 0.000000]\n",
            "8572: [discriminator loss: 0.6902593374252319, acc: 0.375] [gan loss: 0.899387, acc: 0.140625]\n",
            "8573: [discriminator loss: 0.7202808856964111, acc: 0.078125] [gan loss: 1.140361, acc: 0.000000]\n",
            "8574: [discriminator loss: 0.6955161094665527, acc: 0.3203125] [gan loss: 0.966986, acc: 0.062500]\n",
            "8575: [discriminator loss: 0.7038854360580444, acc: 0.2109375] [gan loss: 1.232906, acc: 0.000000]\n",
            "8576: [discriminator loss: 0.7092561721801758, acc: 0.3203125] [gan loss: 0.985347, acc: 0.046875]\n",
            "8577: [discriminator loss: 0.701244592666626, acc: 0.21875] [gan loss: 1.163291, acc: 0.015625]\n",
            "8578: [discriminator loss: 0.7149076461791992, acc: 0.28125] [gan loss: 0.968515, acc: 0.062500]\n",
            "8579: [discriminator loss: 0.7209120988845825, acc: 0.140625] [gan loss: 1.352706, acc: 0.000000]\n",
            "8580: [discriminator loss: 0.709865391254425, acc: 0.390625] [gan loss: 0.881289, acc: 0.109375]\n",
            "8581: [discriminator loss: 0.713223934173584, acc: 0.1171875] [gan loss: 1.362153, acc: 0.000000]\n",
            "8582: [discriminator loss: 0.7094954252243042, acc: 0.4296875] [gan loss: 0.660838, acc: 0.640625]\n",
            "8583: [discriminator loss: 0.7582885026931763, acc: 0.0390625] [gan loss: 1.615284, acc: 0.000000]\n",
            "8584: [discriminator loss: 0.6972067952156067, acc: 0.4921875] [gan loss: 0.711343, acc: 0.453125]\n",
            "8585: [discriminator loss: 0.7647234201431274, acc: 0.0234375] [gan loss: 1.481226, acc: 0.000000]\n",
            "8586: [discriminator loss: 0.7128556966781616, acc: 0.40625] [gan loss: 0.653046, acc: 0.593750]\n",
            "8587: [discriminator loss: 0.7908597588539124, acc: 0.015625] [gan loss: 1.323321, acc: 0.000000]\n",
            "8588: [discriminator loss: 0.7241595983505249, acc: 0.4296875] [gan loss: 0.648892, acc: 0.562500]\n",
            "8589: [discriminator loss: 0.7560662031173706, acc: 0.03125] [gan loss: 1.326519, acc: 0.000000]\n",
            "8590: [discriminator loss: 0.6980006694793701, acc: 0.4375] [gan loss: 0.826344, acc: 0.281250]\n",
            "8591: [discriminator loss: 0.7506847381591797, acc: 0.078125] [gan loss: 1.318006, acc: 0.000000]\n",
            "8592: [discriminator loss: 0.6954753398895264, acc: 0.453125] [gan loss: 0.790509, acc: 0.265625]\n",
            "8593: [discriminator loss: 0.7161113619804382, acc: 0.0859375] [gan loss: 1.289715, acc: 0.000000]\n",
            "8594: [discriminator loss: 0.7048370242118835, acc: 0.390625] [gan loss: 0.822832, acc: 0.218750]\n",
            "8595: [discriminator loss: 0.734942615032196, acc: 0.0859375] [gan loss: 1.307966, acc: 0.000000]\n",
            "8596: [discriminator loss: 0.6737573146820068, acc: 0.40625] [gan loss: 0.788998, acc: 0.296875]\n",
            "8597: [discriminator loss: 0.7519181966781616, acc: 0.046875] [gan loss: 1.320505, acc: 0.000000]\n",
            "8598: [discriminator loss: 0.7007352709770203, acc: 0.3671875] [gan loss: 0.783056, acc: 0.312500]\n",
            "8599: [discriminator loss: 0.7486942410469055, acc: 0.0546875] [gan loss: 1.394728, acc: 0.000000]\n",
            "8600: [discriminator loss: 0.6841883063316345, acc: 0.4453125] [gan loss: 0.635849, acc: 0.671875]\n",
            "8601: [discriminator loss: 0.8079258799552917, acc: 0.0] [gan loss: 1.558535, acc: 0.000000]\n",
            "8602: [discriminator loss: 0.713696300983429, acc: 0.484375] [gan loss: 0.612487, acc: 0.671875]\n",
            "8603: [discriminator loss: 0.7761204838752747, acc: 0.0390625] [gan loss: 1.356239, acc: 0.000000]\n",
            "8604: [discriminator loss: 0.6945273876190186, acc: 0.4609375] [gan loss: 0.784532, acc: 0.218750]\n",
            "8605: [discriminator loss: 0.7464499473571777, acc: 0.0703125] [gan loss: 1.182881, acc: 0.000000]\n",
            "8606: [discriminator loss: 0.6941787004470825, acc: 0.3046875] [gan loss: 0.912564, acc: 0.062500]\n",
            "8607: [discriminator loss: 0.7153048515319824, acc: 0.1171875] [gan loss: 1.180674, acc: 0.000000]\n",
            "8608: [discriminator loss: 0.7073455452919006, acc: 0.2421875] [gan loss: 1.006950, acc: 0.078125]\n",
            "8609: [discriminator loss: 0.6959934234619141, acc: 0.203125] [gan loss: 1.138283, acc: 0.000000]\n",
            "8610: [discriminator loss: 0.6900875568389893, acc: 0.2890625] [gan loss: 1.003661, acc: 0.093750]\n",
            "8611: [discriminator loss: 0.6938126683235168, acc: 0.171875] [gan loss: 1.217346, acc: 0.000000]\n",
            "8612: [discriminator loss: 0.7131600379943848, acc: 0.3125] [gan loss: 0.942075, acc: 0.046875]\n",
            "8613: [discriminator loss: 0.718597412109375, acc: 0.140625] [gan loss: 1.326005, acc: 0.000000]\n",
            "8614: [discriminator loss: 0.7029565572738647, acc: 0.40625] [gan loss: 0.833907, acc: 0.234375]\n",
            "8615: [discriminator loss: 0.6904948949813843, acc: 0.1171875] [gan loss: 1.439059, acc: 0.015625]\n",
            "8616: [discriminator loss: 0.7066517472267151, acc: 0.375] [gan loss: 0.831962, acc: 0.281250]\n",
            "8617: [discriminator loss: 0.7498672008514404, acc: 0.0546875] [gan loss: 1.445422, acc: 0.000000]\n",
            "8618: [discriminator loss: 0.6889197826385498, acc: 0.4765625] [gan loss: 0.684962, acc: 0.515625]\n",
            "8619: [discriminator loss: 0.775484561920166, acc: 0.0234375] [gan loss: 1.646739, acc: 0.000000]\n",
            "8620: [discriminator loss: 0.7331923246383667, acc: 0.4765625] [gan loss: 0.591525, acc: 0.781250]\n",
            "8621: [discriminator loss: 0.7945730686187744, acc: 0.0078125] [gan loss: 1.435953, acc: 0.000000]\n",
            "8622: [discriminator loss: 0.7095826864242554, acc: 0.4921875] [gan loss: 0.626933, acc: 0.734375]\n",
            "8623: [discriminator loss: 0.8306674957275391, acc: 0.0078125] [gan loss: 1.399479, acc: 0.000000]\n",
            "8624: [discriminator loss: 0.7237992286682129, acc: 0.4453125] [gan loss: 0.688730, acc: 0.578125]\n",
            "8625: [discriminator loss: 0.7833330035209656, acc: 0.015625] [gan loss: 1.232380, acc: 0.000000]\n",
            "8626: [discriminator loss: 0.701316237449646, acc: 0.390625] [gan loss: 0.790547, acc: 0.281250]\n",
            "8627: [discriminator loss: 0.7285630106925964, acc: 0.078125] [gan loss: 1.069925, acc: 0.015625]\n",
            "8628: [discriminator loss: 0.6840716600418091, acc: 0.3125] [gan loss: 0.967703, acc: 0.078125]\n",
            "8629: [discriminator loss: 0.6773030161857605, acc: 0.2890625] [gan loss: 0.944002, acc: 0.140625]\n",
            "8630: [discriminator loss: 0.7095712423324585, acc: 0.234375] [gan loss: 1.067810, acc: 0.031250]\n",
            "8631: [discriminator loss: 0.7232092618942261, acc: 0.2578125] [gan loss: 0.945399, acc: 0.031250]\n",
            "8632: [discriminator loss: 0.7320069670677185, acc: 0.1171875] [gan loss: 1.258339, acc: 0.000000]\n",
            "8633: [discriminator loss: 0.6991835832595825, acc: 0.3125] [gan loss: 0.954391, acc: 0.062500]\n",
            "8634: [discriminator loss: 0.7246613502502441, acc: 0.1640625] [gan loss: 1.231331, acc: 0.015625]\n",
            "8635: [discriminator loss: 0.704311728477478, acc: 0.328125] [gan loss: 0.945911, acc: 0.125000]\n",
            "8636: [discriminator loss: 0.7205550670623779, acc: 0.125] [gan loss: 1.326719, acc: 0.000000]\n",
            "8637: [discriminator loss: 0.7134205102920532, acc: 0.390625] [gan loss: 0.734325, acc: 0.531250]\n",
            "8638: [discriminator loss: 0.7555953860282898, acc: 0.0234375] [gan loss: 1.385085, acc: 0.000000]\n",
            "8639: [discriminator loss: 0.7026369571685791, acc: 0.484375] [gan loss: 0.615104, acc: 0.687500]\n",
            "8640: [discriminator loss: 0.7948812246322632, acc: 0.0390625] [gan loss: 1.357040, acc: 0.000000]\n",
            "8641: [discriminator loss: 0.7239236235618591, acc: 0.421875] [gan loss: 0.760133, acc: 0.359375]\n",
            "8642: [discriminator loss: 0.74146568775177, acc: 0.078125] [gan loss: 1.269817, acc: 0.000000]\n",
            "8643: [discriminator loss: 0.7062236070632935, acc: 0.390625] [gan loss: 0.846022, acc: 0.218750]\n",
            "8644: [discriminator loss: 0.7775899767875671, acc: 0.0390625] [gan loss: 1.342169, acc: 0.000000]\n",
            "8645: [discriminator loss: 0.7079947590827942, acc: 0.453125] [gan loss: 0.730401, acc: 0.390625]\n",
            "8646: [discriminator loss: 0.7438950538635254, acc: 0.015625] [gan loss: 1.303773, acc: 0.000000]\n",
            "8647: [discriminator loss: 0.7188391089439392, acc: 0.4375] [gan loss: 0.752063, acc: 0.375000]\n",
            "8648: [discriminator loss: 0.7532789707183838, acc: 0.0234375] [gan loss: 1.370954, acc: 0.000000]\n",
            "8649: [discriminator loss: 0.6733237504959106, acc: 0.453125] [gan loss: 0.759870, acc: 0.328125]\n",
            "8650: [discriminator loss: 0.7213413715362549, acc: 0.09375] [gan loss: 1.227857, acc: 0.000000]\n",
            "8651: [discriminator loss: 0.6965910196304321, acc: 0.3671875] [gan loss: 0.800497, acc: 0.343750]\n",
            "8652: [discriminator loss: 0.7340676784515381, acc: 0.0859375] [gan loss: 1.343014, acc: 0.000000]\n",
            "8653: [discriminator loss: 0.7152330875396729, acc: 0.359375] [gan loss: 0.826533, acc: 0.218750]\n",
            "8654: [discriminator loss: 0.709650456905365, acc: 0.1484375] [gan loss: 1.336624, acc: 0.000000]\n",
            "8655: [discriminator loss: 0.6959584951400757, acc: 0.4375] [gan loss: 0.769357, acc: 0.375000]\n",
            "8656: [discriminator loss: 0.7770479917526245, acc: 0.0859375] [gan loss: 1.351164, acc: 0.000000]\n",
            "8657: [discriminator loss: 0.6904285550117493, acc: 0.453125] [gan loss: 0.668012, acc: 0.484375]\n",
            "8658: [discriminator loss: 0.7808644771575928, acc: 0.046875] [gan loss: 1.367028, acc: 0.000000]\n",
            "8659: [discriminator loss: 0.7088669538497925, acc: 0.421875] [gan loss: 0.774072, acc: 0.421875]\n",
            "8660: [discriminator loss: 0.7296667098999023, acc: 0.0859375] [gan loss: 1.286115, acc: 0.000000]\n",
            "8661: [discriminator loss: 0.6884158849716187, acc: 0.4375] [gan loss: 0.805147, acc: 0.250000]\n",
            "8662: [discriminator loss: 0.7230269908905029, acc: 0.0703125] [gan loss: 1.362765, acc: 0.000000]\n",
            "8663: [discriminator loss: 0.6979432702064514, acc: 0.421875] [gan loss: 0.801247, acc: 0.328125]\n",
            "8664: [discriminator loss: 0.7329323291778564, acc: 0.1015625] [gan loss: 1.270220, acc: 0.000000]\n",
            "8665: [discriminator loss: 0.6992594003677368, acc: 0.421875] [gan loss: 0.866338, acc: 0.156250]\n",
            "8666: [discriminator loss: 0.7369837164878845, acc: 0.0859375] [gan loss: 1.357978, acc: 0.000000]\n",
            "8667: [discriminator loss: 0.7205079793930054, acc: 0.4296875] [gan loss: 0.748073, acc: 0.375000]\n",
            "8668: [discriminator loss: 0.7440106868743896, acc: 0.03125] [gan loss: 1.459695, acc: 0.000000]\n",
            "8669: [discriminator loss: 0.6901514530181885, acc: 0.5] [gan loss: 0.628028, acc: 0.718750]\n",
            "8670: [discriminator loss: 0.7831664085388184, acc: 0.0078125] [gan loss: 1.532659, acc: 0.000000]\n",
            "8671: [discriminator loss: 0.7127779722213745, acc: 0.5] [gan loss: 0.596207, acc: 0.781250]\n",
            "8672: [discriminator loss: 0.7899906635284424, acc: 0.015625] [gan loss: 1.381592, acc: 0.000000]\n",
            "8673: [discriminator loss: 0.7039061188697815, acc: 0.4453125] [gan loss: 0.679864, acc: 0.562500]\n",
            "8674: [discriminator loss: 0.7813178896903992, acc: 0.0390625] [gan loss: 1.341748, acc: 0.000000]\n",
            "8675: [discriminator loss: 0.7147573232650757, acc: 0.40625] [gan loss: 0.858255, acc: 0.187500]\n",
            "8676: [discriminator loss: 0.7375888228416443, acc: 0.09375] [gan loss: 1.192050, acc: 0.000000]\n",
            "8677: [discriminator loss: 0.7076310515403748, acc: 0.3203125] [gan loss: 0.935436, acc: 0.078125]\n",
            "8678: [discriminator loss: 0.7181147336959839, acc: 0.15625] [gan loss: 1.103683, acc: 0.000000]\n",
            "8679: [discriminator loss: 0.6862407922744751, acc: 0.2890625] [gan loss: 0.950103, acc: 0.125000]\n",
            "8680: [discriminator loss: 0.7013494968414307, acc: 0.234375] [gan loss: 1.024218, acc: 0.062500]\n",
            "8681: [discriminator loss: 0.7552598714828491, acc: 0.15625] [gan loss: 1.283390, acc: 0.000000]\n",
            "8682: [discriminator loss: 0.7119039297103882, acc: 0.390625] [gan loss: 0.809963, acc: 0.218750]\n",
            "8683: [discriminator loss: 0.7344241142272949, acc: 0.09375] [gan loss: 1.305020, acc: 0.000000]\n",
            "8684: [discriminator loss: 0.6940876841545105, acc: 0.421875] [gan loss: 0.727691, acc: 0.328125]\n",
            "8685: [discriminator loss: 0.7219512462615967, acc: 0.0546875] [gan loss: 1.380999, acc: 0.000000]\n",
            "8686: [discriminator loss: 0.6764520406723022, acc: 0.4765625] [gan loss: 0.732017, acc: 0.421875]\n",
            "8687: [discriminator loss: 0.7441715598106384, acc: 0.0546875] [gan loss: 1.403100, acc: 0.000000]\n",
            "8688: [discriminator loss: 0.7211785316467285, acc: 0.453125] [gan loss: 0.679296, acc: 0.562500]\n",
            "8689: [discriminator loss: 0.7392564415931702, acc: 0.0390625] [gan loss: 1.375753, acc: 0.015625]\n",
            "8690: [discriminator loss: 0.7079123854637146, acc: 0.4453125] [gan loss: 0.795908, acc: 0.234375]\n",
            "8691: [discriminator loss: 0.7233254909515381, acc: 0.09375] [gan loss: 1.240771, acc: 0.031250]\n",
            "8692: [discriminator loss: 0.6595890522003174, acc: 0.4609375] [gan loss: 0.787741, acc: 0.390625]\n",
            "8693: [discriminator loss: 0.7520109415054321, acc: 0.078125] [gan loss: 1.184837, acc: 0.015625]\n",
            "8694: [discriminator loss: 0.691792368888855, acc: 0.390625] [gan loss: 0.806344, acc: 0.250000]\n",
            "8695: [discriminator loss: 0.727936327457428, acc: 0.0546875] [gan loss: 1.318656, acc: 0.000000]\n",
            "8696: [discriminator loss: 0.6825283765792847, acc: 0.4140625] [gan loss: 0.785547, acc: 0.328125]\n",
            "8697: [discriminator loss: 0.7646795511245728, acc: 0.078125] [gan loss: 1.498445, acc: 0.000000]\n",
            "8698: [discriminator loss: 0.7222402691841125, acc: 0.4140625] [gan loss: 0.755463, acc: 0.296875]\n",
            "8699: [discriminator loss: 0.7265298366546631, acc: 0.0390625] [gan loss: 1.226323, acc: 0.000000]\n",
            "8700: [discriminator loss: 0.7011339664459229, acc: 0.390625] [gan loss: 0.727001, acc: 0.484375]\n",
            "8701: [discriminator loss: 0.7482364177703857, acc: 0.09375] [gan loss: 1.267290, acc: 0.015625]\n",
            "8702: [discriminator loss: 0.7286093235015869, acc: 0.3828125] [gan loss: 0.816873, acc: 0.187500]\n",
            "8703: [discriminator loss: 0.7114561200141907, acc: 0.1015625] [gan loss: 1.192954, acc: 0.015625]\n",
            "8704: [discriminator loss: 0.6972112655639648, acc: 0.328125] [gan loss: 0.870404, acc: 0.156250]\n",
            "8705: [discriminator loss: 0.6924721002578735, acc: 0.140625] [gan loss: 1.282621, acc: 0.000000]\n",
            "8706: [discriminator loss: 0.7038154602050781, acc: 0.359375] [gan loss: 0.908595, acc: 0.156250]\n",
            "8707: [discriminator loss: 0.7085710763931274, acc: 0.125] [gan loss: 1.339315, acc: 0.000000]\n",
            "8708: [discriminator loss: 0.7036299705505371, acc: 0.4296875] [gan loss: 0.759237, acc: 0.281250]\n",
            "8709: [discriminator loss: 0.7409709692001343, acc: 0.0625] [gan loss: 1.427783, acc: 0.000000]\n",
            "8710: [discriminator loss: 0.6924047470092773, acc: 0.453125] [gan loss: 0.666809, acc: 0.625000]\n",
            "8711: [discriminator loss: 0.775422215461731, acc: 0.03125] [gan loss: 1.464764, acc: 0.000000]\n",
            "8712: [discriminator loss: 0.7061960101127625, acc: 0.421875] [gan loss: 0.750988, acc: 0.343750]\n",
            "8713: [discriminator loss: 0.7546663880348206, acc: 0.0703125] [gan loss: 1.367331, acc: 0.000000]\n",
            "8714: [discriminator loss: 0.6923186779022217, acc: 0.453125] [gan loss: 0.732798, acc: 0.390625]\n",
            "8715: [discriminator loss: 0.7549538612365723, acc: 0.0390625] [gan loss: 1.451074, acc: 0.000000]\n",
            "8716: [discriminator loss: 0.6836512088775635, acc: 0.4765625] [gan loss: 0.739418, acc: 0.375000]\n",
            "8717: [discriminator loss: 0.7560908794403076, acc: 0.046875] [gan loss: 1.479729, acc: 0.000000]\n",
            "8718: [discriminator loss: 0.6998891830444336, acc: 0.4765625] [gan loss: 0.677792, acc: 0.593750]\n",
            "8719: [discriminator loss: 0.7596665024757385, acc: 0.015625] [gan loss: 1.304827, acc: 0.000000]\n",
            "8720: [discriminator loss: 0.6903794407844543, acc: 0.390625] [gan loss: 0.849650, acc: 0.156250]\n",
            "8721: [discriminator loss: 0.7641423344612122, acc: 0.078125] [gan loss: 1.212861, acc: 0.000000]\n",
            "8722: [discriminator loss: 0.7218595743179321, acc: 0.3203125] [gan loss: 0.858386, acc: 0.125000]\n",
            "8723: [discriminator loss: 0.7190841436386108, acc: 0.1328125] [gan loss: 1.082129, acc: 0.015625]\n",
            "8724: [discriminator loss: 0.7060875296592712, acc: 0.2734375] [gan loss: 1.028710, acc: 0.000000]\n",
            "8725: [discriminator loss: 0.7055918574333191, acc: 0.1640625] [gan loss: 1.289145, acc: 0.015625]\n",
            "8726: [discriminator loss: 0.6865221858024597, acc: 0.390625] [gan loss: 0.761804, acc: 0.421875]\n",
            "8727: [discriminator loss: 0.7410260438919067, acc: 0.0234375] [gan loss: 1.465167, acc: 0.000000]\n",
            "8728: [discriminator loss: 0.7021499872207642, acc: 0.46875] [gan loss: 0.684751, acc: 0.515625]\n",
            "8729: [discriminator loss: 0.7673489451408386, acc: 0.015625] [gan loss: 1.378360, acc: 0.000000]\n",
            "8730: [discriminator loss: 0.694475531578064, acc: 0.46875] [gan loss: 0.762043, acc: 0.312500]\n",
            "8731: [discriminator loss: 0.736175537109375, acc: 0.0703125] [gan loss: 1.258288, acc: 0.015625]\n",
            "8732: [discriminator loss: 0.6751305460929871, acc: 0.4375] [gan loss: 0.805056, acc: 0.234375]\n",
            "8733: [discriminator loss: 0.7369070053100586, acc: 0.0859375] [gan loss: 1.314336, acc: 0.015625]\n",
            "8734: [discriminator loss: 0.6954711675643921, acc: 0.4296875] [gan loss: 0.772866, acc: 0.343750]\n",
            "8735: [discriminator loss: 0.7482610940933228, acc: 0.0703125] [gan loss: 1.219056, acc: 0.015625]\n",
            "8736: [discriminator loss: 0.6955195665359497, acc: 0.3984375] [gan loss: 0.848342, acc: 0.203125]\n",
            "8737: [discriminator loss: 0.7521328926086426, acc: 0.0703125] [gan loss: 1.406736, acc: 0.000000]\n",
            "8738: [discriminator loss: 0.6975809335708618, acc: 0.453125] [gan loss: 0.766196, acc: 0.343750]\n",
            "8739: [discriminator loss: 0.7359598278999329, acc: 0.0546875] [gan loss: 1.368245, acc: 0.000000]\n",
            "8740: [discriminator loss: 0.7044392824172974, acc: 0.46875] [gan loss: 0.751490, acc: 0.328125]\n",
            "8741: [discriminator loss: 0.7262558937072754, acc: 0.0859375] [gan loss: 1.310643, acc: 0.000000]\n",
            "8742: [discriminator loss: 0.6992273926734924, acc: 0.3671875] [gan loss: 0.854875, acc: 0.109375]\n",
            "8743: [discriminator loss: 0.7222819328308105, acc: 0.125] [gan loss: 1.298688, acc: 0.000000]\n",
            "8744: [discriminator loss: 0.7144657373428345, acc: 0.3671875] [gan loss: 0.741827, acc: 0.421875]\n",
            "8745: [discriminator loss: 0.7264686822891235, acc: 0.046875] [gan loss: 1.391336, acc: 0.000000]\n",
            "8746: [discriminator loss: 0.680031418800354, acc: 0.484375] [gan loss: 0.666204, acc: 0.578125]\n",
            "8747: [discriminator loss: 0.8031418919563293, acc: 0.0390625] [gan loss: 1.604882, acc: 0.000000]\n",
            "8748: [discriminator loss: 0.7023693919181824, acc: 0.4921875] [gan loss: 0.654595, acc: 0.656250]\n",
            "8749: [discriminator loss: 0.7591484785079956, acc: 0.015625] [gan loss: 1.406388, acc: 0.000000]\n",
            "8750: [discriminator loss: 0.7070594429969788, acc: 0.4375] [gan loss: 0.708512, acc: 0.468750]\n",
            "8751: [discriminator loss: 0.7266902327537537, acc: 0.0546875] [gan loss: 1.128354, acc: 0.000000]\n",
            "8752: [discriminator loss: 0.6969985961914062, acc: 0.3359375] [gan loss: 0.916510, acc: 0.093750]\n",
            "8753: [discriminator loss: 0.727686882019043, acc: 0.15625] [gan loss: 1.122996, acc: 0.000000]\n",
            "8754: [discriminator loss: 0.6964292526245117, acc: 0.359375] [gan loss: 0.831935, acc: 0.218750]\n",
            "8755: [discriminator loss: 0.7282008528709412, acc: 0.140625] [gan loss: 1.166340, acc: 0.015625]\n",
            "8756: [discriminator loss: 0.6963549852371216, acc: 0.2734375] [gan loss: 1.031094, acc: 0.031250]\n",
            "8757: [discriminator loss: 0.7316909432411194, acc: 0.15625] [gan loss: 1.160426, acc: 0.000000]\n",
            "8758: [discriminator loss: 0.729573130607605, acc: 0.2890625] [gan loss: 0.967603, acc: 0.078125]\n",
            "8759: [discriminator loss: 0.72230464220047, acc: 0.1796875] [gan loss: 1.268935, acc: 0.000000]\n",
            "8760: [discriminator loss: 0.6969736814498901, acc: 0.3046875] [gan loss: 0.875868, acc: 0.109375]\n",
            "8761: [discriminator loss: 0.706709623336792, acc: 0.1484375] [gan loss: 1.182931, acc: 0.000000]\n",
            "8762: [discriminator loss: 0.6949398517608643, acc: 0.375] [gan loss: 0.843270, acc: 0.203125]\n",
            "8763: [discriminator loss: 0.7465842962265015, acc: 0.0859375] [gan loss: 1.483971, acc: 0.000000]\n",
            "8764: [discriminator loss: 0.6795029044151306, acc: 0.46875] [gan loss: 0.641653, acc: 0.640625]\n",
            "8765: [discriminator loss: 0.7926464676856995, acc: 0.03125] [gan loss: 1.569526, acc: 0.000000]\n",
            "8766: [discriminator loss: 0.7099955677986145, acc: 0.4921875] [gan loss: 0.586975, acc: 0.703125]\n",
            "8767: [discriminator loss: 0.7901954650878906, acc: 0.015625] [gan loss: 1.440913, acc: 0.000000]\n",
            "8768: [discriminator loss: 0.6708412766456604, acc: 0.484375] [gan loss: 0.699427, acc: 0.546875]\n",
            "8769: [discriminator loss: 0.7487242221832275, acc: 0.0546875] [gan loss: 1.352184, acc: 0.000000]\n",
            "8770: [discriminator loss: 0.6977576017379761, acc: 0.3984375] [gan loss: 0.778851, acc: 0.328125]\n",
            "8771: [discriminator loss: 0.7902214527130127, acc: 0.03125] [gan loss: 1.206686, acc: 0.015625]\n",
            "8772: [discriminator loss: 0.6855637431144714, acc: 0.4453125] [gan loss: 0.798572, acc: 0.359375]\n",
            "8773: [discriminator loss: 0.7439397573471069, acc: 0.1015625] [gan loss: 1.249974, acc: 0.000000]\n",
            "8774: [discriminator loss: 0.6915362477302551, acc: 0.3671875] [gan loss: 0.770186, acc: 0.359375]\n",
            "8775: [discriminator loss: 0.7412310838699341, acc: 0.171875] [gan loss: 1.204502, acc: 0.015625]\n",
            "8776: [discriminator loss: 0.6720703840255737, acc: 0.390625] [gan loss: 0.827211, acc: 0.281250]\n",
            "8777: [discriminator loss: 0.7314478158950806, acc: 0.0859375] [gan loss: 1.395902, acc: 0.000000]\n",
            "8778: [discriminator loss: 0.6802818179130554, acc: 0.4609375] [gan loss: 0.770895, acc: 0.359375]\n",
            "8779: [discriminator loss: 0.774895429611206, acc: 0.0703125] [gan loss: 1.320865, acc: 0.000000]\n",
            "8780: [discriminator loss: 0.6947897672653198, acc: 0.4140625] [gan loss: 0.739336, acc: 0.328125]\n",
            "8781: [discriminator loss: 0.7512632012367249, acc: 0.0703125] [gan loss: 1.375944, acc: 0.000000]\n",
            "8782: [discriminator loss: 0.6928614377975464, acc: 0.4453125] [gan loss: 0.699738, acc: 0.515625]\n",
            "8783: [discriminator loss: 0.7711197137832642, acc: 0.0078125] [gan loss: 1.387743, acc: 0.000000]\n",
            "8784: [discriminator loss: 0.6902816295623779, acc: 0.453125] [gan loss: 0.748281, acc: 0.390625]\n",
            "8785: [discriminator loss: 0.744915246963501, acc: 0.046875] [gan loss: 1.287500, acc: 0.015625]\n",
            "8786: [discriminator loss: 0.7027918100357056, acc: 0.375] [gan loss: 0.945683, acc: 0.062500]\n",
            "8787: [discriminator loss: 0.7018282413482666, acc: 0.1875] [gan loss: 1.113900, acc: 0.000000]\n",
            "8788: [discriminator loss: 0.6958588361740112, acc: 0.3203125] [gan loss: 0.864460, acc: 0.171875]\n",
            "8789: [discriminator loss: 0.7109987735748291, acc: 0.125] [gan loss: 1.232293, acc: 0.000000]\n",
            "8790: [discriminator loss: 0.6907572746276855, acc: 0.328125] [gan loss: 0.898183, acc: 0.093750]\n",
            "8791: [discriminator loss: 0.7025485634803772, acc: 0.1796875] [gan loss: 1.297284, acc: 0.000000]\n",
            "8792: [discriminator loss: 0.7103206515312195, acc: 0.3984375] [gan loss: 0.811628, acc: 0.156250]\n",
            "8793: [discriminator loss: 0.7241913080215454, acc: 0.109375] [gan loss: 1.466147, acc: 0.000000]\n",
            "8794: [discriminator loss: 0.7156804203987122, acc: 0.4921875] [gan loss: 0.610074, acc: 0.703125]\n",
            "8795: [discriminator loss: 0.8051454424858093, acc: 0.015625] [gan loss: 1.603987, acc: 0.000000]\n",
            "8796: [discriminator loss: 0.6863325238227844, acc: 0.4921875] [gan loss: 0.648144, acc: 0.609375]\n",
            "8797: [discriminator loss: 0.7695713043212891, acc: 0.03125] [gan loss: 1.361604, acc: 0.000000]\n",
            "8798: [discriminator loss: 0.6865901947021484, acc: 0.46875] [gan loss: 0.687935, acc: 0.484375]\n",
            "8799: [discriminator loss: 0.778964638710022, acc: 0.0390625] [gan loss: 1.408572, acc: 0.000000]\n",
            "8800: [discriminator loss: 0.6988021731376648, acc: 0.453125] [gan loss: 0.737656, acc: 0.390625]\n",
            "8801: [discriminator loss: 0.7610490918159485, acc: 0.046875] [gan loss: 1.276288, acc: 0.000000]\n",
            "8802: [discriminator loss: 0.690365731716156, acc: 0.4140625] [gan loss: 0.887559, acc: 0.093750]\n",
            "8803: [discriminator loss: 0.7534739971160889, acc: 0.0625] [gan loss: 1.383490, acc: 0.000000]\n",
            "8804: [discriminator loss: 0.702253520488739, acc: 0.3984375] [gan loss: 0.745955, acc: 0.406250]\n",
            "8805: [discriminator loss: 0.7471332550048828, acc: 0.0390625] [gan loss: 1.360519, acc: 0.000000]\n",
            "8806: [discriminator loss: 0.6633782386779785, acc: 0.4609375] [gan loss: 0.804683, acc: 0.234375]\n",
            "8807: [discriminator loss: 0.736168622970581, acc: 0.046875] [gan loss: 1.202249, acc: 0.000000]\n",
            "8808: [discriminator loss: 0.6793879270553589, acc: 0.359375] [gan loss: 0.876719, acc: 0.156250]\n",
            "8809: [discriminator loss: 0.7257782220840454, acc: 0.1484375] [gan loss: 1.302911, acc: 0.000000]\n",
            "8810: [discriminator loss: 0.70599365234375, acc: 0.34375] [gan loss: 0.924883, acc: 0.187500]\n",
            "8811: [discriminator loss: 0.7293677926063538, acc: 0.109375] [gan loss: 1.294698, acc: 0.000000]\n",
            "8812: [discriminator loss: 0.6845247149467468, acc: 0.421875] [gan loss: 0.765517, acc: 0.296875]\n",
            "8813: [discriminator loss: 0.7489554286003113, acc: 0.046875] [gan loss: 1.291601, acc: 0.000000]\n",
            "8814: [discriminator loss: 0.6938798427581787, acc: 0.4375] [gan loss: 0.707593, acc: 0.421875]\n",
            "8815: [discriminator loss: 0.7434620261192322, acc: 0.0546875] [gan loss: 1.426126, acc: 0.000000]\n",
            "8816: [discriminator loss: 0.6975506544113159, acc: 0.4765625] [gan loss: 0.663422, acc: 0.546875]\n",
            "8817: [discriminator loss: 0.7673921585083008, acc: 0.0078125] [gan loss: 1.446489, acc: 0.000000]\n",
            "8818: [discriminator loss: 0.7190964221954346, acc: 0.421875] [gan loss: 0.811599, acc: 0.250000]\n",
            "8819: [discriminator loss: 0.7437431812286377, acc: 0.078125] [gan loss: 1.314540, acc: 0.015625]\n",
            "8820: [discriminator loss: 0.685966968536377, acc: 0.4453125] [gan loss: 0.830070, acc: 0.281250]\n",
            "8821: [discriminator loss: 0.7344777584075928, acc: 0.109375] [gan loss: 1.298746, acc: 0.015625]\n",
            "8822: [discriminator loss: 0.6718810796737671, acc: 0.453125] [gan loss: 0.708590, acc: 0.484375]\n",
            "8823: [discriminator loss: 0.755823016166687, acc: 0.0546875] [gan loss: 1.237706, acc: 0.031250]\n",
            "8824: [discriminator loss: 0.6903536319732666, acc: 0.390625] [gan loss: 0.812504, acc: 0.265625]\n",
            "8825: [discriminator loss: 0.7196465134620667, acc: 0.1640625] [gan loss: 1.075696, acc: 0.031250]\n",
            "8826: [discriminator loss: 0.6807526350021362, acc: 0.28125] [gan loss: 0.991379, acc: 0.015625]\n",
            "8827: [discriminator loss: 0.7166079878807068, acc: 0.2265625] [gan loss: 1.083371, acc: 0.000000]\n",
            "8828: [discriminator loss: 0.7157769203186035, acc: 0.28125] [gan loss: 1.052811, acc: 0.093750]\n",
            "8829: [discriminator loss: 0.7089906930923462, acc: 0.15625] [gan loss: 1.316471, acc: 0.000000]\n",
            "8830: [discriminator loss: 0.6947927474975586, acc: 0.4296875] [gan loss: 0.777956, acc: 0.328125]\n",
            "8831: [discriminator loss: 0.754054844379425, acc: 0.0546875] [gan loss: 1.557861, acc: 0.015625]\n",
            "8832: [discriminator loss: 0.7021975517272949, acc: 0.4765625] [gan loss: 0.603864, acc: 0.750000]\n",
            "8833: [discriminator loss: 0.8352893590927124, acc: 0.0] [gan loss: 1.654567, acc: 0.000000]\n",
            "8834: [discriminator loss: 0.7698047161102295, acc: 0.46875] [gan loss: 0.606849, acc: 0.718750]\n",
            "8835: [discriminator loss: 0.8007006645202637, acc: 0.0390625] [gan loss: 1.253579, acc: 0.015625]\n",
            "8836: [discriminator loss: 0.6908413171768188, acc: 0.4375] [gan loss: 0.846114, acc: 0.187500]\n",
            "8837: [discriminator loss: 0.7252538204193115, acc: 0.09375] [gan loss: 1.131065, acc: 0.000000]\n",
            "8838: [discriminator loss: 0.6882574558258057, acc: 0.328125] [gan loss: 0.985193, acc: 0.015625]\n",
            "8839: [discriminator loss: 0.7234984040260315, acc: 0.15625] [gan loss: 1.109423, acc: 0.015625]\n",
            "8840: [discriminator loss: 0.7119027376174927, acc: 0.25] [gan loss: 0.965299, acc: 0.046875]\n",
            "8841: [discriminator loss: 0.725953221321106, acc: 0.1953125] [gan loss: 1.047719, acc: 0.031250]\n",
            "8842: [discriminator loss: 0.7060742974281311, acc: 0.2109375] [gan loss: 1.147702, acc: 0.000000]\n",
            "8843: [discriminator loss: 0.6861754655838013, acc: 0.3984375] [gan loss: 0.900647, acc: 0.093750]\n",
            "8844: [discriminator loss: 0.722825288772583, acc: 0.1484375] [gan loss: 1.247961, acc: 0.000000]\n",
            "8845: [discriminator loss: 0.6941843032836914, acc: 0.3515625] [gan loss: 0.912341, acc: 0.109375]\n",
            "8846: [discriminator loss: 0.6952605247497559, acc: 0.2109375] [gan loss: 1.192405, acc: 0.000000]\n",
            "8847: [discriminator loss: 0.7065097093582153, acc: 0.3203125] [gan loss: 0.937757, acc: 0.078125]\n",
            "8848: [discriminator loss: 0.7065263986587524, acc: 0.1875] [gan loss: 1.223488, acc: 0.031250]\n",
            "8849: [discriminator loss: 0.6930463314056396, acc: 0.390625] [gan loss: 0.864163, acc: 0.125000]\n",
            "8850: [discriminator loss: 0.7062851190567017, acc: 0.1484375] [gan loss: 1.236215, acc: 0.000000]\n",
            "8851: [discriminator loss: 0.7094981670379639, acc: 0.3203125] [gan loss: 0.860660, acc: 0.109375]\n",
            "8852: [discriminator loss: 0.7185866832733154, acc: 0.1328125] [gan loss: 1.665626, acc: 0.000000]\n",
            "8853: [discriminator loss: 0.6965327262878418, acc: 0.4765625] [gan loss: 0.642527, acc: 0.671875]\n",
            "8854: [discriminator loss: 0.7783271670341492, acc: 0.015625] [gan loss: 1.682762, acc: 0.000000]\n",
            "8855: [discriminator loss: 0.7151924967765808, acc: 0.484375] [gan loss: 0.540661, acc: 0.906250]\n",
            "8856: [discriminator loss: 0.7708705067634583, acc: 0.0078125] [gan loss: 1.262277, acc: 0.000000]\n",
            "8857: [discriminator loss: 0.7144759893417358, acc: 0.4375] [gan loss: 0.673048, acc: 0.593750]\n",
            "8858: [discriminator loss: 0.8017549514770508, acc: 0.0390625] [gan loss: 1.387559, acc: 0.000000]\n",
            "8859: [discriminator loss: 0.7077133655548096, acc: 0.46875] [gan loss: 0.655832, acc: 0.562500]\n",
            "8860: [discriminator loss: 0.7609656453132629, acc: 0.0625] [gan loss: 1.392729, acc: 0.000000]\n",
            "8861: [discriminator loss: 0.7076047658920288, acc: 0.375] [gan loss: 0.811794, acc: 0.234375]\n",
            "8862: [discriminator loss: 0.7493817806243896, acc: 0.0859375] [gan loss: 1.341360, acc: 0.000000]\n",
            "8863: [discriminator loss: 0.7137137055397034, acc: 0.4140625] [gan loss: 0.858124, acc: 0.125000]\n",
            "8864: [discriminator loss: 0.7325023412704468, acc: 0.09375] [gan loss: 1.101574, acc: 0.000000]\n",
            "8865: [discriminator loss: 0.6966720819473267, acc: 0.3203125] [gan loss: 0.904677, acc: 0.109375]\n",
            "8866: [discriminator loss: 0.7350060343742371, acc: 0.109375] [gan loss: 1.247700, acc: 0.000000]\n",
            "8867: [discriminator loss: 0.7049092054367065, acc: 0.375] [gan loss: 0.942884, acc: 0.062500]\n",
            "8868: [discriminator loss: 0.7263909578323364, acc: 0.15625] [gan loss: 1.038737, acc: 0.062500]\n",
            "8869: [discriminator loss: 0.7030563950538635, acc: 0.234375] [gan loss: 1.036279, acc: 0.031250]\n",
            "8870: [discriminator loss: 0.7063482999801636, acc: 0.21875] [gan loss: 1.219606, acc: 0.000000]\n",
            "8871: [discriminator loss: 0.6826048493385315, acc: 0.375] [gan loss: 0.875618, acc: 0.250000]\n",
            "8872: [discriminator loss: 0.738758385181427, acc: 0.0625] [gan loss: 1.371982, acc: 0.000000]\n",
            "8873: [discriminator loss: 0.6943831443786621, acc: 0.4453125] [gan loss: 0.684150, acc: 0.546875]\n",
            "8874: [discriminator loss: 0.7661993503570557, acc: 0.0390625] [gan loss: 1.546606, acc: 0.000000]\n",
            "8875: [discriminator loss: 0.7156850099563599, acc: 0.4296875] [gan loss: 0.685703, acc: 0.609375]\n",
            "8876: [discriminator loss: 0.7568175792694092, acc: 0.0546875] [gan loss: 1.356839, acc: 0.000000]\n",
            "8877: [discriminator loss: 0.7078219652175903, acc: 0.4140625] [gan loss: 0.777243, acc: 0.406250]\n",
            "8878: [discriminator loss: 0.7501147985458374, acc: 0.0859375] [gan loss: 1.176387, acc: 0.015625]\n",
            "8879: [discriminator loss: 0.6845304369926453, acc: 0.359375] [gan loss: 0.890945, acc: 0.093750]\n",
            "8880: [discriminator loss: 0.7284778952598572, acc: 0.1328125] [gan loss: 1.243282, acc: 0.000000]\n",
            "8881: [discriminator loss: 0.6917641162872314, acc: 0.375] [gan loss: 0.841818, acc: 0.187500]\n",
            "8882: [discriminator loss: 0.7363489866256714, acc: 0.1171875] [gan loss: 1.261467, acc: 0.000000]\n",
            "8883: [discriminator loss: 0.6946670413017273, acc: 0.421875] [gan loss: 0.763798, acc: 0.406250]\n",
            "8884: [discriminator loss: 0.719825267791748, acc: 0.1171875] [gan loss: 1.377428, acc: 0.015625]\n",
            "8885: [discriminator loss: 0.6666871905326843, acc: 0.421875] [gan loss: 0.875694, acc: 0.265625]\n",
            "8886: [discriminator loss: 0.7358005046844482, acc: 0.1484375] [gan loss: 1.293795, acc: 0.015625]\n",
            "8887: [discriminator loss: 0.7073295712471008, acc: 0.3671875] [gan loss: 0.897199, acc: 0.171875]\n",
            "8888: [discriminator loss: 0.766781210899353, acc: 0.09375] [gan loss: 1.325966, acc: 0.000000]\n",
            "8889: [discriminator loss: 0.7044496536254883, acc: 0.453125] [gan loss: 0.633474, acc: 0.671875]\n",
            "8890: [discriminator loss: 0.8220866322517395, acc: 0.0234375] [gan loss: 1.736663, acc: 0.000000]\n",
            "8891: [discriminator loss: 0.7193679809570312, acc: 0.5] [gan loss: 0.582853, acc: 0.765625]\n",
            "8892: [discriminator loss: 0.8113147616386414, acc: 0.0078125] [gan loss: 1.283489, acc: 0.000000]\n",
            "8893: [discriminator loss: 0.7167277336120605, acc: 0.421875] [gan loss: 0.750050, acc: 0.406250]\n",
            "8894: [discriminator loss: 0.7356674671173096, acc: 0.0859375] [gan loss: 1.171847, acc: 0.000000]\n",
            "8895: [discriminator loss: 0.7142618894577026, acc: 0.2734375] [gan loss: 0.907417, acc: 0.109375]\n",
            "8896: [discriminator loss: 0.7202240824699402, acc: 0.1640625] [gan loss: 1.176241, acc: 0.000000]\n",
            "8897: [discriminator loss: 0.7149295806884766, acc: 0.265625] [gan loss: 0.910608, acc: 0.078125]\n",
            "8898: [discriminator loss: 0.7337203025817871, acc: 0.15625] [gan loss: 1.190402, acc: 0.000000]\n",
            "8899: [discriminator loss: 0.7145761847496033, acc: 0.2890625] [gan loss: 0.924639, acc: 0.093750]\n",
            "8900: [discriminator loss: 0.7182546257972717, acc: 0.171875] [gan loss: 1.266696, acc: 0.015625]\n",
            "8901: [discriminator loss: 0.7029783725738525, acc: 0.3828125] [gan loss: 0.862081, acc: 0.109375]\n",
            "8902: [discriminator loss: 0.7231458425521851, acc: 0.1484375] [gan loss: 1.328777, acc: 0.000000]\n",
            "8903: [discriminator loss: 0.7013471722602844, acc: 0.375] [gan loss: 0.874051, acc: 0.078125]\n",
            "8904: [discriminator loss: 0.7173886299133301, acc: 0.1171875] [gan loss: 1.243789, acc: 0.000000]\n",
            "8905: [discriminator loss: 0.7052141427993774, acc: 0.3828125] [gan loss: 0.903890, acc: 0.078125]\n",
            "8906: [discriminator loss: 0.7065258622169495, acc: 0.125] [gan loss: 1.257268, acc: 0.000000]\n",
            "8907: [discriminator loss: 0.6960387229919434, acc: 0.3984375] [gan loss: 0.816513, acc: 0.312500]\n",
            "8908: [discriminator loss: 0.7243291139602661, acc: 0.1484375] [gan loss: 1.348483, acc: 0.000000]\n",
            "8909: [discriminator loss: 0.6718398332595825, acc: 0.453125] [gan loss: 0.692997, acc: 0.531250]\n",
            "8910: [discriminator loss: 0.7912958860397339, acc: 0.0546875] [gan loss: 1.676294, acc: 0.000000]\n",
            "8911: [discriminator loss: 0.7040224671363831, acc: 0.4921875] [gan loss: 0.578726, acc: 0.796875]\n",
            "8912: [discriminator loss: 0.8256090879440308, acc: 0.0] [gan loss: 1.492321, acc: 0.000000]\n",
            "8913: [discriminator loss: 0.6978836059570312, acc: 0.4921875] [gan loss: 0.628891, acc: 0.640625]\n",
            "8914: [discriminator loss: 0.7818742990493774, acc: 0.0234375] [gan loss: 1.425078, acc: 0.000000]\n",
            "8915: [discriminator loss: 0.690379798412323, acc: 0.4375] [gan loss: 0.816842, acc: 0.234375]\n",
            "8916: [discriminator loss: 0.7184703350067139, acc: 0.1171875] [gan loss: 1.127985, acc: 0.000000]\n",
            "8917: [discriminator loss: 0.700539767742157, acc: 0.3125] [gan loss: 0.954511, acc: 0.031250]\n",
            "8918: [discriminator loss: 0.7094802260398865, acc: 0.1015625] [gan loss: 1.127159, acc: 0.015625]\n",
            "8919: [discriminator loss: 0.699215292930603, acc: 0.3359375] [gan loss: 0.979679, acc: 0.015625]\n",
            "8920: [discriminator loss: 0.703378438949585, acc: 0.21875] [gan loss: 1.177022, acc: 0.031250]\n",
            "8921: [discriminator loss: 0.6972856521606445, acc: 0.3203125] [gan loss: 0.822231, acc: 0.265625]\n",
            "8922: [discriminator loss: 0.7199992537498474, acc: 0.1640625] [gan loss: 1.197217, acc: 0.000000]\n",
            "8923: [discriminator loss: 0.6988934278488159, acc: 0.3359375] [gan loss: 0.863041, acc: 0.218750]\n",
            "8924: [discriminator loss: 0.7382520437240601, acc: 0.0859375] [gan loss: 1.368901, acc: 0.015625]\n",
            "8925: [discriminator loss: 0.7159351706504822, acc: 0.3671875] [gan loss: 0.858359, acc: 0.203125]\n",
            "8926: [discriminator loss: 0.7764008045196533, acc: 0.09375] [gan loss: 1.334780, acc: 0.000000]\n",
            "8927: [discriminator loss: 0.6639712452888489, acc: 0.4609375] [gan loss: 0.668161, acc: 0.593750]\n",
            "8928: [discriminator loss: 0.7535778284072876, acc: 0.0625] [gan loss: 1.424266, acc: 0.000000]\n",
            "8929: [discriminator loss: 0.7060753107070923, acc: 0.3984375] [gan loss: 0.735792, acc: 0.468750]\n",
            "8930: [discriminator loss: 0.7264633178710938, acc: 0.078125] [gan loss: 1.388914, acc: 0.000000]\n",
            "8931: [discriminator loss: 0.7171399593353271, acc: 0.3984375] [gan loss: 0.843360, acc: 0.265625]\n",
            "8932: [discriminator loss: 0.7369732856750488, acc: 0.125] [gan loss: 1.353299, acc: 0.000000]\n",
            "8933: [discriminator loss: 0.6994436383247375, acc: 0.3515625] [gan loss: 0.889360, acc: 0.093750]\n",
            "8934: [discriminator loss: 0.7184610366821289, acc: 0.1015625] [gan loss: 1.374557, acc: 0.000000]\n",
            "8935: [discriminator loss: 0.6910063028335571, acc: 0.4140625] [gan loss: 0.830486, acc: 0.250000]\n",
            "8936: [discriminator loss: 0.7528213858604431, acc: 0.0625] [gan loss: 1.410933, acc: 0.000000]\n",
            "8937: [discriminator loss: 0.6951364874839783, acc: 0.4375] [gan loss: 0.677772, acc: 0.500000]\n",
            "8938: [discriminator loss: 0.7636284828186035, acc: 0.015625] [gan loss: 1.470576, acc: 0.000000]\n",
            "8939: [discriminator loss: 0.7308976054191589, acc: 0.46875] [gan loss: 0.647889, acc: 0.671875]\n",
            "8940: [discriminator loss: 0.7761582732200623, acc: 0.0234375] [gan loss: 1.206578, acc: 0.000000]\n",
            "8941: [discriminator loss: 0.6954524517059326, acc: 0.4453125] [gan loss: 0.691457, acc: 0.656250]\n",
            "8942: [discriminator loss: 0.7295286655426025, acc: 0.140625] [gan loss: 1.207065, acc: 0.000000]\n",
            "8943: [discriminator loss: 0.7344667911529541, acc: 0.28125] [gan loss: 0.950260, acc: 0.093750]\n",
            "8944: [discriminator loss: 0.7114416360855103, acc: 0.1796875] [gan loss: 1.169884, acc: 0.000000]\n",
            "8945: [discriminator loss: 0.6906023025512695, acc: 0.3828125] [gan loss: 0.980089, acc: 0.078125]\n",
            "8946: [discriminator loss: 0.7255152463912964, acc: 0.125] [gan loss: 1.277910, acc: 0.000000]\n",
            "8947: [discriminator loss: 0.6893320679664612, acc: 0.3828125] [gan loss: 0.880159, acc: 0.156250]\n",
            "8948: [discriminator loss: 0.7045593857765198, acc: 0.1484375] [gan loss: 1.325849, acc: 0.000000]\n",
            "8949: [discriminator loss: 0.7030535936355591, acc: 0.3671875] [gan loss: 0.795194, acc: 0.281250]\n",
            "8950: [discriminator loss: 0.7335666418075562, acc: 0.09375] [gan loss: 1.380002, acc: 0.000000]\n",
            "8951: [discriminator loss: 0.6887897849082947, acc: 0.4140625] [gan loss: 0.752782, acc: 0.406250]\n",
            "8952: [discriminator loss: 0.7871507406234741, acc: 0.0703125] [gan loss: 1.468881, acc: 0.000000]\n",
            "8953: [discriminator loss: 0.7041839361190796, acc: 0.4921875] [gan loss: 0.687339, acc: 0.484375]\n",
            "8954: [discriminator loss: 0.768229603767395, acc: 0.0546875] [gan loss: 1.515908, acc: 0.000000]\n",
            "8955: [discriminator loss: 0.7092846035957336, acc: 0.453125] [gan loss: 0.670574, acc: 0.531250]\n",
            "8956: [discriminator loss: 0.8030542135238647, acc: 0.03125] [gan loss: 1.367842, acc: 0.000000]\n",
            "8957: [discriminator loss: 0.6965075135231018, acc: 0.484375] [gan loss: 0.732972, acc: 0.406250]\n",
            "8958: [discriminator loss: 0.7477642893791199, acc: 0.0234375] [gan loss: 1.275522, acc: 0.015625]\n",
            "8959: [discriminator loss: 0.7143045663833618, acc: 0.4375] [gan loss: 0.744610, acc: 0.375000]\n",
            "8960: [discriminator loss: 0.7382869720458984, acc: 0.0625] [gan loss: 1.241985, acc: 0.000000]\n",
            "8961: [discriminator loss: 0.7050213813781738, acc: 0.3984375] [gan loss: 0.853281, acc: 0.171875]\n",
            "8962: [discriminator loss: 0.7398468255996704, acc: 0.078125] [gan loss: 1.229456, acc: 0.000000]\n",
            "8963: [discriminator loss: 0.7073118686676025, acc: 0.34375] [gan loss: 0.883308, acc: 0.140625]\n",
            "8964: [discriminator loss: 0.7417932152748108, acc: 0.109375] [gan loss: 1.303456, acc: 0.000000]\n",
            "8965: [discriminator loss: 0.685779869556427, acc: 0.421875] [gan loss: 0.747295, acc: 0.390625]\n",
            "8966: [discriminator loss: 0.6920218467712402, acc: 0.1171875] [gan loss: 1.338851, acc: 0.015625]\n",
            "8967: [discriminator loss: 0.6979719400405884, acc: 0.4140625] [gan loss: 0.830096, acc: 0.250000]\n",
            "8968: [discriminator loss: 0.7305700778961182, acc: 0.09375] [gan loss: 1.296358, acc: 0.000000]\n",
            "8969: [discriminator loss: 0.6952933073043823, acc: 0.390625] [gan loss: 0.904538, acc: 0.140625]\n",
            "8970: [discriminator loss: 0.7041268348693848, acc: 0.15625] [gan loss: 1.151410, acc: 0.000000]\n",
            "8971: [discriminator loss: 0.6817967891693115, acc: 0.390625] [gan loss: 0.928505, acc: 0.140625]\n",
            "8972: [discriminator loss: 0.7223604917526245, acc: 0.15625] [gan loss: 1.271862, acc: 0.000000]\n",
            "8973: [discriminator loss: 0.6811957359313965, acc: 0.4296875] [gan loss: 0.755465, acc: 0.437500]\n",
            "8974: [discriminator loss: 0.7388964891433716, acc: 0.109375] [gan loss: 1.369034, acc: 0.000000]\n",
            "8975: [discriminator loss: 0.6964703798294067, acc: 0.4375] [gan loss: 0.725875, acc: 0.421875]\n",
            "8976: [discriminator loss: 0.7560557126998901, acc: 0.015625] [gan loss: 1.373327, acc: 0.000000]\n",
            "8977: [discriminator loss: 0.6930263042449951, acc: 0.421875] [gan loss: 0.755922, acc: 0.359375]\n",
            "8978: [discriminator loss: 0.7470141649246216, acc: 0.0546875] [gan loss: 1.315063, acc: 0.000000]\n",
            "8979: [discriminator loss: 0.7166966795921326, acc: 0.390625] [gan loss: 0.772961, acc: 0.296875]\n",
            "8980: [discriminator loss: 0.7465168237686157, acc: 0.0234375] [gan loss: 1.505362, acc: 0.000000]\n",
            "8981: [discriminator loss: 0.7072539329528809, acc: 0.4765625] [gan loss: 0.634405, acc: 0.656250]\n",
            "8982: [discriminator loss: 0.7659277319908142, acc: 0.03125] [gan loss: 1.410310, acc: 0.015625]\n",
            "8983: [discriminator loss: 0.692081093788147, acc: 0.46875] [gan loss: 0.706830, acc: 0.453125]\n",
            "8984: [discriminator loss: 0.7473432421684265, acc: 0.0703125] [gan loss: 1.244627, acc: 0.000000]\n",
            "8985: [discriminator loss: 0.6889209151268005, acc: 0.4140625] [gan loss: 0.838107, acc: 0.296875]\n",
            "8986: [discriminator loss: 0.7428157329559326, acc: 0.1171875] [gan loss: 1.234381, acc: 0.000000]\n",
            "8987: [discriminator loss: 0.7064328789710999, acc: 0.375] [gan loss: 0.800201, acc: 0.265625]\n",
            "8988: [discriminator loss: 0.7363516092300415, acc: 0.1015625] [gan loss: 1.272399, acc: 0.000000]\n",
            "8989: [discriminator loss: 0.7156819105148315, acc: 0.4375] [gan loss: 0.707771, acc: 0.484375]\n",
            "8990: [discriminator loss: 0.7488104701042175, acc: 0.0625] [gan loss: 1.256152, acc: 0.000000]\n",
            "8991: [discriminator loss: 0.6915326714515686, acc: 0.4140625] [gan loss: 0.755934, acc: 0.359375]\n",
            "8992: [discriminator loss: 0.7586400508880615, acc: 0.0625] [gan loss: 1.346403, acc: 0.000000]\n",
            "8993: [discriminator loss: 0.7082794904708862, acc: 0.421875] [gan loss: 0.793707, acc: 0.218750]\n",
            "8994: [discriminator loss: 0.7399178743362427, acc: 0.09375] [gan loss: 1.255080, acc: 0.000000]\n",
            "8995: [discriminator loss: 0.702983021736145, acc: 0.3828125] [gan loss: 0.802503, acc: 0.234375]\n",
            "8996: [discriminator loss: 0.7361454963684082, acc: 0.1484375] [gan loss: 1.312654, acc: 0.000000]\n",
            "8997: [discriminator loss: 0.725643515586853, acc: 0.3515625] [gan loss: 0.961209, acc: 0.062500]\n",
            "8998: [discriminator loss: 0.7234727144241333, acc: 0.140625] [gan loss: 1.312794, acc: 0.000000]\n",
            "8999: [discriminator loss: 0.705794095993042, acc: 0.359375] [gan loss: 0.868359, acc: 0.125000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "9000: [discriminator loss: 0.7398337125778198, acc: 0.0703125] [gan loss: 1.399339, acc: 0.000000]\n",
            "9001: [discriminator loss: 0.7032761573791504, acc: 0.46875] [gan loss: 0.677649, acc: 0.578125]\n",
            "9002: [discriminator loss: 0.7615727186203003, acc: 0.046875] [gan loss: 1.527238, acc: 0.000000]\n",
            "9003: [discriminator loss: 0.7011587619781494, acc: 0.46875] [gan loss: 0.749304, acc: 0.406250]\n",
            "9004: [discriminator loss: 0.7452541589736938, acc: 0.078125] [gan loss: 1.337450, acc: 0.000000]\n",
            "9005: [discriminator loss: 0.6898606419563293, acc: 0.4765625] [gan loss: 0.769423, acc: 0.359375]\n",
            "9006: [discriminator loss: 0.7553642988204956, acc: 0.03125] [gan loss: 1.280792, acc: 0.000000]\n",
            "9007: [discriminator loss: 0.7055975198745728, acc: 0.4140625] [gan loss: 0.851735, acc: 0.156250]\n",
            "9008: [discriminator loss: 0.7502273917198181, acc: 0.046875] [gan loss: 1.179370, acc: 0.000000]\n",
            "9009: [discriminator loss: 0.7078235745429993, acc: 0.3671875] [gan loss: 0.900315, acc: 0.062500]\n",
            "9010: [discriminator loss: 0.7297227382659912, acc: 0.1171875] [gan loss: 1.215646, acc: 0.000000]\n",
            "9011: [discriminator loss: 0.7108556032180786, acc: 0.3359375] [gan loss: 0.883831, acc: 0.125000]\n",
            "9012: [discriminator loss: 0.7144050598144531, acc: 0.109375] [gan loss: 1.246742, acc: 0.000000]\n",
            "9013: [discriminator loss: 0.7167751789093018, acc: 0.3046875] [gan loss: 1.039967, acc: 0.000000]\n",
            "9014: [discriminator loss: 0.7084636688232422, acc: 0.2265625] [gan loss: 1.116485, acc: 0.000000]\n",
            "9015: [discriminator loss: 0.6980100870132446, acc: 0.2734375] [gan loss: 0.982424, acc: 0.046875]\n",
            "9016: [discriminator loss: 0.6994567513465881, acc: 0.2109375] [gan loss: 1.241434, acc: 0.000000]\n",
            "9017: [discriminator loss: 0.7064346671104431, acc: 0.3359375] [gan loss: 0.915590, acc: 0.078125]\n",
            "9018: [discriminator loss: 0.715345561504364, acc: 0.1328125] [gan loss: 1.365725, acc: 0.000000]\n",
            "9019: [discriminator loss: 0.691953182220459, acc: 0.4375] [gan loss: 0.740464, acc: 0.359375]\n",
            "9020: [discriminator loss: 0.7531170845031738, acc: 0.03125] [gan loss: 1.399466, acc: 0.000000]\n",
            "9021: [discriminator loss: 0.7115437984466553, acc: 0.4609375] [gan loss: 0.689015, acc: 0.546875]\n",
            "9022: [discriminator loss: 0.7717533111572266, acc: 0.0390625] [gan loss: 1.427297, acc: 0.000000]\n",
            "9023: [discriminator loss: 0.7260037064552307, acc: 0.46875] [gan loss: 0.684695, acc: 0.484375]\n",
            "9024: [discriminator loss: 0.7637975215911865, acc: 0.0625] [gan loss: 1.346291, acc: 0.000000]\n",
            "9025: [discriminator loss: 0.6747603416442871, acc: 0.46875] [gan loss: 0.692546, acc: 0.484375]\n",
            "9026: [discriminator loss: 0.7578383088111877, acc: 0.0390625] [gan loss: 1.243394, acc: 0.015625]\n",
            "9027: [discriminator loss: 0.7043367028236389, acc: 0.3828125] [gan loss: 0.763780, acc: 0.296875]\n",
            "9028: [discriminator loss: 0.7423421740531921, acc: 0.0234375] [gan loss: 1.390666, acc: 0.000000]\n",
            "9029: [discriminator loss: 0.7014985084533691, acc: 0.4375] [gan loss: 0.705865, acc: 0.531250]\n",
            "9030: [discriminator loss: 0.739692747592926, acc: 0.0390625] [gan loss: 1.218744, acc: 0.000000]\n",
            "9031: [discriminator loss: 0.6795850396156311, acc: 0.4375] [gan loss: 0.768301, acc: 0.375000]\n",
            "9032: [discriminator loss: 0.7387781143188477, acc: 0.0859375] [gan loss: 1.354427, acc: 0.000000]\n",
            "9033: [discriminator loss: 0.7064774036407471, acc: 0.453125] [gan loss: 0.750309, acc: 0.390625]\n",
            "9034: [discriminator loss: 0.7478585243225098, acc: 0.0703125] [gan loss: 1.372694, acc: 0.000000]\n",
            "9035: [discriminator loss: 0.6992582082748413, acc: 0.375] [gan loss: 0.789756, acc: 0.296875]\n",
            "9036: [discriminator loss: 0.738017201423645, acc: 0.09375] [gan loss: 1.246570, acc: 0.015625]\n",
            "9037: [discriminator loss: 0.701117217540741, acc: 0.4140625] [gan loss: 0.716039, acc: 0.531250]\n",
            "9038: [discriminator loss: 0.8111931085586548, acc: 0.046875] [gan loss: 1.320215, acc: 0.000000]\n",
            "9039: [discriminator loss: 0.7065749168395996, acc: 0.4609375] [gan loss: 0.779155, acc: 0.312500]\n",
            "9040: [discriminator loss: 0.7125757932662964, acc: 0.109375] [gan loss: 1.215570, acc: 0.000000]\n",
            "9041: [discriminator loss: 0.6899756193161011, acc: 0.40625] [gan loss: 0.804048, acc: 0.281250]\n",
            "9042: [discriminator loss: 0.735789954662323, acc: 0.0703125] [gan loss: 1.269084, acc: 0.015625]\n",
            "9043: [discriminator loss: 0.6896151304244995, acc: 0.4375] [gan loss: 0.704354, acc: 0.421875]\n",
            "9044: [discriminator loss: 0.7711906433105469, acc: 0.03125] [gan loss: 1.304806, acc: 0.000000]\n",
            "9045: [discriminator loss: 0.7087050080299377, acc: 0.421875] [gan loss: 0.765365, acc: 0.375000]\n",
            "9046: [discriminator loss: 0.7211583852767944, acc: 0.09375] [gan loss: 1.188918, acc: 0.015625]\n",
            "9047: [discriminator loss: 0.691031277179718, acc: 0.4453125] [gan loss: 0.777949, acc: 0.281250]\n",
            "9048: [discriminator loss: 0.7518908381462097, acc: 0.046875] [gan loss: 1.263647, acc: 0.000000]\n",
            "9049: [discriminator loss: 0.7050787210464478, acc: 0.4140625] [gan loss: 0.807101, acc: 0.250000]\n",
            "9050: [discriminator loss: 0.734534502029419, acc: 0.0625] [gan loss: 1.353497, acc: 0.000000]\n",
            "9051: [discriminator loss: 0.7219206690788269, acc: 0.4609375] [gan loss: 0.676360, acc: 0.546875]\n",
            "9052: [discriminator loss: 0.7596251964569092, acc: 0.09375] [gan loss: 1.289248, acc: 0.000000]\n",
            "9053: [discriminator loss: 0.6974378824234009, acc: 0.34375] [gan loss: 0.872743, acc: 0.156250]\n",
            "9054: [discriminator loss: 0.7213752269744873, acc: 0.1015625] [gan loss: 1.085313, acc: 0.015625]\n",
            "9055: [discriminator loss: 0.696491003036499, acc: 0.28125] [gan loss: 0.987669, acc: 0.046875]\n",
            "9056: [discriminator loss: 0.7087680101394653, acc: 0.1953125] [gan loss: 1.119224, acc: 0.000000]\n",
            "9057: [discriminator loss: 0.677897572517395, acc: 0.296875] [gan loss: 1.106781, acc: 0.015625]\n",
            "9058: [discriminator loss: 0.7093625068664551, acc: 0.28125] [gan loss: 1.080469, acc: 0.015625]\n",
            "9059: [discriminator loss: 0.6981924772262573, acc: 0.2890625] [gan loss: 1.080435, acc: 0.015625]\n",
            "9060: [discriminator loss: 0.7138253450393677, acc: 0.21875] [gan loss: 1.229516, acc: 0.000000]\n",
            "9061: [discriminator loss: 0.6934943795204163, acc: 0.296875] [gan loss: 1.136222, acc: 0.000000]\n",
            "9062: [discriminator loss: 0.6769701242446899, acc: 0.3515625] [gan loss: 1.004088, acc: 0.046875]\n",
            "9063: [discriminator loss: 0.6846873760223389, acc: 0.21875] [gan loss: 1.202670, acc: 0.000000]\n",
            "9064: [discriminator loss: 0.6903185248374939, acc: 0.328125] [gan loss: 0.949359, acc: 0.062500]\n",
            "9065: [discriminator loss: 0.7511812448501587, acc: 0.078125] [gan loss: 1.596818, acc: 0.000000]\n",
            "9066: [discriminator loss: 0.7262693643569946, acc: 0.46875] [gan loss: 0.542970, acc: 0.875000]\n",
            "9067: [discriminator loss: 0.8038628101348877, acc: 0.0390625] [gan loss: 1.516677, acc: 0.000000]\n",
            "9068: [discriminator loss: 0.7160657644271851, acc: 0.4765625] [gan loss: 0.655594, acc: 0.546875]\n",
            "9069: [discriminator loss: 0.7544692754745483, acc: 0.0703125] [gan loss: 1.265194, acc: 0.000000]\n",
            "9070: [discriminator loss: 0.7013124823570251, acc: 0.421875] [gan loss: 0.784771, acc: 0.390625]\n",
            "9071: [discriminator loss: 0.7352613210678101, acc: 0.1015625] [gan loss: 1.343200, acc: 0.000000]\n",
            "9072: [discriminator loss: 0.7039346694946289, acc: 0.4375] [gan loss: 0.746482, acc: 0.390625]\n",
            "9073: [discriminator loss: 0.745619535446167, acc: 0.0859375] [gan loss: 1.362705, acc: 0.000000]\n",
            "9074: [discriminator loss: 0.7080935835838318, acc: 0.3984375] [gan loss: 0.827865, acc: 0.265625]\n",
            "9075: [discriminator loss: 0.7528868913650513, acc: 0.1015625] [gan loss: 1.371617, acc: 0.000000]\n",
            "9076: [discriminator loss: 0.694733202457428, acc: 0.453125] [gan loss: 0.713785, acc: 0.437500]\n",
            "9077: [discriminator loss: 0.7438783049583435, acc: 0.046875] [gan loss: 1.382174, acc: 0.000000]\n",
            "9078: [discriminator loss: 0.711481511592865, acc: 0.3984375] [gan loss: 0.751068, acc: 0.375000]\n",
            "9079: [discriminator loss: 0.7330382466316223, acc: 0.0703125] [gan loss: 1.101381, acc: 0.000000]\n",
            "9080: [discriminator loss: 0.6985689401626587, acc: 0.390625] [gan loss: 0.840573, acc: 0.250000]\n",
            "9081: [discriminator loss: 0.7332614660263062, acc: 0.140625] [gan loss: 1.294821, acc: 0.000000]\n",
            "9082: [discriminator loss: 0.6876190900802612, acc: 0.3828125] [gan loss: 0.801165, acc: 0.296875]\n",
            "9083: [discriminator loss: 0.7181649208068848, acc: 0.0546875] [gan loss: 1.355207, acc: 0.000000]\n",
            "9084: [discriminator loss: 0.7511348724365234, acc: 0.4296875] [gan loss: 0.689120, acc: 0.562500]\n",
            "9085: [discriminator loss: 0.766179084777832, acc: 0.03125] [gan loss: 1.419992, acc: 0.000000]\n",
            "9086: [discriminator loss: 0.6777567863464355, acc: 0.453125] [gan loss: 0.760842, acc: 0.296875]\n",
            "9087: [discriminator loss: 0.7313257455825806, acc: 0.0625] [gan loss: 1.173257, acc: 0.000000]\n",
            "9088: [discriminator loss: 0.6840827465057373, acc: 0.40625] [gan loss: 0.829875, acc: 0.187500]\n",
            "9089: [discriminator loss: 0.7298963069915771, acc: 0.1015625] [gan loss: 1.112908, acc: 0.015625]\n",
            "9090: [discriminator loss: 0.7144781351089478, acc: 0.3046875] [gan loss: 0.937197, acc: 0.046875]\n",
            "9091: [discriminator loss: 0.7219316363334656, acc: 0.1484375] [gan loss: 1.080061, acc: 0.015625]\n",
            "9092: [discriminator loss: 0.6855979561805725, acc: 0.375] [gan loss: 0.882656, acc: 0.125000]\n",
            "9093: [discriminator loss: 0.7309159636497498, acc: 0.1328125] [gan loss: 1.201928, acc: 0.015625]\n",
            "9094: [discriminator loss: 0.6917616128921509, acc: 0.3515625] [gan loss: 0.892733, acc: 0.109375]\n",
            "9095: [discriminator loss: 0.7176316976547241, acc: 0.1875] [gan loss: 1.266008, acc: 0.000000]\n",
            "9096: [discriminator loss: 0.7005538940429688, acc: 0.3828125] [gan loss: 0.877384, acc: 0.109375]\n",
            "9097: [discriminator loss: 0.7201769351959229, acc: 0.125] [gan loss: 1.315073, acc: 0.000000]\n",
            "9098: [discriminator loss: 0.7033360004425049, acc: 0.4140625] [gan loss: 0.725710, acc: 0.468750]\n",
            "9099: [discriminator loss: 0.7665170431137085, acc: 0.0859375] [gan loss: 1.447797, acc: 0.000000]\n",
            "9100: [discriminator loss: 0.7054949402809143, acc: 0.484375] [gan loss: 0.644262, acc: 0.656250]\n",
            "9101: [discriminator loss: 0.7900068163871765, acc: 0.015625] [gan loss: 1.505847, acc: 0.000000]\n",
            "9102: [discriminator loss: 0.7024320363998413, acc: 0.484375] [gan loss: 0.603587, acc: 0.734375]\n",
            "9103: [discriminator loss: 0.7546126842498779, acc: 0.0] [gan loss: 1.287564, acc: 0.000000]\n",
            "9104: [discriminator loss: 0.6926141977310181, acc: 0.421875] [gan loss: 0.722622, acc: 0.468750]\n",
            "9105: [discriminator loss: 0.7848732471466064, acc: 0.0546875] [gan loss: 1.186742, acc: 0.000000]\n",
            "9106: [discriminator loss: 0.714356005191803, acc: 0.3515625] [gan loss: 0.867135, acc: 0.125000]\n",
            "9107: [discriminator loss: 0.7232667803764343, acc: 0.1015625] [gan loss: 1.131590, acc: 0.000000]\n",
            "9108: [discriminator loss: 0.6984331011772156, acc: 0.3046875] [gan loss: 0.973288, acc: 0.046875]\n",
            "9109: [discriminator loss: 0.7086268663406372, acc: 0.171875] [gan loss: 1.257199, acc: 0.015625]\n",
            "9110: [discriminator loss: 0.7114862203598022, acc: 0.40625] [gan loss: 0.786623, acc: 0.281250]\n",
            "9111: [discriminator loss: 0.7287374138832092, acc: 0.1015625] [gan loss: 1.301879, acc: 0.000000]\n",
            "9112: [discriminator loss: 0.6773972511291504, acc: 0.421875] [gan loss: 0.802315, acc: 0.296875]\n",
            "9113: [discriminator loss: 0.7444445490837097, acc: 0.0625] [gan loss: 1.253698, acc: 0.000000]\n",
            "9114: [discriminator loss: 0.692083477973938, acc: 0.34375] [gan loss: 0.926282, acc: 0.156250]\n",
            "9115: [discriminator loss: 0.6977688074111938, acc: 0.2421875] [gan loss: 1.087568, acc: 0.046875]\n",
            "9116: [discriminator loss: 0.7163311243057251, acc: 0.203125] [gan loss: 0.995619, acc: 0.109375]\n",
            "9117: [discriminator loss: 0.6855493783950806, acc: 0.265625] [gan loss: 1.081375, acc: 0.031250]\n",
            "9118: [discriminator loss: 0.7123732566833496, acc: 0.2578125] [gan loss: 1.098379, acc: 0.015625]\n",
            "9119: [discriminator loss: 0.6854602098464966, acc: 0.2578125] [gan loss: 1.130095, acc: 0.031250]\n",
            "9120: [discriminator loss: 0.7017762064933777, acc: 0.2890625] [gan loss: 1.073298, acc: 0.031250]\n",
            "9121: [discriminator loss: 0.7271256446838379, acc: 0.25] [gan loss: 1.106664, acc: 0.015625]\n",
            "9122: [discriminator loss: 0.6828018426895142, acc: 0.328125] [gan loss: 0.954255, acc: 0.093750]\n",
            "9123: [discriminator loss: 0.7372779846191406, acc: 0.1171875] [gan loss: 1.463536, acc: 0.000000]\n",
            "9124: [discriminator loss: 0.7163216471672058, acc: 0.4375] [gan loss: 0.633263, acc: 0.671875]\n",
            "9125: [discriminator loss: 0.7992534637451172, acc: 0.0546875] [gan loss: 1.665413, acc: 0.000000]\n",
            "9126: [discriminator loss: 0.7354464530944824, acc: 0.4921875] [gan loss: 0.623398, acc: 0.718750]\n",
            "9127: [discriminator loss: 0.7745556235313416, acc: 0.0234375] [gan loss: 1.407693, acc: 0.000000]\n",
            "9128: [discriminator loss: 0.7132444977760315, acc: 0.421875] [gan loss: 0.793055, acc: 0.312500]\n",
            "9129: [discriminator loss: 0.7157502770423889, acc: 0.0703125] [gan loss: 1.278332, acc: 0.000000]\n",
            "9130: [discriminator loss: 0.7201480865478516, acc: 0.34375] [gan loss: 0.796374, acc: 0.281250]\n",
            "9131: [discriminator loss: 0.7511798143386841, acc: 0.09375] [gan loss: 1.270607, acc: 0.000000]\n",
            "9132: [discriminator loss: 0.7259092330932617, acc: 0.3671875] [gan loss: 0.770680, acc: 0.296875]\n",
            "9133: [discriminator loss: 0.7315467596054077, acc: 0.1015625] [gan loss: 1.338693, acc: 0.000000]\n",
            "9134: [discriminator loss: 0.6828510165214539, acc: 0.484375] [gan loss: 0.729470, acc: 0.468750]\n",
            "9135: [discriminator loss: 0.7284606695175171, acc: 0.0390625] [gan loss: 1.364994, acc: 0.000000]\n",
            "9136: [discriminator loss: 0.6691316366195679, acc: 0.4921875] [gan loss: 0.653891, acc: 0.656250]\n",
            "9137: [discriminator loss: 0.7728611826896667, acc: 0.0625] [gan loss: 1.286916, acc: 0.000000]\n",
            "9138: [discriminator loss: 0.709083080291748, acc: 0.3984375] [gan loss: 0.713709, acc: 0.453125]\n",
            "9139: [discriminator loss: 0.7649902701377869, acc: 0.078125] [gan loss: 1.282351, acc: 0.000000]\n",
            "9140: [discriminator loss: 0.6896920800209045, acc: 0.4140625] [gan loss: 0.782800, acc: 0.312500]\n",
            "9141: [discriminator loss: 0.7637243270874023, acc: 0.03125] [gan loss: 1.369473, acc: 0.000000]\n",
            "9142: [discriminator loss: 0.6705343127250671, acc: 0.453125] [gan loss: 0.786222, acc: 0.296875]\n",
            "9143: [discriminator loss: 0.7300513982772827, acc: 0.0859375] [gan loss: 1.307930, acc: 0.015625]\n",
            "9144: [discriminator loss: 0.6830564141273499, acc: 0.4453125] [gan loss: 0.762795, acc: 0.343750]\n",
            "9145: [discriminator loss: 0.7459792494773865, acc: 0.0625] [gan loss: 1.265957, acc: 0.000000]\n",
            "9146: [discriminator loss: 0.7307518720626831, acc: 0.4375] [gan loss: 0.687975, acc: 0.484375]\n",
            "9147: [discriminator loss: 0.7689176201820374, acc: 0.0234375] [gan loss: 1.395019, acc: 0.000000]\n",
            "9148: [discriminator loss: 0.7201041579246521, acc: 0.46875] [gan loss: 0.703350, acc: 0.546875]\n",
            "9149: [discriminator loss: 0.7478047609329224, acc: 0.0390625] [gan loss: 1.220807, acc: 0.000000]\n",
            "9150: [discriminator loss: 0.688152551651001, acc: 0.4140625] [gan loss: 0.855999, acc: 0.140625]\n",
            "9151: [discriminator loss: 0.7320451140403748, acc: 0.0859375] [gan loss: 1.113894, acc: 0.015625]\n",
            "9152: [discriminator loss: 0.6943855881690979, acc: 0.328125] [gan loss: 0.939796, acc: 0.093750]\n",
            "9153: [discriminator loss: 0.713295042514801, acc: 0.1328125] [gan loss: 1.121440, acc: 0.015625]\n",
            "9154: [discriminator loss: 0.6944527626037598, acc: 0.359375] [gan loss: 0.816789, acc: 0.281250]\n",
            "9155: [discriminator loss: 0.7221778035163879, acc: 0.1015625] [gan loss: 1.164260, acc: 0.000000]\n",
            "9156: [discriminator loss: 0.7282655239105225, acc: 0.3359375] [gan loss: 0.914401, acc: 0.109375]\n",
            "9157: [discriminator loss: 0.7150936722755432, acc: 0.1640625] [gan loss: 1.096268, acc: 0.015625]\n",
            "9158: [discriminator loss: 0.6997416019439697, acc: 0.2890625] [gan loss: 0.930227, acc: 0.078125]\n",
            "9159: [discriminator loss: 0.721358597278595, acc: 0.171875] [gan loss: 1.110410, acc: 0.000000]\n",
            "9160: [discriminator loss: 0.7086554169654846, acc: 0.3203125] [gan loss: 0.878700, acc: 0.187500]\n",
            "9161: [discriminator loss: 0.7316089868545532, acc: 0.09375] [gan loss: 1.500276, acc: 0.000000]\n",
            "9162: [discriminator loss: 0.724617600440979, acc: 0.4375] [gan loss: 0.783203, acc: 0.296875]\n",
            "9163: [discriminator loss: 0.7774439454078674, acc: 0.0390625] [gan loss: 1.553545, acc: 0.000000]\n",
            "9164: [discriminator loss: 0.710027813911438, acc: 0.4609375] [gan loss: 0.624175, acc: 0.734375]\n",
            "9165: [discriminator loss: 0.7597335577011108, acc: 0.03125] [gan loss: 1.386159, acc: 0.000000]\n",
            "9166: [discriminator loss: 0.6871523857116699, acc: 0.4921875] [gan loss: 0.700221, acc: 0.515625]\n",
            "9167: [discriminator loss: 0.7671525478363037, acc: 0.03125] [gan loss: 1.376511, acc: 0.000000]\n",
            "9168: [discriminator loss: 0.7018078565597534, acc: 0.4609375] [gan loss: 0.761282, acc: 0.328125]\n",
            "9169: [discriminator loss: 0.7469890117645264, acc: 0.0546875] [gan loss: 1.293727, acc: 0.000000]\n",
            "9170: [discriminator loss: 0.6921314001083374, acc: 0.421875] [gan loss: 0.840818, acc: 0.171875]\n",
            "9171: [discriminator loss: 0.7368109822273254, acc: 0.0546875] [gan loss: 1.383116, acc: 0.000000]\n",
            "9172: [discriminator loss: 0.686179518699646, acc: 0.421875] [gan loss: 0.769247, acc: 0.359375]\n",
            "9173: [discriminator loss: 0.7542412281036377, acc: 0.1015625] [gan loss: 1.180571, acc: 0.000000]\n",
            "9174: [discriminator loss: 0.7032984495162964, acc: 0.375] [gan loss: 0.814523, acc: 0.250000]\n",
            "9175: [discriminator loss: 0.7165179252624512, acc: 0.1171875] [gan loss: 1.151150, acc: 0.015625]\n",
            "9176: [discriminator loss: 0.7012094855308533, acc: 0.359375] [gan loss: 0.854527, acc: 0.140625]\n",
            "9177: [discriminator loss: 0.7369882464408875, acc: 0.125] [gan loss: 1.286173, acc: 0.000000]\n",
            "9178: [discriminator loss: 0.6923729777336121, acc: 0.4375] [gan loss: 0.749592, acc: 0.390625]\n",
            "9179: [discriminator loss: 0.7033005356788635, acc: 0.1171875] [gan loss: 1.208293, acc: 0.000000]\n",
            "9180: [discriminator loss: 0.6789419651031494, acc: 0.4140625] [gan loss: 0.841338, acc: 0.203125]\n",
            "9181: [discriminator loss: 0.7290477752685547, acc: 0.1640625] [gan loss: 1.334074, acc: 0.000000]\n",
            "9182: [discriminator loss: 0.6968480348587036, acc: 0.4140625] [gan loss: 0.760101, acc: 0.265625]\n",
            "9183: [discriminator loss: 0.7280241847038269, acc: 0.046875] [gan loss: 1.384001, acc: 0.000000]\n",
            "9184: [discriminator loss: 0.6949376463890076, acc: 0.421875] [gan loss: 0.722017, acc: 0.484375]\n",
            "9185: [discriminator loss: 0.7635626792907715, acc: 0.0390625] [gan loss: 1.485464, acc: 0.000000]\n",
            "9186: [discriminator loss: 0.7114412784576416, acc: 0.4765625] [gan loss: 0.609695, acc: 0.796875]\n",
            "9187: [discriminator loss: 0.7663834095001221, acc: 0.03125] [gan loss: 1.441258, acc: 0.000000]\n",
            "9188: [discriminator loss: 0.6950021982192993, acc: 0.4765625] [gan loss: 0.744815, acc: 0.453125]\n",
            "9189: [discriminator loss: 0.7866852283477783, acc: 0.0390625] [gan loss: 1.344026, acc: 0.000000]\n",
            "9190: [discriminator loss: 0.7258922457695007, acc: 0.4375] [gan loss: 0.772367, acc: 0.296875]\n",
            "9191: [discriminator loss: 0.7614721059799194, acc: 0.03125] [gan loss: 1.278556, acc: 0.000000]\n",
            "9192: [discriminator loss: 0.6919494271278381, acc: 0.453125] [gan loss: 0.817031, acc: 0.203125]\n",
            "9193: [discriminator loss: 0.7325355410575867, acc: 0.0859375] [gan loss: 1.133932, acc: 0.015625]\n",
            "9194: [discriminator loss: 0.6919797658920288, acc: 0.3203125] [gan loss: 0.867512, acc: 0.156250]\n",
            "9195: [discriminator loss: 0.7390286922454834, acc: 0.125] [gan loss: 1.173498, acc: 0.000000]\n",
            "9196: [discriminator loss: 0.6857349276542664, acc: 0.359375] [gan loss: 0.927920, acc: 0.093750]\n",
            "9197: [discriminator loss: 0.707818329334259, acc: 0.1953125] [gan loss: 1.126913, acc: 0.000000]\n",
            "9198: [discriminator loss: 0.7136535048484802, acc: 0.3203125] [gan loss: 0.840992, acc: 0.218750]\n",
            "9199: [discriminator loss: 0.7370064854621887, acc: 0.1171875] [gan loss: 1.156305, acc: 0.000000]\n",
            "9200: [discriminator loss: 0.7109156847000122, acc: 0.3203125] [gan loss: 0.883320, acc: 0.171875]\n",
            "9201: [discriminator loss: 0.7270944714546204, acc: 0.1875] [gan loss: 1.171530, acc: 0.000000]\n",
            "9202: [discriminator loss: 0.695275068283081, acc: 0.375] [gan loss: 0.873990, acc: 0.187500]\n",
            "9203: [discriminator loss: 0.7403731942176819, acc: 0.078125] [gan loss: 1.364082, acc: 0.000000]\n",
            "9204: [discriminator loss: 0.6858871579170227, acc: 0.4921875] [gan loss: 0.680300, acc: 0.562500]\n",
            "9205: [discriminator loss: 0.740864634513855, acc: 0.0234375] [gan loss: 1.444084, acc: 0.000000]\n",
            "9206: [discriminator loss: 0.7010365724563599, acc: 0.46875] [gan loss: 0.695261, acc: 0.500000]\n",
            "9207: [discriminator loss: 0.7596771717071533, acc: 0.046875] [gan loss: 1.394914, acc: 0.000000]\n",
            "9208: [discriminator loss: 0.6954044103622437, acc: 0.4375] [gan loss: 0.742865, acc: 0.453125]\n",
            "9209: [discriminator loss: 0.742035984992981, acc: 0.0546875] [gan loss: 1.377331, acc: 0.000000]\n",
            "9210: [discriminator loss: 0.6803683042526245, acc: 0.4296875] [gan loss: 0.824100, acc: 0.281250]\n",
            "9211: [discriminator loss: 0.7141392827033997, acc: 0.109375] [gan loss: 1.249652, acc: 0.000000]\n",
            "9212: [discriminator loss: 0.6655951142311096, acc: 0.328125] [gan loss: 0.857984, acc: 0.203125]\n",
            "9213: [discriminator loss: 0.7848523259162903, acc: 0.09375] [gan loss: 1.372059, acc: 0.015625]\n",
            "9214: [discriminator loss: 0.6843308210372925, acc: 0.40625] [gan loss: 0.841795, acc: 0.156250]\n",
            "9215: [discriminator loss: 0.7165480852127075, acc: 0.1484375] [gan loss: 1.308569, acc: 0.000000]\n",
            "9216: [discriminator loss: 0.7213699817657471, acc: 0.3203125] [gan loss: 0.987654, acc: 0.062500]\n",
            "9217: [discriminator loss: 0.7060391902923584, acc: 0.265625] [gan loss: 1.012129, acc: 0.031250]\n",
            "9218: [discriminator loss: 0.7091242671012878, acc: 0.25] [gan loss: 0.986211, acc: 0.015625]\n",
            "9219: [discriminator loss: 0.7022268772125244, acc: 0.2421875] [gan loss: 1.111025, acc: 0.000000]\n",
            "9220: [discriminator loss: 0.6974798440933228, acc: 0.28125] [gan loss: 1.191825, acc: 0.000000]\n",
            "9221: [discriminator loss: 0.6910264492034912, acc: 0.328125] [gan loss: 0.927031, acc: 0.078125]\n",
            "9222: [discriminator loss: 0.7547202110290527, acc: 0.0703125] [gan loss: 1.315654, acc: 0.000000]\n",
            "9223: [discriminator loss: 0.6967645883560181, acc: 0.453125] [gan loss: 0.705202, acc: 0.437500]\n",
            "9224: [discriminator loss: 0.7468076944351196, acc: 0.03125] [gan loss: 1.411018, acc: 0.000000]\n",
            "9225: [discriminator loss: 0.7117725610733032, acc: 0.4453125] [gan loss: 0.653837, acc: 0.578125]\n",
            "9226: [discriminator loss: 0.7621887922286987, acc: 0.03125] [gan loss: 1.441669, acc: 0.000000]\n",
            "9227: [discriminator loss: 0.7276929616928101, acc: 0.4765625] [gan loss: 0.605432, acc: 0.718750]\n",
            "9228: [discriminator loss: 0.7809974551200867, acc: 0.015625] [gan loss: 1.357386, acc: 0.000000]\n",
            "9229: [discriminator loss: 0.7138805389404297, acc: 0.4609375] [gan loss: 0.685904, acc: 0.500000]\n",
            "9230: [discriminator loss: 0.7778282165527344, acc: 0.0703125] [gan loss: 1.258721, acc: 0.000000]\n",
            "9231: [discriminator loss: 0.6831716299057007, acc: 0.4296875] [gan loss: 0.821760, acc: 0.265625]\n",
            "9232: [discriminator loss: 0.6960956454277039, acc: 0.1484375] [gan loss: 1.066197, acc: 0.015625]\n",
            "9233: [discriminator loss: 0.7068726420402527, acc: 0.234375] [gan loss: 1.022368, acc: 0.046875]\n",
            "9234: [discriminator loss: 0.7085201740264893, acc: 0.234375] [gan loss: 1.084088, acc: 0.031250]\n",
            "9235: [discriminator loss: 0.7113837003707886, acc: 0.3046875] [gan loss: 0.905665, acc: 0.093750]\n",
            "9236: [discriminator loss: 0.7330620288848877, acc: 0.171875] [gan loss: 1.176165, acc: 0.000000]\n",
            "9237: [discriminator loss: 0.7086460590362549, acc: 0.3359375] [gan loss: 0.960383, acc: 0.015625]\n",
            "9238: [discriminator loss: 0.7500013113021851, acc: 0.078125] [gan loss: 1.303772, acc: 0.000000]\n",
            "9239: [discriminator loss: 0.7117286920547485, acc: 0.421875] [gan loss: 0.813797, acc: 0.140625]\n",
            "9240: [discriminator loss: 0.7127014994621277, acc: 0.09375] [gan loss: 1.376077, acc: 0.000000]\n",
            "9241: [discriminator loss: 0.7028889656066895, acc: 0.4609375] [gan loss: 0.763055, acc: 0.375000]\n",
            "9242: [discriminator loss: 0.7505642175674438, acc: 0.015625] [gan loss: 1.490242, acc: 0.000000]\n",
            "9243: [discriminator loss: 0.7033482789993286, acc: 0.4765625] [gan loss: 0.643839, acc: 0.656250]\n",
            "9244: [discriminator loss: 0.7513301372528076, acc: 0.03125] [gan loss: 1.206640, acc: 0.000000]\n",
            "9245: [discriminator loss: 0.6995277404785156, acc: 0.3671875] [gan loss: 0.813182, acc: 0.281250]\n",
            "9246: [discriminator loss: 0.7224404811859131, acc: 0.171875] [gan loss: 1.190204, acc: 0.015625]\n",
            "9247: [discriminator loss: 0.7069146037101746, acc: 0.34375] [gan loss: 0.953786, acc: 0.125000]\n",
            "9248: [discriminator loss: 0.7043159008026123, acc: 0.25] [gan loss: 1.117899, acc: 0.031250]\n",
            "9249: [discriminator loss: 0.7055524587631226, acc: 0.328125] [gan loss: 0.940672, acc: 0.093750]\n",
            "9250: [discriminator loss: 0.6965894103050232, acc: 0.2265625] [gan loss: 1.087248, acc: 0.078125]\n",
            "9251: [discriminator loss: 0.7118566036224365, acc: 0.3359375] [gan loss: 0.893571, acc: 0.125000]\n",
            "9252: [discriminator loss: 0.7179162502288818, acc: 0.1171875] [gan loss: 1.140901, acc: 0.046875]\n",
            "9253: [discriminator loss: 0.6994493007659912, acc: 0.3984375] [gan loss: 0.739713, acc: 0.390625]\n",
            "9254: [discriminator loss: 0.7455836534500122, acc: 0.09375] [gan loss: 1.448873, acc: 0.000000]\n",
            "9255: [discriminator loss: 0.7333227396011353, acc: 0.3984375] [gan loss: 0.678122, acc: 0.578125]\n",
            "9256: [discriminator loss: 0.788189709186554, acc: 0.0234375] [gan loss: 1.656160, acc: 0.000000]\n",
            "9257: [discriminator loss: 0.696733832359314, acc: 0.484375] [gan loss: 0.630453, acc: 0.703125]\n",
            "9258: [discriminator loss: 0.7873949408531189, acc: 0.0234375] [gan loss: 1.404632, acc: 0.000000]\n",
            "9259: [discriminator loss: 0.6885928511619568, acc: 0.4375] [gan loss: 0.757679, acc: 0.406250]\n",
            "9260: [discriminator loss: 0.7357298135757446, acc: 0.1015625] [gan loss: 1.188353, acc: 0.015625]\n",
            "9261: [discriminator loss: 0.6835987567901611, acc: 0.3671875] [gan loss: 0.864516, acc: 0.156250]\n",
            "9262: [discriminator loss: 0.7383779287338257, acc: 0.1328125] [gan loss: 1.315908, acc: 0.000000]\n",
            "9263: [discriminator loss: 0.7054052352905273, acc: 0.3828125] [gan loss: 0.852623, acc: 0.156250]\n",
            "9264: [discriminator loss: 0.7412554025650024, acc: 0.0703125] [gan loss: 1.430934, acc: 0.000000]\n",
            "9265: [discriminator loss: 0.6936202645301819, acc: 0.40625] [gan loss: 0.802809, acc: 0.265625]\n",
            "9266: [discriminator loss: 0.7407870888710022, acc: 0.078125] [gan loss: 1.401461, acc: 0.000000]\n",
            "9267: [discriminator loss: 0.707811713218689, acc: 0.3671875] [gan loss: 0.739750, acc: 0.343750]\n",
            "9268: [discriminator loss: 0.7838313579559326, acc: 0.078125] [gan loss: 1.441729, acc: 0.000000]\n",
            "9269: [discriminator loss: 0.7010154724121094, acc: 0.484375] [gan loss: 0.697336, acc: 0.515625]\n",
            "9270: [discriminator loss: 0.7598432302474976, acc: 0.0546875] [gan loss: 1.352262, acc: 0.000000]\n",
            "9271: [discriminator loss: 0.7037087082862854, acc: 0.453125] [gan loss: 0.675739, acc: 0.593750]\n",
            "9272: [discriminator loss: 0.784873366355896, acc: 0.0078125] [gan loss: 1.338160, acc: 0.000000]\n",
            "9273: [discriminator loss: 0.7110692262649536, acc: 0.4296875] [gan loss: 0.851196, acc: 0.187500]\n",
            "9274: [discriminator loss: 0.7511464357376099, acc: 0.0703125] [gan loss: 1.428111, acc: 0.000000]\n",
            "9275: [discriminator loss: 0.6935666799545288, acc: 0.453125] [gan loss: 0.705642, acc: 0.531250]\n",
            "9276: [discriminator loss: 0.7390555739402771, acc: 0.03125] [gan loss: 1.210567, acc: 0.000000]\n",
            "9277: [discriminator loss: 0.6936811208724976, acc: 0.375] [gan loss: 0.823519, acc: 0.265625]\n",
            "9278: [discriminator loss: 0.7204990386962891, acc: 0.078125] [gan loss: 1.216869, acc: 0.000000]\n",
            "9279: [discriminator loss: 0.6972817778587341, acc: 0.3203125] [gan loss: 0.952846, acc: 0.156250]\n",
            "9280: [discriminator loss: 0.7087407112121582, acc: 0.203125] [gan loss: 1.131483, acc: 0.015625]\n",
            "9281: [discriminator loss: 0.6896870136260986, acc: 0.34375] [gan loss: 0.901437, acc: 0.093750]\n",
            "9282: [discriminator loss: 0.6993862390518188, acc: 0.171875] [gan loss: 1.115408, acc: 0.000000]\n",
            "9283: [discriminator loss: 0.6913386583328247, acc: 0.3203125] [gan loss: 0.983693, acc: 0.015625]\n",
            "9284: [discriminator loss: 0.7302161455154419, acc: 0.125] [gan loss: 1.297710, acc: 0.000000]\n",
            "9285: [discriminator loss: 0.7109866142272949, acc: 0.3671875] [gan loss: 0.831150, acc: 0.140625]\n",
            "9286: [discriminator loss: 0.7259567379951477, acc: 0.0625] [gan loss: 1.386909, acc: 0.000000]\n",
            "9287: [discriminator loss: 0.7218906283378601, acc: 0.421875] [gan loss: 0.644630, acc: 0.578125]\n",
            "9288: [discriminator loss: 0.7956628799438477, acc: 0.0234375] [gan loss: 1.669513, acc: 0.000000]\n",
            "9289: [discriminator loss: 0.7220877408981323, acc: 0.5] [gan loss: 0.571048, acc: 0.828125]\n",
            "9290: [discriminator loss: 0.814144492149353, acc: 0.0078125] [gan loss: 1.392084, acc: 0.000000]\n",
            "9291: [discriminator loss: 0.7038337588310242, acc: 0.4609375] [gan loss: 0.698177, acc: 0.515625]\n",
            "9292: [discriminator loss: 0.7687295079231262, acc: 0.0625] [gan loss: 1.199350, acc: 0.000000]\n",
            "9293: [discriminator loss: 0.6908227205276489, acc: 0.359375] [gan loss: 0.891827, acc: 0.203125]\n",
            "9294: [discriminator loss: 0.7349448204040527, acc: 0.1328125] [gan loss: 1.045889, acc: 0.015625]\n",
            "9295: [discriminator loss: 0.6989561319351196, acc: 0.28125] [gan loss: 0.956931, acc: 0.109375]\n",
            "9296: [discriminator loss: 0.7456648349761963, acc: 0.1484375] [gan loss: 1.123528, acc: 0.000000]\n",
            "9297: [discriminator loss: 0.6931601166725159, acc: 0.296875] [gan loss: 0.962914, acc: 0.093750]\n",
            "9298: [discriminator loss: 0.7139801383018494, acc: 0.1796875] [gan loss: 1.148342, acc: 0.000000]\n",
            "9299: [discriminator loss: 0.6852254867553711, acc: 0.359375] [gan loss: 0.895696, acc: 0.171875]\n",
            "9300: [discriminator loss: 0.7226672172546387, acc: 0.09375] [gan loss: 1.271658, acc: 0.015625]\n",
            "9301: [discriminator loss: 0.7144493460655212, acc: 0.359375] [gan loss: 0.826856, acc: 0.156250]\n",
            "9302: [discriminator loss: 0.7349162101745605, acc: 0.109375] [gan loss: 1.389983, acc: 0.000000]\n",
            "9303: [discriminator loss: 0.7070069313049316, acc: 0.4375] [gan loss: 0.752720, acc: 0.312500]\n",
            "9304: [discriminator loss: 0.7696092128753662, acc: 0.046875] [gan loss: 1.351126, acc: 0.000000]\n",
            "9305: [discriminator loss: 0.7160919904708862, acc: 0.4609375] [gan loss: 0.713054, acc: 0.468750]\n",
            "9306: [discriminator loss: 0.7405274510383606, acc: 0.0390625] [gan loss: 1.322941, acc: 0.000000]\n",
            "9307: [discriminator loss: 0.6987510919570923, acc: 0.46875] [gan loss: 0.833152, acc: 0.218750]\n",
            "9308: [discriminator loss: 0.7335073947906494, acc: 0.0859375] [gan loss: 1.373629, acc: 0.000000]\n",
            "9309: [discriminator loss: 0.7060114741325378, acc: 0.3671875] [gan loss: 0.825184, acc: 0.218750]\n",
            "9310: [discriminator loss: 0.7444467544555664, acc: 0.03125] [gan loss: 1.197132, acc: 0.015625]\n",
            "9311: [discriminator loss: 0.7025690674781799, acc: 0.3828125] [gan loss: 0.799028, acc: 0.234375]\n",
            "9312: [discriminator loss: 0.7455514669418335, acc: 0.0859375] [gan loss: 1.241637, acc: 0.000000]\n",
            "9313: [discriminator loss: 0.7041100263595581, acc: 0.4375] [gan loss: 0.676563, acc: 0.562500]\n",
            "9314: [discriminator loss: 0.7372580766677856, acc: 0.1015625] [gan loss: 1.312195, acc: 0.015625]\n",
            "9315: [discriminator loss: 0.716090977191925, acc: 0.40625] [gan loss: 0.776063, acc: 0.328125]\n",
            "9316: [discriminator loss: 0.7746200561523438, acc: 0.078125] [gan loss: 1.291770, acc: 0.000000]\n",
            "9317: [discriminator loss: 0.7132291793823242, acc: 0.4453125] [gan loss: 0.629877, acc: 0.734375]\n",
            "9318: [discriminator loss: 0.8134094476699829, acc: 0.0078125] [gan loss: 1.556958, acc: 0.000000]\n",
            "9319: [discriminator loss: 0.7293657660484314, acc: 0.4765625] [gan loss: 0.624088, acc: 0.750000]\n",
            "9320: [discriminator loss: 0.7839961051940918, acc: 0.0234375] [gan loss: 1.382017, acc: 0.000000]\n",
            "9321: [discriminator loss: 0.7303557395935059, acc: 0.4296875] [gan loss: 0.703636, acc: 0.500000]\n",
            "9322: [discriminator loss: 0.7686164379119873, acc: 0.078125] [gan loss: 1.255205, acc: 0.000000]\n",
            "9323: [discriminator loss: 0.7241302728652954, acc: 0.3515625] [gan loss: 0.886960, acc: 0.125000]\n",
            "9324: [discriminator loss: 0.7342033386230469, acc: 0.1328125] [gan loss: 1.129532, acc: 0.031250]\n",
            "9325: [discriminator loss: 0.7157971858978271, acc: 0.2734375] [gan loss: 0.993098, acc: 0.015625]\n",
            "9326: [discriminator loss: 0.706180214881897, acc: 0.1796875] [gan loss: 1.081732, acc: 0.015625]\n",
            "9327: [discriminator loss: 0.6997491121292114, acc: 0.2578125] [gan loss: 1.068588, acc: 0.000000]\n",
            "9328: [discriminator loss: 0.7086023092269897, acc: 0.2421875] [gan loss: 0.994409, acc: 0.031250]\n",
            "9329: [discriminator loss: 0.7100203037261963, acc: 0.15625] [gan loss: 1.259564, acc: 0.000000]\n",
            "9330: [discriminator loss: 0.72172611951828, acc: 0.28125] [gan loss: 1.004223, acc: 0.000000]\n",
            "9331: [discriminator loss: 0.7106329202651978, acc: 0.1953125] [gan loss: 1.273836, acc: 0.000000]\n",
            "9332: [discriminator loss: 0.6894722580909729, acc: 0.3671875] [gan loss: 0.882918, acc: 0.140625]\n",
            "9333: [discriminator loss: 0.7394477725028992, acc: 0.0859375] [gan loss: 1.306568, acc: 0.000000]\n",
            "9334: [discriminator loss: 0.7087885141372681, acc: 0.40625] [gan loss: 0.731601, acc: 0.421875]\n",
            "9335: [discriminator loss: 0.7587890028953552, acc: 0.078125] [gan loss: 1.340334, acc: 0.000000]\n",
            "9336: [discriminator loss: 0.7121388912200928, acc: 0.453125] [gan loss: 0.748084, acc: 0.406250]\n",
            "9337: [discriminator loss: 0.7637428045272827, acc: 0.046875] [gan loss: 1.488259, acc: 0.000000]\n",
            "9338: [discriminator loss: 0.7091836333274841, acc: 0.4609375] [gan loss: 0.619774, acc: 0.687500]\n",
            "9339: [discriminator loss: 0.772588849067688, acc: 0.0] [gan loss: 1.312117, acc: 0.000000]\n",
            "9340: [discriminator loss: 0.720381498336792, acc: 0.4375] [gan loss: 0.744610, acc: 0.390625]\n",
            "9341: [discriminator loss: 0.7501764893531799, acc: 0.0390625] [gan loss: 1.367223, acc: 0.000000]\n",
            "9342: [discriminator loss: 0.703013002872467, acc: 0.46875] [gan loss: 0.737832, acc: 0.437500]\n",
            "9343: [discriminator loss: 0.7558764815330505, acc: 0.03125] [gan loss: 1.407274, acc: 0.000000]\n",
            "9344: [discriminator loss: 0.6894994974136353, acc: 0.46875] [gan loss: 0.696127, acc: 0.484375]\n",
            "9345: [discriminator loss: 0.7359769344329834, acc: 0.078125] [gan loss: 1.255404, acc: 0.000000]\n",
            "9346: [discriminator loss: 0.7082303762435913, acc: 0.375] [gan loss: 0.878041, acc: 0.171875]\n",
            "9347: [discriminator loss: 0.7464274168014526, acc: 0.140625] [gan loss: 1.076288, acc: 0.046875]\n",
            "9348: [discriminator loss: 0.6839461326599121, acc: 0.296875] [gan loss: 0.980431, acc: 0.046875]\n",
            "9349: [discriminator loss: 0.7212832570075989, acc: 0.2109375] [gan loss: 0.957828, acc: 0.031250]\n",
            "9350: [discriminator loss: 0.7005838751792908, acc: 0.2421875] [gan loss: 1.071349, acc: 0.031250]\n",
            "9351: [discriminator loss: 0.7072737216949463, acc: 0.3046875] [gan loss: 0.985494, acc: 0.031250]\n",
            "9352: [discriminator loss: 0.7358287572860718, acc: 0.125] [gan loss: 1.299914, acc: 0.000000]\n",
            "9353: [discriminator loss: 0.6842736005783081, acc: 0.4296875] [gan loss: 0.800550, acc: 0.281250]\n",
            "9354: [discriminator loss: 0.736391007900238, acc: 0.109375] [gan loss: 1.589757, acc: 0.000000]\n",
            "9355: [discriminator loss: 0.6951025724411011, acc: 0.46875] [gan loss: 0.629389, acc: 0.703125]\n",
            "9356: [discriminator loss: 0.7613893151283264, acc: 0.0625] [gan loss: 1.527833, acc: 0.000000]\n",
            "9357: [discriminator loss: 0.7244577407836914, acc: 0.4765625] [gan loss: 0.681730, acc: 0.578125]\n",
            "9358: [discriminator loss: 0.757671594619751, acc: 0.0390625] [gan loss: 1.344650, acc: 0.000000]\n",
            "9359: [discriminator loss: 0.711430549621582, acc: 0.4765625] [gan loss: 0.784602, acc: 0.343750]\n",
            "9360: [discriminator loss: 0.7413440942764282, acc: 0.0703125] [gan loss: 1.202895, acc: 0.015625]\n",
            "9361: [discriminator loss: 0.695061981678009, acc: 0.4375] [gan loss: 0.810538, acc: 0.234375]\n",
            "9362: [discriminator loss: 0.7598187923431396, acc: 0.078125] [gan loss: 1.311393, acc: 0.000000]\n",
            "9363: [discriminator loss: 0.7188695669174194, acc: 0.4375] [gan loss: 0.759821, acc: 0.328125]\n",
            "9364: [discriminator loss: 0.7408447265625, acc: 0.0703125] [gan loss: 1.146185, acc: 0.000000]\n",
            "9365: [discriminator loss: 0.6975297927856445, acc: 0.375] [gan loss: 0.888550, acc: 0.093750]\n",
            "9366: [discriminator loss: 0.7157391309738159, acc: 0.140625] [gan loss: 1.263816, acc: 0.000000]\n",
            "9367: [discriminator loss: 0.6986274123191833, acc: 0.4375] [gan loss: 0.847902, acc: 0.203125]\n",
            "9368: [discriminator loss: 0.7605381608009338, acc: 0.0703125] [gan loss: 1.254987, acc: 0.000000]\n",
            "9369: [discriminator loss: 0.7204850912094116, acc: 0.390625] [gan loss: 0.825690, acc: 0.218750]\n",
            "9370: [discriminator loss: 0.7151097059249878, acc: 0.1640625] [gan loss: 1.192842, acc: 0.000000]\n",
            "9371: [discriminator loss: 0.7131403684616089, acc: 0.390625] [gan loss: 0.809821, acc: 0.250000]\n",
            "9372: [discriminator loss: 0.7556275725364685, acc: 0.1171875] [gan loss: 1.379746, acc: 0.000000]\n",
            "9373: [discriminator loss: 0.6952701807022095, acc: 0.4609375] [gan loss: 0.712643, acc: 0.421875]\n",
            "9374: [discriminator loss: 0.7590512037277222, acc: 0.015625] [gan loss: 1.472358, acc: 0.000000]\n",
            "9375: [discriminator loss: 0.7124758958816528, acc: 0.484375] [gan loss: 0.671162, acc: 0.562500]\n",
            "9376: [discriminator loss: 0.7633715271949768, acc: 0.03125] [gan loss: 1.426332, acc: 0.000000]\n",
            "9377: [discriminator loss: 0.6925593614578247, acc: 0.453125] [gan loss: 0.755991, acc: 0.359375]\n",
            "9378: [discriminator loss: 0.7485989332199097, acc: 0.0390625] [gan loss: 1.242410, acc: 0.000000]\n",
            "9379: [discriminator loss: 0.6814466118812561, acc: 0.40625] [gan loss: 0.846648, acc: 0.218750]\n",
            "9380: [discriminator loss: 0.7363753914833069, acc: 0.125] [gan loss: 1.302463, acc: 0.000000]\n",
            "9381: [discriminator loss: 0.6939301490783691, acc: 0.421875] [gan loss: 0.874877, acc: 0.203125]\n",
            "9382: [discriminator loss: 0.7113146781921387, acc: 0.125] [gan loss: 1.269734, acc: 0.000000]\n",
            "9383: [discriminator loss: 0.695961594581604, acc: 0.421875] [gan loss: 0.764936, acc: 0.328125]\n",
            "9384: [discriminator loss: 0.766219973564148, acc: 0.0703125] [gan loss: 1.313148, acc: 0.000000]\n",
            "9385: [discriminator loss: 0.7013565301895142, acc: 0.4296875] [gan loss: 0.753525, acc: 0.312500]\n",
            "9386: [discriminator loss: 0.7460155487060547, acc: 0.078125] [gan loss: 1.253957, acc: 0.000000]\n",
            "9387: [discriminator loss: 0.704068124294281, acc: 0.390625] [gan loss: 0.834377, acc: 0.203125]\n",
            "9388: [discriminator loss: 0.7494230270385742, acc: 0.046875] [gan loss: 1.170937, acc: 0.000000]\n",
            "9389: [discriminator loss: 0.7096438407897949, acc: 0.3203125] [gan loss: 0.998767, acc: 0.062500]\n",
            "9390: [discriminator loss: 0.7121063470840454, acc: 0.21875] [gan loss: 1.161628, acc: 0.000000]\n",
            "9391: [discriminator loss: 0.68958979845047, acc: 0.359375] [gan loss: 0.920897, acc: 0.093750]\n",
            "9392: [discriminator loss: 0.7348905801773071, acc: 0.1328125] [gan loss: 1.361221, acc: 0.000000]\n",
            "9393: [discriminator loss: 0.6956589818000793, acc: 0.4140625] [gan loss: 0.824072, acc: 0.296875]\n",
            "9394: [discriminator loss: 0.7037243247032166, acc: 0.1171875] [gan loss: 1.214731, acc: 0.000000]\n",
            "9395: [discriminator loss: 0.7008921504020691, acc: 0.3984375] [gan loss: 0.756105, acc: 0.343750]\n",
            "9396: [discriminator loss: 0.7474186420440674, acc: 0.0625] [gan loss: 1.362084, acc: 0.000000]\n",
            "9397: [discriminator loss: 0.7221797704696655, acc: 0.4296875] [gan loss: 0.744802, acc: 0.437500]\n",
            "9398: [discriminator loss: 0.7352159023284912, acc: 0.046875] [gan loss: 1.276943, acc: 0.000000]\n",
            "9399: [discriminator loss: 0.6727794408798218, acc: 0.4375] [gan loss: 0.731758, acc: 0.500000]\n",
            "9400: [discriminator loss: 0.7893763184547424, acc: 0.0546875] [gan loss: 1.400017, acc: 0.000000]\n",
            "9401: [discriminator loss: 0.7248281240463257, acc: 0.4453125] [gan loss: 0.700461, acc: 0.515625]\n",
            "9402: [discriminator loss: 0.7593154907226562, acc: 0.03125] [gan loss: 1.166580, acc: 0.015625]\n",
            "9403: [discriminator loss: 0.7087961435317993, acc: 0.3359375] [gan loss: 1.057879, acc: 0.015625]\n",
            "9404: [discriminator loss: 0.677807629108429, acc: 0.28125] [gan loss: 0.963143, acc: 0.156250]\n",
            "9405: [discriminator loss: 0.7165420055389404, acc: 0.2890625] [gan loss: 1.153434, acc: 0.015625]\n",
            "9406: [discriminator loss: 0.7108892202377319, acc: 0.3046875] [gan loss: 0.957769, acc: 0.062500]\n",
            "9407: [discriminator loss: 0.7062755823135376, acc: 0.1796875] [gan loss: 1.256844, acc: 0.000000]\n",
            "9408: [discriminator loss: 0.683499276638031, acc: 0.40625] [gan loss: 0.827010, acc: 0.203125]\n",
            "9409: [discriminator loss: 0.7451779246330261, acc: 0.0859375] [gan loss: 1.394713, acc: 0.000000]\n",
            "9410: [discriminator loss: 0.7242141962051392, acc: 0.453125] [gan loss: 0.692300, acc: 0.515625]\n",
            "9411: [discriminator loss: 0.7857851982116699, acc: 0.0078125] [gan loss: 1.652885, acc: 0.000000]\n",
            "9412: [discriminator loss: 0.7049823999404907, acc: 0.4921875] [gan loss: 0.678893, acc: 0.625000]\n",
            "9413: [discriminator loss: 0.7670834064483643, acc: 0.03125] [gan loss: 1.306480, acc: 0.000000]\n",
            "9414: [discriminator loss: 0.7029497623443604, acc: 0.453125] [gan loss: 0.743673, acc: 0.343750]\n",
            "9415: [discriminator loss: 0.8162850141525269, acc: 0.015625] [gan loss: 1.374433, acc: 0.000000]\n",
            "9416: [discriminator loss: 0.6746504306793213, acc: 0.4921875] [gan loss: 0.727684, acc: 0.484375]\n",
            "9417: [discriminator loss: 0.7383725643157959, acc: 0.046875] [gan loss: 1.286525, acc: 0.000000]\n",
            "9418: [discriminator loss: 0.7308566570281982, acc: 0.3515625] [gan loss: 0.903860, acc: 0.062500]\n",
            "9419: [discriminator loss: 0.7367181181907654, acc: 0.09375] [gan loss: 1.181837, acc: 0.015625]\n",
            "9420: [discriminator loss: 0.6957848072052002, acc: 0.40625] [gan loss: 0.781130, acc: 0.281250]\n",
            "9421: [discriminator loss: 0.7858623266220093, acc: 0.0546875] [gan loss: 1.268820, acc: 0.000000]\n",
            "9422: [discriminator loss: 0.6892589926719666, acc: 0.4609375] [gan loss: 0.758860, acc: 0.343750]\n",
            "9423: [discriminator loss: 0.7376410365104675, acc: 0.0546875] [gan loss: 1.151623, acc: 0.015625]\n",
            "9424: [discriminator loss: 0.7311078310012817, acc: 0.296875] [gan loss: 0.888882, acc: 0.125000]\n",
            "9425: [discriminator loss: 0.7281394004821777, acc: 0.0859375] [gan loss: 1.129406, acc: 0.000000]\n",
            "9426: [discriminator loss: 0.7063880562782288, acc: 0.2890625] [gan loss: 0.979196, acc: 0.062500]\n",
            "9427: [discriminator loss: 0.7287164330482483, acc: 0.171875] [gan loss: 1.138529, acc: 0.000000]\n",
            "9428: [discriminator loss: 0.7044444680213928, acc: 0.359375] [gan loss: 0.954519, acc: 0.015625]\n",
            "9429: [discriminator loss: 0.7277525067329407, acc: 0.140625] [gan loss: 1.297621, acc: 0.000000]\n",
            "9430: [discriminator loss: 0.6857801675796509, acc: 0.40625] [gan loss: 0.810692, acc: 0.250000]\n",
            "9431: [discriminator loss: 0.7552294731140137, acc: 0.046875] [gan loss: 1.402980, acc: 0.000000]\n",
            "9432: [discriminator loss: 0.6891900897026062, acc: 0.484375] [gan loss: 0.618119, acc: 0.750000]\n",
            "9433: [discriminator loss: 0.8016650676727295, acc: 0.0] [gan loss: 1.676709, acc: 0.000000]\n",
            "9434: [discriminator loss: 0.7367045879364014, acc: 0.4921875] [gan loss: 0.592917, acc: 0.734375]\n",
            "9435: [discriminator loss: 0.7709886431694031, acc: 0.0078125] [gan loss: 1.297358, acc: 0.000000]\n",
            "9436: [discriminator loss: 0.699059247970581, acc: 0.46875] [gan loss: 0.755740, acc: 0.343750]\n",
            "9437: [discriminator loss: 0.7200635671615601, acc: 0.0859375] [gan loss: 1.154862, acc: 0.015625]\n",
            "9438: [discriminator loss: 0.7119627594947815, acc: 0.3359375] [gan loss: 0.874857, acc: 0.093750]\n",
            "9439: [discriminator loss: 0.719924807548523, acc: 0.1640625] [gan loss: 1.083271, acc: 0.031250]\n",
            "9440: [discriminator loss: 0.7274640798568726, acc: 0.234375] [gan loss: 1.028425, acc: 0.000000]\n",
            "9441: [discriminator loss: 0.7079823017120361, acc: 0.234375] [gan loss: 1.076398, acc: 0.046875]\n",
            "9442: [discriminator loss: 0.7324795722961426, acc: 0.25] [gan loss: 1.001460, acc: 0.015625]\n",
            "9443: [discriminator loss: 0.7029547095298767, acc: 0.25] [gan loss: 0.989533, acc: 0.031250]\n",
            "9444: [discriminator loss: 0.7017898559570312, acc: 0.2734375] [gan loss: 1.022226, acc: 0.000000]\n",
            "9445: [discriminator loss: 0.7192021608352661, acc: 0.2421875] [gan loss: 1.050539, acc: 0.015625]\n",
            "9446: [discriminator loss: 0.6899425983428955, acc: 0.2421875] [gan loss: 1.045346, acc: 0.015625]\n",
            "9447: [discriminator loss: 0.7158781290054321, acc: 0.2265625] [gan loss: 1.086153, acc: 0.000000]\n",
            "9448: [discriminator loss: 0.7354722023010254, acc: 0.2265625] [gan loss: 0.988893, acc: 0.031250]\n",
            "9449: [discriminator loss: 0.7301774024963379, acc: 0.15625] [gan loss: 1.270288, acc: 0.000000]\n",
            "9450: [discriminator loss: 0.700755774974823, acc: 0.375] [gan loss: 0.801156, acc: 0.296875]\n",
            "9451: [discriminator loss: 0.7439494132995605, acc: 0.078125] [gan loss: 1.512794, acc: 0.000000]\n",
            "9452: [discriminator loss: 0.7098352313041687, acc: 0.484375] [gan loss: 0.591506, acc: 0.812500]\n",
            "9453: [discriminator loss: 0.8131469488143921, acc: 0.0] [gan loss: 1.574392, acc: 0.000000]\n",
            "9454: [discriminator loss: 0.7317825555801392, acc: 0.5] [gan loss: 0.624964, acc: 0.765625]\n",
            "9455: [discriminator loss: 0.780582070350647, acc: 0.0] [gan loss: 1.289195, acc: 0.000000]\n",
            "9456: [discriminator loss: 0.6815177202224731, acc: 0.453125] [gan loss: 0.797782, acc: 0.250000]\n",
            "9457: [discriminator loss: 0.7445327043533325, acc: 0.0859375] [gan loss: 1.187821, acc: 0.000000]\n",
            "9458: [discriminator loss: 0.6911033987998962, acc: 0.390625] [gan loss: 0.888533, acc: 0.171875]\n",
            "9459: [discriminator loss: 0.7391791939735413, acc: 0.1640625] [gan loss: 1.063537, acc: 0.078125]\n",
            "9460: [discriminator loss: 0.7006725668907166, acc: 0.296875] [gan loss: 0.971238, acc: 0.078125]\n",
            "9461: [discriminator loss: 0.7024509310722351, acc: 0.2265625] [gan loss: 1.079045, acc: 0.000000]\n",
            "9462: [discriminator loss: 0.7247378826141357, acc: 0.25] [gan loss: 1.153138, acc: 0.000000]\n",
            "9463: [discriminator loss: 0.688770055770874, acc: 0.3046875] [gan loss: 0.900568, acc: 0.078125]\n",
            "9464: [discriminator loss: 0.7059940099716187, acc: 0.1796875] [gan loss: 1.335359, acc: 0.000000]\n",
            "9465: [discriminator loss: 0.716921329498291, acc: 0.3671875] [gan loss: 0.793535, acc: 0.296875]\n",
            "9466: [discriminator loss: 0.7226788997650146, acc: 0.15625] [gan loss: 1.266432, acc: 0.000000]\n",
            "9467: [discriminator loss: 0.68656986951828, acc: 0.421875] [gan loss: 0.807131, acc: 0.265625]\n",
            "9468: [discriminator loss: 0.7686812877655029, acc: 0.0703125] [gan loss: 1.423825, acc: 0.000000]\n",
            "9469: [discriminator loss: 0.6967795491218567, acc: 0.453125] [gan loss: 0.748051, acc: 0.359375]\n",
            "9470: [discriminator loss: 0.7256994247436523, acc: 0.046875] [gan loss: 1.424889, acc: 0.000000]\n",
            "9471: [discriminator loss: 0.7063416242599487, acc: 0.4375] [gan loss: 0.620899, acc: 0.640625]\n",
            "9472: [discriminator loss: 0.7802557945251465, acc: 0.015625] [gan loss: 1.479887, acc: 0.000000]\n",
            "9473: [discriminator loss: 0.7137104272842407, acc: 0.4453125] [gan loss: 0.666082, acc: 0.546875]\n",
            "9474: [discriminator loss: 0.7835947871208191, acc: 0.0703125] [gan loss: 1.267655, acc: 0.000000]\n",
            "9475: [discriminator loss: 0.7163783311843872, acc: 0.3984375] [gan loss: 0.777782, acc: 0.343750]\n",
            "9476: [discriminator loss: 0.7603261470794678, acc: 0.0390625] [gan loss: 1.275615, acc: 0.000000]\n",
            "9477: [discriminator loss: 0.7171241044998169, acc: 0.3515625] [gan loss: 0.886156, acc: 0.234375]\n",
            "9478: [discriminator loss: 0.7259516716003418, acc: 0.140625] [gan loss: 1.179323, acc: 0.000000]\n",
            "9479: [discriminator loss: 0.7011959552764893, acc: 0.328125] [gan loss: 0.968498, acc: 0.046875]\n",
            "9480: [discriminator loss: 0.7156241536140442, acc: 0.2109375] [gan loss: 1.112266, acc: 0.015625]\n",
            "9481: [discriminator loss: 0.6973562836647034, acc: 0.3359375] [gan loss: 0.883846, acc: 0.078125]\n",
            "9482: [discriminator loss: 0.7176293134689331, acc: 0.1171875] [gan loss: 1.190058, acc: 0.000000]\n",
            "9483: [discriminator loss: 0.700414776802063, acc: 0.40625] [gan loss: 0.859106, acc: 0.187500]\n",
            "9484: [discriminator loss: 0.7697693109512329, acc: 0.1015625] [gan loss: 1.250948, acc: 0.000000]\n",
            "9485: [discriminator loss: 0.691123366355896, acc: 0.4140625] [gan loss: 0.827885, acc: 0.265625]\n",
            "9486: [discriminator loss: 0.7461838722229004, acc: 0.0546875] [gan loss: 1.332155, acc: 0.000000]\n",
            "9487: [discriminator loss: 0.7012726664543152, acc: 0.40625] [gan loss: 0.793072, acc: 0.296875]\n",
            "9488: [discriminator loss: 0.7207839488983154, acc: 0.0625] [gan loss: 1.405709, acc: 0.000000]\n",
            "9489: [discriminator loss: 0.7058002352714539, acc: 0.484375] [gan loss: 0.690833, acc: 0.484375]\n",
            "9490: [discriminator loss: 0.7667620778083801, acc: 0.0546875] [gan loss: 1.388944, acc: 0.000000]\n",
            "9491: [discriminator loss: 0.7086494565010071, acc: 0.5] [gan loss: 0.709904, acc: 0.468750]\n",
            "9492: [discriminator loss: 0.8030635714530945, acc: 0.0703125] [gan loss: 1.355100, acc: 0.000000]\n",
            "9493: [discriminator loss: 0.7136812806129456, acc: 0.4453125] [gan loss: 0.734169, acc: 0.359375]\n",
            "9494: [discriminator loss: 0.7618961930274963, acc: 0.0390625] [gan loss: 1.254939, acc: 0.015625]\n",
            "9495: [discriminator loss: 0.7066992521286011, acc: 0.390625] [gan loss: 0.799901, acc: 0.281250]\n",
            "9496: [discriminator loss: 0.7573984265327454, acc: 0.0703125] [gan loss: 1.255800, acc: 0.000000]\n",
            "9497: [discriminator loss: 0.693776547908783, acc: 0.390625] [gan loss: 0.878455, acc: 0.156250]\n",
            "9498: [discriminator loss: 0.7396144270896912, acc: 0.1015625] [gan loss: 1.244935, acc: 0.000000]\n",
            "9499: [discriminator loss: 0.7050085067749023, acc: 0.40625] [gan loss: 0.780750, acc: 0.296875]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAIuCAYAAABdOBlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3dZ5xVRbb38WrJdJNzkpxFggSRICA4wgzJgHmMqOjMRVFHvY4JHUXlIyOCiCJXFFFRAUF0AEmKKI5AK0hOkhRomtB0Itj3xfPcz7hWFed0OLHO7/vuf3rvOlu6end59upVSXl5eQYAAMA350T7AgAAAMKBRQ4AAPASixwAAOAlFjkAAMBLLHIAAICXWOQAAAAvFQ/0xaSkJP6+3FN5eXlJkXov5pG/IjWPmEP+4l6EUDjbPOKTHAAA4CUWOQAAwEsscgAAgJdY5AAAAC+xyAEAAF5ikQMAALzEIgcAAHiJRQ4AAPASixwAAOAlFjkAAMBLLHIAAICXAu5dBaDokpOTRc7MzIzSlQDAf/Tu3VvkZ599VuQePXpY55w+fTqs1xRqfJIDAAC8xCIHAAB4iUUOAADwUlJeXt7Zv5iUdPYvIq7l5eUlReq9fJ5HZcuWFTk9Pd06pmTJkiLrZ9qdOnUS+ccff7TGCPRzGk2RmkexOodKlSolcsOGDa1j9u7dK/KJEyfCek3xhntR5FSrVk3kgwcPFniMCy64QOQ1a9YU6ZpC5WzziE9yAACAl1jkAAAAL7HIAQAAXqJPTgglJdmPBHUtRYkSJUROSUkR+ciRI6G/MBTaOefI/w9YuXKlyF26dCnwmHoOpKamipyfPhTDhg0TedGiRSJT9xEeej60b99e5Dlz5ljnrF69WmT9/Z4wYYLIrrquM2fOiBxvvUoQecWKFbNeu+GGG4o87tSpU0XWNTp6rkYbn+QAAAAvscgBAABeYpEDAAC85EWfnPzUwgQ7Jz99SPQzzuLFZUnTqVOngp5TuXJlkdu0aSPyF198EfQ6QoHeFLYKFSpYr3300Uci671etKysLOu1ffv2idyoUSORdR8dl99++01kPV8nTpwo8ty5c60xFi9eHPR9CirR+uTUq1dP5K1bt4qs++YYY9co6Lxr1y6RXb1LmjZtKvIzzzwj8vvvvy9yWlqaNUas4l4UHvo+Y4wxf/zjH0UeP358gcfNzs4WuUaNGiJnZGQUeMxQoE8OAABIKCxyAACAl1jkAAAAL7HIAQAAXorJZoC6KPjcc88VuXTp0iK7iux0Aagu3NTNtFzFy7pouG3btiJXqlRJ5LVr11pj9OnTR+SBAweKPGnSJOscRIersV+vXr1E1s3gdOHpggULrDF0gbrexFHPRf0extjzU+dbb71V5Nq1a1tj6I0/Dx06ZB2DwHRhcX4an+nv1cmTJ0XW9zMXXdz5yiuviPziiy+KnJycbI2h74Hwi55nV199tXXMpk2bRNZFxHouun4vHjhwQGRdjL9hw4bgFxtBfJIDAAC8xCIHAAB4iUUOAADwUtRrclzP/HTjq4cfflhk3XTv0UcftcbIyckROT/N/jRdk6PrL/R1uDZE05sx7tmzJ2BG5Jx//vkif/zxx9Yxup5Gb3y5fPlykXW9hTHGXHXVVQHHPHbsmMiu2gm9MWjVqlVF7ty5s8iDBg2yxti/f7/II0eOtI5BYP369RNZ/3y77jP6tS+//FLkWbNmifzBBx9YY+g5ouu2dC2FrgU0JnKNRhEdep7pOi1j7N9RevPY7t27F/h9c3NzC3xOJPFJDgAA8BKLHAAA4CUWOQAAwEtRr8lx9YjQvWPq1q0r8h/+8AeRw7UZnX7WGOzZo661MMaYVq1aiXz77beLrPsWvPXWWwW4QhSE/v7o2ghXbxFN18tce+21Iuu5aYwxo0ePFlnXcun6MV3nYYx97S1atBD53//+t8i//PKLNcavv/5qvYaCueuuu0TW3ytXPZX+Xlx++eUi6zouV13P2LFjRf7b3/4W8DrvuOMO67WlS5eKnJ8eP4hfrt+tui/O+vXrRdY1Oa65qOdzrG8Gyyc5AADASyxyAACAl1jkAAAAL0W9Jsf1XHj69Oki62fJu3btCuclFZrrefyIESNELleunMi69w5CR/cSeeyxx0QuW7Zs0DH091TvXfT555+LrPvZFIar147e30r3htJ1PnXq1LHGmD17dpGvLdHoPl66Fkpz1TAsXLhQ5ML0FZk3b57I999/v8i6/4nuo2SMMc2bNxc51vYYQmjpnl7G2PtM3XnnnQHHcM3nMWPGiJyRkVGIq4scPskBAABeYpEDAAC8xCIHAAB4iUUOAADwUkwWHs+cOVPkzMzMSF1OkbiKPStUqCCybsa0cePGsF5TonBt9HrbbbeJ/Pe//11kXZh89OhRa4y9e/eK3L59e5F1QXC46AJAXcCum9K9+uqr1hhHjhwJ/YV5Thf0uhp+/p5rPuh7QGH07t1bZD139fzQmxwbY8zLL78sst7EVd+bEN9c98QZM2YEPeb3dKNSY+w/xijM5teRxCc5AADASyxyAACAl1jkAAAALyUFep6WlJQU8odt+hmgqyGbrtNxPReMRe+++6712nXXXSeybtqmm8tF6vlmXl5e4IexIRSJedS1a1frmAULFohcpkwZkQ8cOCBy27ZtrTEOHz4scrSeP+smXtu2bRO5ZMmSIn/00UfWGMOGDRM5FP8tkZpH4ZhD+aHnTFZWVsDj09PTrdcaN24s8vHjx0XW3zvXPXH79u0iV6xYMeB1uOjvt6511DWF+jrDJd7vRbHKNUf0Jr3694/makyqx9W/n6N1jzzbPOKTHAAA4CUWOQAAwEsscgAAgJci3idHP69zbVbn6p0Ti3SvimuvvTboOboOJNZ7DMQq3a9k0qRJ1jG6n4PuE6N73qSlpYXo6kJv5MiRIus6Dj2PatasGfZrSgQNGzYs0PHJycnWa5UrVxb52LFjIuveOq4xdD3VSy+9JLLui+Pq56N7/qSkpIj8yy+/iOzq7xOpvlAoOP39dW3+rH9nBePa5FPX5Oh5E2v4JAcAAHiJRQ4AAPASixwAAOClqO9d5Xruq//uXj8HdtXxRMO4ceNEDrYPiDHGDB48OFyXk1D0vGnevLl1jO5J9Ne//lXkgwcPhv7CQsBVT3HfffcFPEfXH+kaDmOo/yoM3Y8oGFffkR49eoi8f/9+kXUthf66Mcbs2bNH5Hbt2oms96Fat26dNcb69esDXqvuz9OkSRNrjE2bNlmvITp0fY3u4aTr9grD9fu5Ro0aIlOTAwAAEAUscgAAgJdY5AAAAC+xyAEAAF4Ke+FxsA05O3fubJ2zatUqkWvVqiVyixYtRNYbzRljzJIlS0TWRah79+4VOTU11Rpj9OjRIi9cuFBk3eTLVdipCwbXrl1rHYPg9DzSc8RVrPv222+LvHLlytBfWBi4ioyDNfF67733RNYb8aFwXBsUBuJqZKrHCPaHE/kpENfvM3v2bJFdfwQxZMgQkefNmyey/hm6/vrrrTEee+yxoNeG8Bg/frzIf/nLX0TOzx++FJRrLpYvXz7k7xNOfJIDAAC8xCIHAAB4iUUOAADwUthrcq644gqRL7jgApGHDx9unfPBBx+IrGtyevXqJXKlSpWCXkeJEiVE1hvvuTbiGzp0aNBxf8+1ed2NN94oMg3ZCqd06dIi165dW2TXv6tudqafJdepU0dkV3NAXfugm+6FQtWqVUV+9tlng56j59qjjz4qMvMsPHQ9jZ5jrvnx4YcfihyJ742rPuPo0aMiZ2dni1yuXDmRW7VqFfoLg1OZMmVE3rdvn3VMfn7PhdrXX39tvfbtt99G/DqKgk9yAACAl1jkAAAAL7HIAQAAXgp5TU6jRo1EfuONN0TWG365nmHrup37779fZL05nevv9vWmd5Fw7Ngx67VnnnlG5P79+4vs6vEDm+5ZpGt0XJYtWyay3kiuWrVqInfo0CHoGBkZGUHfN5gxY8aI/OCDD4rs6omj6zhuvvlmkXfv3l3k60Jwum6rbt26Irvqbb788kuRe/bsKbKrlq+oXNeh57vuWaZRkxM6ukZK97GqXr16JC/nrPQmnwMGDLCOKWjvqGjjkxwAAOAlFjkAAMBLLHIAAICXQl6To2sD9H4oO3bsENnVJ+err74SWdftdO3aVWS9B4sxxjRt2jTgdej+Fq69j3QNx9atW0XWz9r//Oc/W2Po+qEXX3xR5CeeeELkQ4cOWWPAmGnTpol87733iuyqY9G1D7qeQp/jqqnSvXT0HGjSpInIru9fp06dRB45cmTA63DtfzRu3DiRde8VREbr1q1FPnz4sMiu+0iXLl1E/vjjj0XWexDpffWMCd5bR9d86J43xhhz5MiRgOfo9/jiiy8CvifcXP/2K1asELkwNTj6vlCYulP9PZ4/f77IgwYNCnh8POKTHAAA4CUWOQAAwEsscgAAgJdY5AAAAC8lBSosSkpKKnDVkS5m00WVuoi4MIVN+Ska1u+ri8F0gdXUqVOtMXSTLn2tuvBLF6EaY8y6desCXpf+eseOHYNeRyjk5eXZO/iFSWHmUTAPPPCAyFdeeaV1jC7U0/+OuhmabrBnjN3ccubMmSIHm+/G2JvDBqMbSBpjb9qp3ydaTSUjNY/CMYcKQ//M63n41FNPWecE28RTN4bTf6xgjDEnTpwQWW+uqVWpUsV6bdOmTQGP0U3edEG0McZMmTIl4PsWRrzfi/Q9QG+2aYz9b6+bSOoxXE1H9fc8FMXLuqlqOH7XRMrZ5hGf5AAAAC+xyAEAAF5ikQMAALwU8pqcYDU4wY7PzzmhEKwRVmHoDfCMMWbhwoUit2nTRuQ777xT5E8++cQaIy0trcjXpsXbc3BdC6H/HV977TXrHF3bULJkSZF1nUNqaqo1RufOnUXW9TWu+RuMnt+33XabyLppmzHGzJ07V+RYadKVaDU5WuPGjUX+9ttvrWOqVq0acAxda+HaALFXr14i79+/X+TnnntO5GHDhlljJCcni6zn0LZt20TWGyUbY//MhEK83Ys0Xe/pqsnRTXL1v72ujXE1FdU1VMHuPa57RO3atUXW9WDxjJocAACQUFjkAAAAL7HIAQAAXgr5Bp0FraeJRP2NSzhqGlzPUdevXy/y8uXLRe7bt6/IeiM3Y8JTkxNvdA2V3ihzzJgx1jnvv/++yPp7npKSInKPHj2KconO9zDG3nC2d+/eIuufAf3faoxdkxTP/Sx8sn37dpFd81Bvyqu/v7qPzubNm60xrrnmGpG7desmst602LV5o56bx48fF3nNmjUiV65c2RojHDU5sU5/v3RfNl27tHr1amsMXXdVsWLFgO/pqu/U9PczNzdXZNeG0T7V4OQXn+QAAAAvscgBAABeYpEDAAC8FPI+OZD0s9e33npL5Dlz5oj89ttvW2OEo24p3ntT5Ieudbj77rtFbtCggciuZ9i6Z4mu48nJyRH5hhtusMZYsGCByNGqQwuHRO+To7n2KVuyZInIen863e/EtRefnme6r0p++pPpmo3vv/9e5CFDhoicnp5ujREO8XYv0v/WOpcvX94654cffhBZ712VH7oeat++fSLruixXvy2f0ScHAAAkFBY5AADASyxyAACAl1jkAAAAL1F4HGF6w8fmzZuLPH36dOuccDQujLdiP8QmCo8Lrn///iKPGzdO5IYNG1rn6OZ+utj1zJkzIrs2m9VN7T7//HORX3jhBZEzMjKsMcLBt3uRa+PMqVOninzTTTcFHEP/sYIxxgwYMEBkn/6AIRQoPAYAAAmFRQ4AAPASixwAAOClkG/QicA6deokst7MLRz1N4h/up6CeRK/dC3MsWPHRH7ttdesc8aPHy+yrtn45ZdfRK5QoYI1xuOPPy7y5MmTRWbT19Bw1co89NBDIuu6q8GDB4t89OjR0F9YguKTHAAA4CUWOQAAwEsscgAAgJfok5OgfOtNgeigT078yM8mntGQCPcivVlw48aNRd6wYUMkL8dL9MkBAAAJhUUOAADwEoscAADgJWpyElQiPAdH+FGTg6LiXoRQoCYHAAAkFBY5AADASyxyAACAl1jkAAAAL7HIAQAAXmKRAwAAvMQiBwAAeIlFDgAA8FLAZoAAAADxik9yAACAl1jkAAAAL7HIAQAAXmKRAwAAvMQiBwAAeIlFDgAA8BKLHAAA4CUWOQAAwEsscgAAgJdY5AAAAC+xyAEAAF5ikQMAALzEIgcAAHiJRQ4AAPASixwAAOAlFjkAAMBLLHIAAICXWOQAAAAvscgBAABeYpEDAAC8xCIHAAB4iUUOAADwEoscAADgpeKBvpiUlJQXqQtBZOXl5SVF6r2YR/6K1DxiDvmLexFC4WzziE9yAACAl1jkAAAAL7HIAQAAXmKRAwAAvMQiBwAAeIlFDgAA8BKLHAAA4CUWOQAAwEsscgAAgJdY5AAAAC+xyAEAAF4KuHcVAABITElJ9nZQAwYMEDk1NVXk9PR0kbOzs0N/YQXAJzkAAMBLLHIAAICXWOQAAAAvUZMDAADMk08+KfL1119vHVOzZk2RmzVrJvKpU6dCfl1FwSc5AADASyxyAACAl1jkAAAAL1GTE2G678A558h1ZvHi9rckNzc3rNcEIL60bt1a5Llz54pcsmRJ65xHH31U5HfeeUfkvLy8EF0d4sXYsWNFvu+++4KeM3v2bJEPHDgg8m+//Vb0CwshPskBAABeYpEDAAC8xCIHAAB4KSnQc9ikpCQe0haArq8xxn4+qWtuOnbsKPLWrVutMfReIKF4dp6Xl2dvShImPs0jXVOV6HUMkZpHPs2hiy66SOQZM2aIXK9ePescPe9cewoFs2HDBpGvuuqqgF+PFO5FkdOkSROR161bJ3Lp0qVFPn78uDVGlSpVRD59+nSIrq5ozjaP+CQHAAB4iUUOAADwEoscAADgJRY5AADASzQD/P9KlChhvVanTh2RdVO+ChUqiLx7925rDF0g+Pjjj4s8fPhwkb/77jtrjH/+858i/+tf/7KOQdG5ijlr164t8pEjR0TOysoK6zX9H12wfuutt4r84YcfiqyvE5GRnJxsvfb111+L3LZt2wKPe+bMGZH1HyPo+bFixQprjEsuuUTkypUrF/g6ED9c97MrrrhCZN00Uv+hjG4yaUzsFBrnF5/kAAAAL7HIAQAAXmKRAwAAvJSwNTkvv/yyyLo2xhj72aOuc9C1Mu+++641hn4uqp+LV6pUSeQuXbpYY+iGgdTkhIermePhw4dFDrb5XH5qu/S8yszMDPh1Y+zNFDt06CCyruWiJic63n77bes1XYOjG0jqe0TLli2tMTZt2lSg6yhWrJj12pYtW0SOt9oKFEz9+vWt1/Qmrfp+lp2dLfIDDzwQ+guLMD7JAQAAXmKRAwAAvMQiBwAAeMnbmhz9nHvXrl0i6/4nGRkZ1hhvvvmmyP/zP/8j8s6dO0WuVauWNca9994rcvPmzUXW/S+mTZtmjfHMM89Yr/lO18cEq4Vx0X2M9OZzuq7FVcewd+9ekZ944gmRJ02aJPKNN95ojXHxxReLrOfVwYMHRb7lllusMapVqyayrq9ITU21zkH46Xq5IUOGWMfk5OSIrOfIRx99FPLr0hsvGmNMo0aNRB4xYoTI3377bcivA5Gj75mffPKJdUypUqVE1htw6t+TuiYxHvFJDgAA8BKLHAAA4CUWOQAAwEtJumeD+GJS0tm/GEP0/hvGGLNt2zaR69WrJ7KuhRkwYIA1xsKFCwt0Hb1797Zemz17tsh6b5uBAweKHKkeOHl5efbGJmFSmHmk9+JZv369yEuXLhX55ptvtsY4duyYyFWrVtXXJbKrT06wnib5EWyfoT59+oi8aNEia4yUlBSRN2/eLHKbNm0KfF2hEKl5FCv3In2vWb16tcgtWrSwztH71T333HMhvy49p06dOmUdo+dyq1atRC5oL55QifV7UbzQv0vGjBljHaN7MOlax379+oms77Ox7GzziE9yAACAl1jkAAAAL7HIAQAAXmKRAwAAvORFM8CxY8dar504cUJkvRldp06dRM5PMzVddKobf7kKvXSh8dNPPy3yl19+GfR9E9Hll18ust5srmfPniIfOnTIGiMrK0tkXZypi+5080Bj7AaBOusxdHMtY+xNG3Nzc0X++eefRdZz1xi78Piqq66yjkH46T9Y0IXGO3bssM55/vnnQ34deh7qOeWiG0hGq9AY4XHttdeKXKVKFesYPX/1H1u4CtbjHZ/kAAAAL7HIAQAAXmKRAwAAvBSXNTl9+/YVefjw4dYxmZmZIufn2Xkw+vmlrrVwee2110TWGzrqupFEpOtNjDGmV69eIl922WUi//vf/xZZ11wZYzc/08+b9ffT1RgzWPM/XRuhn3kbE3xz0WbNmolcs2bNoGPoOh5Eht4oVTv33HOt166++mqRP/zwQ5Fdc1fT968ZM2aIrOfy0aNHrTF0IzjEt7Jly4qsG/fVqVPHOkdvKKvvb3qzYB/wSQ4AAPASixwAAOAlFjkAAMBLcbFBZ/Xq1UXes2ePyCVKlLDO2bdvn8i6r4jewFNvomiMMWXKlBH5iy++EPnCCy8U2fVsfdiwYSLrDTujhU3xYse6detEbt26tXWMns/NmzcXOVq1XYm2QafutbR7926RXTU7wXor6c1W9UayxhhTunRpkcuVKyfyxo0bRXbNoVjFvSh/KlasKLLegFh/z/XmsS4ZGRki16pVS+R46pvDBp0AACChsMgBAABeYpEDAAC8FBd9cnQ/AN0TwtXLpHbt2iKvXLlS5MOHD4t88uRJawxd6xOsR4Z+RmqMMXPnzg14DhKPnr9NmzYV2VUnp/st0V8pOnTd3UUXXSSyqw5C74mma3RatWoV9H2PHDkisq7tu+aaa4KOgfim62d0P60FCxaIXLJkSWsM/XtuxIgRIsdTDU5+8UkOAADwEoscAADgJRY5AADASyxyAACAl+Ki8Fg3/+vXr5/Ib731lnVOlSpVAo6Rm5sr8quvvmqN4Xrt9/bv3y9yt27drGNcGzYisU2bNk3kUqVKiewqKp4/f77Iung52CagCI9du3aJXKNGDesYvWnn9OnTRW7fvr3I+ntrjDHly5cX+YUXXhDZx4JRSPp3Sbt27UTWjfxctmzZIvLChQuLfmExjk9yAACAl1jkAAAAL7HIAQAAXoqLmhz9LHLZsmUiN2jQoMjvoTfbNMZ+zv3rr7+K3KFDB5HT0tKKfB3wj66xuO666wIer+eZMcb88MMPIgfaWBfR49qkd8eOHSLr7/+4ceNE7tq1qzVGcnKyyHqzRiSeJUuWiKyb4rruEe+++67Ix48fD/2FxRg+yQEAAF5ikQMAALzEIgcAAHgpLmpywqF169Yir1ixwjpGP18/77zzRHZtyAloenNYXaNz4sQJkbt06WKNQQ2OP/SGw/379xdZbwxsjN0HSW/WiMRTpkyZAp8zZ84ckRPhvsInOQAAwEsscgAAgJdY5AAAAC8lbE3OmjVrRHbtF3PppZeKTA0Ogunevbv1WseOHUXWz8EnTJgg8pEjR0J/YYgI3avEGGP++Mc/ivzhhx+KrPuAHTp0yBqjXLlyIk+ZMkXkVq1aFeg6Ef/Gjx8f8OvZ2dnWa7t37w7X5cQsPskBAABeYpEDAAC8xCIHAAB4iUUOAADwUsQLj0uWLCmyq+A3NzdX5MI0LEpJSRF548aNAa8jNTXVGuOrr74q8PsisdSuXVvkxYsXW8cUK1ZM5DfffFPkp59+WmRdiIr4oe8rxhjTp08fkV9++WWRH3/8cZHfeecda4zBgweH4Orgk6uvvjrg12fNmmW9lpOTE67LiVl8kgMAALzEIgcAAHiJRQ4AAPBSUqB6l6SkpJDv3nXfffeJ/Pzzz1vH7Nq1S2TdHKtatWoiL1q0yBpjxIgRImdmZop88uRJkRs2bGiNcfz4ces1X+Tl5dldy8IkHPMoWnQNWVpamsjly5e3zvnll19Evuqqq0RetWqVyPG0aV6k5lGsziG92aauvzLGmDp16oi8efPmgGMmJydbr+3bt0/krKwskXVtGHPILVbnUX7ouaV/h+l706BBg6wx5s2bF/oLixFnm0d8kgMAALzEIgcAAHiJRQ4AAPBS2Pvk6A3rdP2B7oljjDGNGzcWWT/TLlWqlMh33XVX0Pc9ffq0yFWrVhU5np5hI3JKly4t8kMPPSRyxYoVRd60aZM1xoABA0TWNWeIX3pjzNWrV1vHuObE71WoUEHkG264wTpGb6z4yiuviMz9y39PPPGEyPp3p/4d99lnn4X9muIBn+QAAAAvscgBAABeYpEDAAC8FPaaHP2sWNfk3HHHHdY5d999t8jDhw8POKZrP45OnTqJPHv27IBjAHq/M2OMmTJlisjDhg0TWdd+LV++3Brj559/DsHVIRbo/YJ0b5J169ZZ5+jaCX2O3u+sQYMG1hg//vijyK+//nrQa4Vf+vbtK7KuF8zIyBCZPfD+Hz7JAQAAXmKRAwAAvMQiBwAAeIlFDgAA8FLEN+h0vIf1mt6ITDc5QtGxKZ6te/fu1mtLliwJeM7nn38u8uDBg0N6TbHO9w069f1Jb3o4efJkkQ8cOGCNoQtEdWGx3jx4zZo11hj9+vUT2ac/nOBelD9Lly4VuU2bNiJv3bpV5K5du4b9mmIJG3QCAICEwiIHAAB4iUUOAADwUtRrckqUKGG9pjfkZEPD0EvE5+C6CdvIkSNFfv75561z9PxcsWKFyJdeeqnI2dnZRbnEuON7TY42f/58kS+55BKR9ebB+XH8+HGR9QbFxhiTlpZW4HHjRSLeixB61OQAAICEwiIHAAB4iUUOAADwUtRrcnr37m29pp9rf/HFFyLTN6foEuE5uJ5HF154ociffvqpyK4NOo8dOyZy9erVRT558mRRLjHuJVpNjq7rql27tsiuDTrT09NFbtGihcinTp0K0dXFp0S4FyH8qMkBAAAJhUUOAADwEoscAADgpajX5Li0a9dO5E2bNomck5MTycvxUiI8B09OThZ5+vTpIv/2228iT5061Rpj0aJFIid6DY6WaDU5CL1EuBch/KjJAQAACYVFDgAA8BKLHAAA4CUWOQAAwEsxWXiM8AYJLcMAACAASURBVKPYD6FA4TGKinsRQoHCYwAAkFBY5AAAAC+xyAEAAF4KWJMDAAAQr/gkBwAAeIlFDgAA8BKLHAAA4CUWOQAAwEsscgAAgJdY5AAAAC+xyAEAAF5ikQMAALzEIgcAAHiJRQ4AAPASixwAAOAlFjkAAMBLLHIAAICXWOQAAAAvscgBAABeYpEDAAC8xCIHAAB4iUUOAADwEoscAADgJRY5AADASyxyAACAl1jkAAAAL7HIAQAAXioe6ItJSUl5kboQRFZeXl5SpN6LeeSvSM0j5pC/uBchFM42j/gkBwAAeIlFDgAA8BKLHAAA4CUWOQAAwEsscgAAgJdY5AAAAC+xyAEAAF5ikQMAALzEIgcAAHiJRQ4AAPASixwAAOClgHtX+aRVq1YiP/LIIyIPGTLEOqdEiRIinzp1SuQePXqInJqaWpRLRBw45xz5/wW//fabyElJ9vYpXbp0EVnPk5ycnBBdHQBEVpUqVUSuVKmSyLt377bOOXnyZFiv6ff4JAcAAHiJRQ4AAPASixwAAOClpLy8vLN/MSnp7F+MIeXKlbNe279/v8gpKSmRuhyhbdu2Iq9fv15kXdMRKXl5eXbxSJjEyzwqVqyY9VqpUqVEnj9/vsgdO3YUuTDzTNfk6GfarmNiRaTmUbzMIRQc96LwcNUH6prCMWPGiKzrBy+88EJrDF2rquXm5oqsa3aMMSYzMzPgGIVxtnnEJzkAAMBLLHIAAICXWOQAAAAvxWWfnOTkZJGPHDliHaOfPQaqPTLG/YxQ11ssWrRI5Jo1a4qclZVljaFrONq3by9yWlpawOtC5Lhqcho2bCiyfkZdunTpIr+vHuPEiRPWMVOnThX5jjvuKPL7AvDXu+++a732ww8/iDxq1CiR9e/N/Dh69KjI06dPFzna9YR8kgMAALzEIgcAAHiJRQ4AAPBSXPTJqVevnsizZs0SuX79+tY5ur6ievXqIp85cyZEV/cfjz76qPXaTTfdJHKvXr1E1v18IoXeFLZ27dpZry1fvlxk3ZNJ9znS+5u5XitTpozIxYvL0jjXz6Ser4MGDRJ5wYIFAa8rXOiTE5yuc2jatKnI+h7gqg+MVj+tSOBeFBo1atQQecOGDdYxun5V/+48ffq0yLqvmzHGfP/99yJPnjxZ5HXr1okcjt+1LvTJAQAACYVFDgAA8BKLHAAA4CUWOQAAwEtxUXh86aWXiqyLiHft2mWds2LFinBektOmTZus13TRtN4AzVXYFQkU+9lcjf3Wrl0rct26dUXevHmzyJUrV7bG2L59u8jdu3cXWW+kV7JkSWsM/XOqCwSPHz8usi5CNCY8xasUHgenC81ff/11kQcOHCiyq3j9559/FrlixYoiN2jQQOTs7GxrDH3M4cOHndcbadyLCkdvlLlmzRqRzzvvvKBj6PuK3lxz5MiR1jmrVq0SWRcz7969O+j7hgOFxwAAIKGwyAEAAF5ikQMAALwUFzU5mq5hCLb5Zricf/75IuvNz4wx5tChQyLXqlVL5Eg1StJ4Dm43adO1XsbY9V5btmwRed++fSK7GnB17txZ5G7duomsazD0s3Zjgs9xvTlsmzZtrGNctWtFRU1OcLrJ5OLFi0XW9TWF2SQxPzIyMkSuUKGCyNG6j3IvKhx9n9B1ea7avgMHDojcokULkcuXLy9ylSpVrDF+/PHHgNcVa/OIT3IAAICXWOQAAAAvscgBAABeisuanGgpVaqUyLoXha4VMsaY1q1bi+yq2YgGnoPbz6yHDRtmHfP2228XaMyjR49ar+l5kZKSEvDrO3bssMbQPU50zxv933Ls2LGgY7iOKShqciT9vTXGmLS0NJH1fURz3ZN1vYU+Jj/1NSdPnhRZbzarey9FCveiwtH1M0uXLhXZ1QfpD3/4g8h6nujNgl2/01x9nGIBNTkAACChsMgBAABeYpEDAAC8RE1OAPr55NatW0XWNQ4nTpywxtDPvWMFz8GNOffcc0WeOnWqdUyfPn1Edj2j/j3Xz5OeF3pe6X4XPXr0sMbQ9RLTp08XuVmzZkGvQ9dk6BqkefPmWecEk+g1ObqnTU5OjnWMq+/R7+nv1U8//WQdo/c70/usDRkyRORx48ZZY+g9tC677DKRFyxYEPA6w4V7Uf7oefTQQw+JrO9nd955pzVGtHrYRAI1OQAAIKGwyAEAAF5ikQMAALzEIgcAAHgpYQqPdaFecnKyyKtWrbLOadSokci66FQ3ZNNjGuMuRIwFiVjsp5tn6Q1V69evb50TrNBYN9z605/+ZB2zceNGkXUTSd1cKz/FgbrIcNq0aSJ36dLFOkcXnur30Q0F89McLtEKjytXrizynj17RC5btmzQMdLT00Vu2rRpwK8Xxv79+63X9ObAgwcPFnnu3LlFft/CSMR7UWFceeWVIo8aNUrkf/zjHyIvXLjQGkP/zorWBtHhQOExAABIKCxyAACAl1jkAAAALxUPfkjs07UExhgzY8YMka+44gqRdT2C69mkrsfQNQr33HNPwK8juooVKyby/PnzRdZ1LcHqb4wxZtOmTSIPHTo04NfDZffu3SLrZnArVqywzjnvvPNE1s/nfZu/+r6gmyHmh272N2fOHJF1nZP+NzXG3gi1WrVqQc8pKH2dNWrUCHqObiCJ6NH3Hr3hqjH2vGnfvr3Is2bNEnnbtm3WGLpO57vvvhP5vffeC36xcYZPcgAAgJdY5AAAAC+xyAEAAF7yoiZn9OjR1mu6B4Smn4G6anL0xooffPCByPq5t2szziNHjgS8DoSGq0fRhg0bRNbfL13HkJ+alJEjR4q8ZcuW/F5iWGVlZYn86quvWsfo1wpToxJP9M+4zvnpR9SzZ0+R9T1B19tUr17dGkP3QQoHvVmwntsu9erVC9PVoKA+/vhjkQ8ePGgd065dO5H1Rr96fjdv3twao2XLliLre97tt98u8iWXXHKWK44ffJIDAAC8xCIHAAB4iUUOAADwUlzuXaWfN7v65OgaDf28UvcYqFOnjjWG7nmyY8cOkadMmSLyU089ZY2xZs0akfNTBxAJ8bZfjP6eX3zxxSK//fbb1jkpKSkiV6xYMeB7uPqVLFq0SORBgwaJXJh9p8JBz2/dM8MYu5eOvvZSpUqJnJ//lkTbuypWff/99yJfcMEF1jG6/kL3+IlWn6R4uxeFQseOHUX++uuvRS5RooR1TrA+Xvn5edX3ON1LTNP9uIxx7/EXC9i7CgAAJBQWOQAAwEsscgAAgJdY5AAAAC/FZeFxpOiiLP1v9cknn4jsagaoC49HjRoVoqsrmngr9rv//vtFHjNmjMhpaWnWOfPmzRO5UqVKIi9fvlzkCRMmWGPogudQbKYYDrpwvn///tYxr7/+ushLly4V+bLLLhM5P03sKDyODTk5OSLrInJjjFm/fr3I+o8vKDyOHr1Z8DvvvGMd06NHD5EPHDggst7Us3HjxtYYulmlbpr72GOPiey63/Xu3VvklStXWsdEA4XHAAAgobDIAQAAXmKRAwAAvERNThHo597PPvusdcz1118vsn7m+cYbb4T+wvIh1p+D61qYzMxMkfW//cyZM60x7rnnHpGPHj0qsmtT1lilG4Hp5/OzZ88W2dX4UI/xzTffiNytW7cCXxc1OdHRoUMHkVevXh30nPLly4uckZER0msqrFi/F8UKfc/T98js7Owiv8ewYcNEfv/9961j9O+09957r8jvGwrU5AAAgITCIgcAAHiJRQ4AAPBSXNTk6A3B9u7dK3Ks1Fb89NNP1mstW7YU+auvvhJZbzQZKbH+HLxRo0Yi681S9QZ2S5Ysscbo16+fyLHa40bXyrh+JnU9xa5du0TWNTiuMR544AGRU1NTRdZ9c/KDmpzI0HMk2Fz+5ZdfrNd0LyU2C4au69G9sVybgq5YsULknj17hv7CCoGaHAAAkFBY5AAAAC+xyAEAAF4qHu0LcNF7cCxcuFBkvW/RW2+9ZY0RjefNel8QF/3fBrcdO3aI3LdvX5GXLVsmcufOna0xdP3TddddJ/LPP/9chCsMnbJly4o8aNAg6xg950uXLi2ynu+6hskYY8aPHy9yrNSyQXLtgbd58+aA5+jvf+vWrYMeg8RTvLj8lT916lSRdY1Oenq6NcbVV18d+gsLIz7JAQAAXmKRAwAAvMQiBwAAeIlFDgAA8FJMNgPUhXe7d+8O+PXk5GRrjNzc3NBfmKIbJR0/ftw6Rjetq1y5sshZWVmhv7B8iPcGXCNGjBD5H//4h3WMnid6rutC3EmTJlljHD58WOSuXbuKrJvwLViwwBqjU6dOAc8ZO3ZswK8bYxca681Gf/zxR5EHDhxojRGOuUYzwKJr0qSJyK6i8WLFiomsm7bVrFlTZFfBaKyK93tRrNJFxsbYG/nqP+jQhcetWrWyxti+fXsIri70aAYIAAASCoscAADgJRY5AADASzFZk6NrXXJyckQuWbKkyE8//bQ1xuOPPx7y69LPODMzM0V2NVfr3r27yGvWrAn5dRVGvD8H13Pg0KFD1jF6U0tNb3Lo2oxOf091LcSJEyeCjqHrsFzH/J6rnmzVqlUiL168WGRdkxSpzUipySk4Xfdw8uTJoOecPn1a5I4dO4q8fv36ol9YlMT7vcjxHtZrkWjE+PXXX4v8zTffWMeMHDky4BhXXXWVyHPnzrWOidWNjqnJAQAACYVFDgAA8BKLHAAA4KWYrMnRLr74YpF1PYLrGaHesPHgwYMi//rrryJXqVLFGkPX9dx9990i63+7ffv2WWPoHhi6piNa4v05uH7u7eqT8/DDDwc8JxT099P1/S1TpozIet7oup6///3v1hgbN24UWW8+Gom+UC7U5Eiu+8jEiRNF1r1J9Dk7d+60xujQoYPIuk9SPIv3e9HatWtFbtasmXWMrisdN26cyO+//77I+veTMfb9S/eP05s/6zouY4zZv3+/yHpexVN/JY2aHAAAkFBY5AAAAC+xyAEAAF6Ki5oc7a677hL5pZdeso7RPW303i+aq15DP9PUdQ9PPvmkyPo5qzHx11MgHCIxj1z7tOh9V/T3q2fPniK76ik0Vy+k39N7XRlj1+DMnDlT5AkTJoj8888/B31f1/P2aEj0mpxatWqJnJqaah2j55Xuk5OdnS1y48aNrTFcNRq+iPd70Zw5c0QePHhwgcfQP8+u+1kw+j4zbNgw65iPP/444DnxjJocAACQUFjkAAAAL7HIAQAAXmKRAwAAvBSXhcdacnKy9dqoUaNEXrBggcg9evQQefv27dYY3333nci6kVI8i/div8KoWLGiyI0aNRJ56NCh1jl6wzrd8LFmzZoi66ZfxtjN/XQjP91AMFqN/Qoj0QqPdWPHGTNmiFy/fn3rnIYNG4o8efJkkfX8iJWi8kiJ93uR/qOWqVOnWsdcfvnlIuvfWfkpANbzYsuWLSK3adMm6Bg+o/AYAAAkFBY5AADASyxyAACAl7yoyUHBxftz8HBwNYTUr+nmjrppl270ZowxJ0+eDMHVxSbfa3L093/ZsmUi642AdWM/Y4zp06ePyD/88IPIPjVkKwzf7kX52QhYH5OfOZDo8yQYanIAAEBCYZEDAAC8xCIHAAB4qeC7gAGecj3zDvYcPNF6miSaKVOmiKw3ddXf//T0dGuMn376SWRqK/xGfU1s4ZMcAADgJRY5AADASyxyAACAl6jJAYCz0PuMXX/99SI/9thjIr/88svWGHpvMgCRwyc5AADASyxyAACAl1jkAAAAL7HIAQAAXmKDzgTl26Z4iA7fN+hE+HEvQiiwQScAAEgoLHIAAICXWOQAAAAvBazJAQAAiFd8kgMAALzEIgcAAHiJRQ4AAPASixwAAOAlFjkAAMBLLHIAAICXWOQAAAAvscgBAABeYpEDAAC8xCIHAAB4iUUOAADwEoscAADgJRY5AADASyxyAACAl1jkAAAAL7HIAQAAXmKRAwAAvMQiBwAAeIlFDgAA8BKLHAAA4CUWOQAAwEsscgAAgJdY5AAAAC8VD/TFpKSkvEhdCCIrLy8vKVLvxTzyV6TmEXPIX9yLEApnm0d8kgMAALzEIgcAAHiJRQ4AAPASixwAAOAlFjkAAMBLLHIAAICXWOQAAAAvscgBAABeYpEDAAC8xCIHAAB4iUUOAADwUsC9q3xy4YUXirxs2TKRS5UqZZ2Tlye3ORk9enTA/NtvvxXhCgEA8Ms559ifpUTydyWf5AAAAC+xyAEAAF5ikQMAALyUpOtOxBeTks7+xRg3bdo0kf/85z+H/D2OHz8u8syZM61jhg8fHvL3DYW8vLykSL1XvMyjBg0aWK999tlnIhcvLsvYcnNzRT5x4kTQ9xk8eLDIBw8ezOcVxp5IzaN4mUOFUb16dZFPnTplHTNhwgSRK1WqJHJmZqbIVatWtcZo0aKFyHoujxo1SuR33nnnLFccWtyLbMWKFbNe07Utl19+uchjx44VuVatWkHH0JKSCv6tOHPmjMhHjhwRuV+/ftY5qampBX6fYM42j/gkBwAAeIlFDgAA8BKLHAAA4KW47JOje9q46iD0M80DBw6IrJ9h33rrrdYY69atE3nNmjUin3vuuSL36dPHGqNMmTIiZ2dnW8cgMvSc+NOf/iSyq6ZK1y3oZ9b5eYat6970XNRzYsCAAdYYuq8T4oeug2jYsKHIixYtEtl1jyhfvrzIdevWFTlQbeX/0XNV9yp56623RN66das1xrfffhv0fRCcvhe1bNlSZFc9VPPmzUXWv1vyQ3/Pg9XouOi5pmtySpQoIXL9+vWtMcJRk3M2fJIDAAC8xCIHAAB4iUUOAADwUlzU5JQuXVpk/Qzb5YMPPhD52muvLfJ16D4q8+bNE7ljx47WOexnFTt0LdfQoUNF1vU3xhTumbWm+57o99HPsMeNG2eNceONN4q8fv36Il8XQk/3rzHGmC+//FLklJQUkStXriyyvncZY9cHNmrUSOSSJUuK/NBDD1lj6PvX/PnzRdZ9VZo1a2aNQU1OaOjeSI888ojI5513nnWO6/70e/mpy1qyZInIX3zxhcjff/+9yBs2bLDGyMnJEVnXE+k+YHqeRRqf5AAAAC+xyAEAAF5ikQMAALzEIgcAAHgpLjbo1EV3uimSbtJnjDEXXnhhWK/JGGNq1Kgh8jPPPGMdoze9y8jICOs15Reb4tnFmjNmzLCOGTRokMjffPONyG3bthXZVWQ3ceJEkZs2bSqynquuInl9bc8995zIe/bssc6JhETfoFMXpm/evNk6RjcN/fzzz0W+8sorRT59+nSIri6w66+/XmS9qXHNmjWtc9LS0kJ+HYl4L2rcuLHIQ4YMEfnZZ5+1ztH3K/27W/+Ry86dO60xOnToILKea/lpbqqbVer/lvT0dJGPHTtmjaEbCIYCG3QCAICEwiIHAAB4iUUOAADwUkw2A9TPHvUzbd0U6ciRI2G/Jhf97LF9+/bWMbpxEmLHyZMnRdY1CsYYc8MNN4j85ptvFvl9V65cGfA69HsaYzfY+q//+q8iXwcKTt97Nm7cKLKuTzDGbtyo6y+i5cEHHxRZ12PoTSQROtu3bxd58eLFIusGksYYc/HFF4ucnJwssm46WhitWrUSefLkydYx/fv3F1nPE/17Mdr4JAcAAHiJRQ4AAPASixwAAOClmOyTozdSzMzMFFk/O9a9SoyJzAaGukfGG2+8YR3z6aefijx79uywXlN+JWJvilihn2HrvhOuWogTJ06IXLt2bZH1z0ik+N4nR/+M635Fw4YNE9nVR0ZvxhgtekPhAQMGiLx7926RdT8nY8LTw4d7UeRUrFhR5L/+9a8iP/nkkyLv3bvXGkNvyKnvX/nZKDQc6JMDAAASCoscAADgJRY5AADASzHZJ0f/vb9+Lq6FYz+V/NB7hcycOdM6pnv37iLPmTNH5Gg9v0ThlC5dWmTd46ZOnTrWOaNHjxb5pptuElnXmLnmxF/+8heRo1WDk2gaNWok8sCBA0XOzc0NeHy0VK5c2XqtV69eAc/p27evyJHaQwvhUa9ePes1/funWbNmIh86dEjk//7v/7bGyMrKCsHVRQ6f5AAAAC+xyAEAAF5ikQMAALzEIgcAAHgpJguPdUHvvn37RNbFnW3atLHG+PXXX0N/YUHoJobG2Juo5afIFLGjfv36Iu/atSvs77l69WrrtXfeeSfs7wtbt27dRNb3Jv1HErFSEL5//37rNX1/evzxx0XeuXNnWK8JoZWSkiJypUqVRN66dat1jt7MWm8Mes8994isf/fGIz7JAQAAXmKRAwAAvMQiBwAAeCkma3K0jRs3ily3bl2Rp0yZYp2jaykioUqVKtZruoajXLlyIh87diycl4QCcG2MGYmNXoPVoCF6Bg0aJHLZsmVF1o1Iy5QpY40RieZpt9xyi8glS5a0jtGNCz/77DOR9TxE9JQoUcJ6TW9E/fXXX4v85Zdfirxw4UJrDN1UVNfoZGRkFOg64wGf5AAAAC+xyAEAAF5ikQMAALyUFKhPS1JSUkw0cbn44otFXrp0qciu54gVKlQI6zUZY/e82b17t3WM3ihv1qxZIt94442hv7B8yMvLSwp+VGjEyjwqDF3vddtttwU83lXXoDeY/fzzz0Vu3bq1yHfeeac1xr/+9a+A7xstkZpH0ZpDus5Bb7iruWq4LrjgApH1PbcwG2F+9NFHIl9++eUinzlzxjqncePGIu/ZsyfgdUUK9yKb6+f90ksvFVn//tFcc+C7774TWW/aqjccjidnm0d8kgMAALzEIgcAAHiJRQ4AAPBSXPTJ0T0g9LPIwjzTDoWmTZuK7HqeqftqDB48OKzXhNC66667RB47dqzIFStWFFnPCWOMGTp0qMh6z5lq1aqJPGLECGuMWK3J8Z3eAypYHYSurzLGmJYtW4r85ptvity8eXORc3JyrDF0nZa+j+jrctUp6v8W9s2LHfr7p3vDGWNMjx49RNa/W/T3U++rZoxdH9ivXz+R9V6LM2fOPMsVxw8+yQEAAF5ikQMAALzEIgcAAHiJRQ4AAPBSXDQD1BuRXXTRRSI/8cQT1jmjR48u0HuULl3aek0XbumiUl3wPHv2bGsMXYioiwp18Vik0IArf3RBoC7c07l4cbuWX2/iqIsKO3ToILKrqWTDhg1FjpWiUd+bAeoNN9PT0wMe/8Ybb1iv6Z95/YcUumGoq7hZv6abTuo/eqhRo4Y1hi5GTrQ5ZEz83ItcG6w+8sgjIl955ZUiN2vWTOQlS5ZYY1x22WUiHzhwQGQ9j/SmoMbYm3rGCpoBAgCAhMIiBwAAeIlFDgAA8FJcNAPs2rVrwK9XqlSpwGN26dJF5Dlz5ljHZGdni6w3Mzt06JDIffr0scbYsWOHyMWKFRNZ13S4NnhE9Oi6hQYNGgT8uqueZsiQISJ/+OGHAcdw1WnFSv1EotF1d7fffrvIixYtEtn1fdJ1efp+prOruam+T+jaiTVr1ohct25da4wNGzZYryE2uebAs88+K/LevXtF1vemvn37WmPoTTt1I1Jd+3XfffdZY7z44osiuxpPxhI+yQEAAF5ikQMAALzEIgcAAHgpLvrk6GvU+YorrrDO0T1r9LNGveHhnj17rDH0xnm6Jkc/36xdu7Y1xs6dO0XOzc0VuXz58tY5kUBviuh57rnnRH744YdF3rx5s3WO3uQxVmp0fO+To+namHLlyons6m/yySefiLxv3z6RdZ+kW2+91Rrj008/FVn3KtG1FfreZYy7XiwWcC8KD13/aYy9Oex5550ncrt27YKOm5mZKXKVKlVEjtaG2fTJAQAACYVFDgAA8BKLHAAA4KW46JOTlZUlst5nqmbNmtY5+nmk3lPohRdeEFnvj2WMvedMMAcPHrRe07UTrmf2SCyNGjUK+HXd7wKxQ/ex0nV5FSpUsM5ZuHChyLouQvc7cdVbPfrooyL/85//DHidunYI0aVrQsePHy/yyJEjRXbNgYLW4em5aYwxN998s8i6Budvf/ubyEOHDrXG0HVoP/zwg8i6L1S08ZMAAAC8xCIHAAB4iUUOAADwEoscAADgpbgoPJ48ebLI9957r8gPPvigdY7eNEw3wjr//PNFXrlypTVGQTfP/O6776zXdKHxjTfeGHAM+KdZs2YiDxgwIODxs2bNCuflIIROnDgh8rZt26xjnnzyySK/z8SJE0UeN26cyPpe5fojCESGLjI2xpgZM2aIvHbtWpErVqwocnp6eugvzAT/gxy9cbXrOlxNb3+vRIkSIp86daoglxhyfJIDAAC8xCIHAAB4iUUOAADwUlxs0KmfV6alpYnsanylj1mzZo3Ix48fF7lHjx7WGIsWLRJZbzx2yy23nOWK/0M/s69cubLI0Xpe6dumePrZsjF2Uz39b/3++++L/Mgjj1hjHD16VOSffvpJ5FGjRomcmppqjTF37lyRdZ3WsWPHRO7cubM1xpYtW6zXYkGibdAZK4I1huvfv7/1mt6UOFb4di+67LLLrNdefPFFkZs0aSLy7bffLrK+NxljN/fTtT/XXHONyOvWrbPGWLp0qcj692Tjxo2tc7QDBw6I3K9fP5E3bdoUdIxwYINOAACQUFjkAAAAL7HIAQAAXoqLPjm6LkJvcLhx40brHF37ovvi6PqM8uXLW2PoZ40pKSki6+firj46r7zySsD3hZt+3lyrVi2Rd+zYIfKhQ4esMfQ5X331lcgtWrQQWW+SZ4wxZcqUEblt27YiuzaHDUbPge+//17kw4cPF3hMJBZdn6H7n7jqMRAe+t++d+/e1jEtW7YMOIbuezRp0iTrmEGDBok8b948kfXG1a7fNfpa9Tm6RmfmzJnWGCtWrBB569at1jGxhE9yAACAl1jkAAAAL7HIAQAAXoqLPjnBuPrklC1bVmRdT6P3E9J/+2+MMbm5uSLv379f5HPPPVfk7du3W2ME62cRLfHWm0L3pwn2jNtFP2/etWuXyHpOGGPXjaYGawAAAiFJREFUBrlqt4LR9RMPP/ywyBMmTBA5JyenwO8RLfTJiQ5d/6fnqa4/M8aYnj17hvWaCive7kWOMUXW+1QZY8wVV1whst7fSf+ecO1/lZ9jfs9Vk7Nq1SqRly1bJvLo0aNF1r3hXNcRK+iTAwAAEgqLHAAA4CUWOQAAwEsscgAAgJe8KDxGwcVbsV+FChVE1hth1q5d2zpHF6QHy66fBf2aPkcXm+sGXcYY88QTT4isN4eNZxQeR8eGDRtE1oX4ixcvts6ZOHGiyLNnzw79hRVCvN2LgtEb8BpjN6ddvXq1yLopn/5DGRe9EWa3bt1E1sXNxtiNdWO1iLgwKDwGAAAJhUUOAADwEoscAADgJWpyElS8PwfXG80tXLjQOuahhx4S+aWXXhJ5+fLlIl977bXWGE8//bTIejPNnTt3ipydnW2N4dNzb42anOjQG8dOmzZN5G3btlnn6Hn41FNPiXzy5MkQXV3BxPu9CLGBmhwAAJBQWOQAAAAvscgBAABeoiYnQfn2HDw5Odl6LSsrK+A5PtfKRAo1OSgq3+5FiA5qcgAAQEJhkQMAALzEIgcAAHipeLQvAAiFzMzMaF8CACDG8EkOAADwEoscAADgJRY5AADASyxyAACAl1jkAAAAL7HIAQAAXmKRAwAAvMQiBwAAeCngBp0AAADxik9yAACAl1jkAAAAL7HIAQAAXmKRAwAAvMQiBwAAeIlFDgAA8NL/AkQFJQCmWW3iAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "9500: [discriminator loss: 0.7105836868286133, acc: 0.140625] [gan loss: 1.244293, acc: 0.000000]\n",
            "9501: [discriminator loss: 0.7213767766952515, acc: 0.296875] [gan loss: 0.912284, acc: 0.062500]\n",
            "9502: [discriminator loss: 0.7636008262634277, acc: 0.1640625] [gan loss: 1.207491, acc: 0.000000]\n",
            "9503: [discriminator loss: 0.7262914776802063, acc: 0.3359375] [gan loss: 0.818296, acc: 0.187500]\n",
            "9504: [discriminator loss: 0.7135128974914551, acc: 0.1015625] [gan loss: 1.318785, acc: 0.000000]\n",
            "9505: [discriminator loss: 0.6925432085990906, acc: 0.3828125] [gan loss: 0.825710, acc: 0.250000]\n",
            "9506: [discriminator loss: 0.7310873866081238, acc: 0.046875] [gan loss: 1.276204, acc: 0.015625]\n",
            "9507: [discriminator loss: 0.6909899711608887, acc: 0.4453125] [gan loss: 0.711325, acc: 0.500000]\n",
            "9508: [discriminator loss: 0.7373387217521667, acc: 0.078125] [gan loss: 1.368063, acc: 0.000000]\n",
            "9509: [discriminator loss: 0.7299646139144897, acc: 0.453125] [gan loss: 0.651765, acc: 0.640625]\n",
            "9510: [discriminator loss: 0.7954568862915039, acc: 0.046875] [gan loss: 1.470243, acc: 0.000000]\n",
            "9511: [discriminator loss: 0.6930198669433594, acc: 0.5] [gan loss: 0.668870, acc: 0.640625]\n",
            "9512: [discriminator loss: 0.7561520338058472, acc: 0.0234375] [gan loss: 1.388413, acc: 0.000000]\n",
            "9513: [discriminator loss: 0.6890623569488525, acc: 0.453125] [gan loss: 0.694356, acc: 0.500000]\n",
            "9514: [discriminator loss: 0.763480544090271, acc: 0.0625] [gan loss: 1.247187, acc: 0.000000]\n",
            "9515: [discriminator loss: 0.6969659328460693, acc: 0.4296875] [gan loss: 0.795502, acc: 0.296875]\n",
            "9516: [discriminator loss: 0.7366750240325928, acc: 0.09375] [gan loss: 1.199257, acc: 0.000000]\n",
            "9517: [discriminator loss: 0.72099769115448, acc: 0.3515625] [gan loss: 0.838700, acc: 0.234375]\n",
            "9518: [discriminator loss: 0.7357520461082458, acc: 0.1015625] [gan loss: 1.230522, acc: 0.015625]\n",
            "9519: [discriminator loss: 0.6985931396484375, acc: 0.3671875] [gan loss: 0.885383, acc: 0.140625]\n",
            "9520: [discriminator loss: 0.7415610551834106, acc: 0.1328125] [gan loss: 1.324602, acc: 0.000000]\n",
            "9521: [discriminator loss: 0.6877979040145874, acc: 0.453125] [gan loss: 0.774195, acc: 0.375000]\n",
            "9522: [discriminator loss: 0.7714599370956421, acc: 0.0234375] [gan loss: 1.539888, acc: 0.000000]\n",
            "9523: [discriminator loss: 0.7041854858398438, acc: 0.4765625] [gan loss: 0.637944, acc: 0.703125]\n",
            "9524: [discriminator loss: 0.7622578740119934, acc: 0.03125] [gan loss: 1.242668, acc: 0.000000]\n",
            "9525: [discriminator loss: 0.6941988468170166, acc: 0.453125] [gan loss: 0.723872, acc: 0.453125]\n",
            "9526: [discriminator loss: 0.7590885162353516, acc: 0.0703125] [gan loss: 1.333665, acc: 0.000000]\n",
            "9527: [discriminator loss: 0.7184907793998718, acc: 0.4140625] [gan loss: 0.819266, acc: 0.234375]\n",
            "9528: [discriminator loss: 0.742812991142273, acc: 0.0625] [gan loss: 1.272722, acc: 0.000000]\n",
            "9529: [discriminator loss: 0.722491443157196, acc: 0.421875] [gan loss: 0.788653, acc: 0.203125]\n",
            "9530: [discriminator loss: 0.7405674457550049, acc: 0.03125] [gan loss: 1.302778, acc: 0.000000]\n",
            "9531: [discriminator loss: 0.7069366574287415, acc: 0.4140625] [gan loss: 0.869070, acc: 0.171875]\n",
            "9532: [discriminator loss: 0.75352942943573, acc: 0.0859375] [gan loss: 1.314635, acc: 0.000000]\n",
            "9533: [discriminator loss: 0.6848311424255371, acc: 0.375] [gan loss: 0.877286, acc: 0.062500]\n",
            "9534: [discriminator loss: 0.7469851970672607, acc: 0.0703125] [gan loss: 1.210474, acc: 0.000000]\n",
            "9535: [discriminator loss: 0.7252690196037292, acc: 0.28125] [gan loss: 0.953649, acc: 0.062500]\n",
            "9536: [discriminator loss: 0.7086580991744995, acc: 0.203125] [gan loss: 0.968061, acc: 0.046875]\n",
            "9537: [discriminator loss: 0.7096840143203735, acc: 0.1953125] [gan loss: 1.118530, acc: 0.000000]\n",
            "9538: [discriminator loss: 0.7115277051925659, acc: 0.34375] [gan loss: 0.890821, acc: 0.125000]\n",
            "9539: [discriminator loss: 0.7199351787567139, acc: 0.140625] [gan loss: 1.213552, acc: 0.000000]\n",
            "9540: [discriminator loss: 0.6879794597625732, acc: 0.421875] [gan loss: 0.749374, acc: 0.375000]\n",
            "9541: [discriminator loss: 0.7566313147544861, acc: 0.0546875] [gan loss: 1.435283, acc: 0.000000]\n",
            "9542: [discriminator loss: 0.7447637915611267, acc: 0.3984375] [gan loss: 0.748317, acc: 0.328125]\n",
            "9543: [discriminator loss: 0.7294341325759888, acc: 0.0546875] [gan loss: 1.375620, acc: 0.000000]\n",
            "9544: [discriminator loss: 0.7124103307723999, acc: 0.390625] [gan loss: 0.689005, acc: 0.593750]\n",
            "9545: [discriminator loss: 0.7511388659477234, acc: 0.046875] [gan loss: 1.392720, acc: 0.000000]\n",
            "9546: [discriminator loss: 0.7007766962051392, acc: 0.453125] [gan loss: 0.791596, acc: 0.296875]\n",
            "9547: [discriminator loss: 0.7082341313362122, acc: 0.1171875] [gan loss: 1.201755, acc: 0.046875]\n",
            "9548: [discriminator loss: 0.6868345737457275, acc: 0.421875] [gan loss: 0.836104, acc: 0.281250]\n",
            "9549: [discriminator loss: 0.7392295598983765, acc: 0.125] [gan loss: 1.291981, acc: 0.000000]\n",
            "9550: [discriminator loss: 0.7044730186462402, acc: 0.40625] [gan loss: 0.723492, acc: 0.421875]\n",
            "9551: [discriminator loss: 0.7367660999298096, acc: 0.0625] [gan loss: 1.329646, acc: 0.000000]\n",
            "9552: [discriminator loss: 0.7176408767700195, acc: 0.3984375] [gan loss: 0.761094, acc: 0.421875]\n",
            "9553: [discriminator loss: 0.765638530254364, acc: 0.0234375] [gan loss: 1.532398, acc: 0.000000]\n",
            "9554: [discriminator loss: 0.6977588534355164, acc: 0.5] [gan loss: 0.610315, acc: 0.734375]\n",
            "9555: [discriminator loss: 0.8075993061065674, acc: 0.0078125] [gan loss: 1.547361, acc: 0.000000]\n",
            "9556: [discriminator loss: 0.7002534866333008, acc: 0.484375] [gan loss: 0.652323, acc: 0.640625]\n",
            "9557: [discriminator loss: 0.7824968695640564, acc: 0.0078125] [gan loss: 1.222046, acc: 0.000000]\n",
            "9558: [discriminator loss: 0.6840508580207825, acc: 0.3984375] [gan loss: 0.823818, acc: 0.203125]\n",
            "9559: [discriminator loss: 0.7262272834777832, acc: 0.0859375] [gan loss: 1.130282, acc: 0.000000]\n",
            "9560: [discriminator loss: 0.6986384987831116, acc: 0.34375] [gan loss: 0.848449, acc: 0.171875]\n",
            "9561: [discriminator loss: 0.6998025178909302, acc: 0.171875] [gan loss: 1.108589, acc: 0.000000]\n",
            "9562: [discriminator loss: 0.6975271701812744, acc: 0.34375] [gan loss: 0.916205, acc: 0.109375]\n",
            "9563: [discriminator loss: 0.708415150642395, acc: 0.125] [gan loss: 1.270475, acc: 0.000000]\n",
            "9564: [discriminator loss: 0.6848403811454773, acc: 0.390625] [gan loss: 0.878919, acc: 0.156250]\n",
            "9565: [discriminator loss: 0.7294731140136719, acc: 0.1015625] [gan loss: 1.256361, acc: 0.000000]\n",
            "9566: [discriminator loss: 0.7204529643058777, acc: 0.390625] [gan loss: 0.764372, acc: 0.390625]\n",
            "9567: [discriminator loss: 0.7162927389144897, acc: 0.078125] [gan loss: 1.172341, acc: 0.000000]\n",
            "9568: [discriminator loss: 0.6908553242683411, acc: 0.3203125] [gan loss: 0.865139, acc: 0.187500]\n",
            "9569: [discriminator loss: 0.7425024509429932, acc: 0.0859375] [gan loss: 1.286950, acc: 0.000000]\n",
            "9570: [discriminator loss: 0.7080268263816833, acc: 0.4140625] [gan loss: 0.773910, acc: 0.343750]\n",
            "9571: [discriminator loss: 0.7499879598617554, acc: 0.0703125] [gan loss: 1.229285, acc: 0.000000]\n",
            "9572: [discriminator loss: 0.7249376177787781, acc: 0.3515625] [gan loss: 0.882756, acc: 0.078125]\n",
            "9573: [discriminator loss: 0.7327613830566406, acc: 0.109375] [gan loss: 1.235542, acc: 0.000000]\n",
            "9574: [discriminator loss: 0.7047870755195618, acc: 0.3828125] [gan loss: 0.906689, acc: 0.109375]\n",
            "9575: [discriminator loss: 0.7125377655029297, acc: 0.1796875] [gan loss: 1.268512, acc: 0.000000]\n",
            "9576: [discriminator loss: 0.712149977684021, acc: 0.3359375] [gan loss: 0.979738, acc: 0.046875]\n",
            "9577: [discriminator loss: 0.7309962511062622, acc: 0.1640625] [gan loss: 1.263938, acc: 0.000000]\n",
            "9578: [discriminator loss: 0.7011685967445374, acc: 0.40625] [gan loss: 0.735608, acc: 0.421875]\n",
            "9579: [discriminator loss: 0.725311279296875, acc: 0.078125] [gan loss: 1.503405, acc: 0.000000]\n",
            "9580: [discriminator loss: 0.7032885551452637, acc: 0.4453125] [gan loss: 0.722780, acc: 0.484375]\n",
            "9581: [discriminator loss: 0.7897405028343201, acc: 0.0390625] [gan loss: 1.657928, acc: 0.000000]\n",
            "9582: [discriminator loss: 0.716005802154541, acc: 0.5] [gan loss: 0.536255, acc: 0.875000]\n",
            "9583: [discriminator loss: 0.8218579292297363, acc: 0.0078125] [gan loss: 1.449740, acc: 0.000000]\n",
            "9584: [discriminator loss: 0.6885733604431152, acc: 0.4921875] [gan loss: 0.696715, acc: 0.515625]\n",
            "9585: [discriminator loss: 0.7770630717277527, acc: 0.0078125] [gan loss: 1.317166, acc: 0.000000]\n",
            "9586: [discriminator loss: 0.7127113342285156, acc: 0.421875] [gan loss: 0.736198, acc: 0.484375]\n",
            "9587: [discriminator loss: 0.747313380241394, acc: 0.0234375] [gan loss: 1.268821, acc: 0.000000]\n",
            "9588: [discriminator loss: 0.6839061975479126, acc: 0.4609375] [gan loss: 0.870096, acc: 0.203125]\n",
            "9589: [discriminator loss: 0.7225762605667114, acc: 0.1015625] [gan loss: 1.207911, acc: 0.000000]\n",
            "9590: [discriminator loss: 0.7333506345748901, acc: 0.328125] [gan loss: 0.935678, acc: 0.031250]\n",
            "9591: [discriminator loss: 0.7340710759162903, acc: 0.125] [gan loss: 1.043350, acc: 0.015625]\n",
            "9592: [discriminator loss: 0.6889597773551941, acc: 0.34375] [gan loss: 0.976831, acc: 0.031250]\n",
            "9593: [discriminator loss: 0.6924982070922852, acc: 0.234375] [gan loss: 1.048811, acc: 0.093750]\n",
            "9594: [discriminator loss: 0.7160924673080444, acc: 0.2578125] [gan loss: 0.972224, acc: 0.078125]\n",
            "9595: [discriminator loss: 0.7112023830413818, acc: 0.203125] [gan loss: 1.080055, acc: 0.031250]\n",
            "9596: [discriminator loss: 0.7111156582832336, acc: 0.265625] [gan loss: 1.127736, acc: 0.015625]\n",
            "9597: [discriminator loss: 0.7156199812889099, acc: 0.234375] [gan loss: 0.990134, acc: 0.015625]\n",
            "9598: [discriminator loss: 0.7037397027015686, acc: 0.1796875] [gan loss: 1.131552, acc: 0.000000]\n",
            "9599: [discriminator loss: 0.6997109651565552, acc: 0.3125] [gan loss: 0.886528, acc: 0.062500]\n",
            "9600: [discriminator loss: 0.7121829986572266, acc: 0.109375] [gan loss: 1.098022, acc: 0.015625]\n",
            "9601: [discriminator loss: 0.7050403356552124, acc: 0.2890625] [gan loss: 1.062231, acc: 0.046875]\n",
            "9602: [discriminator loss: 0.7025560140609741, acc: 0.234375] [gan loss: 1.253759, acc: 0.000000]\n",
            "9603: [discriminator loss: 0.6918429732322693, acc: 0.4140625] [gan loss: 0.822176, acc: 0.156250]\n",
            "9604: [discriminator loss: 0.7368584871292114, acc: 0.0703125] [gan loss: 1.468035, acc: 0.000000]\n",
            "9605: [discriminator loss: 0.7091524600982666, acc: 0.4921875] [gan loss: 0.563740, acc: 0.828125]\n",
            "9606: [discriminator loss: 0.8117151856422424, acc: 0.0] [gan loss: 1.698421, acc: 0.000000]\n",
            "9607: [discriminator loss: 0.7325341701507568, acc: 0.5] [gan loss: 0.579667, acc: 0.843750]\n",
            "9608: [discriminator loss: 0.7973240613937378, acc: 0.0078125] [gan loss: 1.329179, acc: 0.000000]\n",
            "9609: [discriminator loss: 0.7006153464317322, acc: 0.453125] [gan loss: 0.764444, acc: 0.343750]\n",
            "9610: [discriminator loss: 0.7426165342330933, acc: 0.0625] [gan loss: 1.100591, acc: 0.000000]\n",
            "9611: [discriminator loss: 0.7015079259872437, acc: 0.3203125] [gan loss: 0.893533, acc: 0.125000]\n",
            "9612: [discriminator loss: 0.703423023223877, acc: 0.2109375] [gan loss: 0.951752, acc: 0.031250]\n",
            "9613: [discriminator loss: 0.7148905992507935, acc: 0.2109375] [gan loss: 0.971691, acc: 0.031250]\n",
            "9614: [discriminator loss: 0.7146386504173279, acc: 0.1953125] [gan loss: 1.069415, acc: 0.015625]\n",
            "9615: [discriminator loss: 0.7231056690216064, acc: 0.2421875] [gan loss: 1.036151, acc: 0.031250]\n",
            "9616: [discriminator loss: 0.6970199346542358, acc: 0.2421875] [gan loss: 1.051427, acc: 0.031250]\n",
            "9617: [discriminator loss: 0.7249637246131897, acc: 0.2421875] [gan loss: 1.111324, acc: 0.015625]\n",
            "9618: [discriminator loss: 0.6931725740432739, acc: 0.3203125] [gan loss: 0.892063, acc: 0.093750]\n",
            "9619: [discriminator loss: 0.7169038653373718, acc: 0.1953125] [gan loss: 1.208913, acc: 0.000000]\n",
            "9620: [discriminator loss: 0.7257217168807983, acc: 0.2890625] [gan loss: 0.903662, acc: 0.109375]\n",
            "9621: [discriminator loss: 0.7091995477676392, acc: 0.171875] [gan loss: 1.218009, acc: 0.000000]\n",
            "9622: [discriminator loss: 0.6952157020568848, acc: 0.4375] [gan loss: 0.775829, acc: 0.328125]\n",
            "9623: [discriminator loss: 0.7440207004547119, acc: 0.0546875] [gan loss: 1.445569, acc: 0.000000]\n",
            "9624: [discriminator loss: 0.7007836103439331, acc: 0.46875] [gan loss: 0.638515, acc: 0.734375]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "iYqoQLDvnonz",
        "outputId": "92a141dc-11af-47cc-bdec-42c6a634a65b"
      },
      "source": [
        "for layer in gen.layers:\n",
        "    print(layer.name)\n",
        "    try:\n",
        "        df = pd.DataFrame(layer.get_weights()[0].T.reshape(-1, 128))\n",
        "        \n",
        "        display(df.describe())\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": 65,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dense_7\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4900.000000</td>\n",
              "      <td>4.900000e+03</td>\n",
              "      <td>4900.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.000069</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>-0.000246</td>\n",
              "      <td>-0.000389</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>-0.000427</td>\n",
              "      <td>0.000075</td>\n",
              "      <td>-0.000246</td>\n",
              "      <td>0.000095</td>\n",
              "      <td>-0.000308</td>\n",
              "      <td>0.000098</td>\n",
              "      <td>0.000124</td>\n",
              "      <td>-0.000031</td>\n",
              "      <td>-0.000307</td>\n",
              "      <td>0.000160</td>\n",
              "      <td>0.000224</td>\n",
              "      <td>0.000137</td>\n",
              "      <td>0.000248</td>\n",
              "      <td>0.000167</td>\n",
              "      <td>0.000164</td>\n",
              "      <td>0.000365</td>\n",
              "      <td>-0.000042</td>\n",
              "      <td>0.000035</td>\n",
              "      <td>0.000530</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>-0.000171</td>\n",
              "      <td>-0.000195</td>\n",
              "      <td>-0.000005</td>\n",
              "      <td>-0.000347</td>\n",
              "      <td>-0.000040</td>\n",
              "      <td>0.000150</td>\n",
              "      <td>-0.000087</td>\n",
              "      <td>0.000447</td>\n",
              "      <td>-0.000047</td>\n",
              "      <td>-0.000072</td>\n",
              "      <td>-0.000325</td>\n",
              "      <td>0.000050</td>\n",
              "      <td>0.000103</td>\n",
              "      <td>-0.000239</td>\n",
              "      <td>-0.000060</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000005</td>\n",
              "      <td>0.000136</td>\n",
              "      <td>0.000246</td>\n",
              "      <td>-0.000185</td>\n",
              "      <td>-0.000143</td>\n",
              "      <td>-0.000050</td>\n",
              "      <td>0.000155</td>\n",
              "      <td>-0.000010</td>\n",
              "      <td>-0.000276</td>\n",
              "      <td>-0.000246</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>-0.000016</td>\n",
              "      <td>-0.000414</td>\n",
              "      <td>-0.000267</td>\n",
              "      <td>0.000226</td>\n",
              "      <td>0.000011</td>\n",
              "      <td>-0.000030</td>\n",
              "      <td>0.000029</td>\n",
              "      <td>-0.000020</td>\n",
              "      <td>-0.000192</td>\n",
              "      <td>-0.000091</td>\n",
              "      <td>0.000159</td>\n",
              "      <td>0.000256</td>\n",
              "      <td>0.000102</td>\n",
              "      <td>-0.000019</td>\n",
              "      <td>-0.000068</td>\n",
              "      <td>-0.000023</td>\n",
              "      <td>0.000121</td>\n",
              "      <td>0.000003</td>\n",
              "      <td>0.000086</td>\n",
              "      <td>-0.000087</td>\n",
              "      <td>-0.000227</td>\n",
              "      <td>-0.000086</td>\n",
              "      <td>0.000022</td>\n",
              "      <td>0.000013</td>\n",
              "      <td>-0.000096</td>\n",
              "      <td>-0.000401</td>\n",
              "      <td>5.957027e-07</td>\n",
              "      <td>0.000179</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>0.016103</td>\n",
              "      <td>0.016009</td>\n",
              "      <td>0.015963</td>\n",
              "      <td>0.015942</td>\n",
              "      <td>0.016070</td>\n",
              "      <td>0.015848</td>\n",
              "      <td>0.016133</td>\n",
              "      <td>0.015997</td>\n",
              "      <td>0.015982</td>\n",
              "      <td>0.016010</td>\n",
              "      <td>0.015915</td>\n",
              "      <td>0.015976</td>\n",
              "      <td>0.015867</td>\n",
              "      <td>0.015906</td>\n",
              "      <td>0.016069</td>\n",
              "      <td>0.015895</td>\n",
              "      <td>0.015754</td>\n",
              "      <td>0.016090</td>\n",
              "      <td>0.015861</td>\n",
              "      <td>0.015635</td>\n",
              "      <td>0.016014</td>\n",
              "      <td>0.016186</td>\n",
              "      <td>0.016256</td>\n",
              "      <td>0.015888</td>\n",
              "      <td>0.016053</td>\n",
              "      <td>0.015836</td>\n",
              "      <td>0.016187</td>\n",
              "      <td>0.016245</td>\n",
              "      <td>0.016028</td>\n",
              "      <td>0.016017</td>\n",
              "      <td>0.015599</td>\n",
              "      <td>0.016088</td>\n",
              "      <td>0.015823</td>\n",
              "      <td>0.016281</td>\n",
              "      <td>0.016000</td>\n",
              "      <td>0.016114</td>\n",
              "      <td>0.015783</td>\n",
              "      <td>0.016103</td>\n",
              "      <td>0.015993</td>\n",
              "      <td>0.015704</td>\n",
              "      <td>...</td>\n",
              "      <td>0.016103</td>\n",
              "      <td>0.015952</td>\n",
              "      <td>0.015696</td>\n",
              "      <td>0.015794</td>\n",
              "      <td>0.016017</td>\n",
              "      <td>0.016024</td>\n",
              "      <td>0.016027</td>\n",
              "      <td>0.015958</td>\n",
              "      <td>0.015827</td>\n",
              "      <td>0.015963</td>\n",
              "      <td>0.015971</td>\n",
              "      <td>0.015902</td>\n",
              "      <td>0.016034</td>\n",
              "      <td>0.015729</td>\n",
              "      <td>0.015756</td>\n",
              "      <td>0.015937</td>\n",
              "      <td>0.016023</td>\n",
              "      <td>0.015916</td>\n",
              "      <td>0.015939</td>\n",
              "      <td>0.015761</td>\n",
              "      <td>0.015652</td>\n",
              "      <td>0.016129</td>\n",
              "      <td>0.016256</td>\n",
              "      <td>0.015850</td>\n",
              "      <td>0.015763</td>\n",
              "      <td>0.015920</td>\n",
              "      <td>0.015829</td>\n",
              "      <td>0.016016</td>\n",
              "      <td>0.015624</td>\n",
              "      <td>0.015596</td>\n",
              "      <td>0.016050</td>\n",
              "      <td>0.015852</td>\n",
              "      <td>0.016081</td>\n",
              "      <td>0.015815</td>\n",
              "      <td>0.015893</td>\n",
              "      <td>0.015696</td>\n",
              "      <td>0.015682</td>\n",
              "      <td>0.015978</td>\n",
              "      <td>1.586747e-02</td>\n",
              "      <td>0.016013</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>-0.050662</td>\n",
              "      <td>-0.056566</td>\n",
              "      <td>-0.047858</td>\n",
              "      <td>-0.052223</td>\n",
              "      <td>-0.051778</td>\n",
              "      <td>-0.049979</td>\n",
              "      <td>-0.048961</td>\n",
              "      <td>-0.049009</td>\n",
              "      <td>-0.053986</td>\n",
              "      <td>-0.049038</td>\n",
              "      <td>-0.055496</td>\n",
              "      <td>-0.055069</td>\n",
              "      <td>-0.052850</td>\n",
              "      <td>-0.051343</td>\n",
              "      <td>-0.050135</td>\n",
              "      <td>-0.049535</td>\n",
              "      <td>-0.048424</td>\n",
              "      <td>-0.052492</td>\n",
              "      <td>-0.053316</td>\n",
              "      <td>-0.051398</td>\n",
              "      <td>-0.051718</td>\n",
              "      <td>-0.052692</td>\n",
              "      <td>-0.051074</td>\n",
              "      <td>-0.051689</td>\n",
              "      <td>-0.053868</td>\n",
              "      <td>-0.053013</td>\n",
              "      <td>-0.052125</td>\n",
              "      <td>-0.050060</td>\n",
              "      <td>-0.048467</td>\n",
              "      <td>-0.051554</td>\n",
              "      <td>-0.051730</td>\n",
              "      <td>-0.052605</td>\n",
              "      <td>-0.050294</td>\n",
              "      <td>-0.048793</td>\n",
              "      <td>-0.052176</td>\n",
              "      <td>-0.052783</td>\n",
              "      <td>-0.052998</td>\n",
              "      <td>-0.053946</td>\n",
              "      <td>-0.051080</td>\n",
              "      <td>-0.052839</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.048751</td>\n",
              "      <td>-0.050488</td>\n",
              "      <td>-0.048256</td>\n",
              "      <td>-0.056001</td>\n",
              "      <td>-0.050027</td>\n",
              "      <td>-0.051208</td>\n",
              "      <td>-0.051011</td>\n",
              "      <td>-0.051318</td>\n",
              "      <td>-0.048296</td>\n",
              "      <td>-0.047597</td>\n",
              "      <td>-0.050714</td>\n",
              "      <td>-0.052033</td>\n",
              "      <td>-0.048447</td>\n",
              "      <td>-0.047155</td>\n",
              "      <td>-0.054002</td>\n",
              "      <td>-0.052449</td>\n",
              "      <td>-0.049919</td>\n",
              "      <td>-0.047899</td>\n",
              "      <td>-0.047154</td>\n",
              "      <td>-0.050602</td>\n",
              "      <td>-0.050574</td>\n",
              "      <td>-0.051119</td>\n",
              "      <td>-0.057837</td>\n",
              "      <td>-0.053811</td>\n",
              "      <td>-0.051000</td>\n",
              "      <td>-0.050327</td>\n",
              "      <td>-0.048186</td>\n",
              "      <td>-0.051915</td>\n",
              "      <td>-0.050349</td>\n",
              "      <td>-0.050998</td>\n",
              "      <td>-0.050761</td>\n",
              "      <td>-0.051319</td>\n",
              "      <td>-0.052571</td>\n",
              "      <td>-0.046300</td>\n",
              "      <td>-0.050310</td>\n",
              "      <td>-0.052049</td>\n",
              "      <td>-0.051891</td>\n",
              "      <td>-0.047145</td>\n",
              "      <td>-4.917764e-02</td>\n",
              "      <td>-0.050998</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>-0.010461</td>\n",
              "      <td>-0.010124</td>\n",
              "      <td>-0.010366</td>\n",
              "      <td>-0.010858</td>\n",
              "      <td>-0.010045</td>\n",
              "      <td>-0.010739</td>\n",
              "      <td>-0.010444</td>\n",
              "      <td>-0.010700</td>\n",
              "      <td>-0.010209</td>\n",
              "      <td>-0.010758</td>\n",
              "      <td>-0.010406</td>\n",
              "      <td>-0.009950</td>\n",
              "      <td>-0.010523</td>\n",
              "      <td>-0.010389</td>\n",
              "      <td>-0.010437</td>\n",
              "      <td>-0.010209</td>\n",
              "      <td>-0.010135</td>\n",
              "      <td>-0.010365</td>\n",
              "      <td>-0.010017</td>\n",
              "      <td>-0.010207</td>\n",
              "      <td>-0.009853</td>\n",
              "      <td>-0.010496</td>\n",
              "      <td>-0.009985</td>\n",
              "      <td>-0.009658</td>\n",
              "      <td>-0.010540</td>\n",
              "      <td>-0.010195</td>\n",
              "      <td>-0.010942</td>\n",
              "      <td>-0.010514</td>\n",
              "      <td>-0.010765</td>\n",
              "      <td>-0.010575</td>\n",
              "      <td>-0.009829</td>\n",
              "      <td>-0.010032</td>\n",
              "      <td>-0.009983</td>\n",
              "      <td>-0.010650</td>\n",
              "      <td>-0.010426</td>\n",
              "      <td>-0.010877</td>\n",
              "      <td>-0.010118</td>\n",
              "      <td>-0.010484</td>\n",
              "      <td>-0.010700</td>\n",
              "      <td>-0.010233</td>\n",
              "      <td>...</td>\n",
              "      <td>-0.010436</td>\n",
              "      <td>-0.010443</td>\n",
              "      <td>-0.009862</td>\n",
              "      <td>-0.010661</td>\n",
              "      <td>-0.010490</td>\n",
              "      <td>-0.010348</td>\n",
              "      <td>-0.010193</td>\n",
              "      <td>-0.010468</td>\n",
              "      <td>-0.010723</td>\n",
              "      <td>-0.010738</td>\n",
              "      <td>-0.010230</td>\n",
              "      <td>-0.010753</td>\n",
              "      <td>-0.010415</td>\n",
              "      <td>-0.010535</td>\n",
              "      <td>-0.010514</td>\n",
              "      <td>-0.009971</td>\n",
              "      <td>-0.010319</td>\n",
              "      <td>-0.010401</td>\n",
              "      <td>-0.010346</td>\n",
              "      <td>-0.010338</td>\n",
              "      <td>-0.010577</td>\n",
              "      <td>-0.010542</td>\n",
              "      <td>-0.010385</td>\n",
              "      <td>-0.010049</td>\n",
              "      <td>-0.010368</td>\n",
              "      <td>-0.010216</td>\n",
              "      <td>-0.010506</td>\n",
              "      <td>-0.010117</td>\n",
              "      <td>-0.009997</td>\n",
              "      <td>-0.009900</td>\n",
              "      <td>-0.010422</td>\n",
              "      <td>-0.010453</td>\n",
              "      <td>-0.010594</td>\n",
              "      <td>-0.010154</td>\n",
              "      <td>-0.010321</td>\n",
              "      <td>-0.010307</td>\n",
              "      <td>-0.010270</td>\n",
              "      <td>-0.010442</td>\n",
              "      <td>-1.008660e-02</td>\n",
              "      <td>-0.010088</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.000243</td>\n",
              "      <td>0.000179</td>\n",
              "      <td>-0.000226</td>\n",
              "      <td>-0.000066</td>\n",
              "      <td>-0.000044</td>\n",
              "      <td>-0.000687</td>\n",
              "      <td>-0.000109</td>\n",
              "      <td>-0.000108</td>\n",
              "      <td>0.000061</td>\n",
              "      <td>-0.000102</td>\n",
              "      <td>-0.000020</td>\n",
              "      <td>0.000091</td>\n",
              "      <td>-0.000054</td>\n",
              "      <td>-0.000004</td>\n",
              "      <td>0.000324</td>\n",
              "      <td>0.000230</td>\n",
              "      <td>0.000386</td>\n",
              "      <td>0.000219</td>\n",
              "      <td>0.000130</td>\n",
              "      <td>-0.000011</td>\n",
              "      <td>0.000464</td>\n",
              "      <td>-0.000147</td>\n",
              "      <td>-0.000080</td>\n",
              "      <td>0.000309</td>\n",
              "      <td>0.000348</td>\n",
              "      <td>0.000108</td>\n",
              "      <td>-0.000229</td>\n",
              "      <td>-0.000020</td>\n",
              "      <td>-0.000345</td>\n",
              "      <td>0.000109</td>\n",
              "      <td>0.000030</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>0.000436</td>\n",
              "      <td>0.000099</td>\n",
              "      <td>-0.000139</td>\n",
              "      <td>-0.000397</td>\n",
              "      <td>-0.000303</td>\n",
              "      <td>0.000145</td>\n",
              "      <td>-0.000513</td>\n",
              "      <td>-0.000020</td>\n",
              "      <td>...</td>\n",
              "      <td>0.000105</td>\n",
              "      <td>0.000157</td>\n",
              "      <td>0.000358</td>\n",
              "      <td>-0.000335</td>\n",
              "      <td>-0.000250</td>\n",
              "      <td>-0.000379</td>\n",
              "      <td>-0.000043</td>\n",
              "      <td>0.000163</td>\n",
              "      <td>-0.000240</td>\n",
              "      <td>-0.000223</td>\n",
              "      <td>0.000252</td>\n",
              "      <td>0.000093</td>\n",
              "      <td>-0.000064</td>\n",
              "      <td>-0.000334</td>\n",
              "      <td>-0.000192</td>\n",
              "      <td>0.000743</td>\n",
              "      <td>0.000023</td>\n",
              "      <td>-0.000064</td>\n",
              "      <td>0.000134</td>\n",
              "      <td>0.000072</td>\n",
              "      <td>0.000033</td>\n",
              "      <td>-0.000148</td>\n",
              "      <td>0.000138</td>\n",
              "      <td>0.000270</td>\n",
              "      <td>0.000037</td>\n",
              "      <td>0.000010</td>\n",
              "      <td>-0.000045</td>\n",
              "      <td>0.000142</td>\n",
              "      <td>0.000133</td>\n",
              "      <td>0.000017</td>\n",
              "      <td>0.000058</td>\n",
              "      <td>0.000041</td>\n",
              "      <td>-0.000055</td>\n",
              "      <td>0.000038</td>\n",
              "      <td>-0.000012</td>\n",
              "      <td>-0.000174</td>\n",
              "      <td>0.000244</td>\n",
              "      <td>-0.000656</td>\n",
              "      <td>-4.578785e-06</td>\n",
              "      <td>0.000014</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.010587</td>\n",
              "      <td>0.010223</td>\n",
              "      <td>0.010316</td>\n",
              "      <td>0.010025</td>\n",
              "      <td>0.010371</td>\n",
              "      <td>0.010107</td>\n",
              "      <td>0.010433</td>\n",
              "      <td>0.010311</td>\n",
              "      <td>0.010287</td>\n",
              "      <td>0.010068</td>\n",
              "      <td>0.010576</td>\n",
              "      <td>0.010272</td>\n",
              "      <td>0.010273</td>\n",
              "      <td>0.010091</td>\n",
              "      <td>0.010654</td>\n",
              "      <td>0.010229</td>\n",
              "      <td>0.010287</td>\n",
              "      <td>0.010863</td>\n",
              "      <td>0.010568</td>\n",
              "      <td>0.010350</td>\n",
              "      <td>0.010905</td>\n",
              "      <td>0.010243</td>\n",
              "      <td>0.010186</td>\n",
              "      <td>0.011059</td>\n",
              "      <td>0.010565</td>\n",
              "      <td>0.009822</td>\n",
              "      <td>0.010646</td>\n",
              "      <td>0.010733</td>\n",
              "      <td>0.010040</td>\n",
              "      <td>0.010785</td>\n",
              "      <td>0.010148</td>\n",
              "      <td>0.010230</td>\n",
              "      <td>0.010819</td>\n",
              "      <td>0.010761</td>\n",
              "      <td>0.010234</td>\n",
              "      <td>0.010096</td>\n",
              "      <td>0.010524</td>\n",
              "      <td>0.010557</td>\n",
              "      <td>0.010303</td>\n",
              "      <td>0.010342</td>\n",
              "      <td>...</td>\n",
              "      <td>0.010540</td>\n",
              "      <td>0.010723</td>\n",
              "      <td>0.010467</td>\n",
              "      <td>0.010083</td>\n",
              "      <td>0.010125</td>\n",
              "      <td>0.010267</td>\n",
              "      <td>0.010636</td>\n",
              "      <td>0.010311</td>\n",
              "      <td>0.009867</td>\n",
              "      <td>0.010101</td>\n",
              "      <td>0.010494</td>\n",
              "      <td>0.010497</td>\n",
              "      <td>0.010655</td>\n",
              "      <td>0.009656</td>\n",
              "      <td>0.009905</td>\n",
              "      <td>0.010652</td>\n",
              "      <td>0.010466</td>\n",
              "      <td>0.010315</td>\n",
              "      <td>0.010446</td>\n",
              "      <td>0.009815</td>\n",
              "      <td>0.009855</td>\n",
              "      <td>0.010253</td>\n",
              "      <td>0.010734</td>\n",
              "      <td>0.010659</td>\n",
              "      <td>0.010560</td>\n",
              "      <td>0.010461</td>\n",
              "      <td>0.010598</td>\n",
              "      <td>0.010365</td>\n",
              "      <td>0.009882</td>\n",
              "      <td>0.010413</td>\n",
              "      <td>0.010705</td>\n",
              "      <td>0.010365</td>\n",
              "      <td>0.010636</td>\n",
              "      <td>0.010221</td>\n",
              "      <td>0.010241</td>\n",
              "      <td>0.009869</td>\n",
              "      <td>0.009867</td>\n",
              "      <td>0.010149</td>\n",
              "      <td>1.017719e-02</td>\n",
              "      <td>0.010680</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.050622</td>\n",
              "      <td>0.049044</td>\n",
              "      <td>0.048141</td>\n",
              "      <td>0.051485</td>\n",
              "      <td>0.049033</td>\n",
              "      <td>0.050716</td>\n",
              "      <td>0.053266</td>\n",
              "      <td>0.051621</td>\n",
              "      <td>0.053073</td>\n",
              "      <td>0.050091</td>\n",
              "      <td>0.057099</td>\n",
              "      <td>0.052694</td>\n",
              "      <td>0.055025</td>\n",
              "      <td>0.050771</td>\n",
              "      <td>0.050006</td>\n",
              "      <td>0.051279</td>\n",
              "      <td>0.053202</td>\n",
              "      <td>0.050979</td>\n",
              "      <td>0.049481</td>\n",
              "      <td>0.054269</td>\n",
              "      <td>0.054215</td>\n",
              "      <td>0.054858</td>\n",
              "      <td>0.053210</td>\n",
              "      <td>0.051305</td>\n",
              "      <td>0.047457</td>\n",
              "      <td>0.052928</td>\n",
              "      <td>0.055734</td>\n",
              "      <td>0.053647</td>\n",
              "      <td>0.051194</td>\n",
              "      <td>0.049096</td>\n",
              "      <td>0.050130</td>\n",
              "      <td>0.052037</td>\n",
              "      <td>0.049947</td>\n",
              "      <td>0.045067</td>\n",
              "      <td>0.050594</td>\n",
              "      <td>0.051095</td>\n",
              "      <td>0.058272</td>\n",
              "      <td>0.056099</td>\n",
              "      <td>0.056286</td>\n",
              "      <td>0.051035</td>\n",
              "      <td>...</td>\n",
              "      <td>0.051791</td>\n",
              "      <td>0.050091</td>\n",
              "      <td>0.048282</td>\n",
              "      <td>0.052198</td>\n",
              "      <td>0.050873</td>\n",
              "      <td>0.055201</td>\n",
              "      <td>0.049848</td>\n",
              "      <td>0.047670</td>\n",
              "      <td>0.053604</td>\n",
              "      <td>0.048104</td>\n",
              "      <td>0.049559</td>\n",
              "      <td>0.048001</td>\n",
              "      <td>0.050799</td>\n",
              "      <td>0.046908</td>\n",
              "      <td>0.049170</td>\n",
              "      <td>0.049857</td>\n",
              "      <td>0.058256</td>\n",
              "      <td>0.054281</td>\n",
              "      <td>0.050786</td>\n",
              "      <td>0.054115</td>\n",
              "      <td>0.048243</td>\n",
              "      <td>0.049564</td>\n",
              "      <td>0.047287</td>\n",
              "      <td>0.052254</td>\n",
              "      <td>0.047719</td>\n",
              "      <td>0.049433</td>\n",
              "      <td>0.051785</td>\n",
              "      <td>0.057360</td>\n",
              "      <td>0.052942</td>\n",
              "      <td>0.052957</td>\n",
              "      <td>0.051992</td>\n",
              "      <td>0.047676</td>\n",
              "      <td>0.056648</td>\n",
              "      <td>0.050250</td>\n",
              "      <td>0.049086</td>\n",
              "      <td>0.048619</td>\n",
              "      <td>0.051039</td>\n",
              "      <td>0.049509</td>\n",
              "      <td>5.289202e-02</td>\n",
              "      <td>0.049883</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  128 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "               0            1    ...           126          127\n",
              "count  4900.000000  4900.000000  ...  4.900000e+03  4900.000000\n",
              "mean      0.000069     0.000102  ...  5.957027e-07     0.000179\n",
              "std       0.016103     0.016009  ...  1.586747e-02     0.016013\n",
              "min      -0.050662    -0.056566  ... -4.917764e-02    -0.050998\n",
              "25%      -0.010461    -0.010124  ... -1.008660e-02    -0.010088\n",
              "50%       0.000243     0.000179  ... -4.578785e-06     0.000014\n",
              "75%       0.010587     0.010223  ...  1.017719e-02     0.010680\n",
              "max       0.050622     0.049044  ...  5.289202e-02     0.049883\n",
              "\n",
              "[8 rows x 128 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "reshape_4\n",
            "conv_trans_block_14\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>1.009667</td>\n",
              "      <td>1.011762</td>\n",
              "      <td>1.012202</td>\n",
              "      <td>0.999938</td>\n",
              "      <td>0.985055</td>\n",
              "      <td>1.007311</td>\n",
              "      <td>1.003887</td>\n",
              "      <td>1.006909</td>\n",
              "      <td>0.980367</td>\n",
              "      <td>0.990979</td>\n",
              "      <td>1.007995</td>\n",
              "      <td>1.008603</td>\n",
              "      <td>0.982435</td>\n",
              "      <td>0.99572</td>\n",
              "      <td>1.002329</td>\n",
              "      <td>0.992653</td>\n",
              "      <td>0.987569</td>\n",
              "      <td>1.010866</td>\n",
              "      <td>1.002767</td>\n",
              "      <td>0.978246</td>\n",
              "      <td>1.009169</td>\n",
              "      <td>1.009043</td>\n",
              "      <td>0.978408</td>\n",
              "      <td>0.988885</td>\n",
              "      <td>0.991227</td>\n",
              "      <td>1.003236</td>\n",
              "      <td>0.999237</td>\n",
              "      <td>1.008865</td>\n",
              "      <td>1.002729</td>\n",
              "      <td>1.003449</td>\n",
              "      <td>1.001694</td>\n",
              "      <td>0.990352</td>\n",
              "      <td>0.994587</td>\n",
              "      <td>0.982006</td>\n",
              "      <td>0.993089</td>\n",
              "      <td>0.996235</td>\n",
              "      <td>0.97917</td>\n",
              "      <td>0.994326</td>\n",
              "      <td>0.994341</td>\n",
              "      <td>0.992834</td>\n",
              "      <td>...</td>\n",
              "      <td>0.98772</td>\n",
              "      <td>0.998055</td>\n",
              "      <td>1.005141</td>\n",
              "      <td>1.008542</td>\n",
              "      <td>0.992476</td>\n",
              "      <td>1.013149</td>\n",
              "      <td>0.994101</td>\n",
              "      <td>1.000908</td>\n",
              "      <td>1.008154</td>\n",
              "      <td>0.997087</td>\n",
              "      <td>0.988208</td>\n",
              "      <td>1.00025</td>\n",
              "      <td>1.007042</td>\n",
              "      <td>0.986021</td>\n",
              "      <td>1.00124</td>\n",
              "      <td>0.999967</td>\n",
              "      <td>1.010749</td>\n",
              "      <td>0.989907</td>\n",
              "      <td>0.993326</td>\n",
              "      <td>0.993137</td>\n",
              "      <td>1.007583</td>\n",
              "      <td>1.012241</td>\n",
              "      <td>0.994379</td>\n",
              "      <td>0.996485</td>\n",
              "      <td>0.995146</td>\n",
              "      <td>0.995418</td>\n",
              "      <td>0.998302</td>\n",
              "      <td>0.992474</td>\n",
              "      <td>0.999278</td>\n",
              "      <td>0.98676</td>\n",
              "      <td>1.00208</td>\n",
              "      <td>0.970148</td>\n",
              "      <td>0.996148</td>\n",
              "      <td>1.002422</td>\n",
              "      <td>1.004608</td>\n",
              "      <td>1.005918</td>\n",
              "      <td>1.002921</td>\n",
              "      <td>1.004027</td>\n",
              "      <td>0.975106</td>\n",
              "      <td>1.005918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>1.009667</td>\n",
              "      <td>1.011762</td>\n",
              "      <td>1.012202</td>\n",
              "      <td>0.999938</td>\n",
              "      <td>0.985055</td>\n",
              "      <td>1.007311</td>\n",
              "      <td>1.003887</td>\n",
              "      <td>1.006909</td>\n",
              "      <td>0.980367</td>\n",
              "      <td>0.990979</td>\n",
              "      <td>1.007995</td>\n",
              "      <td>1.008603</td>\n",
              "      <td>0.982435</td>\n",
              "      <td>0.99572</td>\n",
              "      <td>1.002329</td>\n",
              "      <td>0.992653</td>\n",
              "      <td>0.987569</td>\n",
              "      <td>1.010866</td>\n",
              "      <td>1.002767</td>\n",
              "      <td>0.978246</td>\n",
              "      <td>1.009169</td>\n",
              "      <td>1.009043</td>\n",
              "      <td>0.978408</td>\n",
              "      <td>0.988885</td>\n",
              "      <td>0.991227</td>\n",
              "      <td>1.003236</td>\n",
              "      <td>0.999237</td>\n",
              "      <td>1.008865</td>\n",
              "      <td>1.002729</td>\n",
              "      <td>1.003449</td>\n",
              "      <td>1.001694</td>\n",
              "      <td>0.990352</td>\n",
              "      <td>0.994587</td>\n",
              "      <td>0.982006</td>\n",
              "      <td>0.993089</td>\n",
              "      <td>0.996235</td>\n",
              "      <td>0.97917</td>\n",
              "      <td>0.994326</td>\n",
              "      <td>0.994341</td>\n",
              "      <td>0.992834</td>\n",
              "      <td>...</td>\n",
              "      <td>0.98772</td>\n",
              "      <td>0.998055</td>\n",
              "      <td>1.005141</td>\n",
              "      <td>1.008542</td>\n",
              "      <td>0.992476</td>\n",
              "      <td>1.013149</td>\n",
              "      <td>0.994101</td>\n",
              "      <td>1.000908</td>\n",
              "      <td>1.008154</td>\n",
              "      <td>0.997087</td>\n",
              "      <td>0.988208</td>\n",
              "      <td>1.00025</td>\n",
              "      <td>1.007042</td>\n",
              "      <td>0.986021</td>\n",
              "      <td>1.00124</td>\n",
              "      <td>0.999967</td>\n",
              "      <td>1.010749</td>\n",
              "      <td>0.989907</td>\n",
              "      <td>0.993326</td>\n",
              "      <td>0.993137</td>\n",
              "      <td>1.007583</td>\n",
              "      <td>1.012241</td>\n",
              "      <td>0.994379</td>\n",
              "      <td>0.996485</td>\n",
              "      <td>0.995146</td>\n",
              "      <td>0.995418</td>\n",
              "      <td>0.998302</td>\n",
              "      <td>0.992474</td>\n",
              "      <td>0.999278</td>\n",
              "      <td>0.98676</td>\n",
              "      <td>1.00208</td>\n",
              "      <td>0.970148</td>\n",
              "      <td>0.996148</td>\n",
              "      <td>1.002422</td>\n",
              "      <td>1.004608</td>\n",
              "      <td>1.005918</td>\n",
              "      <td>1.002921</td>\n",
              "      <td>1.004027</td>\n",
              "      <td>0.975106</td>\n",
              "      <td>1.005918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>1.009667</td>\n",
              "      <td>1.011762</td>\n",
              "      <td>1.012202</td>\n",
              "      <td>0.999938</td>\n",
              "      <td>0.985055</td>\n",
              "      <td>1.007311</td>\n",
              "      <td>1.003887</td>\n",
              "      <td>1.006909</td>\n",
              "      <td>0.980367</td>\n",
              "      <td>0.990979</td>\n",
              "      <td>1.007995</td>\n",
              "      <td>1.008603</td>\n",
              "      <td>0.982435</td>\n",
              "      <td>0.99572</td>\n",
              "      <td>1.002329</td>\n",
              "      <td>0.992653</td>\n",
              "      <td>0.987569</td>\n",
              "      <td>1.010866</td>\n",
              "      <td>1.002767</td>\n",
              "      <td>0.978246</td>\n",
              "      <td>1.009169</td>\n",
              "      <td>1.009043</td>\n",
              "      <td>0.978408</td>\n",
              "      <td>0.988885</td>\n",
              "      <td>0.991227</td>\n",
              "      <td>1.003236</td>\n",
              "      <td>0.999237</td>\n",
              "      <td>1.008865</td>\n",
              "      <td>1.002729</td>\n",
              "      <td>1.003449</td>\n",
              "      <td>1.001694</td>\n",
              "      <td>0.990352</td>\n",
              "      <td>0.994587</td>\n",
              "      <td>0.982006</td>\n",
              "      <td>0.993089</td>\n",
              "      <td>0.996235</td>\n",
              "      <td>0.97917</td>\n",
              "      <td>0.994326</td>\n",
              "      <td>0.994341</td>\n",
              "      <td>0.992834</td>\n",
              "      <td>...</td>\n",
              "      <td>0.98772</td>\n",
              "      <td>0.998055</td>\n",
              "      <td>1.005141</td>\n",
              "      <td>1.008542</td>\n",
              "      <td>0.992476</td>\n",
              "      <td>1.013149</td>\n",
              "      <td>0.994101</td>\n",
              "      <td>1.000908</td>\n",
              "      <td>1.008154</td>\n",
              "      <td>0.997087</td>\n",
              "      <td>0.988208</td>\n",
              "      <td>1.00025</td>\n",
              "      <td>1.007042</td>\n",
              "      <td>0.986021</td>\n",
              "      <td>1.00124</td>\n",
              "      <td>0.999967</td>\n",
              "      <td>1.010749</td>\n",
              "      <td>0.989907</td>\n",
              "      <td>0.993326</td>\n",
              "      <td>0.993137</td>\n",
              "      <td>1.007583</td>\n",
              "      <td>1.012241</td>\n",
              "      <td>0.994379</td>\n",
              "      <td>0.996485</td>\n",
              "      <td>0.995146</td>\n",
              "      <td>0.995418</td>\n",
              "      <td>0.998302</td>\n",
              "      <td>0.992474</td>\n",
              "      <td>0.999278</td>\n",
              "      <td>0.98676</td>\n",
              "      <td>1.00208</td>\n",
              "      <td>0.970148</td>\n",
              "      <td>0.996148</td>\n",
              "      <td>1.002422</td>\n",
              "      <td>1.004608</td>\n",
              "      <td>1.005918</td>\n",
              "      <td>1.002921</td>\n",
              "      <td>1.004027</td>\n",
              "      <td>0.975106</td>\n",
              "      <td>1.005918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>1.009667</td>\n",
              "      <td>1.011762</td>\n",
              "      <td>1.012202</td>\n",
              "      <td>0.999938</td>\n",
              "      <td>0.985055</td>\n",
              "      <td>1.007311</td>\n",
              "      <td>1.003887</td>\n",
              "      <td>1.006909</td>\n",
              "      <td>0.980367</td>\n",
              "      <td>0.990979</td>\n",
              "      <td>1.007995</td>\n",
              "      <td>1.008603</td>\n",
              "      <td>0.982435</td>\n",
              "      <td>0.99572</td>\n",
              "      <td>1.002329</td>\n",
              "      <td>0.992653</td>\n",
              "      <td>0.987569</td>\n",
              "      <td>1.010866</td>\n",
              "      <td>1.002767</td>\n",
              "      <td>0.978246</td>\n",
              "      <td>1.009169</td>\n",
              "      <td>1.009043</td>\n",
              "      <td>0.978408</td>\n",
              "      <td>0.988885</td>\n",
              "      <td>0.991227</td>\n",
              "      <td>1.003236</td>\n",
              "      <td>0.999237</td>\n",
              "      <td>1.008865</td>\n",
              "      <td>1.002729</td>\n",
              "      <td>1.003449</td>\n",
              "      <td>1.001694</td>\n",
              "      <td>0.990352</td>\n",
              "      <td>0.994587</td>\n",
              "      <td>0.982006</td>\n",
              "      <td>0.993089</td>\n",
              "      <td>0.996235</td>\n",
              "      <td>0.97917</td>\n",
              "      <td>0.994326</td>\n",
              "      <td>0.994341</td>\n",
              "      <td>0.992834</td>\n",
              "      <td>...</td>\n",
              "      <td>0.98772</td>\n",
              "      <td>0.998055</td>\n",
              "      <td>1.005141</td>\n",
              "      <td>1.008542</td>\n",
              "      <td>0.992476</td>\n",
              "      <td>1.013149</td>\n",
              "      <td>0.994101</td>\n",
              "      <td>1.000908</td>\n",
              "      <td>1.008154</td>\n",
              "      <td>0.997087</td>\n",
              "      <td>0.988208</td>\n",
              "      <td>1.00025</td>\n",
              "      <td>1.007042</td>\n",
              "      <td>0.986021</td>\n",
              "      <td>1.00124</td>\n",
              "      <td>0.999967</td>\n",
              "      <td>1.010749</td>\n",
              "      <td>0.989907</td>\n",
              "      <td>0.993326</td>\n",
              "      <td>0.993137</td>\n",
              "      <td>1.007583</td>\n",
              "      <td>1.012241</td>\n",
              "      <td>0.994379</td>\n",
              "      <td>0.996485</td>\n",
              "      <td>0.995146</td>\n",
              "      <td>0.995418</td>\n",
              "      <td>0.998302</td>\n",
              "      <td>0.992474</td>\n",
              "      <td>0.999278</td>\n",
              "      <td>0.98676</td>\n",
              "      <td>1.00208</td>\n",
              "      <td>0.970148</td>\n",
              "      <td>0.996148</td>\n",
              "      <td>1.002422</td>\n",
              "      <td>1.004608</td>\n",
              "      <td>1.005918</td>\n",
              "      <td>1.002921</td>\n",
              "      <td>1.004027</td>\n",
              "      <td>0.975106</td>\n",
              "      <td>1.005918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>1.009667</td>\n",
              "      <td>1.011762</td>\n",
              "      <td>1.012202</td>\n",
              "      <td>0.999938</td>\n",
              "      <td>0.985055</td>\n",
              "      <td>1.007311</td>\n",
              "      <td>1.003887</td>\n",
              "      <td>1.006909</td>\n",
              "      <td>0.980367</td>\n",
              "      <td>0.990979</td>\n",
              "      <td>1.007995</td>\n",
              "      <td>1.008603</td>\n",
              "      <td>0.982435</td>\n",
              "      <td>0.99572</td>\n",
              "      <td>1.002329</td>\n",
              "      <td>0.992653</td>\n",
              "      <td>0.987569</td>\n",
              "      <td>1.010866</td>\n",
              "      <td>1.002767</td>\n",
              "      <td>0.978246</td>\n",
              "      <td>1.009169</td>\n",
              "      <td>1.009043</td>\n",
              "      <td>0.978408</td>\n",
              "      <td>0.988885</td>\n",
              "      <td>0.991227</td>\n",
              "      <td>1.003236</td>\n",
              "      <td>0.999237</td>\n",
              "      <td>1.008865</td>\n",
              "      <td>1.002729</td>\n",
              "      <td>1.003449</td>\n",
              "      <td>1.001694</td>\n",
              "      <td>0.990352</td>\n",
              "      <td>0.994587</td>\n",
              "      <td>0.982006</td>\n",
              "      <td>0.993089</td>\n",
              "      <td>0.996235</td>\n",
              "      <td>0.97917</td>\n",
              "      <td>0.994326</td>\n",
              "      <td>0.994341</td>\n",
              "      <td>0.992834</td>\n",
              "      <td>...</td>\n",
              "      <td>0.98772</td>\n",
              "      <td>0.998055</td>\n",
              "      <td>1.005141</td>\n",
              "      <td>1.008542</td>\n",
              "      <td>0.992476</td>\n",
              "      <td>1.013149</td>\n",
              "      <td>0.994101</td>\n",
              "      <td>1.000908</td>\n",
              "      <td>1.008154</td>\n",
              "      <td>0.997087</td>\n",
              "      <td>0.988208</td>\n",
              "      <td>1.00025</td>\n",
              "      <td>1.007042</td>\n",
              "      <td>0.986021</td>\n",
              "      <td>1.00124</td>\n",
              "      <td>0.999967</td>\n",
              "      <td>1.010749</td>\n",
              "      <td>0.989907</td>\n",
              "      <td>0.993326</td>\n",
              "      <td>0.993137</td>\n",
              "      <td>1.007583</td>\n",
              "      <td>1.012241</td>\n",
              "      <td>0.994379</td>\n",
              "      <td>0.996485</td>\n",
              "      <td>0.995146</td>\n",
              "      <td>0.995418</td>\n",
              "      <td>0.998302</td>\n",
              "      <td>0.992474</td>\n",
              "      <td>0.999278</td>\n",
              "      <td>0.98676</td>\n",
              "      <td>1.00208</td>\n",
              "      <td>0.970148</td>\n",
              "      <td>0.996148</td>\n",
              "      <td>1.002422</td>\n",
              "      <td>1.004608</td>\n",
              "      <td>1.005918</td>\n",
              "      <td>1.002921</td>\n",
              "      <td>1.004027</td>\n",
              "      <td>0.975106</td>\n",
              "      <td>1.005918</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>1.009667</td>\n",
              "      <td>1.011762</td>\n",
              "      <td>1.012202</td>\n",
              "      <td>0.999938</td>\n",
              "      <td>0.985055</td>\n",
              "      <td>1.007311</td>\n",
              "      <td>1.003887</td>\n",
              "      <td>1.006909</td>\n",
              "      <td>0.980367</td>\n",
              "      <td>0.990979</td>\n",
              "      <td>1.007995</td>\n",
              "      <td>1.008603</td>\n",
              "      <td>0.982435</td>\n",
              "      <td>0.99572</td>\n",
              "      <td>1.002329</td>\n",
              "      <td>0.992653</td>\n",
              "      <td>0.987569</td>\n",
              "      <td>1.010866</td>\n",
              "      <td>1.002767</td>\n",
              "      <td>0.978246</td>\n",
              "      <td>1.009169</td>\n",
              "      <td>1.009043</td>\n",
              "      <td>0.978408</td>\n",
              "      <td>0.988885</td>\n",
              "      <td>0.991227</td>\n",
              "      <td>1.003236</td>\n",
              "      <td>0.999237</td>\n",
              "      <td>1.008865</td>\n",
              "      <td>1.002729</td>\n",
              "      <td>1.003449</td>\n",
              "      <td>1.001694</td>\n",
              "      <td>0.990352</td>\n",
              "      <td>0.994587</td>\n",
              "      <td>0.982006</td>\n",
              "      <td>0.993089</td>\n",
              "      <td>0.996235</td>\n",
              "      <td>0.97917</td>\n",
              "      <td>0.994326</td>\n",
              "      <td>0.994341</td>\n",
              "      <td>0.992834</td>\n",
              "      <td>...</td>\n",
              "      <td>0.98772</td>\n",
              "      <td>0.998055</td>\n",
              "      <td>1.005141</td>\n",
              "      <td>1.008542</td>\n",
              "      <td>0.992476</td>\n",
              "      <td>1.013149</td>\n",
              "      <td>0.994101</td>\n",
              "      <td>1.000908</td>\n",
              "      <td>1.008154</td>\n",
              "      <td>0.997087</td>\n",
              "      <td>0.988208</td>\n",
              "      <td>1.00025</td>\n",
              "      <td>1.007042</td>\n",
              "      <td>0.986021</td>\n",
              "      <td>1.00124</td>\n",
              "      <td>0.999967</td>\n",
              "      <td>1.010749</td>\n",
              "      <td>0.989907</td>\n",
              "      <td>0.993326</td>\n",
              "      <td>0.993137</td>\n",
              "      <td>1.007583</td>\n",
              "      <td>1.012241</td>\n",
              "      <td>0.994379</td>\n",
              "      <td>0.996485</td>\n",
              "      <td>0.995146</td>\n",
              "      <td>0.995418</td>\n",
              "      <td>0.998302</td>\n",
              "      <td>0.992474</td>\n",
              "      <td>0.999278</td>\n",
              "      <td>0.98676</td>\n",
              "      <td>1.00208</td>\n",
              "      <td>0.970148</td>\n",
              "      <td>0.996148</td>\n",
              "      <td>1.002422</td>\n",
              "      <td>1.004608</td>\n",
              "      <td>1.005918</td>\n",
              "      <td>1.002921</td>\n",
              "      <td>1.004027</td>\n",
              "      <td>0.975106</td>\n",
              "      <td>1.005918</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  128 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0         1         2    ...       125       126       127\n",
              "count  1.000000  1.000000  1.000000  ...  1.000000  1.000000  1.000000\n",
              "mean   1.009667  1.011762  1.012202  ...  1.004027  0.975106  1.005918\n",
              "std         NaN       NaN       NaN  ...       NaN       NaN       NaN\n",
              "min    1.009667  1.011762  1.012202  ...  1.004027  0.975106  1.005918\n",
              "25%    1.009667  1.011762  1.012202  ...  1.004027  0.975106  1.005918\n",
              "50%    1.009667  1.011762  1.012202  ...  1.004027  0.975106  1.005918\n",
              "75%    1.009667  1.011762  1.012202  ...  1.004027  0.975106  1.005918\n",
              "max    1.009667  1.011762  1.012202  ...  1.004027  0.975106  1.005918\n",
              "\n",
              "[8 rows x 128 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "conv_trans_block_15\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>88</th>\n",
              "      <th>89</th>\n",
              "      <th>90</th>\n",
              "      <th>91</th>\n",
              "      <th>92</th>\n",
              "      <th>93</th>\n",
              "      <th>94</th>\n",
              "      <th>95</th>\n",
              "      <th>96</th>\n",
              "      <th>97</th>\n",
              "      <th>98</th>\n",
              "      <th>99</th>\n",
              "      <th>100</th>\n",
              "      <th>101</th>\n",
              "      <th>102</th>\n",
              "      <th>103</th>\n",
              "      <th>104</th>\n",
              "      <th>105</th>\n",
              "      <th>106</th>\n",
              "      <th>107</th>\n",
              "      <th>108</th>\n",
              "      <th>109</th>\n",
              "      <th>110</th>\n",
              "      <th>111</th>\n",
              "      <th>112</th>\n",
              "      <th>113</th>\n",
              "      <th>114</th>\n",
              "      <th>115</th>\n",
              "      <th>116</th>\n",
              "      <th>117</th>\n",
              "      <th>118</th>\n",
              "      <th>119</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>count</th>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.00000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "      <td>1.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>mean</th>\n",
              "      <td>0.993737</td>\n",
              "      <td>0.99683</td>\n",
              "      <td>1.002031</td>\n",
              "      <td>1.013713</td>\n",
              "      <td>1.006133</td>\n",
              "      <td>1.001171</td>\n",
              "      <td>1.009853</td>\n",
              "      <td>1.001342</td>\n",
              "      <td>1.008162</td>\n",
              "      <td>1.00205</td>\n",
              "      <td>0.993886</td>\n",
              "      <td>1.000062</td>\n",
              "      <td>0.997335</td>\n",
              "      <td>0.987785</td>\n",
              "      <td>1.00197</td>\n",
              "      <td>0.995764</td>\n",
              "      <td>0.991636</td>\n",
              "      <td>0.994293</td>\n",
              "      <td>0.996284</td>\n",
              "      <td>0.996068</td>\n",
              "      <td>0.993233</td>\n",
              "      <td>1.003574</td>\n",
              "      <td>0.99276</td>\n",
              "      <td>1.003377</td>\n",
              "      <td>0.990974</td>\n",
              "      <td>1.000663</td>\n",
              "      <td>0.994784</td>\n",
              "      <td>0.991328</td>\n",
              "      <td>0.989916</td>\n",
              "      <td>0.999591</td>\n",
              "      <td>0.991639</td>\n",
              "      <td>0.996234</td>\n",
              "      <td>1.008404</td>\n",
              "      <td>1.003346</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.999117</td>\n",
              "      <td>0.993729</td>\n",
              "      <td>0.995164</td>\n",
              "      <td>0.994916</td>\n",
              "      <td>0.995566</td>\n",
              "      <td>...</td>\n",
              "      <td>1.005608</td>\n",
              "      <td>0.989219</td>\n",
              "      <td>0.987627</td>\n",
              "      <td>0.999931</td>\n",
              "      <td>1.006638</td>\n",
              "      <td>1.00298</td>\n",
              "      <td>0.992956</td>\n",
              "      <td>0.998666</td>\n",
              "      <td>0.98926</td>\n",
              "      <td>0.99663</td>\n",
              "      <td>1.005869</td>\n",
              "      <td>1.003529</td>\n",
              "      <td>1.005053</td>\n",
              "      <td>0.994999</td>\n",
              "      <td>1.011835</td>\n",
              "      <td>1.001468</td>\n",
              "      <td>1.006677</td>\n",
              "      <td>0.998059</td>\n",
              "      <td>0.99398</td>\n",
              "      <td>1.001571</td>\n",
              "      <td>0.996599</td>\n",
              "      <td>0.993639</td>\n",
              "      <td>0.996694</td>\n",
              "      <td>1.006816</td>\n",
              "      <td>1.014685</td>\n",
              "      <td>0.989651</td>\n",
              "      <td>1.012758</td>\n",
              "      <td>1.00731</td>\n",
              "      <td>0.991233</td>\n",
              "      <td>1.019647</td>\n",
              "      <td>0.996789</td>\n",
              "      <td>1.003528</td>\n",
              "      <td>0.992621</td>\n",
              "      <td>0.992612</td>\n",
              "      <td>1.00195</td>\n",
              "      <td>0.996996</td>\n",
              "      <td>0.99308</td>\n",
              "      <td>0.990122</td>\n",
              "      <td>0.996108</td>\n",
              "      <td>0.996281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>std</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>...</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>min</th>\n",
              "      <td>0.993737</td>\n",
              "      <td>0.99683</td>\n",
              "      <td>1.002031</td>\n",
              "      <td>1.013713</td>\n",
              "      <td>1.006133</td>\n",
              "      <td>1.001171</td>\n",
              "      <td>1.009853</td>\n",
              "      <td>1.001342</td>\n",
              "      <td>1.008162</td>\n",
              "      <td>1.00205</td>\n",
              "      <td>0.993886</td>\n",
              "      <td>1.000062</td>\n",
              "      <td>0.997335</td>\n",
              "      <td>0.987785</td>\n",
              "      <td>1.00197</td>\n",
              "      <td>0.995764</td>\n",
              "      <td>0.991636</td>\n",
              "      <td>0.994293</td>\n",
              "      <td>0.996284</td>\n",
              "      <td>0.996068</td>\n",
              "      <td>0.993233</td>\n",
              "      <td>1.003574</td>\n",
              "      <td>0.99276</td>\n",
              "      <td>1.003377</td>\n",
              "      <td>0.990974</td>\n",
              "      <td>1.000663</td>\n",
              "      <td>0.994784</td>\n",
              "      <td>0.991328</td>\n",
              "      <td>0.989916</td>\n",
              "      <td>0.999591</td>\n",
              "      <td>0.991639</td>\n",
              "      <td>0.996234</td>\n",
              "      <td>1.008404</td>\n",
              "      <td>1.003346</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.999117</td>\n",
              "      <td>0.993729</td>\n",
              "      <td>0.995164</td>\n",
              "      <td>0.994916</td>\n",
              "      <td>0.995566</td>\n",
              "      <td>...</td>\n",
              "      <td>1.005608</td>\n",
              "      <td>0.989219</td>\n",
              "      <td>0.987627</td>\n",
              "      <td>0.999931</td>\n",
              "      <td>1.006638</td>\n",
              "      <td>1.00298</td>\n",
              "      <td>0.992956</td>\n",
              "      <td>0.998666</td>\n",
              "      <td>0.98926</td>\n",
              "      <td>0.99663</td>\n",
              "      <td>1.005869</td>\n",
              "      <td>1.003529</td>\n",
              "      <td>1.005053</td>\n",
              "      <td>0.994999</td>\n",
              "      <td>1.011835</td>\n",
              "      <td>1.001468</td>\n",
              "      <td>1.006677</td>\n",
              "      <td>0.998059</td>\n",
              "      <td>0.99398</td>\n",
              "      <td>1.001571</td>\n",
              "      <td>0.996599</td>\n",
              "      <td>0.993639</td>\n",
              "      <td>0.996694</td>\n",
              "      <td>1.006816</td>\n",
              "      <td>1.014685</td>\n",
              "      <td>0.989651</td>\n",
              "      <td>1.012758</td>\n",
              "      <td>1.00731</td>\n",
              "      <td>0.991233</td>\n",
              "      <td>1.019647</td>\n",
              "      <td>0.996789</td>\n",
              "      <td>1.003528</td>\n",
              "      <td>0.992621</td>\n",
              "      <td>0.992612</td>\n",
              "      <td>1.00195</td>\n",
              "      <td>0.996996</td>\n",
              "      <td>0.99308</td>\n",
              "      <td>0.990122</td>\n",
              "      <td>0.996108</td>\n",
              "      <td>0.996281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25%</th>\n",
              "      <td>0.993737</td>\n",
              "      <td>0.99683</td>\n",
              "      <td>1.002031</td>\n",
              "      <td>1.013713</td>\n",
              "      <td>1.006133</td>\n",
              "      <td>1.001171</td>\n",
              "      <td>1.009853</td>\n",
              "      <td>1.001342</td>\n",
              "      <td>1.008162</td>\n",
              "      <td>1.00205</td>\n",
              "      <td>0.993886</td>\n",
              "      <td>1.000062</td>\n",
              "      <td>0.997335</td>\n",
              "      <td>0.987785</td>\n",
              "      <td>1.00197</td>\n",
              "      <td>0.995764</td>\n",
              "      <td>0.991636</td>\n",
              "      <td>0.994293</td>\n",
              "      <td>0.996284</td>\n",
              "      <td>0.996068</td>\n",
              "      <td>0.993233</td>\n",
              "      <td>1.003574</td>\n",
              "      <td>0.99276</td>\n",
              "      <td>1.003377</td>\n",
              "      <td>0.990974</td>\n",
              "      <td>1.000663</td>\n",
              "      <td>0.994784</td>\n",
              "      <td>0.991328</td>\n",
              "      <td>0.989916</td>\n",
              "      <td>0.999591</td>\n",
              "      <td>0.991639</td>\n",
              "      <td>0.996234</td>\n",
              "      <td>1.008404</td>\n",
              "      <td>1.003346</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.999117</td>\n",
              "      <td>0.993729</td>\n",
              "      <td>0.995164</td>\n",
              "      <td>0.994916</td>\n",
              "      <td>0.995566</td>\n",
              "      <td>...</td>\n",
              "      <td>1.005608</td>\n",
              "      <td>0.989219</td>\n",
              "      <td>0.987627</td>\n",
              "      <td>0.999931</td>\n",
              "      <td>1.006638</td>\n",
              "      <td>1.00298</td>\n",
              "      <td>0.992956</td>\n",
              "      <td>0.998666</td>\n",
              "      <td>0.98926</td>\n",
              "      <td>0.99663</td>\n",
              "      <td>1.005869</td>\n",
              "      <td>1.003529</td>\n",
              "      <td>1.005053</td>\n",
              "      <td>0.994999</td>\n",
              "      <td>1.011835</td>\n",
              "      <td>1.001468</td>\n",
              "      <td>1.006677</td>\n",
              "      <td>0.998059</td>\n",
              "      <td>0.99398</td>\n",
              "      <td>1.001571</td>\n",
              "      <td>0.996599</td>\n",
              "      <td>0.993639</td>\n",
              "      <td>0.996694</td>\n",
              "      <td>1.006816</td>\n",
              "      <td>1.014685</td>\n",
              "      <td>0.989651</td>\n",
              "      <td>1.012758</td>\n",
              "      <td>1.00731</td>\n",
              "      <td>0.991233</td>\n",
              "      <td>1.019647</td>\n",
              "      <td>0.996789</td>\n",
              "      <td>1.003528</td>\n",
              "      <td>0.992621</td>\n",
              "      <td>0.992612</td>\n",
              "      <td>1.00195</td>\n",
              "      <td>0.996996</td>\n",
              "      <td>0.99308</td>\n",
              "      <td>0.990122</td>\n",
              "      <td>0.996108</td>\n",
              "      <td>0.996281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>50%</th>\n",
              "      <td>0.993737</td>\n",
              "      <td>0.99683</td>\n",
              "      <td>1.002031</td>\n",
              "      <td>1.013713</td>\n",
              "      <td>1.006133</td>\n",
              "      <td>1.001171</td>\n",
              "      <td>1.009853</td>\n",
              "      <td>1.001342</td>\n",
              "      <td>1.008162</td>\n",
              "      <td>1.00205</td>\n",
              "      <td>0.993886</td>\n",
              "      <td>1.000062</td>\n",
              "      <td>0.997335</td>\n",
              "      <td>0.987785</td>\n",
              "      <td>1.00197</td>\n",
              "      <td>0.995764</td>\n",
              "      <td>0.991636</td>\n",
              "      <td>0.994293</td>\n",
              "      <td>0.996284</td>\n",
              "      <td>0.996068</td>\n",
              "      <td>0.993233</td>\n",
              "      <td>1.003574</td>\n",
              "      <td>0.99276</td>\n",
              "      <td>1.003377</td>\n",
              "      <td>0.990974</td>\n",
              "      <td>1.000663</td>\n",
              "      <td>0.994784</td>\n",
              "      <td>0.991328</td>\n",
              "      <td>0.989916</td>\n",
              "      <td>0.999591</td>\n",
              "      <td>0.991639</td>\n",
              "      <td>0.996234</td>\n",
              "      <td>1.008404</td>\n",
              "      <td>1.003346</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.999117</td>\n",
              "      <td>0.993729</td>\n",
              "      <td>0.995164</td>\n",
              "      <td>0.994916</td>\n",
              "      <td>0.995566</td>\n",
              "      <td>...</td>\n",
              "      <td>1.005608</td>\n",
              "      <td>0.989219</td>\n",
              "      <td>0.987627</td>\n",
              "      <td>0.999931</td>\n",
              "      <td>1.006638</td>\n",
              "      <td>1.00298</td>\n",
              "      <td>0.992956</td>\n",
              "      <td>0.998666</td>\n",
              "      <td>0.98926</td>\n",
              "      <td>0.99663</td>\n",
              "      <td>1.005869</td>\n",
              "      <td>1.003529</td>\n",
              "      <td>1.005053</td>\n",
              "      <td>0.994999</td>\n",
              "      <td>1.011835</td>\n",
              "      <td>1.001468</td>\n",
              "      <td>1.006677</td>\n",
              "      <td>0.998059</td>\n",
              "      <td>0.99398</td>\n",
              "      <td>1.001571</td>\n",
              "      <td>0.996599</td>\n",
              "      <td>0.993639</td>\n",
              "      <td>0.996694</td>\n",
              "      <td>1.006816</td>\n",
              "      <td>1.014685</td>\n",
              "      <td>0.989651</td>\n",
              "      <td>1.012758</td>\n",
              "      <td>1.00731</td>\n",
              "      <td>0.991233</td>\n",
              "      <td>1.019647</td>\n",
              "      <td>0.996789</td>\n",
              "      <td>1.003528</td>\n",
              "      <td>0.992621</td>\n",
              "      <td>0.992612</td>\n",
              "      <td>1.00195</td>\n",
              "      <td>0.996996</td>\n",
              "      <td>0.99308</td>\n",
              "      <td>0.990122</td>\n",
              "      <td>0.996108</td>\n",
              "      <td>0.996281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>75%</th>\n",
              "      <td>0.993737</td>\n",
              "      <td>0.99683</td>\n",
              "      <td>1.002031</td>\n",
              "      <td>1.013713</td>\n",
              "      <td>1.006133</td>\n",
              "      <td>1.001171</td>\n",
              "      <td>1.009853</td>\n",
              "      <td>1.001342</td>\n",
              "      <td>1.008162</td>\n",
              "      <td>1.00205</td>\n",
              "      <td>0.993886</td>\n",
              "      <td>1.000062</td>\n",
              "      <td>0.997335</td>\n",
              "      <td>0.987785</td>\n",
              "      <td>1.00197</td>\n",
              "      <td>0.995764</td>\n",
              "      <td>0.991636</td>\n",
              "      <td>0.994293</td>\n",
              "      <td>0.996284</td>\n",
              "      <td>0.996068</td>\n",
              "      <td>0.993233</td>\n",
              "      <td>1.003574</td>\n",
              "      <td>0.99276</td>\n",
              "      <td>1.003377</td>\n",
              "      <td>0.990974</td>\n",
              "      <td>1.000663</td>\n",
              "      <td>0.994784</td>\n",
              "      <td>0.991328</td>\n",
              "      <td>0.989916</td>\n",
              "      <td>0.999591</td>\n",
              "      <td>0.991639</td>\n",
              "      <td>0.996234</td>\n",
              "      <td>1.008404</td>\n",
              "      <td>1.003346</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.999117</td>\n",
              "      <td>0.993729</td>\n",
              "      <td>0.995164</td>\n",
              "      <td>0.994916</td>\n",
              "      <td>0.995566</td>\n",
              "      <td>...</td>\n",
              "      <td>1.005608</td>\n",
              "      <td>0.989219</td>\n",
              "      <td>0.987627</td>\n",
              "      <td>0.999931</td>\n",
              "      <td>1.006638</td>\n",
              "      <td>1.00298</td>\n",
              "      <td>0.992956</td>\n",
              "      <td>0.998666</td>\n",
              "      <td>0.98926</td>\n",
              "      <td>0.99663</td>\n",
              "      <td>1.005869</td>\n",
              "      <td>1.003529</td>\n",
              "      <td>1.005053</td>\n",
              "      <td>0.994999</td>\n",
              "      <td>1.011835</td>\n",
              "      <td>1.001468</td>\n",
              "      <td>1.006677</td>\n",
              "      <td>0.998059</td>\n",
              "      <td>0.99398</td>\n",
              "      <td>1.001571</td>\n",
              "      <td>0.996599</td>\n",
              "      <td>0.993639</td>\n",
              "      <td>0.996694</td>\n",
              "      <td>1.006816</td>\n",
              "      <td>1.014685</td>\n",
              "      <td>0.989651</td>\n",
              "      <td>1.012758</td>\n",
              "      <td>1.00731</td>\n",
              "      <td>0.991233</td>\n",
              "      <td>1.019647</td>\n",
              "      <td>0.996789</td>\n",
              "      <td>1.003528</td>\n",
              "      <td>0.992621</td>\n",
              "      <td>0.992612</td>\n",
              "      <td>1.00195</td>\n",
              "      <td>0.996996</td>\n",
              "      <td>0.99308</td>\n",
              "      <td>0.990122</td>\n",
              "      <td>0.996108</td>\n",
              "      <td>0.996281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>max</th>\n",
              "      <td>0.993737</td>\n",
              "      <td>0.99683</td>\n",
              "      <td>1.002031</td>\n",
              "      <td>1.013713</td>\n",
              "      <td>1.006133</td>\n",
              "      <td>1.001171</td>\n",
              "      <td>1.009853</td>\n",
              "      <td>1.001342</td>\n",
              "      <td>1.008162</td>\n",
              "      <td>1.00205</td>\n",
              "      <td>0.993886</td>\n",
              "      <td>1.000062</td>\n",
              "      <td>0.997335</td>\n",
              "      <td>0.987785</td>\n",
              "      <td>1.00197</td>\n",
              "      <td>0.995764</td>\n",
              "      <td>0.991636</td>\n",
              "      <td>0.994293</td>\n",
              "      <td>0.996284</td>\n",
              "      <td>0.996068</td>\n",
              "      <td>0.993233</td>\n",
              "      <td>1.003574</td>\n",
              "      <td>0.99276</td>\n",
              "      <td>1.003377</td>\n",
              "      <td>0.990974</td>\n",
              "      <td>1.000663</td>\n",
              "      <td>0.994784</td>\n",
              "      <td>0.991328</td>\n",
              "      <td>0.989916</td>\n",
              "      <td>0.999591</td>\n",
              "      <td>0.991639</td>\n",
              "      <td>0.996234</td>\n",
              "      <td>1.008404</td>\n",
              "      <td>1.003346</td>\n",
              "      <td>0.991031</td>\n",
              "      <td>0.999117</td>\n",
              "      <td>0.993729</td>\n",
              "      <td>0.995164</td>\n",
              "      <td>0.994916</td>\n",
              "      <td>0.995566</td>\n",
              "      <td>...</td>\n",
              "      <td>1.005608</td>\n",
              "      <td>0.989219</td>\n",
              "      <td>0.987627</td>\n",
              "      <td>0.999931</td>\n",
              "      <td>1.006638</td>\n",
              "      <td>1.00298</td>\n",
              "      <td>0.992956</td>\n",
              "      <td>0.998666</td>\n",
              "      <td>0.98926</td>\n",
              "      <td>0.99663</td>\n",
              "      <td>1.005869</td>\n",
              "      <td>1.003529</td>\n",
              "      <td>1.005053</td>\n",
              "      <td>0.994999</td>\n",
              "      <td>1.011835</td>\n",
              "      <td>1.001468</td>\n",
              "      <td>1.006677</td>\n",
              "      <td>0.998059</td>\n",
              "      <td>0.99398</td>\n",
              "      <td>1.001571</td>\n",
              "      <td>0.996599</td>\n",
              "      <td>0.993639</td>\n",
              "      <td>0.996694</td>\n",
              "      <td>1.006816</td>\n",
              "      <td>1.014685</td>\n",
              "      <td>0.989651</td>\n",
              "      <td>1.012758</td>\n",
              "      <td>1.00731</td>\n",
              "      <td>0.991233</td>\n",
              "      <td>1.019647</td>\n",
              "      <td>0.996789</td>\n",
              "      <td>1.003528</td>\n",
              "      <td>0.992621</td>\n",
              "      <td>0.992612</td>\n",
              "      <td>1.00195</td>\n",
              "      <td>0.996996</td>\n",
              "      <td>0.99308</td>\n",
              "      <td>0.990122</td>\n",
              "      <td>0.996108</td>\n",
              "      <td>0.996281</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>8 rows  128 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0        1         2    ...       125       126       127\n",
              "count  1.000000  1.00000  1.000000  ...  1.000000  1.000000  1.000000\n",
              "mean   0.993737  0.99683  1.002031  ...  0.990122  0.996108  0.996281\n",
              "std         NaN      NaN       NaN  ...       NaN       NaN       NaN\n",
              "min    0.993737  0.99683  1.002031  ...  0.990122  0.996108  0.996281\n",
              "25%    0.993737  0.99683  1.002031  ...  0.990122  0.996108  0.996281\n",
              "50%    0.993737  0.99683  1.002031  ...  0.990122  0.996108  0.996281\n",
              "75%    0.993737  0.99683  1.002031  ...  0.990122  0.996108  0.996281\n",
              "max    0.993737  0.99683  1.002031  ...  0.990122  0.996108  0.996281\n",
              "\n",
              "[8 rows x 128 columns]"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "conv_trans_block_16\n",
            "conv_trans_block_17\n",
            "activation_21\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMpUwSf1tO3_",
        "outputId": "c2892ec8-98b7-432e-97d3-144cb418f8a0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "source": [
        "for layer in gen.layers:\n",
        "    print(layer.name)\n"
      ],
      "execution_count": 46,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dense_3\n",
            "reshape_2\n",
            "conv_trans_block_6\n",
            "conv_trans_block_7\n",
            "conv_trans_block_8\n",
            "conv_trans_block_9\n",
            "activation_11\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwRUls75tSP0"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYZ9u7zosR6s"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}