{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "gans_n_classes(1)(3).ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "wvCMx6WZrec9"
      },
      "source": [
        "import os, math\n",
        "from PIL import Image, ImageDraw, ImageFont\n",
        "from tensorflow.keras.datasets import cifar10, mnist\n",
        "import matplotlib.pyplot as plt\n",
        "import random\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.layers import Dense, Input, BatchNormalization \n",
        "from tensorflow.keras.layers import Conv2D, Flatten, Activation\n",
        "from tensorflow.keras.layers import Reshape, Conv2DTranspose, LeakyReLU\n",
        "from tensorflow.keras.models import Model, Sequential\n",
        "from tensorflow.keras.optimizers import RMSprop\n",
        "from tensorflow.keras.losses import mse, SparseCategoricalCrossentropy\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gnQNYvHILnK0"
      },
      "source": [
        "tf.random.set_seed(42)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "df9LBqdbriOh"
      },
      "source": [
        "(x_train, y_train),(x_test, _) = mnist.load_data()\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ceVV7TVkmAYs"
      },
      "source": [
        "x_train = x_train.reshape(-1, 28, 28, 1)\n",
        "x_train = x_train / 255"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LW9SJ9vprkAr",
        "outputId": "b81dba6d-f900-4758-b587-983e64d73ade"
      },
      "source": [
        "x_train.max()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gJserdiD0huu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6f28a847-5632-48fc-c303-e9dd209219e6"
      },
      "source": [
        "image_size = x_train[0].shape[1]\n",
        "image_size"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "28"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KyFFv7MotmPM"
      },
      "source": [
        "class ConvTransBlock(tf.keras.layers.Layer):\n",
        "    def __init__(self, filters, kernel_size, strides):\n",
        "        super().__init__()\n",
        "        self.bn = BatchNormalization()  # check training flag\n",
        "        self.act = Activation(activation='relu')\n",
        "        self.conv2D_trans = Conv2DTranspose(filters=filters,\n",
        "                                kernel_size=kernel_size,\n",
        "                                strides=strides,\n",
        "                                padding='same')\n",
        "        \n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.bn(inputs)\n",
        "        x = self.act(x)\n",
        "        return self.conv2D_trans(x)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tRIsDGBlxYOA"
      },
      "source": [
        "class Generator(tf.keras.models.Model):\n",
        "    def __init__(self, filters, kernel_size, resize_img):\n",
        "        super().__init__()\n",
        "        self.dense1 = Dense(resize_img * resize_img * filters[0])\n",
        "        self.reshape = Reshape([resize_img, resize_img, filters[0]])\n",
        "        self.conv2dtrans = []\n",
        "        for i, _filter in enumerate(filters):\n",
        "            if i <= 1:\n",
        "                strides = 2\n",
        "            else:\n",
        "                strides = 1\n",
        "            self.conv2dtrans.append(ConvTransBlock(_filter, kernel_size, strides))\n",
        "        \n",
        "        self.act = Activation(\"sigmoid\")\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = self.dense1(inputs)\n",
        "        x = self.reshape(x)\n",
        "        for conv in self.conv2dtrans:\n",
        "            x = conv(x)\n",
        "        return self.act(x)\n",
        "    \n",
        "    def model(self):\n",
        "        x = Input(shape=[1, 6272])\n",
        "        return Model(inputs=x, outputs=self.call(x))\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_DEoDlZSU5xC"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mUfyrKaXi7hx"
      },
      "source": [
        "def leaky_conv(filters, kernel_size, strides):\n",
        "    return Sequential([LeakyReLU(alpha=0.2),\n",
        "                       Conv2D(filters=filters,\n",
        "                       kernel_size=kernel_size,\n",
        "                       strides=strides,\n",
        "                       padding='same')])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nuHfmqqgl1UE"
      },
      "source": [
        "class Discriminator(tf.keras.models.Model):\n",
        "    def __init__(self, filters, kernel_size):\n",
        "        super().__init__()\n",
        "        self.leaky_convs = []\n",
        "        for i, _filter in enumerate(filters):\n",
        "            if i < len(filters) - 1:\n",
        "                strides = 2\n",
        "            else:\n",
        "                strides = 1\n",
        "            self.leaky_convs.append(leaky_conv(_filter, kernel_size, strides))\n",
        "        self.flat = Flatten()\n",
        "        self.dense = Dense(1, activation='sigmoid')\n",
        "\n",
        "    def call(self, inputs, training=False):\n",
        "        x = inputs\n",
        "        for conv in self.leaky_convs:\n",
        "            x = conv(x)\n",
        "        x = self.flat(x)\n",
        "        return self.dense(x)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mprUKHR-qyH_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f2ba8d7c-648f-46e1-ba8a-3e3fa3df1d7d"
      },
      "source": [
        "# latent_size = 2??\n",
        "# simple disc\n",
        "# delete activation as input in disc\n",
        "\n",
        "\n",
        "latent_size = 2\n",
        "batch_size = 512\n",
        "train_steps = 40000\n",
        "disc_lr = 2e-4\n",
        "disc_decay = 6e-8\n",
        "gen_lr = disc_lr/4\n",
        "gen_decay = disc_decay/4\n",
        "\n",
        "disc_lr, disc_decay, gen_lr, gen_decay"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(0.0002, 6e-08, 5e-05, 1.5e-08)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yYGp4FOZ8_lI"
      },
      "source": [
        "disc_kernel_size = 4\n",
        "disc_layers_filters = [32, 64, 128]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gNBiGfK4s5v4"
      },
      "source": [
        "gen_resize_img = image_size // 4\n",
        "gen_kernel_size = 4\n",
        "gen_layers_filter = [128, 64, 32, 1]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "80EQpJupsYsu"
      },
      "source": [
        "gen = Generator(gen_layers_filter, gen_kernel_size, gen_resize_img)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HsGHWgavtkcd"
      },
      "source": [
        "disc = Discriminator(disc_layers_filters, disc_kernel_size)\n",
        "disc.compile(loss='binary_crossentropy',\n",
        "             optimizer=RMSprop(learning_rate=disc_lr, decay=disc_decay),\n",
        "             metrics=['accuracy'])\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_dQJ7JIUuAF1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e4b38483-cfcc-4f88-f2da-09b1abfbcaa4"
      },
      "source": [
        "def build_gan():\n",
        "    # disc.trainable = False\n",
        "    gan = Sequential(name='gan_model')\n",
        "    gan.add(Input([latent_size,]))\n",
        "    gan.add(gen)\n",
        "    gan.add(disc)\n",
        "    gan.summary()\n",
        "    gan.compile(loss='binary_crossentropy',\n",
        "                optimizer=RMSprop(learning_rate=gen_lr, decay=gen_decay),\n",
        "                metrics=['accuracy'])            \n",
        "    return gan   \n",
        "\n",
        "\n",
        "gan = build_gan()\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Model: \"gan_model\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "generator (Generator)        (None, 28, 28, 1)         446945    \n",
            "_________________________________________________________________\n",
            "discriminator (Discriminator (None, 1)                 170849    \n",
            "=================================================================\n",
            "Total params: 617,794\n",
            "Trainable params: 617,090\n",
            "Non-trainable params: 704\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "diElXoXzFgLe",
        "outputId": "05e5d06b-5389-43ba-84cd-c944f56d7de1"
      },
      "source": [
        "for layer in gan.layers:\n",
        "    print(layer.trainable)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "True\n",
            "True\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D9zEUeIpNjsK"
      },
      "source": [
        "def plot_images(generator,\n",
        "                noise_input,\n",
        "                noise_label=None,\n",
        "                noise_codes=None,\n",
        "                show=False,\n",
        "                step=0,\n",
        "                model_name=\"gan\"):\n",
        "    \"\"\"Generate fake images and plot them\n",
        "\n",
        "    For visualization purposes, generate fake images\n",
        "    then plot them in a square grid\n",
        "\n",
        "    # Arguments\n",
        "        generator (Model): The Generator Model for \n",
        "            fake images generation\n",
        "        noise_input (ndarray): Array of z-vectors\n",
        "        show (bool): Whether to show plot or not\n",
        "        step (int): Appended to filename of the save images\n",
        "        model_name (string): Model name\n",
        "\n",
        "    \"\"\"\n",
        "    # os.makedirs(model_name, exist_ok=True)\n",
        "    # filename = os.path.join(model_name, \"%05d.png\" % step)\n",
        "    rows = int(math.sqrt(noise_input.shape[0]))\n",
        "    if noise_label is not None:\n",
        "        noise_input = [noise_input, noise_label]\n",
        "        if noise_codes is not None:\n",
        "            noise_input += noise_codes\n",
        "\n",
        "    images = generator.predict(noise_input)\n",
        "    plt.figure(figsize=(10, 10))\n",
        "    num_images = images.shape[0]\n",
        "    image_size = images.shape[1]\n",
        "    for i in range(num_images):\n",
        "        plt.subplot(rows, rows, i + 1)\n",
        "        image = np.reshape(images[i], [image_size, image_size])\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        plt.axis('off')\n",
        "    # plt.savefig(filename)\n",
        "    if show:\n",
        "        \n",
        "        plt.show()\n",
        "    else:\n",
        "        plt.close('all')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sbex8Ww8H03-"
      },
      "source": [
        "# Train the Discriminator and Adversarial Networks\n",
        "def train(models, x_train):\n",
        "    generator, discriminator, gan = models \n",
        "    # batch_size, latent_size, train_step, model_name = params\n",
        "    save_interval = 500\n",
        "    noise_input = np.random.uniform(-1.0, 1.0, size=[16, latent_size]) \n",
        "    train_size = x_train.shape[0]\n",
        "\n",
        "    for i in range(train_steps):\n",
        "        rand_indexes = np.random.randint(0, train_size, size=batch_size)\n",
        "        real_images = x_train[rand_indexes]\n",
        "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "        fake_images = generator.predict(noise)        \n",
        "        x = np.concatenate((real_images, fake_images)) # same size\n",
        "        y = np.ones([2 * batch_size, 1])\n",
        "        y[batch_size:, :] = 0.0\n",
        "        # experimental\n",
        "        # y[: batch_size] = 1\n",
        "        discriminator.trainable = True\n",
        "        loss, acc = discriminator.train_on_batch(x, y)\n",
        "        # log = \"%d: [discriminator loss: %f, acc: %f]\" % (i, loss, acc)\n",
        "        log = f'{i}: [discriminator loss: {loss}, acc: {acc}]'\n",
        "        discriminator.trainable = False\n",
        "\n",
        "        noise = np.random.uniform(-1.0, 1.0, size=[batch_size, latent_size])\n",
        "        y = np.ones([batch_size, 1])        \n",
        "        loss, acc = gan.train_on_batch(noise, y)\n",
        "        log = \"%s [gan loss: %f, acc: %f]\" % (log, loss, acc)\n",
        "        print(log)\n",
        "        if (i + 1) % save_interval == 0:\n",
        "            # plot generator images on a periodic basis\n",
        "            plot_images(generator,\n",
        "                        noise_input=noise_input,\n",
        "                        show=True,\n",
        "                        step=(i + 1),\n",
        "                        model_name='gan_model')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "prYbCy5Q3dbw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "749cd0bd-1714-4093-d249-e8e312d27910"
      },
      "source": [
        "print(x_train.shape, x_train.min(), x_train.max())\n",
        "        "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(60000, 28, 28, 1) 0.0 1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3_7RVfcXIbaR",
        "outputId": "ae93bddd-4df2-42c5-84ee-c898124a4b95"
      },
      "source": [
        "train(models=[gen, disc, gan], x_train=x_train)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0: [discriminator loss: 0.6887925863265991, acc: 0.7060546875] [gan loss: 0.814798, acc: 0.000000]\n",
            "1: [discriminator loss: 0.6429523229598999, acc: 0.966796875] [gan loss: 0.814822, acc: 0.000000]\n",
            "2: [discriminator loss: 0.607060432434082, acc: 0.9951171875] [gan loss: 0.822665, acc: 0.000000]\n",
            "3: [discriminator loss: 0.5676516890525818, acc: 1.0] [gan loss: 0.827955, acc: 0.000000]\n",
            "4: [discriminator loss: 0.5279223918914795, acc: 0.9970703125] [gan loss: 0.820941, acc: 0.000000]\n",
            "5: [discriminator loss: 0.485856294631958, acc: 1.0] [gan loss: 0.781545, acc: 0.000000]\n",
            "6: [discriminator loss: 0.4402754008769989, acc: 1.0] [gan loss: 0.733953, acc: 0.185547]\n",
            "7: [discriminator loss: 0.39427459239959717, acc: 1.0] [gan loss: 0.644206, acc: 0.828125]\n",
            "8: [discriminator loss: 0.3476865291595459, acc: 1.0] [gan loss: 0.565402, acc: 0.933594]\n",
            "9: [discriminator loss: 0.29942572116851807, acc: 1.0] [gan loss: 0.419309, acc: 1.000000]\n",
            "10: [discriminator loss: 0.2564335763454437, acc: 1.0] [gan loss: 0.373466, acc: 1.000000]\n",
            "11: [discriminator loss: 0.21533989906311035, acc: 1.0] [gan loss: 0.191961, acc: 1.000000]\n",
            "12: [discriminator loss: 0.17847855389118195, acc: 1.0] [gan loss: 0.172758, acc: 1.000000]\n",
            "13: [discriminator loss: 0.1472412496805191, acc: 1.0] [gan loss: 0.076881, acc: 1.000000]\n",
            "14: [discriminator loss: 0.1211373507976532, acc: 1.0] [gan loss: 0.078048, acc: 1.000000]\n",
            "15: [discriminator loss: 0.09928002208471298, acc: 1.0] [gan loss: 0.034189, acc: 1.000000]\n",
            "16: [discriminator loss: 0.08150678873062134, acc: 1.0] [gan loss: 0.034841, acc: 1.000000]\n",
            "17: [discriminator loss: 0.06855154037475586, acc: 1.0] [gan loss: 0.016102, acc: 1.000000]\n",
            "18: [discriminator loss: 0.05579720437526703, acc: 1.0] [gan loss: 0.018796, acc: 1.000000]\n",
            "19: [discriminator loss: 0.04791460558772087, acc: 1.0] [gan loss: 0.008500, acc: 1.000000]\n",
            "20: [discriminator loss: 0.03982207924127579, acc: 1.0] [gan loss: 0.008717, acc: 1.000000]\n",
            "21: [discriminator loss: 0.03523508831858635, acc: 1.0] [gan loss: 0.004623, acc: 1.000000]\n",
            "22: [discriminator loss: 0.030083179473876953, acc: 1.0] [gan loss: 0.005199, acc: 1.000000]\n",
            "23: [discriminator loss: 0.025905052199959755, acc: 1.0] [gan loss: 0.003120, acc: 1.000000]\n",
            "24: [discriminator loss: 0.022272329777479172, acc: 1.0] [gan loss: 0.002967, acc: 1.000000]\n",
            "25: [discriminator loss: 0.019723087549209595, acc: 1.0] [gan loss: 0.001890, acc: 1.000000]\n",
            "26: [discriminator loss: 0.017588622868061066, acc: 1.0] [gan loss: 0.001764, acc: 1.000000]\n",
            "27: [discriminator loss: 0.01498060580343008, acc: 1.0] [gan loss: 0.001237, acc: 1.000000]\n",
            "28: [discriminator loss: 0.014086535200476646, acc: 1.0] [gan loss: 0.001012, acc: 1.000000]\n",
            "29: [discriminator loss: 0.011793769896030426, acc: 1.0] [gan loss: 0.000947, acc: 1.000000]\n",
            "30: [discriminator loss: 0.01092283520847559, acc: 1.0] [gan loss: 0.000718, acc: 1.000000]\n",
            "31: [discriminator loss: 0.009631703607738018, acc: 1.0] [gan loss: 0.000673, acc: 1.000000]\n",
            "32: [discriminator loss: 0.00825984962284565, acc: 1.0] [gan loss: 0.000595, acc: 1.000000]\n",
            "33: [discriminator loss: 0.00775242829695344, acc: 1.0] [gan loss: 0.000460, acc: 1.000000]\n",
            "34: [discriminator loss: 0.006980872713029385, acc: 1.0] [gan loss: 0.000361, acc: 1.000000]\n",
            "35: [discriminator loss: 0.006612498313188553, acc: 1.0] [gan loss: 0.000343, acc: 1.000000]\n",
            "36: [discriminator loss: 0.0057280780747532845, acc: 1.0] [gan loss: 0.000312, acc: 1.000000]\n",
            "37: [discriminator loss: 0.005215676501393318, acc: 1.0] [gan loss: 0.000237, acc: 1.000000]\n",
            "38: [discriminator loss: 0.005032160319387913, acc: 1.0] [gan loss: 0.000215, acc: 1.000000]\n",
            "39: [discriminator loss: 0.00459687877446413, acc: 1.0] [gan loss: 0.000166, acc: 1.000000]\n",
            "40: [discriminator loss: 0.004098065197467804, acc: 1.0] [gan loss: 0.000169, acc: 1.000000]\n",
            "41: [discriminator loss: 0.0038936282508075237, acc: 1.0] [gan loss: 0.000140, acc: 1.000000]\n",
            "42: [discriminator loss: 0.0033038363326340914, acc: 1.0] [gan loss: 0.000144, acc: 1.000000]\n",
            "43: [discriminator loss: 0.0031711915507912636, acc: 1.0] [gan loss: 0.000100, acc: 1.000000]\n",
            "44: [discriminator loss: 0.0028614106122404337, acc: 1.0] [gan loss: 0.000102, acc: 1.000000]\n",
            "45: [discriminator loss: 0.0028701857663691044, acc: 1.0] [gan loss: 0.000074, acc: 1.000000]\n",
            "46: [discriminator loss: 0.0024667030666023493, acc: 1.0] [gan loss: 0.000084, acc: 1.000000]\n",
            "47: [discriminator loss: 0.002421954646706581, acc: 1.0] [gan loss: 0.000054, acc: 1.000000]\n",
            "48: [discriminator loss: 0.0022093080915510654, acc: 1.0] [gan loss: 0.000065, acc: 1.000000]\n",
            "49: [discriminator loss: 0.0022464406210929155, acc: 1.0] [gan loss: 0.000044, acc: 1.000000]\n",
            "50: [discriminator loss: 0.0019506211392581463, acc: 1.0] [gan loss: 0.000052, acc: 1.000000]\n",
            "51: [discriminator loss: 0.0018462633015587926, acc: 1.0] [gan loss: 0.000032, acc: 1.000000]\n",
            "52: [discriminator loss: 0.001644625561311841, acc: 1.0] [gan loss: 0.000035, acc: 1.000000]\n",
            "53: [discriminator loss: 0.0014514740323647857, acc: 1.0] [gan loss: 0.000033, acc: 1.000000]\n",
            "54: [discriminator loss: 0.0013720126589760184, acc: 1.0] [gan loss: 0.000029, acc: 1.000000]\n",
            "55: [discriminator loss: 0.0014085696311667562, acc: 1.0] [gan loss: 0.000017, acc: 1.000000]\n",
            "56: [discriminator loss: 0.0014412677846848965, acc: 1.0] [gan loss: 0.000018, acc: 1.000000]\n",
            "57: [discriminator loss: 0.0010239739203825593, acc: 1.0] [gan loss: 0.000023, acc: 1.000000]\n",
            "58: [discriminator loss: 0.0011229855008423328, acc: 1.0] [gan loss: 0.000014, acc: 1.000000]\n",
            "59: [discriminator loss: 0.0010949359275400639, acc: 1.0] [gan loss: 0.000016, acc: 1.000000]\n",
            "60: [discriminator loss: 0.000984940561465919, acc: 1.0] [gan loss: 0.000013, acc: 1.000000]\n",
            "61: [discriminator loss: 0.0008993085357360542, acc: 1.0] [gan loss: 0.000011, acc: 1.000000]\n",
            "62: [discriminator loss: 0.0008379353675991297, acc: 1.0] [gan loss: 0.000010, acc: 1.000000]\n",
            "63: [discriminator loss: 0.0007956229383125901, acc: 1.0] [gan loss: 0.000009, acc: 1.000000]\n",
            "64: [discriminator loss: 0.0007168499869294465, acc: 1.0] [gan loss: 0.000009, acc: 1.000000]\n",
            "65: [discriminator loss: 0.0007263345760293305, acc: 1.0] [gan loss: 0.000010, acc: 1.000000]\n",
            "66: [discriminator loss: 0.0008907316951081157, acc: 1.0] [gan loss: 0.000004, acc: 1.000000]\n",
            "67: [discriminator loss: 0.0010638092644512653, acc: 1.0] [gan loss: 0.000004, acc: 1.000000]\n",
            "68: [discriminator loss: 0.0006326729780994356, acc: 1.0] [gan loss: 0.000008, acc: 1.000000]\n",
            "69: [discriminator loss: 0.000609964074101299, acc: 1.0] [gan loss: 0.000004, acc: 1.000000]\n",
            "70: [discriminator loss: 0.0005056247464381158, acc: 1.0] [gan loss: 0.000005, acc: 1.000000]\n",
            "71: [discriminator loss: 0.0004329028888605535, acc: 1.0] [gan loss: 0.000005, acc: 1.000000]\n",
            "72: [discriminator loss: 0.0004001786874141544, acc: 1.0] [gan loss: 0.000005, acc: 1.000000]\n",
            "73: [discriminator loss: 0.0003899390867445618, acc: 1.0] [gan loss: 0.000004, acc: 1.000000]\n",
            "74: [discriminator loss: 0.001024767174385488, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "75: [discriminator loss: 0.0012044628383591771, acc: 1.0] [gan loss: 0.000012, acc: 1.000000]\n",
            "76: [discriminator loss: 0.0006821413990110159, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "77: [discriminator loss: 0.00031000873423181474, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "78: [discriminator loss: 0.0003003650053869933, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "79: [discriminator loss: 0.0002523184521123767, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "80: [discriminator loss: 0.00022740464191883802, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "81: [discriminator loss: 0.00034362333826720715, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "82: [discriminator loss: 0.00023205381876323372, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "83: [discriminator loss: 0.00020121285342611372, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "84: [discriminator loss: 0.0001866567472461611, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "85: [discriminator loss: 0.0002266031369799748, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "86: [discriminator loss: 0.00017444536206312478, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "87: [discriminator loss: 0.00018513674149289727, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "88: [discriminator loss: 0.00015606230590492487, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "89: [discriminator loss: 0.0001551973109599203, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "90: [discriminator loss: 0.0004150894528720528, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "91: [discriminator loss: 0.00037513324059545994, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "92: [discriminator loss: 0.0002706357336137444, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "93: [discriminator loss: 0.00014152847870718688, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "94: [discriminator loss: 0.0001605558645678684, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "95: [discriminator loss: 0.00011338043987052515, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "96: [discriminator loss: 0.00010116794146597385, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "97: [discriminator loss: 0.00015945309132803231, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "98: [discriminator loss: 0.0001110171724576503, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "99: [discriminator loss: 0.0025224334094673395, acc: 0.9990234375] [gan loss: 0.000000, acc: 1.000000]\n",
            "100: [discriminator loss: 0.010311903432011604, acc: 1.0] [gan loss: 0.000005, acc: 1.000000]\n",
            "101: [discriminator loss: 9.36981086852029e-05, acc: 1.0] [gan loss: 0.000004, acc: 1.000000]\n",
            "102: [discriminator loss: 7.762704626657069e-05, acc: 1.0] [gan loss: 0.000004, acc: 1.000000]\n",
            "103: [discriminator loss: 7.356467540375888e-05, acc: 1.0] [gan loss: 0.000004, acc: 1.000000]\n",
            "104: [discriminator loss: 7.090373401297256e-05, acc: 1.0] [gan loss: 0.000003, acc: 1.000000]\n",
            "105: [discriminator loss: 8.139463898260146e-05, acc: 1.0] [gan loss: 0.000003, acc: 1.000000]\n",
            "106: [discriminator loss: 9.692809544503689e-05, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "107: [discriminator loss: 8.679104212205857e-05, acc: 1.0] [gan loss: 0.000003, acc: 1.000000]\n",
            "108: [discriminator loss: 0.00010081684740725905, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "109: [discriminator loss: 0.00022706593153998256, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "110: [discriminator loss: 0.0006617428734898567, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "111: [discriminator loss: 0.00012860260903835297, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "112: [discriminator loss: 8.225323108490556e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "113: [discriminator loss: 8.543262083549052e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "114: [discriminator loss: 8.66347982082516e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "115: [discriminator loss: 8.830349543131888e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "116: [discriminator loss: 8.792187873041257e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "117: [discriminator loss: 0.00012070847151335329, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "118: [discriminator loss: 9.661960211815313e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "119: [discriminator loss: 8.05361196398735e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "120: [discriminator loss: 8.517134847352281e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "121: [discriminator loss: 7.645859295735136e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "122: [discriminator loss: 8.53814126458019e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "123: [discriminator loss: 7.887263200245798e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "124: [discriminator loss: 7.859131437726319e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "125: [discriminator loss: 8.214696572395042e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "126: [discriminator loss: 6.701232632622123e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "127: [discriminator loss: 9.950284584192559e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "128: [discriminator loss: 6.93166148266755e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "129: [discriminator loss: 7.33041888452135e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "130: [discriminator loss: 6.797074456699193e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "131: [discriminator loss: 0.0007120562950149179, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "132: [discriminator loss: 0.0003233257448300719, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "133: [discriminator loss: 0.00011110306513728574, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "134: [discriminator loss: 8.26355826575309e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "135: [discriminator loss: 6.935550482012331e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "136: [discriminator loss: 6.216205656528473e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "137: [discriminator loss: 5.337246329872869e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "138: [discriminator loss: 6.405443127732724e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "139: [discriminator loss: 7.168603042373434e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "140: [discriminator loss: 6.701007077936083e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "141: [discriminator loss: 5.4203115723794326e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "142: [discriminator loss: 4.415270814206451e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "143: [discriminator loss: 3.8051635783631355e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "144: [discriminator loss: 5.45833318028599e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "145: [discriminator loss: 3.610140993259847e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "146: [discriminator loss: 4.479433846427128e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "147: [discriminator loss: 3.2588977774139494e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "148: [discriminator loss: 0.0006940829916857183, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "149: [discriminator loss: 0.0013115726178511977, acc: 1.0] [gan loss: 0.000007, acc: 1.000000]\n",
            "150: [discriminator loss: 1.6209894965868443e-05, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "151: [discriminator loss: 2.378152566961944e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "152: [discriminator loss: 1.8633036233950406e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "153: [discriminator loss: 1.5956546121742576e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "154: [discriminator loss: 2.709294858505018e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "155: [discriminator loss: 1.4986466339905746e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "156: [discriminator loss: 1.562211946293246e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "157: [discriminator loss: 1.8530325178289786e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "158: [discriminator loss: 2.296276215929538e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "159: [discriminator loss: 5.9849622630281374e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "160: [discriminator loss: 2.4132865291903727e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "161: [discriminator loss: 2.3775104637024924e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "162: [discriminator loss: 2.633547228469979e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "163: [discriminator loss: 2.7048366973758675e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "164: [discriminator loss: 2.2056430680095218e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "165: [discriminator loss: 2.1263131202431396e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "166: [discriminator loss: 2.6325122234993614e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "167: [discriminator loss: 2.1714191461796872e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "168: [discriminator loss: 2.17022534343414e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "169: [discriminator loss: 2.0501587641774677e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "170: [discriminator loss: 4.6274108171928674e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "171: [discriminator loss: 2.6706069547799416e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "172: [discriminator loss: 2.2708183678332716e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "173: [discriminator loss: 6.247939018066972e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "174: [discriminator loss: 3.69751505786553e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "175: [discriminator loss: 2.335969293199014e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "176: [discriminator loss: 3.3027616154868156e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "177: [discriminator loss: 2.143362507922575e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "178: [discriminator loss: 3.3639636967564e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "179: [discriminator loss: 2.131655128323473e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "180: [discriminator loss: 1.6015148503356613e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "181: [discriminator loss: 1.4206746527634095e-05, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "182: [discriminator loss: 1.7942644262802787e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "183: [discriminator loss: 0.0013235422084107995, acc: 0.9990234375] [gan loss: 0.000000, acc: 1.000000]\n",
            "184: [discriminator loss: 0.018125571310520172, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "185: [discriminator loss: 1.096841151593253e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "186: [discriminator loss: 1.3719538401346654e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "187: [discriminator loss: 3.469972580205649e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "188: [discriminator loss: 2.2484589862870052e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "189: [discriminator loss: 2.39164728554897e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "190: [discriminator loss: 2.8091424610465765e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "191: [discriminator loss: 0.0002541321446187794, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "192: [discriminator loss: 4.388528395793401e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "193: [discriminator loss: 5.0583359552547336e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "194: [discriminator loss: 0.00031276923255063593, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "195: [discriminator loss: 7.912356522865593e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "196: [discriminator loss: 8.417909702984616e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "197: [discriminator loss: 9.070367377717048e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "198: [discriminator loss: 9.82320198090747e-05, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "199: [discriminator loss: 0.00010298925189999864, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "200: [discriminator loss: 0.00010867047967622057, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "201: [discriminator loss: 0.0001154609490185976, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "202: [discriminator loss: 0.00012060078006470576, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "203: [discriminator loss: 0.00012408228940330446, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "204: [discriminator loss: 0.00012751230678986758, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "205: [discriminator loss: 0.00013079121708869934, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "206: [discriminator loss: 0.00034045311622321606, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "207: [discriminator loss: 0.00018464514869265258, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "208: [discriminator loss: 0.000171908744960092, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "209: [discriminator loss: 0.00016035667795222253, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "210: [discriminator loss: 0.00016051370766945183, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "211: [discriminator loss: 0.00015450204955413938, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "212: [discriminator loss: 0.00015363073907792568, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "213: [discriminator loss: 0.00015585817163810134, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "214: [discriminator loss: 0.00017110300541389734, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "215: [discriminator loss: 0.0003008610219694674, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "216: [discriminator loss: 0.00020963058341294527, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "217: [discriminator loss: 0.00018440619169268757, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "218: [discriminator loss: 0.00016814311675261706, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "219: [discriminator loss: 0.00017934117931872606, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "220: [discriminator loss: 0.00017782293434720486, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "221: [discriminator loss: 0.00020133747602812946, acc: 1.0] [gan loss: 0.000000, acc: 1.000000]\n",
            "222: [discriminator loss: 0.00022831519891042262, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "223: [discriminator loss: 0.00025550147984176874, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "224: [discriminator loss: 0.00023172050714492798, acc: 1.0] [gan loss: 0.000001, acc: 1.000000]\n",
            "225: [discriminator loss: 0.00152006302960217, acc: 0.9990234375] [gan loss: 0.000000, acc: 1.000000]\n",
            "226: [discriminator loss: 0.0054533397778868675, acc: 1.0] [gan loss: 0.007093, acc: 1.000000]\n",
            "227: [discriminator loss: 5.078817412140779e-05, acc: 1.0] [gan loss: 0.000019, acc: 1.000000]\n",
            "228: [discriminator loss: 5.351146319299005e-05, acc: 1.0] [gan loss: 0.000030, acc: 1.000000]\n",
            "229: [discriminator loss: 6.033773752278648e-05, acc: 1.0] [gan loss: 0.000013, acc: 1.000000]\n",
            "230: [discriminator loss: 0.00020339046022854745, acc: 1.0] [gan loss: 0.000016, acc: 1.000000]\n",
            "231: [discriminator loss: 8.515211811754853e-05, acc: 1.0] [gan loss: 0.000005, acc: 1.000000]\n",
            "232: [discriminator loss: 7.723473390797153e-05, acc: 1.0] [gan loss: 0.000018, acc: 1.000000]\n",
            "233: [discriminator loss: 7.069684215821326e-05, acc: 1.0] [gan loss: 0.000015, acc: 1.000000]\n",
            "234: [discriminator loss: 8.53057936183177e-05, acc: 1.0] [gan loss: 0.000022, acc: 1.000000]\n",
            "235: [discriminator loss: 0.00010917314648395404, acc: 1.0] [gan loss: 0.000013, acc: 1.000000]\n",
            "236: [discriminator loss: 7.447609095834196e-05, acc: 1.0] [gan loss: 0.000018, acc: 1.000000]\n",
            "237: [discriminator loss: 7.368098886217922e-05, acc: 1.0] [gan loss: 0.000036, acc: 1.000000]\n",
            "238: [discriminator loss: 0.0032715070992708206, acc: 0.9990234375] [gan loss: 0.000000, acc: 1.000000]\n",
            "239: [discriminator loss: 0.002207502955570817, acc: 1.0] [gan loss: 0.002105, acc: 1.000000]\n",
            "240: [discriminator loss: 6.0134163504699245e-05, acc: 1.0] [gan loss: 0.000060, acc: 1.000000]\n",
            "241: [discriminator loss: 6.0215748817427084e-05, acc: 1.0] [gan loss: 0.000085, acc: 1.000000]\n",
            "242: [discriminator loss: 6.941666651982814e-05, acc: 1.0] [gan loss: 0.000110, acc: 1.000000]\n",
            "243: [discriminator loss: 6.525041681015864e-05, acc: 1.0] [gan loss: 0.000130, acc: 1.000000]\n",
            "244: [discriminator loss: 7.443263166351244e-05, acc: 1.0] [gan loss: 0.000147, acc: 1.000000]\n",
            "245: [discriminator loss: 7.836954318918288e-05, acc: 1.0] [gan loss: 0.000199, acc: 1.000000]\n",
            "246: [discriminator loss: 8.294895815197378e-05, acc: 1.0] [gan loss: 0.000181, acc: 1.000000]\n",
            "247: [discriminator loss: 9.661115473136306e-05, acc: 1.0] [gan loss: 0.000228, acc: 1.000000]\n",
            "248: [discriminator loss: 0.00010961556108668447, acc: 1.0] [gan loss: 0.000203, acc: 1.000000]\n",
            "249: [discriminator loss: 0.00011440500384196639, acc: 1.0] [gan loss: 0.000243, acc: 1.000000]\n",
            "250: [discriminator loss: 0.00015004845045041293, acc: 1.0] [gan loss: 0.000181, acc: 1.000000]\n",
            "251: [discriminator loss: 0.00012939461157657206, acc: 1.0] [gan loss: 0.000232, acc: 1.000000]\n",
            "252: [discriminator loss: 0.00012535530549939722, acc: 1.0] [gan loss: 0.000269, acc: 1.000000]\n",
            "253: [discriminator loss: 0.00019340970902703702, acc: 1.0] [gan loss: 0.000077, acc: 1.000000]\n",
            "254: [discriminator loss: 0.0004249710473231971, acc: 1.0] [gan loss: 0.000021, acc: 1.000000]\n",
            "255: [discriminator loss: 0.00035692332312464714, acc: 1.0] [gan loss: 0.001937, acc: 1.000000]\n",
            "256: [discriminator loss: 0.0008780899224802852, acc: 1.0] [gan loss: 0.126548, acc: 1.000000]\n",
            "257: [discriminator loss: 0.00018205129890702665, acc: 1.0] [gan loss: 0.000468, acc: 1.000000]\n",
            "258: [discriminator loss: 0.00026792471180669963, acc: 1.0] [gan loss: 0.000066, acc: 1.000000]\n",
            "259: [discriminator loss: 0.00030203862115740776, acc: 1.0] [gan loss: 0.003824, acc: 1.000000]\n",
            "260: [discriminator loss: 9.15321143111214e-05, acc: 1.0] [gan loss: 0.002043, acc: 1.000000]\n",
            "261: [discriminator loss: 0.00038065153057686985, acc: 1.0] [gan loss: 0.000028, acc: 1.000000]\n",
            "262: [discriminator loss: 0.0006160467164590955, acc: 1.0] [gan loss: 0.227838, acc: 1.000000]\n",
            "263: [discriminator loss: 0.00020669445802923292, acc: 1.0] [gan loss: 0.003303, acc: 1.000000]\n",
            "264: [discriminator loss: 7.354651461355388e-05, acc: 1.0] [gan loss: 0.008625, acc: 1.000000]\n",
            "265: [discriminator loss: 6.51482623652555e-05, acc: 1.0] [gan loss: 0.011570, acc: 1.000000]\n",
            "266: [discriminator loss: 0.00013942865189164877, acc: 1.0] [gan loss: 0.003210, acc: 1.000000]\n",
            "267: [discriminator loss: 0.00011015193013008684, acc: 1.0] [gan loss: 0.025585, acc: 1.000000]\n",
            "268: [discriminator loss: 6.403761653928086e-05, acc: 1.0] [gan loss: 0.030243, acc: 1.000000]\n",
            "269: [discriminator loss: 7.092142914189026e-05, acc: 1.0] [gan loss: 0.047779, acc: 1.000000]\n",
            "270: [discriminator loss: 8.574485400458798e-05, acc: 1.0] [gan loss: 0.050297, acc: 1.000000]\n",
            "271: [discriminator loss: 0.000124668818898499, acc: 1.0] [gan loss: 0.131654, acc: 1.000000]\n",
            "272: [discriminator loss: 0.00032101324177347124, acc: 1.0] [gan loss: 2.145368, acc: 0.033203]\n",
            "273: [discriminator loss: 0.0003184335073456168, acc: 1.0] [gan loss: 3.371121, acc: 0.029297]\n",
            "274: [discriminator loss: 0.0009382909629493952, acc: 1.0] [gan loss: 0.000002, acc: 1.000000]\n",
            "275: [discriminator loss: 0.1030009388923645, acc: 0.966796875] [gan loss: 7.829788, acc: 0.000000]\n",
            "276: [discriminator loss: 0.0002680571051314473, acc: 1.0] [gan loss: 2.174906, acc: 0.269531]\n",
            "277: [discriminator loss: 0.0007775258854962885, acc: 1.0] [gan loss: 0.039069, acc: 1.000000]\n",
            "278: [discriminator loss: 0.0013054517330601811, acc: 1.0] [gan loss: 0.021874, acc: 1.000000]\n",
            "279: [discriminator loss: 0.001132648903876543, acc: 1.0] [gan loss: 0.020800, acc: 1.000000]\n",
            "280: [discriminator loss: 0.0012630324345082045, acc: 1.0] [gan loss: 0.018443, acc: 1.000000]\n",
            "281: [discriminator loss: 0.010057157836854458, acc: 0.9990234375] [gan loss: 0.012416, acc: 1.000000]\n",
            "282: [discriminator loss: 0.00168385892175138, acc: 1.0] [gan loss: 0.016075, acc: 1.000000]\n",
            "283: [discriminator loss: 0.0017586590256541967, acc: 1.0] [gan loss: 0.022959, acc: 1.000000]\n",
            "284: [discriminator loss: 0.0015227739932015538, acc: 1.0] [gan loss: 0.021034, acc: 1.000000]\n",
            "285: [discriminator loss: 0.00842264574021101, acc: 0.9990234375] [gan loss: 0.015891, acc: 1.000000]\n",
            "286: [discriminator loss: 0.003361667972058058, acc: 0.9990234375] [gan loss: 0.015451, acc: 1.000000]\n",
            "287: [discriminator loss: 0.0026413206942379475, acc: 1.0] [gan loss: 0.019787, acc: 1.000000]\n",
            "288: [discriminator loss: 0.002523475792258978, acc: 1.0] [gan loss: 0.015227, acc: 1.000000]\n",
            "289: [discriminator loss: 0.0019873082637786865, acc: 1.0] [gan loss: 0.029543, acc: 1.000000]\n",
            "290: [discriminator loss: 0.003705814015120268, acc: 0.9990234375] [gan loss: 0.016664, acc: 1.000000]\n",
            "291: [discriminator loss: 0.0018938526045531034, acc: 1.0] [gan loss: 0.032503, acc: 1.000000]\n",
            "292: [discriminator loss: 0.0015184060903266072, acc: 1.0] [gan loss: 0.034529, acc: 1.000000]\n",
            "293: [discriminator loss: 0.0015025176107883453, acc: 1.0] [gan loss: 0.037274, acc: 1.000000]\n",
            "294: [discriminator loss: 0.0023047199938446283, acc: 0.9990234375] [gan loss: 0.016216, acc: 1.000000]\n",
            "295: [discriminator loss: 0.0013737204717472196, acc: 1.0] [gan loss: 0.033444, acc: 1.000000]\n",
            "296: [discriminator loss: 0.0016178661026060581, acc: 1.0] [gan loss: 0.040051, acc: 1.000000]\n",
            "297: [discriminator loss: 0.0016088038682937622, acc: 1.0] [gan loss: 0.044700, acc: 1.000000]\n",
            "298: [discriminator loss: 0.008206342346966267, acc: 0.9990234375] [gan loss: 0.015131, acc: 1.000000]\n",
            "299: [discriminator loss: 0.0019934396259486675, acc: 1.0] [gan loss: 0.075502, acc: 1.000000]\n",
            "300: [discriminator loss: 0.005212501622736454, acc: 0.998046875] [gan loss: 0.017222, acc: 1.000000]\n",
            "301: [discriminator loss: 0.0028380935546010733, acc: 1.0] [gan loss: 0.174463, acc: 1.000000]\n",
            "302: [discriminator loss: 0.00713898753747344, acc: 0.9990234375] [gan loss: 0.492036, acc: 1.000000]\n",
            "303: [discriminator loss: 0.01574936881661415, acc: 1.0] [gan loss: 8.356913, acc: 0.000000]\n",
            "304: [discriminator loss: 0.007613141089677811, acc: 0.9970703125] [gan loss: 0.093664, acc: 1.000000]\n",
            "305: [discriminator loss: 0.01932569034397602, acc: 0.998046875] [gan loss: 0.401373, acc: 0.941406]\n",
            "306: [discriminator loss: 0.012795185670256615, acc: 1.0] [gan loss: 6.159379, acc: 0.000000]\n",
            "307: [discriminator loss: 0.022127948701381683, acc: 0.9951171875] [gan loss: 0.000441, acc: 1.000000]\n",
            "308: [discriminator loss: 0.01926538348197937, acc: 0.9990234375] [gan loss: 2.916700, acc: 0.000000]\n",
            "309: [discriminator loss: 0.0009717835346236825, acc: 1.0] [gan loss: 0.008394, acc: 1.000000]\n",
            "310: [discriminator loss: 0.004824742674827576, acc: 0.998046875] [gan loss: 0.003144, acc: 1.000000]\n",
            "311: [discriminator loss: 0.0050924369134008884, acc: 0.998046875] [gan loss: 0.001535, acc: 1.000000]\n",
            "312: [discriminator loss: 0.002769834827631712, acc: 1.0] [gan loss: 0.016456, acc: 1.000000]\n",
            "313: [discriminator loss: 0.0010101600782945752, acc: 1.0] [gan loss: 0.019288, acc: 1.000000]\n",
            "314: [discriminator loss: 0.006870503071695566, acc: 0.9990234375] [gan loss: 0.003261, acc: 1.000000]\n",
            "315: [discriminator loss: 0.003793436335399747, acc: 0.9990234375] [gan loss: 0.008315, acc: 1.000000]\n",
            "316: [discriminator loss: 0.0014349713455885649, acc: 1.0] [gan loss: 0.031592, acc: 1.000000]\n",
            "317: [discriminator loss: 0.015076822601258755, acc: 0.998046875] [gan loss: 0.002131, acc: 1.000000]\n",
            "318: [discriminator loss: 0.003725475398823619, acc: 1.0] [gan loss: 0.231703, acc: 1.000000]\n",
            "319: [discriminator loss: 0.00037568234256468713, acc: 1.0] [gan loss: 0.037032, acc: 1.000000]\n",
            "320: [discriminator loss: 0.0015551361721009016, acc: 0.9990234375] [gan loss: 0.012543, acc: 1.000000]\n",
            "321: [discriminator loss: 0.0006704141851514578, acc: 1.0] [gan loss: 0.026689, acc: 1.000000]\n",
            "322: [discriminator loss: 0.0035541970282793045, acc: 0.998046875] [gan loss: 0.001568, acc: 1.000000]\n",
            "323: [discriminator loss: 0.0026635269168764353, acc: 1.0] [gan loss: 0.133057, acc: 1.000000]\n",
            "324: [discriminator loss: 0.00040745240403339267, acc: 1.0] [gan loss: 0.042106, acc: 1.000000]\n",
            "325: [discriminator loss: 0.00034573444281704724, acc: 1.0] [gan loss: 0.036255, acc: 1.000000]\n",
            "326: [discriminator loss: 0.003133081365376711, acc: 0.9990234375] [gan loss: 0.004382, acc: 1.000000]\n",
            "327: [discriminator loss: 0.000992146204225719, acc: 1.0] [gan loss: 0.029258, acc: 1.000000]\n",
            "328: [discriminator loss: 0.00040155963506549597, acc: 1.0] [gan loss: 0.046547, acc: 1.000000]\n",
            "329: [discriminator loss: 0.0006532215629704297, acc: 1.0] [gan loss: 0.025200, acc: 1.000000]\n",
            "330: [discriminator loss: 0.0010167664149776101, acc: 1.0] [gan loss: 0.015389, acc: 1.000000]\n",
            "331: [discriminator loss: 0.0006940034800209105, acc: 1.0] [gan loss: 0.037810, acc: 1.000000]\n",
            "332: [discriminator loss: 0.00047686888137832284, acc: 1.0] [gan loss: 0.050518, acc: 1.000000]\n",
            "333: [discriminator loss: 0.0005182027816772461, acc: 1.0] [gan loss: 0.040906, acc: 1.000000]\n",
            "334: [discriminator loss: 0.0004660488630179316, acc: 1.0] [gan loss: 0.049878, acc: 1.000000]\n",
            "335: [discriminator loss: 0.00040931825060397387, acc: 1.0] [gan loss: 0.058739, acc: 1.000000]\n",
            "336: [discriminator loss: 0.0005244244239293039, acc: 1.0] [gan loss: 0.061597, acc: 1.000000]\n",
            "337: [discriminator loss: 0.0011156725231558084, acc: 1.0] [gan loss: 0.011702, acc: 1.000000]\n",
            "338: [discriminator loss: 0.0011195528786629438, acc: 1.0] [gan loss: 0.147642, acc: 1.000000]\n",
            "339: [discriminator loss: 0.007753871846944094, acc: 0.9990234375] [gan loss: 0.056046, acc: 1.000000]\n",
            "340: [discriminator loss: 0.0016878012102097273, acc: 1.0] [gan loss: 1.416643, acc: 0.078125]\n",
            "341: [discriminator loss: 0.021378492936491966, acc: 0.9931640625] [gan loss: 14.610624, acc: 0.000000]\n",
            "342: [discriminator loss: 0.03200256824493408, acc: 0.9912109375] [gan loss: 0.200636, acc: 1.000000]\n",
            "343: [discriminator loss: 0.012943440116941929, acc: 0.9990234375] [gan loss: 6.859460, acc: 0.007812]\n",
            "344: [discriminator loss: 0.0030842216219753027, acc: 1.0] [gan loss: 2.925418, acc: 0.078125]\n",
            "345: [discriminator loss: 0.021041294559836388, acc: 0.9931640625] [gan loss: 10.323489, acc: 0.000000]\n",
            "346: [discriminator loss: 0.011743147857487202, acc: 0.9970703125] [gan loss: 3.265197, acc: 0.105469]\n",
            "347: [discriminator loss: 0.04018446058034897, acc: 0.990234375] [gan loss: 9.010329, acc: 0.003906]\n",
            "348: [discriminator loss: 0.02536533959209919, acc: 0.994140625] [gan loss: 0.996563, acc: 0.441406]\n",
            "349: [discriminator loss: 0.057717833667993546, acc: 0.98828125] [gan loss: 11.372149, acc: 0.000000]\n",
            "350: [discriminator loss: 0.06837951391935349, acc: 0.98046875] [gan loss: 0.384716, acc: 0.935547]\n",
            "351: [discriminator loss: 0.01215679757297039, acc: 0.9970703125] [gan loss: 0.236262, acc: 1.000000]\n",
            "352: [discriminator loss: 0.016658879816532135, acc: 0.99609375] [gan loss: 0.146494, acc: 1.000000]\n",
            "353: [discriminator loss: 0.008503492921590805, acc: 0.9990234375] [gan loss: 0.279428, acc: 1.000000]\n",
            "354: [discriminator loss: 0.0072553642094135284, acc: 0.9990234375] [gan loss: 0.185892, acc: 1.000000]\n",
            "355: [discriminator loss: 0.005022193305194378, acc: 1.0] [gan loss: 0.245097, acc: 1.000000]\n",
            "356: [discriminator loss: 0.008265499025583267, acc: 0.9990234375] [gan loss: 0.166598, acc: 1.000000]\n",
            "357: [discriminator loss: 0.01325023453682661, acc: 0.998046875] [gan loss: 0.116685, acc: 1.000000]\n",
            "358: [discriminator loss: 0.007118350826203823, acc: 0.9990234375] [gan loss: 0.293358, acc: 1.000000]\n",
            "359: [discriminator loss: 0.004007103852927685, acc: 1.0] [gan loss: 0.276003, acc: 1.000000]\n",
            "360: [discriminator loss: 0.00476881954818964, acc: 0.9990234375] [gan loss: 0.152524, acc: 1.000000]\n",
            "361: [discriminator loss: 0.003702595131471753, acc: 1.0] [gan loss: 0.209357, acc: 1.000000]\n",
            "362: [discriminator loss: 0.01234231237322092, acc: 0.998046875] [gan loss: 0.044710, acc: 1.000000]\n",
            "363: [discriminator loss: 0.00821448303759098, acc: 0.998046875] [gan loss: 0.214649, acc: 1.000000]\n",
            "364: [discriminator loss: 0.005677446722984314, acc: 0.998046875] [gan loss: 0.165274, acc: 1.000000]\n",
            "365: [discriminator loss: 0.014586086384952068, acc: 0.99609375] [gan loss: 0.026182, acc: 1.000000]\n",
            "366: [discriminator loss: 0.006197503302246332, acc: 1.0] [gan loss: 0.968002, acc: 0.015625]\n",
            "367: [discriminator loss: 0.007016553543508053, acc: 1.0] [gan loss: 0.187788, acc: 1.000000]\n",
            "368: [discriminator loss: 0.020100073888897896, acc: 0.994140625] [gan loss: 0.001358, acc: 1.000000]\n",
            "369: [discriminator loss: 0.026802344247698784, acc: 0.9990234375] [gan loss: 3.864842, acc: 0.000000]\n",
            "370: [discriminator loss: 0.009868081659078598, acc: 0.998046875] [gan loss: 0.006925, acc: 1.000000]\n",
            "371: [discriminator loss: 0.004464390687644482, acc: 0.9990234375] [gan loss: 0.013552, acc: 1.000000]\n",
            "372: [discriminator loss: 0.0016128388233482838, acc: 1.0] [gan loss: 0.034275, acc: 1.000000]\n",
            "373: [discriminator loss: 0.002093222225084901, acc: 0.9990234375] [gan loss: 0.027511, acc: 1.000000]\n",
            "374: [discriminator loss: 0.004253110382705927, acc: 0.9990234375] [gan loss: 0.011918, acc: 1.000000]\n",
            "375: [discriminator loss: 0.0023324210196733475, acc: 1.0] [gan loss: 0.030774, acc: 1.000000]\n",
            "376: [discriminator loss: 0.002586961258202791, acc: 0.9990234375] [gan loss: 0.024370, acc: 1.000000]\n",
            "377: [discriminator loss: 0.00141843780875206, acc: 1.0] [gan loss: 0.054787, acc: 1.000000]\n",
            "378: [discriminator loss: 0.0009012211230583489, acc: 1.0] [gan loss: 0.058158, acc: 1.000000]\n",
            "379: [discriminator loss: 0.0019038267200812697, acc: 0.9990234375] [gan loss: 0.021759, acc: 1.000000]\n",
            "380: [discriminator loss: 0.0022934467997401953, acc: 1.0] [gan loss: 0.017363, acc: 1.000000]\n",
            "381: [discriminator loss: 0.0016824251506477594, acc: 1.0] [gan loss: 0.041483, acc: 1.000000]\n",
            "382: [discriminator loss: 0.000950241694226861, acc: 1.0] [gan loss: 0.057254, acc: 1.000000]\n",
            "383: [discriminator loss: 0.0015731507446616888, acc: 0.9990234375] [gan loss: 0.028679, acc: 1.000000]\n",
            "384: [discriminator loss: 0.0013607938308268785, acc: 1.0] [gan loss: 0.042410, acc: 1.000000]\n",
            "385: [discriminator loss: 0.0016297774855047464, acc: 1.0] [gan loss: 0.024250, acc: 1.000000]\n",
            "386: [discriminator loss: 0.0011216567363590002, acc: 1.0] [gan loss: 0.048939, acc: 1.000000]\n",
            "387: [discriminator loss: 0.0010467147221788764, acc: 1.0] [gan loss: 0.037210, acc: 1.000000]\n",
            "388: [discriminator loss: 0.0009645144455134869, acc: 1.0] [gan loss: 0.037974, acc: 1.000000]\n",
            "389: [discriminator loss: 0.000768797064665705, acc: 1.0] [gan loss: 0.041177, acc: 1.000000]\n",
            "390: [discriminator loss: 0.0006601677741855383, acc: 1.0] [gan loss: 0.040730, acc: 1.000000]\n",
            "391: [discriminator loss: 0.001715854275971651, acc: 0.9990234375] [gan loss: 0.012149, acc: 1.000000]\n",
            "392: [discriminator loss: 0.01706802472472191, acc: 0.998046875] [gan loss: 0.000635, acc: 1.000000]\n",
            "393: [discriminator loss: 0.007825672626495361, acc: 1.0] [gan loss: 4.540726, acc: 0.000000]\n",
            "394: [discriminator loss: 0.0016614500200375915, acc: 0.9990234375] [gan loss: 0.009301, acc: 1.000000]\n",
            "395: [discriminator loss: 0.0009611449204385281, acc: 1.0] [gan loss: 0.019657, acc: 1.000000]\n",
            "396: [discriminator loss: 0.003977152518928051, acc: 0.9990234375] [gan loss: 0.004300, acc: 1.000000]\n",
            "397: [discriminator loss: 0.002209346741437912, acc: 1.0] [gan loss: 0.028520, acc: 1.000000]\n",
            "398: [discriminator loss: 0.000855351216159761, acc: 1.0] [gan loss: 0.034505, acc: 1.000000]\n",
            "399: [discriminator loss: 0.0005640384624712169, acc: 1.0] [gan loss: 0.068652, acc: 1.000000]\n",
            "400: [discriminator loss: 0.0011635888367891312, acc: 1.0] [gan loss: 0.020444, acc: 1.000000]\n",
            "401: [discriminator loss: 0.0013764564646407962, acc: 1.0] [gan loss: 0.023184, acc: 1.000000]\n",
            "402: [discriminator loss: 0.008975805714726448, acc: 0.9990234375] [gan loss: 0.007357, acc: 1.000000]\n",
            "403: [discriminator loss: 0.0015560048632323742, acc: 1.0] [gan loss: 0.439146, acc: 0.994141]\n",
            "404: [discriminator loss: 0.0028787751216441393, acc: 1.0] [gan loss: 1.411541, acc: 0.007812]\n",
            "405: [discriminator loss: 0.006080871447920799, acc: 1.0] [gan loss: 6.753880, acc: 0.000000]\n",
            "406: [discriminator loss: 0.02590096928179264, acc: 0.994140625] [gan loss: 0.000157, acc: 1.000000]\n",
            "407: [discriminator loss: 0.10988810658454895, acc: 0.9814453125] [gan loss: 8.375431, acc: 0.000000]\n",
            "408: [discriminator loss: 0.013526692986488342, acc: 0.99609375] [gan loss: 1.499488, acc: 0.419922]\n",
            "409: [discriminator loss: 0.010383851826190948, acc: 0.9990234375] [gan loss: 0.325994, acc: 0.847656]\n",
            "410: [discriminator loss: 0.02480938285589218, acc: 0.9990234375] [gan loss: 1.271554, acc: 0.373047]\n",
            "411: [discriminator loss: 0.09174700081348419, acc: 0.9931640625] [gan loss: 7.515497, acc: 0.000000]\n",
            "412: [discriminator loss: 0.057619012892246246, acc: 0.9873046875] [gan loss: 0.644174, acc: 0.597656]\n",
            "413: [discriminator loss: 0.2152208387851715, acc: 0.8837890625] [gan loss: 9.324442, acc: 0.000000]\n",
            "414: [discriminator loss: 0.10487782955169678, acc: 0.97265625] [gan loss: 1.653130, acc: 0.175781]\n",
            "415: [discriminator loss: 0.15691159665584564, acc: 0.95703125] [gan loss: 6.510880, acc: 0.000000]\n",
            "416: [discriminator loss: 0.11973948776721954, acc: 0.970703125] [gan loss: 1.419972, acc: 0.222656]\n",
            "417: [discriminator loss: 0.213486909866333, acc: 0.931640625] [gan loss: 7.376153, acc: 0.000000]\n",
            "418: [discriminator loss: 0.13213586807250977, acc: 0.96875] [gan loss: 2.237584, acc: 0.128906]\n",
            "419: [discriminator loss: 0.1630851924419403, acc: 0.986328125] [gan loss: 4.042136, acc: 0.000000]\n",
            "420: [discriminator loss: 0.07250354439020157, acc: 0.98828125] [gan loss: 1.102195, acc: 0.291016]\n",
            "421: [discriminator loss: 0.13744091987609863, acc: 0.984375] [gan loss: 1.919607, acc: 0.000000]\n",
            "422: [discriminator loss: 0.06785565614700317, acc: 0.9892578125] [gan loss: 0.551783, acc: 0.796875]\n",
            "423: [discriminator loss: 0.0922292172908783, acc: 0.986328125] [gan loss: 0.429926, acc: 0.966797]\n",
            "424: [discriminator loss: 0.0599282905459404, acc: 0.98828125] [gan loss: 0.336878, acc: 0.996094]\n",
            "425: [discriminator loss: 0.058654025197029114, acc: 0.9912109375] [gan loss: 0.330791, acc: 0.992188]\n",
            "426: [discriminator loss: 0.043124452233314514, acc: 0.9921875] [gan loss: 0.302038, acc: 1.000000]\n",
            "427: [discriminator loss: 0.035510994493961334, acc: 0.99609375] [gan loss: 0.362932, acc: 0.994141]\n",
            "428: [discriminator loss: 0.05647411197423935, acc: 0.9912109375] [gan loss: 0.247624, acc: 1.000000]\n",
            "429: [discriminator loss: 0.031086452305316925, acc: 0.994140625] [gan loss: 0.309106, acc: 1.000000]\n",
            "430: [discriminator loss: 0.0426788367331028, acc: 0.990234375] [gan loss: 0.215703, acc: 1.000000]\n",
            "431: [discriminator loss: 0.030975131317973137, acc: 0.9931640625] [gan loss: 0.244403, acc: 1.000000]\n",
            "432: [discriminator loss: 0.026051480323076248, acc: 0.99609375] [gan loss: 0.313038, acc: 1.000000]\n",
            "433: [discriminator loss: 0.02374783530831337, acc: 0.9951171875] [gan loss: 0.266553, acc: 1.000000]\n",
            "434: [discriminator loss: 0.013029887340962887, acc: 0.9990234375] [gan loss: 0.324810, acc: 1.000000]\n",
            "435: [discriminator loss: 0.019499080255627632, acc: 0.9970703125] [gan loss: 0.272336, acc: 1.000000]\n",
            "436: [discriminator loss: 0.017413994297385216, acc: 0.998046875] [gan loss: 0.248999, acc: 1.000000]\n",
            "437: [discriminator loss: 0.017916277050971985, acc: 0.9970703125] [gan loss: 0.251684, acc: 1.000000]\n",
            "438: [discriminator loss: 0.02097298763692379, acc: 0.994140625] [gan loss: 0.169322, acc: 1.000000]\n",
            "439: [discriminator loss: 0.015314720571041107, acc: 0.998046875] [gan loss: 0.270523, acc: 1.000000]\n",
            "440: [discriminator loss: 0.017116885632276535, acc: 0.9970703125] [gan loss: 0.203592, acc: 1.000000]\n",
            "441: [discriminator loss: 0.010751809924840927, acc: 0.998046875] [gan loss: 0.197863, acc: 1.000000]\n",
            "442: [discriminator loss: 0.01264904998242855, acc: 0.998046875] [gan loss: 0.212238, acc: 1.000000]\n",
            "443: [discriminator loss: 0.013532261364161968, acc: 0.998046875] [gan loss: 0.171149, acc: 1.000000]\n",
            "444: [discriminator loss: 0.009210251271724701, acc: 0.998046875] [gan loss: 0.168232, acc: 1.000000]\n",
            "445: [discriminator loss: 0.009095851331949234, acc: 0.998046875] [gan loss: 0.144940, acc: 1.000000]\n",
            "446: [discriminator loss: 0.007540548220276833, acc: 0.9990234375] [gan loss: 0.157057, acc: 1.000000]\n",
            "447: [discriminator loss: 0.008686630986630917, acc: 0.998046875] [gan loss: 0.133535, acc: 1.000000]\n",
            "448: [discriminator loss: 0.006939935497939587, acc: 0.9990234375] [gan loss: 0.137990, acc: 1.000000]\n",
            "449: [discriminator loss: 0.005452358163893223, acc: 0.9990234375] [gan loss: 0.149478, acc: 1.000000]\n",
            "450: [discriminator loss: 0.009610036388039589, acc: 0.9970703125] [gan loss: 0.077541, acc: 1.000000]\n",
            "451: [discriminator loss: 0.005621382500976324, acc: 1.0] [gan loss: 0.107980, acc: 1.000000]\n",
            "452: [discriminator loss: 0.004950857721269131, acc: 1.0] [gan loss: 0.090392, acc: 1.000000]\n",
            "453: [discriminator loss: 0.003917408641427755, acc: 1.0] [gan loss: 0.108055, acc: 1.000000]\n",
            "454: [discriminator loss: 0.006104419007897377, acc: 0.998046875] [gan loss: 0.058735, acc: 1.000000]\n",
            "455: [discriminator loss: 0.004003196023404598, acc: 1.0] [gan loss: 0.089882, acc: 1.000000]\n",
            "456: [discriminator loss: 0.002987025771290064, acc: 1.0] [gan loss: 0.091639, acc: 1.000000]\n",
            "457: [discriminator loss: 0.002653662580996752, acc: 1.0] [gan loss: 0.088294, acc: 1.000000]\n",
            "458: [discriminator loss: 0.003716625040397048, acc: 0.9990234375] [gan loss: 0.063259, acc: 1.000000]\n",
            "459: [discriminator loss: 0.004706595093011856, acc: 0.9990234375] [gan loss: 0.044574, acc: 1.000000]\n",
            "460: [discriminator loss: 0.0029806061647832394, acc: 1.0] [gan loss: 0.060504, acc: 1.000000]\n",
            "461: [discriminator loss: 0.009549479000270367, acc: 0.9970703125] [gan loss: 0.025253, acc: 1.000000]\n",
            "462: [discriminator loss: 0.0034450215753167868, acc: 1.0] [gan loss: 0.056642, acc: 1.000000]\n",
            "463: [discriminator loss: 0.0050359624437987804, acc: 0.9990234375] [gan loss: 0.030068, acc: 1.000000]\n",
            "464: [discriminator loss: 0.0024233809672296047, acc: 1.0] [gan loss: 0.059469, acc: 1.000000]\n",
            "465: [discriminator loss: 0.004818820860236883, acc: 0.9990234375] [gan loss: 0.034194, acc: 1.000000]\n",
            "466: [discriminator loss: 0.0020470984745770693, acc: 1.0] [gan loss: 0.060358, acc: 1.000000]\n",
            "467: [discriminator loss: 0.005782672669738531, acc: 0.998046875] [gan loss: 0.015936, acc: 1.000000]\n",
            "468: [discriminator loss: 0.002543524606153369, acc: 1.0] [gan loss: 0.054755, acc: 1.000000]\n",
            "469: [discriminator loss: 0.0019079719204455614, acc: 1.0] [gan loss: 0.041543, acc: 1.000000]\n",
            "470: [discriminator loss: 0.0014879709342494607, acc: 1.0] [gan loss: 0.045384, acc: 1.000000]\n",
            "471: [discriminator loss: 0.0013841051841154695, acc: 1.0] [gan loss: 0.047954, acc: 1.000000]\n",
            "472: [discriminator loss: 0.0021588956005871296, acc: 0.9990234375] [gan loss: 0.025268, acc: 1.000000]\n",
            "473: [discriminator loss: 0.0015923026949167252, acc: 1.0] [gan loss: 0.037133, acc: 1.000000]\n",
            "474: [discriminator loss: 0.0020705421920865774, acc: 0.9990234375] [gan loss: 0.026468, acc: 1.000000]\n",
            "475: [discriminator loss: 0.0015746818389743567, acc: 1.0] [gan loss: 0.037883, acc: 1.000000]\n",
            "476: [discriminator loss: 0.0018875111127272248, acc: 1.0] [gan loss: 0.028060, acc: 1.000000]\n",
            "477: [discriminator loss: 0.0028636448550969362, acc: 0.9990234375] [gan loss: 0.020755, acc: 1.000000]\n",
            "478: [discriminator loss: 0.001954191830009222, acc: 1.0] [gan loss: 0.028257, acc: 1.000000]\n",
            "479: [discriminator loss: 0.005384058225899935, acc: 0.998046875] [gan loss: 0.003980, acc: 1.000000]\n",
            "480: [discriminator loss: 0.003285395447164774, acc: 1.0] [gan loss: 0.169800, acc: 1.000000]\n",
            "481: [discriminator loss: 0.002551126293838024, acc: 1.0] [gan loss: 0.066843, acc: 1.000000]\n",
            "482: [discriminator loss: 0.0018723588436841965, acc: 1.0] [gan loss: 0.025443, acc: 1.000000]\n",
            "483: [discriminator loss: 0.002057421486824751, acc: 0.9990234375] [gan loss: 0.030676, acc: 1.000000]\n",
            "484: [discriminator loss: 0.002028803573921323, acc: 1.0] [gan loss: 0.036482, acc: 1.000000]\n",
            "485: [discriminator loss: 0.005667322780936956, acc: 0.9970703125] [gan loss: 0.002810, acc: 1.000000]\n",
            "486: [discriminator loss: 0.004643088672310114, acc: 1.0] [gan loss: 0.857078, acc: 0.314453]\n",
            "487: [discriminator loss: 0.007346753496676683, acc: 1.0] [gan loss: 0.962441, acc: 0.277344]\n",
            "488: [discriminator loss: 0.0052083819173276424, acc: 0.9990234375] [gan loss: 0.041987, acc: 1.000000]\n",
            "489: [discriminator loss: 0.011262774467468262, acc: 0.9970703125] [gan loss: 0.000343, acc: 1.000000]\n",
            "490: [discriminator loss: 0.01827601157128811, acc: 1.0] [gan loss: 2.674609, acc: 0.013672]\n",
            "491: [discriminator loss: 0.00734688201919198, acc: 0.998046875] [gan loss: 0.011003, acc: 1.000000]\n",
            "492: [discriminator loss: 0.006698458455502987, acc: 0.9990234375] [gan loss: 0.007260, acc: 1.000000]\n",
            "493: [discriminator loss: 0.004969618283212185, acc: 1.0] [gan loss: 0.022163, acc: 1.000000]\n",
            "494: [discriminator loss: 0.007028339430689812, acc: 0.998046875] [gan loss: 0.011327, acc: 1.000000]\n",
            "495: [discriminator loss: 0.004487038590013981, acc: 1.0] [gan loss: 0.041105, acc: 1.000000]\n",
            "496: [discriminator loss: 0.002585235983133316, acc: 1.0] [gan loss: 0.059031, acc: 1.000000]\n",
            "497: [discriminator loss: 0.002645121654495597, acc: 1.0] [gan loss: 0.063277, acc: 1.000000]\n",
            "498: [discriminator loss: 0.0033066675532609224, acc: 1.0] [gan loss: 0.076836, acc: 1.000000]\n",
            "499: [discriminator loss: 0.003005007980391383, acc: 1.0] [gan loss: 0.261127, acc: 1.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "500: [discriminator loss: 0.010309604927897453, acc: 1.0] [gan loss: 1.276872, acc: 0.126953]\n",
            "501: [discriminator loss: 0.12730202078819275, acc: 0.9921875] [gan loss: 6.586894, acc: 0.000000]\n",
            "502: [discriminator loss: 0.17694741487503052, acc: 0.9609375] [gan loss: 0.000706, acc: 1.000000]\n",
            "503: [discriminator loss: 0.2570650577545166, acc: 0.990234375] [gan loss: 3.904898, acc: 0.000000]\n",
            "504: [discriminator loss: 0.23799782991409302, acc: 0.9423828125] [gan loss: 0.000791, acc: 1.000000]\n",
            "505: [discriminator loss: 0.14758005738258362, acc: 0.9912109375] [gan loss: 0.179157, acc: 0.994141]\n",
            "506: [discriminator loss: 0.11596041917800903, acc: 0.9677734375] [gan loss: 0.007715, acc: 1.000000]\n",
            "507: [discriminator loss: 0.06953224539756775, acc: 0.986328125] [gan loss: 0.009203, acc: 1.000000]\n",
            "508: [discriminator loss: 0.06373774260282516, acc: 0.9892578125] [gan loss: 0.011085, acc: 1.000000]\n",
            "509: [discriminator loss: 0.034146323800086975, acc: 0.9921875] [gan loss: 0.016960, acc: 1.000000]\n",
            "510: [discriminator loss: 0.04900072515010834, acc: 0.9892578125] [gan loss: 0.012223, acc: 1.000000]\n",
            "511: [discriminator loss: 0.03228197246789932, acc: 0.9931640625] [gan loss: 0.020181, acc: 1.000000]\n",
            "512: [discriminator loss: 0.055546995252370834, acc: 0.990234375] [gan loss: 0.013889, acc: 1.000000]\n",
            "513: [discriminator loss: 0.03732039034366608, acc: 0.9931640625] [gan loss: 0.019030, acc: 1.000000]\n",
            "514: [discriminator loss: 0.022914912551641464, acc: 0.9970703125] [gan loss: 0.025702, acc: 1.000000]\n",
            "515: [discriminator loss: 0.029789935797452927, acc: 0.9931640625] [gan loss: 0.021216, acc: 1.000000]\n",
            "516: [discriminator loss: 0.022082623094320297, acc: 0.9951171875] [gan loss: 0.026226, acc: 1.000000]\n",
            "517: [discriminator loss: 0.01652458682656288, acc: 0.998046875] [gan loss: 0.037937, acc: 1.000000]\n",
            "518: [discriminator loss: 0.016139095649123192, acc: 0.998046875] [gan loss: 0.038474, acc: 1.000000]\n",
            "519: [discriminator loss: 0.010955443605780602, acc: 0.998046875] [gan loss: 0.044929, acc: 1.000000]\n",
            "520: [discriminator loss: 0.013609514571726322, acc: 0.998046875] [gan loss: 0.039709, acc: 1.000000]\n",
            "521: [discriminator loss: 0.014188467524945736, acc: 0.998046875] [gan loss: 0.033548, acc: 1.000000]\n",
            "522: [discriminator loss: 0.009974404238164425, acc: 0.9990234375] [gan loss: 0.045717, acc: 1.000000]\n",
            "523: [discriminator loss: 0.016214795410633087, acc: 0.9931640625] [gan loss: 0.022926, acc: 1.000000]\n",
            "524: [discriminator loss: 0.014332541264593601, acc: 0.9990234375] [gan loss: 0.060985, acc: 1.000000]\n",
            "525: [discriminator loss: 0.01442041341215372, acc: 0.994140625] [gan loss: 0.025981, acc: 1.000000]\n",
            "526: [discriminator loss: 0.01095153484493494, acc: 0.9990234375] [gan loss: 0.043182, acc: 1.000000]\n",
            "527: [discriminator loss: 0.0077799526043236256, acc: 1.0] [gan loss: 0.059875, acc: 1.000000]\n",
            "528: [discriminator loss: 0.010985519737005234, acc: 0.9970703125] [gan loss: 0.037316, acc: 1.000000]\n",
            "529: [discriminator loss: 0.007202500011771917, acc: 1.0] [gan loss: 0.079673, acc: 1.000000]\n",
            "530: [discriminator loss: 0.010518400929868221, acc: 0.998046875] [gan loss: 0.052889, acc: 1.000000]\n",
            "531: [discriminator loss: 0.009531950578093529, acc: 0.9990234375] [gan loss: 0.062444, acc: 1.000000]\n",
            "532: [discriminator loss: 0.015012787654995918, acc: 0.9970703125] [gan loss: 0.042500, acc: 1.000000]\n",
            "533: [discriminator loss: 0.011840060353279114, acc: 0.998046875] [gan loss: 0.067854, acc: 1.000000]\n",
            "534: [discriminator loss: 0.010484275408089161, acc: 1.0] [gan loss: 0.206416, acc: 1.000000]\n",
            "535: [discriminator loss: 0.03725983574986458, acc: 0.9951171875] [gan loss: 1.368705, acc: 0.000000]\n",
            "536: [discriminator loss: 0.11803239583969116, acc: 0.994140625] [gan loss: 5.847020, acc: 0.000000]\n",
            "537: [discriminator loss: 0.20268738269805908, acc: 0.9375] [gan loss: 0.000393, acc: 1.000000]\n",
            "538: [discriminator loss: 0.4451588988304138, acc: 0.6826171875] [gan loss: 4.136549, acc: 0.000000]\n",
            "539: [discriminator loss: 0.21553421020507812, acc: 0.94140625] [gan loss: 0.041898, acc: 1.000000]\n",
            "540: [discriminator loss: 0.061976827681064606, acc: 0.98828125] [gan loss: 0.064573, acc: 1.000000]\n",
            "541: [discriminator loss: 0.055031668394804, acc: 0.990234375] [gan loss: 0.055031, acc: 1.000000]\n",
            "542: [discriminator loss: 0.04044734314084053, acc: 0.98828125] [gan loss: 0.047836, acc: 1.000000]\n",
            "543: [discriminator loss: 0.04142023250460625, acc: 0.990234375] [gan loss: 0.043451, acc: 1.000000]\n",
            "544: [discriminator loss: 0.034767720848321915, acc: 0.994140625] [gan loss: 0.059874, acc: 1.000000]\n",
            "545: [discriminator loss: 0.039729464799165726, acc: 0.9892578125] [gan loss: 0.047429, acc: 1.000000]\n",
            "546: [discriminator loss: 0.05375567823648453, acc: 0.9892578125] [gan loss: 0.041417, acc: 1.000000]\n",
            "547: [discriminator loss: 0.03603962063789368, acc: 0.99609375] [gan loss: 0.069041, acc: 1.000000]\n",
            "548: [discriminator loss: 0.03754552826285362, acc: 0.9931640625] [gan loss: 0.059497, acc: 1.000000]\n",
            "549: [discriminator loss: 0.035635337233543396, acc: 0.99609375] [gan loss: 0.088643, acc: 1.000000]\n",
            "550: [discriminator loss: 0.02311817742884159, acc: 0.9951171875] [gan loss: 0.111816, acc: 1.000000]\n",
            "551: [discriminator loss: 0.02774580381810665, acc: 0.9951171875] [gan loss: 0.120863, acc: 1.000000]\n",
            "552: [discriminator loss: 0.0374993234872818, acc: 0.994140625] [gan loss: 0.096566, acc: 1.000000]\n",
            "553: [discriminator loss: 0.03496820852160454, acc: 0.9921875] [gan loss: 0.098581, acc: 1.000000]\n",
            "554: [discriminator loss: 0.029865117743611336, acc: 0.9970703125] [gan loss: 0.164048, acc: 1.000000]\n",
            "555: [discriminator loss: 0.0336526557803154, acc: 0.994140625] [gan loss: 0.139337, acc: 1.000000]\n",
            "556: [discriminator loss: 0.030952762812376022, acc: 0.998046875] [gan loss: 0.241414, acc: 1.000000]\n",
            "557: [discriminator loss: 0.03183059021830559, acc: 0.9970703125] [gan loss: 0.349785, acc: 1.000000]\n",
            "558: [discriminator loss: 0.04338163882493973, acc: 0.9970703125] [gan loss: 0.458557, acc: 0.976562]\n",
            "559: [discriminator loss: 0.048439353704452515, acc: 0.994140625] [gan loss: 0.257606, acc: 1.000000]\n",
            "560: [discriminator loss: 0.03823019564151764, acc: 0.994140625] [gan loss: 0.211008, acc: 1.000000]\n",
            "561: [discriminator loss: 0.029442073777318, acc: 0.9951171875] [gan loss: 0.325825, acc: 1.000000]\n",
            "562: [discriminator loss: 0.02673197165131569, acc: 0.998046875] [gan loss: 0.513431, acc: 0.976562]\n",
            "563: [discriminator loss: 0.031657151877880096, acc: 0.9970703125] [gan loss: 0.683244, acc: 0.640625]\n",
            "564: [discriminator loss: 0.023566151037812233, acc: 0.9990234375] [gan loss: 0.896705, acc: 0.000000]\n",
            "565: [discriminator loss: 0.028604231774806976, acc: 0.99609375] [gan loss: 0.452645, acc: 0.990234]\n",
            "566: [discriminator loss: 0.03259911760687828, acc: 0.9931640625] [gan loss: 0.210400, acc: 1.000000]\n",
            "567: [discriminator loss: 0.03031044825911522, acc: 0.9951171875] [gan loss: 0.444246, acc: 1.000000]\n",
            "568: [discriminator loss: 0.022688664495944977, acc: 0.998046875] [gan loss: 0.645356, acc: 0.910156]\n",
            "569: [discriminator loss: 0.02143452689051628, acc: 0.998046875] [gan loss: 0.837519, acc: 0.000000]\n",
            "570: [discriminator loss: 0.024250924587249756, acc: 0.9970703125] [gan loss: 0.863474, acc: 0.000000]\n",
            "571: [discriminator loss: 0.01926371268928051, acc: 0.998046875] [gan loss: 1.228202, acc: 0.000000]\n",
            "572: [discriminator loss: 0.030344223603606224, acc: 0.9951171875] [gan loss: 0.644588, acc: 0.761719]\n",
            "573: [discriminator loss: 0.02141139656305313, acc: 0.998046875] [gan loss: 1.732052, acc: 0.000000]\n",
            "574: [discriminator loss: 0.03336577117443085, acc: 0.9931640625] [gan loss: 0.585456, acc: 0.949219]\n",
            "575: [discriminator loss: 0.03490158170461655, acc: 0.994140625] [gan loss: 2.132595, acc: 0.000000]\n",
            "576: [discriminator loss: 0.01589621603488922, acc: 0.9990234375] [gan loss: 2.306114, acc: 0.000000]\n",
            "577: [discriminator loss: 0.023177284747362137, acc: 0.998046875] [gan loss: 2.529351, acc: 0.000000]\n",
            "578: [discriminator loss: 0.031093889847397804, acc: 0.99609375] [gan loss: 3.428855, acc: 0.000000]\n",
            "579: [discriminator loss: 0.07875656336545944, acc: 0.984375] [gan loss: 1.098208, acc: 0.000000]\n",
            "580: [discriminator loss: 0.19959084689617157, acc: 0.9794921875] [gan loss: 9.273762, acc: 0.000000]\n",
            "581: [discriminator loss: 0.8085134029388428, acc: 0.7451171875] [gan loss: 0.015578, acc: 1.000000]\n",
            "582: [discriminator loss: 0.829764723777771, acc: 0.4990234375] [gan loss: 4.182115, acc: 0.000000]\n",
            "583: [discriminator loss: 0.23164911568164825, acc: 0.919921875] [gan loss: 0.389033, acc: 0.855469]\n",
            "584: [discriminator loss: 0.1582665741443634, acc: 0.9833984375] [gan loss: 0.702513, acc: 0.496094]\n",
            "585: [discriminator loss: 0.1730469912290573, acc: 0.962890625] [gan loss: 0.249710, acc: 1.000000]\n",
            "586: [discriminator loss: 0.14640836417675018, acc: 0.9794921875] [gan loss: 0.330915, acc: 1.000000]\n",
            "587: [discriminator loss: 0.14680400490760803, acc: 0.9697265625] [gan loss: 0.200076, acc: 1.000000]\n",
            "588: [discriminator loss: 0.13777385652065277, acc: 0.97265625] [gan loss: 0.189952, acc: 1.000000]\n",
            "589: [discriminator loss: 0.12462766468524933, acc: 0.978515625] [gan loss: 0.156285, acc: 1.000000]\n",
            "590: [discriminator loss: 0.13992148637771606, acc: 0.970703125] [gan loss: 0.132611, acc: 1.000000]\n",
            "591: [discriminator loss: 0.12064332515001297, acc: 0.98046875] [gan loss: 0.157547, acc: 1.000000]\n",
            "592: [discriminator loss: 0.1434556543827057, acc: 0.9697265625] [gan loss: 0.101782, acc: 1.000000]\n",
            "593: [discriminator loss: 0.1210944652557373, acc: 0.9833984375] [gan loss: 0.195111, acc: 1.000000]\n",
            "594: [discriminator loss: 0.12956656515598297, acc: 0.9755859375] [gan loss: 0.127237, acc: 1.000000]\n",
            "595: [discriminator loss: 0.11974635720252991, acc: 0.978515625] [gan loss: 0.232791, acc: 1.000000]\n",
            "596: [discriminator loss: 0.13542848825454712, acc: 0.9677734375] [gan loss: 0.226671, acc: 0.996094]\n",
            "597: [discriminator loss: 0.12966322898864746, acc: 0.9814453125] [gan loss: 0.430479, acc: 0.951172]\n",
            "598: [discriminator loss: 0.17349815368652344, acc: 0.97265625] [gan loss: 0.490715, acc: 0.947266]\n",
            "599: [discriminator loss: 0.15728771686553955, acc: 0.9736328125] [gan loss: 0.515246, acc: 0.943359]\n",
            "600: [discriminator loss: 0.13850140571594238, acc: 0.97265625] [gan loss: 0.723018, acc: 0.478516]\n",
            "601: [discriminator loss: 0.11706185340881348, acc: 0.9775390625] [gan loss: 0.448102, acc: 0.947266]\n",
            "602: [discriminator loss: 0.11202403157949448, acc: 0.9765625] [gan loss: 0.387296, acc: 0.960938]\n",
            "603: [discriminator loss: 0.09179078042507172, acc: 0.98828125] [gan loss: 0.661360, acc: 0.839844]\n",
            "604: [discriminator loss: 0.08526650816202164, acc: 0.984375] [gan loss: 0.375725, acc: 0.972656]\n",
            "605: [discriminator loss: 0.08430904895067215, acc: 0.984375] [gan loss: 0.403554, acc: 0.972656]\n",
            "606: [discriminator loss: 0.06540904194116592, acc: 0.9921875] [gan loss: 0.605342, acc: 0.904297]\n",
            "607: [discriminator loss: 0.06133139133453369, acc: 0.9912109375] [gan loss: 0.478441, acc: 0.958984]\n",
            "608: [discriminator loss: 0.06096884608268738, acc: 0.9921875] [gan loss: 0.464990, acc: 0.960938]\n",
            "609: [discriminator loss: 0.04633117467164993, acc: 0.99609375] [gan loss: 0.642519, acc: 0.902344]\n",
            "610: [discriminator loss: 0.04700380563735962, acc: 0.994140625] [gan loss: 0.520658, acc: 0.939453]\n",
            "611: [discriminator loss: 0.05055028945207596, acc: 0.990234375] [gan loss: 0.497721, acc: 0.964844]\n",
            "612: [discriminator loss: 0.058535367250442505, acc: 0.990234375] [gan loss: 0.380137, acc: 0.986328]\n",
            "613: [discriminator loss: 0.048578180372714996, acc: 0.9951171875] [gan loss: 1.030975, acc: 0.000000]\n",
            "614: [discriminator loss: 0.05829177796840668, acc: 0.9892578125] [gan loss: 0.372533, acc: 0.982422]\n",
            "615: [discriminator loss: 0.043826524168252945, acc: 0.998046875] [gan loss: 1.576368, acc: 0.000000]\n",
            "616: [discriminator loss: 0.050661515444517136, acc: 0.9912109375] [gan loss: 0.397781, acc: 0.974609]\n",
            "617: [discriminator loss: 0.0456697978079319, acc: 0.9951171875] [gan loss: 1.829595, acc: 0.000000]\n",
            "618: [discriminator loss: 0.05773148685693741, acc: 0.9853515625] [gan loss: 0.195374, acc: 1.000000]\n",
            "619: [discriminator loss: 0.06055781617760658, acc: 0.998046875] [gan loss: 4.356053, acc: 0.000000]\n",
            "620: [discriminator loss: 0.10713440179824829, acc: 0.9560546875] [gan loss: 0.010751, acc: 1.000000]\n",
            "621: [discriminator loss: 0.4086415469646454, acc: 0.5791015625] [gan loss: 7.338501, acc: 0.000000]\n",
            "622: [discriminator loss: 0.28183549642562866, acc: 0.904296875] [gan loss: 0.499904, acc: 0.992188]\n",
            "623: [discriminator loss: 0.0857899934053421, acc: 0.99609375] [gan loss: 1.160019, acc: 0.000000]\n",
            "624: [discriminator loss: 0.09164085239171982, acc: 0.9912109375] [gan loss: 1.054311, acc: 0.000000]\n",
            "625: [discriminator loss: 0.12447106838226318, acc: 0.9873046875] [gan loss: 1.137325, acc: 0.000000]\n",
            "626: [discriminator loss: 0.18464688956737518, acc: 0.96484375] [gan loss: 0.704791, acc: 0.484375]\n",
            "627: [discriminator loss: 0.16998688876628876, acc: 0.9794921875] [gan loss: 1.071874, acc: 0.000000]\n",
            "628: [discriminator loss: 0.23580707609653473, acc: 0.9501953125] [gan loss: 0.258301, acc: 1.000000]\n",
            "629: [discriminator loss: 0.18310576677322388, acc: 0.9814453125] [gan loss: 1.933265, acc: 0.000000]\n",
            "630: [discriminator loss: 0.2640894055366516, acc: 0.9169921875] [gan loss: 0.021708, acc: 1.000000]\n",
            "631: [discriminator loss: 0.447673499584198, acc: 0.5556640625] [gan loss: 4.825272, acc: 0.000000]\n",
            "632: [discriminator loss: 0.6373158097267151, acc: 0.7509765625] [gan loss: 0.010123, acc: 1.000000]\n",
            "633: [discriminator loss: 0.616047203540802, acc: 0.494140625] [gan loss: 2.879088, acc: 0.000000]\n",
            "634: [discriminator loss: 0.4531609117984772, acc: 0.8251953125] [gan loss: 0.033208, acc: 1.000000]\n",
            "635: [discriminator loss: 0.33067598938941956, acc: 0.8994140625] [gan loss: 1.194449, acc: 0.000000]\n",
            "636: [discriminator loss: 0.27498897910118103, acc: 0.912109375] [gan loss: 0.070943, acc: 1.000000]\n",
            "637: [discriminator loss: 0.24014857411384583, acc: 0.9765625] [gan loss: 0.512719, acc: 1.000000]\n",
            "638: [discriminator loss: 0.22947850823402405, acc: 0.94140625] [gan loss: 0.090893, acc: 1.000000]\n",
            "639: [discriminator loss: 0.231312096118927, acc: 0.962890625] [gan loss: 0.363007, acc: 1.000000]\n",
            "640: [discriminator loss: 0.18922261893749237, acc: 0.9560546875] [gan loss: 0.144081, acc: 1.000000]\n",
            "641: [discriminator loss: 0.1982283592224121, acc: 0.9716796875] [gan loss: 0.351739, acc: 1.000000]\n",
            "642: [discriminator loss: 0.19793090224266052, acc: 0.9501953125] [gan loss: 0.142530, acc: 1.000000]\n",
            "643: [discriminator loss: 0.18024152517318726, acc: 0.9736328125] [gan loss: 0.476132, acc: 1.000000]\n",
            "644: [discriminator loss: 0.22010184824466705, acc: 0.9423828125] [gan loss: 0.115741, acc: 1.000000]\n",
            "645: [discriminator loss: 0.20195908844470978, acc: 0.9775390625] [gan loss: 0.707485, acc: 0.361328]\n",
            "646: [discriminator loss: 0.1986202597618103, acc: 0.9541015625] [gan loss: 0.158100, acc: 1.000000]\n",
            "647: [discriminator loss: 0.18886137008666992, acc: 0.9716796875] [gan loss: 0.565539, acc: 0.980469]\n",
            "648: [discriminator loss: 0.19701215624809265, acc: 0.9580078125] [gan loss: 0.170995, acc: 1.000000]\n",
            "649: [discriminator loss: 0.17935390770435333, acc: 0.9697265625] [gan loss: 0.643183, acc: 0.820312]\n",
            "650: [discriminator loss: 0.189308300614357, acc: 0.9560546875] [gan loss: 0.203315, acc: 1.000000]\n",
            "651: [discriminator loss: 0.18073472380638123, acc: 0.9755859375] [gan loss: 0.746734, acc: 0.222656]\n",
            "652: [discriminator loss: 0.20045946538448334, acc: 0.955078125] [gan loss: 0.215066, acc: 1.000000]\n",
            "653: [discriminator loss: 0.201206773519516, acc: 0.9697265625] [gan loss: 0.957834, acc: 0.000000]\n",
            "654: [discriminator loss: 0.23907123506069183, acc: 0.939453125] [gan loss: 0.144192, acc: 1.000000]\n",
            "655: [discriminator loss: 0.2121504545211792, acc: 0.97265625] [gan loss: 2.131987, acc: 0.000000]\n",
            "656: [discriminator loss: 0.2258397340774536, acc: 0.943359375] [gan loss: 0.066050, acc: 1.000000]\n",
            "657: [discriminator loss: 0.30829328298568726, acc: 0.984375] [gan loss: 5.873085, acc: 0.000000]\n",
            "658: [discriminator loss: 0.5777479410171509, acc: 0.7939453125] [gan loss: 0.015248, acc: 1.000000]\n",
            "659: [discriminator loss: 0.7939536571502686, acc: 0.4892578125] [gan loss: 4.925948, acc: 0.000000]\n",
            "660: [discriminator loss: 0.4842151999473572, acc: 0.826171875] [gan loss: 0.165111, acc: 1.000000]\n",
            "661: [discriminator loss: 0.29084283113479614, acc: 0.9765625] [gan loss: 1.617000, acc: 0.000000]\n",
            "662: [discriminator loss: 0.23130527138710022, acc: 0.9423828125] [gan loss: 0.460616, acc: 0.933594]\n",
            "663: [discriminator loss: 0.2790268659591675, acc: 0.9560546875] [gan loss: 0.741826, acc: 0.328125]\n",
            "664: [discriminator loss: 0.2877154052257538, acc: 0.9443359375] [gan loss: 0.777526, acc: 0.251953]\n",
            "665: [discriminator loss: 0.2878807783126831, acc: 0.9560546875] [gan loss: 1.193732, acc: 0.000000]\n",
            "666: [discriminator loss: 0.34998151659965515, acc: 0.94140625] [gan loss: 0.988968, acc: 0.125000]\n",
            "667: [discriminator loss: 0.45707085728645325, acc: 0.90625] [gan loss: 0.448409, acc: 0.964844]\n",
            "668: [discriminator loss: 0.4285777807235718, acc: 0.921875] [gan loss: 0.773519, acc: 0.273438]\n",
            "669: [discriminator loss: 0.4594350755214691, acc: 0.9150390625] [gan loss: 0.615930, acc: 0.746094]\n",
            "670: [discriminator loss: 0.3952600061893463, acc: 0.923828125] [gan loss: 0.889068, acc: 0.107422]\n",
            "671: [discriminator loss: 0.4710955321788788, acc: 0.8984375] [gan loss: 0.459996, acc: 0.955078]\n",
            "672: [discriminator loss: 0.4003160297870636, acc: 0.9375] [gan loss: 1.307985, acc: 0.000000]\n",
            "673: [discriminator loss: 0.40906059741973877, acc: 0.916015625] [gan loss: 0.667839, acc: 0.632812]\n",
            "674: [discriminator loss: 0.36880427598953247, acc: 0.9287109375] [gan loss: 0.793444, acc: 0.195312]\n",
            "675: [discriminator loss: 0.4248785078525543, acc: 0.92578125] [gan loss: 0.773182, acc: 0.314453]\n",
            "676: [discriminator loss: 0.4236335754394531, acc: 0.9140625] [gan loss: 0.447714, acc: 0.988281]\n",
            "677: [discriminator loss: 0.3114086389541626, acc: 0.9609375] [gan loss: 1.986604, acc: 0.000000]\n",
            "678: [discriminator loss: 0.4611082971096039, acc: 0.87890625] [gan loss: 0.061437, acc: 1.000000]\n",
            "679: [discriminator loss: 0.6072479486465454, acc: 0.484375] [gan loss: 4.782016, acc: 0.000000]\n",
            "680: [discriminator loss: 0.9799556732177734, acc: 0.705078125] [gan loss: 0.032564, acc: 1.000000]\n",
            "681: [discriminator loss: 0.8490220308303833, acc: 0.486328125] [gan loss: 2.437560, acc: 0.000000]\n",
            "682: [discriminator loss: 0.4314025938510895, acc: 0.876953125] [gan loss: 0.212141, acc: 1.000000]\n",
            "683: [discriminator loss: 0.34731435775756836, acc: 0.9599609375] [gan loss: 0.892487, acc: 0.144531]\n",
            "684: [discriminator loss: 0.3302360475063324, acc: 0.92578125] [gan loss: 0.307363, acc: 1.000000]\n",
            "685: [discriminator loss: 0.2831389605998993, acc: 0.9638671875] [gan loss: 0.745126, acc: 0.283203]\n",
            "686: [discriminator loss: 0.26069211959838867, acc: 0.9423828125] [gan loss: 0.451251, acc: 0.988281]\n",
            "687: [discriminator loss: 0.30097800493240356, acc: 0.943359375] [gan loss: 0.473809, acc: 0.978516]\n",
            "688: [discriminator loss: 0.2971023917198181, acc: 0.947265625] [gan loss: 0.537223, acc: 0.927734]\n",
            "689: [discriminator loss: 0.24716611206531525, acc: 0.953125] [gan loss: 0.622837, acc: 0.808594]\n",
            "690: [discriminator loss: 0.2923907935619354, acc: 0.9404296875] [gan loss: 0.455645, acc: 0.994141]\n",
            "691: [discriminator loss: 0.2683461904525757, acc: 0.9453125] [gan loss: 0.603592, acc: 0.853516]\n",
            "692: [discriminator loss: 0.2755952775478363, acc: 0.9462890625] [gan loss: 0.496860, acc: 0.966797]\n",
            "693: [discriminator loss: 0.2554881274700165, acc: 0.94921875] [gan loss: 0.691593, acc: 0.552734]\n",
            "694: [discriminator loss: 0.2896192669868469, acc: 0.943359375] [gan loss: 0.449636, acc: 0.994141]\n",
            "695: [discriminator loss: 0.24429133534431458, acc: 0.9658203125] [gan loss: 0.982648, acc: 0.015625]\n",
            "696: [discriminator loss: 0.24467399716377258, acc: 0.951171875] [gan loss: 0.407476, acc: 0.996094]\n",
            "697: [discriminator loss: 0.2541651725769043, acc: 0.9541015625] [gan loss: 0.879007, acc: 0.099609]\n",
            "698: [discriminator loss: 0.28152531385421753, acc: 0.9345703125] [gan loss: 0.320381, acc: 1.000000]\n",
            "699: [discriminator loss: 0.2679741680622101, acc: 0.9609375] [gan loss: 1.324546, acc: 0.000000]\n",
            "700: [discriminator loss: 0.3038376271724701, acc: 0.9326171875] [gan loss: 0.179071, acc: 1.000000]\n",
            "701: [discriminator loss: 0.3158230781555176, acc: 0.9658203125] [gan loss: 2.471757, acc: 0.000000]\n",
            "702: [discriminator loss: 0.3501572608947754, acc: 0.8916015625] [gan loss: 0.028796, acc: 1.000000]\n",
            "703: [discriminator loss: 0.6777579188346863, acc: 0.4853515625] [gan loss: 4.391447, acc: 0.000000]\n",
            "704: [discriminator loss: 0.6239328980445862, acc: 0.814453125] [gan loss: 0.088532, acc: 1.000000]\n",
            "705: [discriminator loss: 0.42571866512298584, acc: 0.6669921875] [gan loss: 1.985333, acc: 0.000000]\n",
            "706: [discriminator loss: 0.3432438373565674, acc: 0.912109375] [gan loss: 0.164992, acc: 1.000000]\n",
            "707: [discriminator loss: 0.2842860519886017, acc: 0.9775390625] [gan loss: 1.195053, acc: 0.000000]\n",
            "708: [discriminator loss: 0.2368791550397873, acc: 0.939453125] [gan loss: 0.304444, acc: 1.000000]\n",
            "709: [discriminator loss: 0.2366761565208435, acc: 0.9697265625] [gan loss: 0.902251, acc: 0.046875]\n",
            "710: [discriminator loss: 0.22695539891719818, acc: 0.9541015625] [gan loss: 0.423489, acc: 0.998047]\n",
            "711: [discriminator loss: 0.24013814330101013, acc: 0.958984375] [gan loss: 0.637054, acc: 0.730469]\n",
            "712: [discriminator loss: 0.2494678646326065, acc: 0.9560546875] [gan loss: 0.500767, acc: 0.974609]\n",
            "713: [discriminator loss: 0.23423251509666443, acc: 0.95703125] [gan loss: 0.694505, acc: 0.560547]\n",
            "714: [discriminator loss: 0.22190916538238525, acc: 0.95703125] [gan loss: 0.698258, acc: 0.566406]\n",
            "715: [discriminator loss: 0.29752010107040405, acc: 0.943359375] [gan loss: 0.382692, acc: 1.000000]\n",
            "716: [discriminator loss: 0.2643097937107086, acc: 0.958984375] [gan loss: 1.221385, acc: 0.000000]\n",
            "717: [discriminator loss: 0.2869621515274048, acc: 0.9443359375] [gan loss: 0.443660, acc: 0.937500]\n",
            "718: [discriminator loss: 0.2976902723312378, acc: 0.9365234375] [gan loss: 0.507869, acc: 0.968750]\n",
            "719: [discriminator loss: 0.23855814337730408, acc: 0.9658203125] [gan loss: 1.411734, acc: 0.000000]\n",
            "720: [discriminator loss: 0.3160167634487152, acc: 0.9130859375] [gan loss: 0.150271, acc: 1.000000]\n",
            "721: [discriminator loss: 0.37431129813194275, acc: 0.9384765625] [gan loss: 3.438661, acc: 0.000000]\n",
            "722: [discriminator loss: 0.5170422792434692, acc: 0.853515625] [gan loss: 0.015028, acc: 1.000000]\n",
            "723: [discriminator loss: 1.0274240970611572, acc: 0.48828125] [gan loss: 3.937196, acc: 0.000000]\n",
            "724: [discriminator loss: 0.6032915115356445, acc: 0.814453125] [gan loss: 0.111457, acc: 1.000000]\n",
            "725: [discriminator loss: 0.45207250118255615, acc: 0.62109375] [gan loss: 1.737831, acc: 0.000000]\n",
            "726: [discriminator loss: 0.33548450469970703, acc: 0.9072265625] [gan loss: 0.264240, acc: 1.000000]\n",
            "727: [discriminator loss: 0.32648542523384094, acc: 0.9580078125] [gan loss: 1.115875, acc: 0.000000]\n",
            "728: [discriminator loss: 0.3145994246006012, acc: 0.93359375] [gan loss: 0.382370, acc: 1.000000]\n",
            "729: [discriminator loss: 0.3158148229122162, acc: 0.9462890625] [gan loss: 0.759039, acc: 0.296875]\n",
            "730: [discriminator loss: 0.3256217837333679, acc: 0.9189453125] [gan loss: 0.362401, acc: 1.000000]\n",
            "731: [discriminator loss: 0.3577018678188324, acc: 0.9306640625] [gan loss: 0.718135, acc: 0.425781]\n",
            "732: [discriminator loss: 0.34081780910491943, acc: 0.9267578125] [gan loss: 0.389771, acc: 1.000000]\n",
            "733: [discriminator loss: 0.3406861126422882, acc: 0.943359375] [gan loss: 0.928440, acc: 0.000000]\n",
            "734: [discriminator loss: 0.37476837635040283, acc: 0.91796875] [gan loss: 0.281270, acc: 1.000000]\n",
            "735: [discriminator loss: 0.37791508436203003, acc: 0.94140625] [gan loss: 1.507709, acc: 0.000000]\n",
            "736: [discriminator loss: 0.41515183448791504, acc: 0.896484375] [gan loss: 0.109331, acc: 1.000000]\n",
            "737: [discriminator loss: 0.49359801411628723, acc: 0.5029296875] [gan loss: 3.662707, acc: 0.000000]\n",
            "738: [discriminator loss: 0.8309351205825806, acc: 0.74609375] [gan loss: 0.012723, acc: 1.000000]\n",
            "739: [discriminator loss: 1.218919038772583, acc: 0.490234375] [gan loss: 2.813130, acc: 0.000000]\n",
            "740: [discriminator loss: 0.6896303296089172, acc: 0.791015625] [gan loss: 0.081793, acc: 1.000000]\n",
            "741: [discriminator loss: 0.7356441020965576, acc: 0.4501953125] [gan loss: 1.986390, acc: 0.000000]\n",
            "742: [discriminator loss: 0.5469081997871399, acc: 0.853515625] [gan loss: 0.213637, acc: 1.000000]\n",
            "743: [discriminator loss: 0.6613418459892273, acc: 0.4228515625] [gan loss: 1.463776, acc: 0.000000]\n",
            "744: [discriminator loss: 0.639585554599762, acc: 0.8642578125] [gan loss: 0.341207, acc: 1.000000]\n",
            "745: [discriminator loss: 0.6705476641654968, acc: 0.4208984375] [gan loss: 1.382897, acc: 0.000000]\n",
            "746: [discriminator loss: 0.732376217842102, acc: 0.8310546875] [gan loss: 0.303890, acc: 1.000000]\n",
            "747: [discriminator loss: 0.7259517908096313, acc: 0.4033203125] [gan loss: 1.784525, acc: 0.000000]\n",
            "748: [discriminator loss: 0.7900250554084778, acc: 0.7861328125] [gan loss: 0.171171, acc: 1.000000]\n",
            "749: [discriminator loss: 0.8507200479507446, acc: 0.43359375] [gan loss: 2.521855, acc: 0.000000]\n",
            "750: [discriminator loss: 0.8069228529930115, acc: 0.7509765625] [gan loss: 0.113643, acc: 1.000000]\n",
            "751: [discriminator loss: 1.0010484457015991, acc: 0.43359375] [gan loss: 2.506699, acc: 0.000000]\n",
            "752: [discriminator loss: 0.8583193421363831, acc: 0.7265625] [gan loss: 0.120344, acc: 1.000000]\n",
            "753: [discriminator loss: 0.9386357665061951, acc: 0.458984375] [gan loss: 2.351415, acc: 0.000000]\n",
            "754: [discriminator loss: 0.7407568097114563, acc: 0.771484375] [gan loss: 0.219338, acc: 1.000000]\n",
            "755: [discriminator loss: 0.7965720891952515, acc: 0.44140625] [gan loss: 1.993685, acc: 0.000000]\n",
            "756: [discriminator loss: 0.6312493681907654, acc: 0.8095703125] [gan loss: 0.352269, acc: 1.000000]\n",
            "757: [discriminator loss: 0.699898362159729, acc: 0.4384765625] [gan loss: 1.825392, acc: 0.000000]\n",
            "758: [discriminator loss: 0.615367591381073, acc: 0.8232421875] [gan loss: 0.386309, acc: 1.000000]\n",
            "759: [discriminator loss: 0.6536992788314819, acc: 0.4501953125] [gan loss: 2.132473, acc: 0.000000]\n",
            "760: [discriminator loss: 0.636231541633606, acc: 0.80859375] [gan loss: 0.318953, acc: 1.000000]\n",
            "761: [discriminator loss: 0.6879533529281616, acc: 0.44140625] [gan loss: 2.204137, acc: 0.000000]\n",
            "762: [discriminator loss: 0.6061695218086243, acc: 0.8076171875] [gan loss: 0.304041, acc: 1.000000]\n",
            "763: [discriminator loss: 0.7087117433547974, acc: 0.4482421875] [gan loss: 2.265370, acc: 0.000000]\n",
            "764: [discriminator loss: 0.5358240604400635, acc: 0.830078125] [gan loss: 0.367787, acc: 1.000000]\n",
            "765: [discriminator loss: 0.6223587393760681, acc: 0.4560546875] [gan loss: 2.318057, acc: 0.000000]\n",
            "766: [discriminator loss: 0.5534151196479797, acc: 0.8271484375] [gan loss: 0.350308, acc: 1.000000]\n",
            "767: [discriminator loss: 0.6324697732925415, acc: 0.46484375] [gan loss: 2.391540, acc: 0.000000]\n",
            "768: [discriminator loss: 0.5017384886741638, acc: 0.8388671875] [gan loss: 0.418447, acc: 1.000000]\n",
            "769: [discriminator loss: 0.5565521121025085, acc: 0.4853515625] [gan loss: 2.497559, acc: 0.000000]\n",
            "770: [discriminator loss: 0.5109269618988037, acc: 0.8359375] [gan loss: 0.411586, acc: 1.000000]\n",
            "771: [discriminator loss: 0.5775264501571655, acc: 0.484375] [gan loss: 2.355690, acc: 0.000000]\n",
            "772: [discriminator loss: 0.47644323110580444, acc: 0.841796875] [gan loss: 0.432379, acc: 1.000000]\n",
            "773: [discriminator loss: 0.6011876463890076, acc: 0.466796875] [gan loss: 2.291459, acc: 0.000000]\n",
            "774: [discriminator loss: 0.4719160199165344, acc: 0.8564453125] [gan loss: 0.519581, acc: 0.933594]\n",
            "775: [discriminator loss: 0.5609568357467651, acc: 0.4892578125] [gan loss: 2.379117, acc: 0.000000]\n",
            "776: [discriminator loss: 0.48679319024086, acc: 0.83203125] [gan loss: 0.471617, acc: 1.000000]\n",
            "777: [discriminator loss: 0.599612832069397, acc: 0.490234375] [gan loss: 2.693797, acc: 0.000000]\n",
            "778: [discriminator loss: 0.6047804951667786, acc: 0.783203125] [gan loss: 0.322018, acc: 1.000000]\n",
            "779: [discriminator loss: 0.7219036817550659, acc: 0.4814453125] [gan loss: 2.636332, acc: 0.000000]\n",
            "780: [discriminator loss: 0.6023533940315247, acc: 0.779296875] [gan loss: 0.356502, acc: 1.000000]\n",
            "781: [discriminator loss: 0.6893151998519897, acc: 0.4873046875] [gan loss: 2.496690, acc: 0.000000]\n",
            "782: [discriminator loss: 0.5471283197402954, acc: 0.7880859375] [gan loss: 0.414958, acc: 1.000000]\n",
            "783: [discriminator loss: 0.6518191695213318, acc: 0.47265625] [gan loss: 2.334240, acc: 0.000000]\n",
            "784: [discriminator loss: 0.4933408498764038, acc: 0.8115234375] [gan loss: 0.522954, acc: 0.992188]\n",
            "785: [discriminator loss: 0.5864698886871338, acc: 0.4833984375] [gan loss: 2.416787, acc: 0.000000]\n",
            "786: [discriminator loss: 0.486521452665329, acc: 0.8193359375] [gan loss: 0.515540, acc: 0.988281]\n",
            "787: [discriminator loss: 0.595567524433136, acc: 0.486328125] [gan loss: 2.651530, acc: 0.000000]\n",
            "788: [discriminator loss: 0.4721633195877075, acc: 0.818359375] [gan loss: 0.532551, acc: 0.974609]\n",
            "789: [discriminator loss: 0.5738422274589539, acc: 0.478515625] [gan loss: 2.679772, acc: 0.000000]\n",
            "790: [discriminator loss: 0.42643630504608154, acc: 0.8349609375] [gan loss: 0.617654, acc: 0.822266]\n",
            "791: [discriminator loss: 0.5233973264694214, acc: 0.5] [gan loss: 2.736969, acc: 0.000000]\n",
            "792: [discriminator loss: 0.40484189987182617, acc: 0.8583984375] [gan loss: 0.680792, acc: 0.619141]\n",
            "793: [discriminator loss: 0.508796751499176, acc: 0.5205078125] [gan loss: 2.664026, acc: 0.000000]\n",
            "794: [discriminator loss: 0.3549043536186218, acc: 0.8818359375] [gan loss: 0.807497, acc: 0.152344]\n",
            "795: [discriminator loss: 0.4379623234272003, acc: 0.615234375] [gan loss: 2.755281, acc: 0.000000]\n",
            "796: [discriminator loss: 0.32455387711524963, acc: 0.900390625] [gan loss: 0.836019, acc: 0.115234]\n",
            "797: [discriminator loss: 0.41643014550209045, acc: 0.65625] [gan loss: 2.737619, acc: 0.000000]\n",
            "798: [discriminator loss: 0.30324530601501465, acc: 0.904296875] [gan loss: 0.935477, acc: 0.007812]\n",
            "799: [discriminator loss: 0.3809036314487457, acc: 0.7744140625] [gan loss: 2.749954, acc: 0.000000]\n",
            "800: [discriminator loss: 0.27589040994644165, acc: 0.935546875] [gan loss: 1.015670, acc: 0.000000]\n",
            "801: [discriminator loss: 0.3616454601287842, acc: 0.845703125] [gan loss: 2.687479, acc: 0.000000]\n",
            "802: [discriminator loss: 0.29818442463874817, acc: 0.923828125] [gan loss: 0.890873, acc: 0.021484]\n",
            "803: [discriminator loss: 0.4030884802341461, acc: 0.6806640625] [gan loss: 2.916032, acc: 0.000000]\n",
            "804: [discriminator loss: 0.31524962186813354, acc: 0.8984375] [gan loss: 0.735666, acc: 0.486328]\n",
            "805: [discriminator loss: 0.45575955510139465, acc: 0.5908203125] [gan loss: 2.824536, acc: 0.000000]\n",
            "806: [discriminator loss: 0.34783512353897095, acc: 0.873046875] [gan loss: 0.618597, acc: 0.726562]\n",
            "807: [discriminator loss: 0.47995585203170776, acc: 0.5751953125] [gan loss: 2.702243, acc: 0.000000]\n",
            "808: [discriminator loss: 0.3470732569694519, acc: 0.875] [gan loss: 0.590936, acc: 0.806641]\n",
            "809: [discriminator loss: 0.4641428291797638, acc: 0.6015625] [gan loss: 2.493005, acc: 0.000000]\n",
            "810: [discriminator loss: 0.3536388576030731, acc: 0.8798828125] [gan loss: 0.585323, acc: 0.826172]\n",
            "811: [discriminator loss: 0.46650323271751404, acc: 0.595703125] [gan loss: 2.333342, acc: 0.000000]\n",
            "812: [discriminator loss: 0.3657591640949249, acc: 0.8857421875] [gan loss: 0.551294, acc: 0.849609]\n",
            "813: [discriminator loss: 0.4702031910419464, acc: 0.59375] [gan loss: 2.274857, acc: 0.000000]\n",
            "814: [discriminator loss: 0.3621283769607544, acc: 0.8779296875] [gan loss: 0.530523, acc: 0.892578]\n",
            "815: [discriminator loss: 0.453144907951355, acc: 0.6220703125] [gan loss: 2.235095, acc: 0.000000]\n",
            "816: [discriminator loss: 0.36589574813842773, acc: 0.873046875] [gan loss: 0.525732, acc: 0.921875]\n",
            "817: [discriminator loss: 0.4600181579589844, acc: 0.60546875] [gan loss: 2.284290, acc: 0.000000]\n",
            "818: [discriminator loss: 0.34915488958358765, acc: 0.8857421875] [gan loss: 0.565718, acc: 0.865234]\n",
            "819: [discriminator loss: 0.44235971570014954, acc: 0.634765625] [gan loss: 2.253770, acc: 0.000000]\n",
            "820: [discriminator loss: 0.3421058654785156, acc: 0.876953125] [gan loss: 0.639099, acc: 0.742188]\n",
            "821: [discriminator loss: 0.4083806872367859, acc: 0.7080078125] [gan loss: 2.232783, acc: 0.000000]\n",
            "822: [discriminator loss: 0.32354509830474854, acc: 0.91015625] [gan loss: 0.634878, acc: 0.714844]\n",
            "823: [discriminator loss: 0.41471296548843384, acc: 0.7216796875] [gan loss: 2.161489, acc: 0.000000]\n",
            "824: [discriminator loss: 0.3304083049297333, acc: 0.9091796875] [gan loss: 0.643906, acc: 0.664062]\n",
            "825: [discriminator loss: 0.43271753191947937, acc: 0.6748046875] [gan loss: 2.198533, acc: 0.000000]\n",
            "826: [discriminator loss: 0.36577028036117554, acc: 0.8818359375] [gan loss: 0.534835, acc: 0.902344]\n",
            "827: [discriminator loss: 0.48012176156044006, acc: 0.58203125] [gan loss: 2.354149, acc: 0.000000]\n",
            "828: [discriminator loss: 0.399883896112442, acc: 0.8203125] [gan loss: 0.452940, acc: 0.998047]\n",
            "829: [discriminator loss: 0.5291349291801453, acc: 0.5205078125] [gan loss: 2.409339, acc: 0.000000]\n",
            "830: [discriminator loss: 0.43343132734298706, acc: 0.798828125] [gan loss: 0.427187, acc: 0.998047]\n",
            "831: [discriminator loss: 0.5655557513237, acc: 0.49609375] [gan loss: 2.432338, acc: 0.000000]\n",
            "832: [discriminator loss: 0.4436713755130768, acc: 0.8037109375] [gan loss: 0.488071, acc: 0.925781]\n",
            "833: [discriminator loss: 0.5726527571678162, acc: 0.5] [gan loss: 2.372279, acc: 0.000000]\n",
            "834: [discriminator loss: 0.45791277289390564, acc: 0.79296875] [gan loss: 0.496785, acc: 0.912109]\n",
            "835: [discriminator loss: 0.5809410810470581, acc: 0.4921875] [gan loss: 2.394879, acc: 0.000000]\n",
            "836: [discriminator loss: 0.47633445262908936, acc: 0.7822265625] [gan loss: 0.516456, acc: 0.894531]\n",
            "837: [discriminator loss: 0.6140955686569214, acc: 0.4912109375] [gan loss: 2.507761, acc: 0.000000]\n",
            "838: [discriminator loss: 0.5255425572395325, acc: 0.73828125] [gan loss: 0.468998, acc: 1.000000]\n",
            "839: [discriminator loss: 0.6654262542724609, acc: 0.490234375] [gan loss: 2.659013, acc: 0.000000]\n",
            "840: [discriminator loss: 0.5606085658073425, acc: 0.7021484375] [gan loss: 0.450641, acc: 1.000000]\n",
            "841: [discriminator loss: 0.6806280612945557, acc: 0.4921875] [gan loss: 2.673678, acc: 0.000000]\n",
            "842: [discriminator loss: 0.5028215050697327, acc: 0.73828125] [gan loss: 0.589658, acc: 0.761719]\n",
            "843: [discriminator loss: 0.6236005425453186, acc: 0.49609375] [gan loss: 2.630422, acc: 0.000000]\n",
            "844: [discriminator loss: 0.47153669595718384, acc: 0.7744140625] [gan loss: 0.712752, acc: 0.474609]\n",
            "845: [discriminator loss: 0.5779272317886353, acc: 0.54296875] [gan loss: 2.654119, acc: 0.000000]\n",
            "846: [discriminator loss: 0.4524046778678894, acc: 0.8095703125] [gan loss: 0.735969, acc: 0.421875]\n",
            "847: [discriminator loss: 0.5642341375350952, acc: 0.5439453125] [gan loss: 2.792307, acc: 0.000000]\n",
            "848: [discriminator loss: 0.4355393052101135, acc: 0.802734375] [gan loss: 0.759094, acc: 0.419922]\n",
            "849: [discriminator loss: 0.5400762557983398, acc: 0.556640625] [gan loss: 2.804989, acc: 0.000000]\n",
            "850: [discriminator loss: 0.4018941819667816, acc: 0.822265625] [gan loss: 0.841958, acc: 0.246094]\n",
            "851: [discriminator loss: 0.501716136932373, acc: 0.59765625] [gan loss: 2.737311, acc: 0.000000]\n",
            "852: [discriminator loss: 0.34348979592323303, acc: 0.884765625] [gan loss: 1.036623, acc: 0.019531]\n",
            "853: [discriminator loss: 0.4373132586479187, acc: 0.703125] [gan loss: 2.635658, acc: 0.000000]\n",
            "854: [discriminator loss: 0.33304086327552795, acc: 0.8916015625] [gan loss: 1.088721, acc: 0.019531]\n",
            "855: [discriminator loss: 0.4131760895252228, acc: 0.740234375] [gan loss: 2.808278, acc: 0.000000]\n",
            "856: [discriminator loss: 0.31465357542037964, acc: 0.8876953125] [gan loss: 1.057624, acc: 0.035156]\n",
            "857: [discriminator loss: 0.4059355854988098, acc: 0.7451171875] [gan loss: 2.992864, acc: 0.000000]\n",
            "858: [discriminator loss: 0.2993825078010559, acc: 0.908203125] [gan loss: 1.087569, acc: 0.029297]\n",
            "859: [discriminator loss: 0.394676148891449, acc: 0.767578125] [gan loss: 2.999635, acc: 0.000000]\n",
            "860: [discriminator loss: 0.3036987781524658, acc: 0.88671875] [gan loss: 0.992099, acc: 0.048828]\n",
            "861: [discriminator loss: 0.415485143661499, acc: 0.6962890625] [gan loss: 3.159509, acc: 0.000000]\n",
            "862: [discriminator loss: 0.3270149827003479, acc: 0.859375] [gan loss: 0.874382, acc: 0.179688]\n",
            "863: [discriminator loss: 0.4320124685764313, acc: 0.6455078125] [gan loss: 3.201353, acc: 0.000000]\n",
            "864: [discriminator loss: 0.304862916469574, acc: 0.8759765625] [gan loss: 0.964000, acc: 0.062500]\n",
            "865: [discriminator loss: 0.4086734354496002, acc: 0.697265625] [gan loss: 3.044511, acc: 0.000000]\n",
            "866: [discriminator loss: 0.310637503862381, acc: 0.884765625] [gan loss: 0.880672, acc: 0.160156]\n",
            "867: [discriminator loss: 0.4211946427822113, acc: 0.654296875] [gan loss: 2.943754, acc: 0.000000]\n",
            "868: [discriminator loss: 0.3238019049167633, acc: 0.884765625] [gan loss: 0.865513, acc: 0.203125]\n",
            "869: [discriminator loss: 0.4221994876861572, acc: 0.66015625] [gan loss: 2.782658, acc: 0.000000]\n",
            "870: [discriminator loss: 0.3353944420814514, acc: 0.875] [gan loss: 0.833297, acc: 0.238281]\n",
            "871: [discriminator loss: 0.4302297830581665, acc: 0.6494140625] [gan loss: 2.697576, acc: 0.000000]\n",
            "872: [discriminator loss: 0.35075780749320984, acc: 0.8564453125] [gan loss: 0.786432, acc: 0.289062]\n",
            "873: [discriminator loss: 0.442315936088562, acc: 0.62890625] [gan loss: 2.658120, acc: 0.000000]\n",
            "874: [discriminator loss: 0.3734126687049866, acc: 0.8310546875] [gan loss: 0.670383, acc: 0.654297]\n",
            "875: [discriminator loss: 0.4674815535545349, acc: 0.599609375] [gan loss: 2.572593, acc: 0.000000]\n",
            "876: [discriminator loss: 0.363156259059906, acc: 0.8447265625] [gan loss: 0.689824, acc: 0.593750]\n",
            "877: [discriminator loss: 0.46312838792800903, acc: 0.6064453125] [gan loss: 2.520669, acc: 0.000000]\n",
            "878: [discriminator loss: 0.3808666169643402, acc: 0.833984375] [gan loss: 0.670610, acc: 0.642578]\n",
            "879: [discriminator loss: 0.5018664598464966, acc: 0.5703125] [gan loss: 2.508422, acc: 0.000000]\n",
            "880: [discriminator loss: 0.41471460461616516, acc: 0.814453125] [gan loss: 0.581800, acc: 0.818359]\n",
            "881: [discriminator loss: 0.5465296506881714, acc: 0.5556640625] [gan loss: 2.636843, acc: 0.000000]\n",
            "882: [discriminator loss: 0.4524163007736206, acc: 0.7841796875] [gan loss: 0.549017, acc: 0.855469]\n",
            "883: [discriminator loss: 0.5921598672866821, acc: 0.5478515625] [gan loss: 2.544886, acc: 0.000000]\n",
            "884: [discriminator loss: 0.4871968924999237, acc: 0.7724609375] [gan loss: 0.550149, acc: 0.818359]\n",
            "885: [discriminator loss: 0.6518152356147766, acc: 0.5458984375] [gan loss: 2.345085, acc: 0.000000]\n",
            "886: [discriminator loss: 0.5839712619781494, acc: 0.763671875] [gan loss: 0.621320, acc: 0.714844]\n",
            "887: [discriminator loss: 0.7544960975646973, acc: 0.4853515625] [gan loss: 2.082982, acc: 0.000000]\n",
            "888: [discriminator loss: 0.6964678168296814, acc: 0.625] [gan loss: 0.812811, acc: 0.369141]\n",
            "889: [discriminator loss: 0.730311393737793, acc: 0.4990234375] [gan loss: 2.044773, acc: 0.000000]\n",
            "890: [discriminator loss: 0.6483325958251953, acc: 0.685546875] [gan loss: 0.883196, acc: 0.318359]\n",
            "891: [discriminator loss: 0.6499922871589661, acc: 0.515625] [gan loss: 2.186977, acc: 0.000000]\n",
            "892: [discriminator loss: 0.5081542730331421, acc: 0.791015625] [gan loss: 1.027285, acc: 0.068359]\n",
            "893: [discriminator loss: 0.49805551767349243, acc: 0.66015625] [gan loss: 2.272279, acc: 0.000000]\n",
            "894: [discriminator loss: 0.4136350452899933, acc: 0.8447265625] [gan loss: 1.113447, acc: 0.029297]\n",
            "895: [discriminator loss: 0.4274485111236572, acc: 0.7734375] [gan loss: 2.371598, acc: 0.000000]\n",
            "896: [discriminator loss: 0.3075743615627289, acc: 0.9130859375] [gan loss: 1.379767, acc: 0.000000]\n",
            "897: [discriminator loss: 0.32009971141815186, acc: 0.9208984375] [gan loss: 2.274621, acc: 0.000000]\n",
            "898: [discriminator loss: 0.26034459471702576, acc: 0.94921875] [gan loss: 1.514339, acc: 0.000000]\n",
            "899: [discriminator loss: 0.2705671489238739, acc: 0.95703125] [gan loss: 2.266954, acc: 0.000000]\n",
            "900: [discriminator loss: 0.21600458025932312, acc: 0.966796875] [gan loss: 1.730080, acc: 0.000000]\n",
            "901: [discriminator loss: 0.24053911864757538, acc: 0.98828125] [gan loss: 2.180513, acc: 0.000000]\n",
            "902: [discriminator loss: 0.21550729870796204, acc: 0.9736328125] [gan loss: 1.803263, acc: 0.000000]\n",
            "903: [discriminator loss: 0.23416857421398163, acc: 0.9892578125] [gan loss: 2.273598, acc: 0.000000]\n",
            "904: [discriminator loss: 0.2224002629518509, acc: 0.974609375] [gan loss: 1.828314, acc: 0.000000]\n",
            "905: [discriminator loss: 0.29993218183517456, acc: 0.986328125] [gan loss: 2.570945, acc: 0.000000]\n",
            "906: [discriminator loss: 0.34035661816596985, acc: 0.8994140625] [gan loss: 1.032513, acc: 0.058594]\n",
            "907: [discriminator loss: 0.5085899829864502, acc: 0.5908203125] [gan loss: 3.622306, acc: 0.000000]\n",
            "908: [discriminator loss: 0.5883901715278625, acc: 0.6904296875] [gan loss: 0.215094, acc: 1.000000]\n",
            "909: [discriminator loss: 0.9019726514816284, acc: 0.498046875] [gan loss: 3.180314, acc: 0.000000]\n",
            "910: [discriminator loss: 0.5146492719650269, acc: 0.75] [gan loss: 0.501969, acc: 0.863281]\n",
            "911: [discriminator loss: 0.6817018389701843, acc: 0.48828125] [gan loss: 2.709689, acc: 0.000000]\n",
            "912: [discriminator loss: 0.5460842847824097, acc: 0.7626953125] [gan loss: 0.521983, acc: 0.849609]\n",
            "913: [discriminator loss: 0.6660522222518921, acc: 0.474609375] [gan loss: 2.433234, acc: 0.000000]\n",
            "914: [discriminator loss: 0.5736021399497986, acc: 0.7626953125] [gan loss: 0.545175, acc: 0.763672]\n",
            "915: [discriminator loss: 0.7124162316322327, acc: 0.458984375] [gan loss: 2.370764, acc: 0.000000]\n",
            "916: [discriminator loss: 0.6294240951538086, acc: 0.7587890625] [gan loss: 0.553436, acc: 0.750000]\n",
            "917: [discriminator loss: 0.7530564665794373, acc: 0.4482421875] [gan loss: 2.338712, acc: 0.000000]\n",
            "918: [discriminator loss: 0.6728450655937195, acc: 0.7314453125] [gan loss: 0.545544, acc: 0.785156]\n",
            "919: [discriminator loss: 0.7890897989273071, acc: 0.4443359375] [gan loss: 2.370448, acc: 0.000000]\n",
            "920: [discriminator loss: 0.6922494173049927, acc: 0.716796875] [gan loss: 0.530428, acc: 0.851562]\n",
            "921: [discriminator loss: 0.8116577863693237, acc: 0.4482421875] [gan loss: 2.479224, acc: 0.000000]\n",
            "922: [discriminator loss: 0.6523274779319763, acc: 0.728515625] [gan loss: 0.615726, acc: 0.705078]\n",
            "923: [discriminator loss: 0.7653114199638367, acc: 0.44921875] [gan loss: 2.456195, acc: 0.000000]\n",
            "924: [discriminator loss: 0.6060841083526611, acc: 0.751953125] [gan loss: 0.668854, acc: 0.634766]\n",
            "925: [discriminator loss: 0.6915248036384583, acc: 0.46875] [gan loss: 2.581825, acc: 0.000000]\n",
            "926: [discriminator loss: 0.5792801976203918, acc: 0.751953125] [gan loss: 0.631654, acc: 0.718750]\n",
            "927: [discriminator loss: 0.6793612241744995, acc: 0.4775390625] [gan loss: 2.580870, acc: 0.000000]\n",
            "928: [discriminator loss: 0.5213176012039185, acc: 0.77734375] [gan loss: 0.734798, acc: 0.542969]\n",
            "929: [discriminator loss: 0.6187069416046143, acc: 0.4775390625] [gan loss: 2.513885, acc: 0.000000]\n",
            "930: [discriminator loss: 0.47287076711654663, acc: 0.8134765625] [gan loss: 0.874953, acc: 0.183594]\n",
            "931: [discriminator loss: 0.5502607822418213, acc: 0.5234375] [gan loss: 2.534980, acc: 0.000000]\n",
            "932: [discriminator loss: 0.45445266366004944, acc: 0.810546875] [gan loss: 0.866536, acc: 0.199219]\n",
            "933: [discriminator loss: 0.5288375020027161, acc: 0.5283203125] [gan loss: 2.743548, acc: 0.000000]\n",
            "934: [discriminator loss: 0.46174317598342896, acc: 0.8017578125] [gan loss: 0.749311, acc: 0.423828]\n",
            "935: [discriminator loss: 0.5565516948699951, acc: 0.49609375] [gan loss: 2.887267, acc: 0.000000]\n",
            "936: [discriminator loss: 0.42279815673828125, acc: 0.8232421875] [gan loss: 0.867734, acc: 0.208984]\n",
            "937: [discriminator loss: 0.5306543111801147, acc: 0.55078125] [gan loss: 2.847032, acc: 0.000000]\n",
            "938: [discriminator loss: 0.3833102583885193, acc: 0.84375] [gan loss: 0.958345, acc: 0.146484]\n",
            "939: [discriminator loss: 0.5225081443786621, acc: 0.611328125] [gan loss: 2.865803, acc: 0.000000]\n",
            "940: [discriminator loss: 0.4004174470901489, acc: 0.8466796875] [gan loss: 0.934681, acc: 0.152344]\n",
            "941: [discriminator loss: 0.5490762591362, acc: 0.53515625] [gan loss: 2.894713, acc: 0.000000]\n",
            "942: [discriminator loss: 0.4173988997936249, acc: 0.8251953125] [gan loss: 0.889812, acc: 0.222656]\n",
            "943: [discriminator loss: 0.5807775855064392, acc: 0.5244140625] [gan loss: 3.028763, acc: 0.000000]\n",
            "944: [discriminator loss: 0.48957717418670654, acc: 0.7861328125] [gan loss: 0.743138, acc: 0.464844]\n",
            "945: [discriminator loss: 0.6722294092178345, acc: 0.5048828125] [gan loss: 3.089882, acc: 0.000000]\n",
            "946: [discriminator loss: 0.5655322074890137, acc: 0.7451171875] [gan loss: 0.695505, acc: 0.519531]\n",
            "947: [discriminator loss: 0.7400619983673096, acc: 0.5087890625] [gan loss: 2.900393, acc: 0.000000]\n",
            "948: [discriminator loss: 0.5749208927154541, acc: 0.7373046875] [gan loss: 0.807998, acc: 0.460938]\n",
            "949: [discriminator loss: 0.706653892993927, acc: 0.5419921875] [gan loss: 2.690291, acc: 0.000000]\n",
            "950: [discriminator loss: 0.5949445366859436, acc: 0.7314453125] [gan loss: 0.770158, acc: 0.474609]\n",
            "951: [discriminator loss: 0.7293550372123718, acc: 0.51953125] [gan loss: 2.714621, acc: 0.000000]\n",
            "952: [discriminator loss: 0.6278371810913086, acc: 0.705078125] [gan loss: 0.750900, acc: 0.519531]\n",
            "953: [discriminator loss: 0.7591041326522827, acc: 0.51171875] [gan loss: 2.613155, acc: 0.000000]\n",
            "954: [discriminator loss: 0.6180976033210754, acc: 0.7314453125] [gan loss: 0.739387, acc: 0.542969]\n",
            "955: [discriminator loss: 0.7428032159805298, acc: 0.5185546875] [gan loss: 2.423228, acc: 0.000000]\n",
            "956: [discriminator loss: 0.6066181659698486, acc: 0.734375] [gan loss: 0.824012, acc: 0.445312]\n",
            "957: [discriminator loss: 0.6996956467628479, acc: 0.525390625] [gan loss: 2.139164, acc: 0.000000]\n",
            "958: [discriminator loss: 0.5899006128311157, acc: 0.7744140625] [gan loss: 0.990048, acc: 0.152344]\n",
            "959: [discriminator loss: 0.6203919649124146, acc: 0.5517578125] [gan loss: 2.044015, acc: 0.000000]\n",
            "960: [discriminator loss: 0.5760456323623657, acc: 0.77734375] [gan loss: 0.907402, acc: 0.234375]\n",
            "961: [discriminator loss: 0.5939816236495972, acc: 0.56640625] [gan loss: 2.076509, acc: 0.000000]\n",
            "962: [discriminator loss: 0.527999222278595, acc: 0.80859375] [gan loss: 0.922022, acc: 0.208984]\n",
            "963: [discriminator loss: 0.5639817714691162, acc: 0.578125] [gan loss: 2.025215, acc: 0.000000]\n",
            "964: [discriminator loss: 0.505325973033905, acc: 0.8017578125] [gan loss: 0.802562, acc: 0.373047]\n",
            "965: [discriminator loss: 0.5593505501747131, acc: 0.6015625] [gan loss: 2.162821, acc: 0.000000]\n",
            "966: [discriminator loss: 0.5046838521957397, acc: 0.7880859375] [gan loss: 0.709939, acc: 0.546875]\n",
            "967: [discriminator loss: 0.6019585132598877, acc: 0.568359375] [gan loss: 2.196274, acc: 0.000000]\n",
            "968: [discriminator loss: 0.5338343381881714, acc: 0.7666015625] [gan loss: 0.565757, acc: 0.763672]\n",
            "969: [discriminator loss: 0.6485654711723328, acc: 0.53515625] [gan loss: 2.296393, acc: 0.000000]\n",
            "970: [discriminator loss: 0.5911471247673035, acc: 0.7109375] [gan loss: 0.451640, acc: 0.882812]\n",
            "971: [discriminator loss: 0.6962453126907349, acc: 0.5068359375] [gan loss: 2.141441, acc: 0.000000]\n",
            "972: [discriminator loss: 0.5325024724006653, acc: 0.771484375] [gan loss: 0.578537, acc: 0.750000]\n",
            "973: [discriminator loss: 0.6340711712837219, acc: 0.5380859375] [gan loss: 1.966810, acc: 0.000000]\n",
            "974: [discriminator loss: 0.5586025714874268, acc: 0.7705078125] [gan loss: 0.545927, acc: 0.781250]\n",
            "975: [discriminator loss: 0.6599700450897217, acc: 0.5068359375] [gan loss: 1.859152, acc: 0.000000]\n",
            "976: [discriminator loss: 0.5791590809822083, acc: 0.7451171875] [gan loss: 0.480883, acc: 0.884766]\n",
            "977: [discriminator loss: 0.69986891746521, acc: 0.4931640625] [gan loss: 2.007774, acc: 0.000000]\n",
            "978: [discriminator loss: 0.661669909954071, acc: 0.6796875] [gan loss: 0.338637, acc: 0.978516]\n",
            "979: [discriminator loss: 0.7956242561340332, acc: 0.48828125] [gan loss: 2.071855, acc: 0.000000]\n",
            "980: [discriminator loss: 0.6979349255561829, acc: 0.6748046875] [gan loss: 0.321576, acc: 0.970703]\n",
            "981: [discriminator loss: 0.8222036957740784, acc: 0.4736328125] [gan loss: 1.841967, acc: 0.000000]\n",
            "982: [discriminator loss: 0.7051098942756653, acc: 0.6865234375] [gan loss: 0.361487, acc: 0.947266]\n",
            "983: [discriminator loss: 0.8168241381645203, acc: 0.47265625] [gan loss: 1.864665, acc: 0.000000]\n",
            "984: [discriminator loss: 0.7568153738975525, acc: 0.6533203125] [gan loss: 0.314501, acc: 0.962891]\n",
            "985: [discriminator loss: 0.8521580100059509, acc: 0.4814453125] [gan loss: 1.854202, acc: 0.000000]\n",
            "986: [discriminator loss: 0.78654944896698, acc: 0.646484375] [gan loss: 0.308762, acc: 0.958984]\n",
            "987: [discriminator loss: 0.8983522057533264, acc: 0.46484375] [gan loss: 1.763262, acc: 0.000000]\n",
            "988: [discriminator loss: 0.7911416292190552, acc: 0.642578125] [gan loss: 0.341682, acc: 0.955078]\n",
            "989: [discriminator loss: 0.879857063293457, acc: 0.466796875] [gan loss: 1.665864, acc: 0.000000]\n",
            "990: [discriminator loss: 0.8171514272689819, acc: 0.595703125] [gan loss: 0.353126, acc: 0.951172]\n",
            "991: [discriminator loss: 0.8818987011909485, acc: 0.462890625] [gan loss: 1.696999, acc: 0.000000]\n",
            "992: [discriminator loss: 0.7810772061347961, acc: 0.607421875] [gan loss: 0.364346, acc: 0.919922]\n",
            "993: [discriminator loss: 0.8960714936256409, acc: 0.4619140625] [gan loss: 1.666689, acc: 0.000000]\n",
            "994: [discriminator loss: 0.7607755064964294, acc: 0.6103515625] [gan loss: 0.380713, acc: 0.914062]\n",
            "995: [discriminator loss: 0.851272702217102, acc: 0.4931640625] [gan loss: 1.742580, acc: 0.000000]\n",
            "996: [discriminator loss: 0.7632896304130554, acc: 0.611328125] [gan loss: 0.342017, acc: 0.935547]\n",
            "997: [discriminator loss: 0.8480387926101685, acc: 0.515625] [gan loss: 1.867756, acc: 0.000000]\n",
            "998: [discriminator loss: 0.760877251625061, acc: 0.62109375] [gan loss: 0.324281, acc: 0.947266]\n",
            "999: [discriminator loss: 0.8594510555267334, acc: 0.509765625] [gan loss: 1.777068, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1000: [discriminator loss: 0.7567264437675476, acc: 0.5810546875] [gan loss: 0.344819, acc: 0.931641]\n",
            "1001: [discriminator loss: 0.8407047986984253, acc: 0.505859375] [gan loss: 1.730919, acc: 0.000000]\n",
            "1002: [discriminator loss: 0.7353090643882751, acc: 0.5966796875] [gan loss: 0.330944, acc: 0.945312]\n",
            "1003: [discriminator loss: 0.824394941329956, acc: 0.5087890625] [gan loss: 1.700323, acc: 0.000000]\n",
            "1004: [discriminator loss: 0.7375137805938721, acc: 0.587890625] [gan loss: 0.314942, acc: 0.957031]\n",
            "1005: [discriminator loss: 0.8209791779518127, acc: 0.5185546875] [gan loss: 1.625362, acc: 0.000000]\n",
            "1006: [discriminator loss: 0.7188063859939575, acc: 0.59765625] [gan loss: 0.325789, acc: 0.958984]\n",
            "1007: [discriminator loss: 0.8252506256103516, acc: 0.515625] [gan loss: 1.606477, acc: 0.000000]\n",
            "1008: [discriminator loss: 0.7091192007064819, acc: 0.6064453125] [gan loss: 0.346771, acc: 0.935547]\n",
            "1009: [discriminator loss: 0.7895944118499756, acc: 0.5185546875] [gan loss: 1.447952, acc: 0.000000]\n",
            "1010: [discriminator loss: 0.7111141085624695, acc: 0.5791015625] [gan loss: 0.334744, acc: 0.945312]\n",
            "1011: [discriminator loss: 0.7992885708808899, acc: 0.5107421875] [gan loss: 1.517584, acc: 0.000000]\n",
            "1012: [discriminator loss: 0.7217488288879395, acc: 0.580078125] [gan loss: 0.300156, acc: 0.962891]\n",
            "1013: [discriminator loss: 0.7890055179595947, acc: 0.5234375] [gan loss: 1.597619, acc: 0.000000]\n",
            "1014: [discriminator loss: 0.7218440175056458, acc: 0.6103515625] [gan loss: 0.281786, acc: 0.978516]\n",
            "1015: [discriminator loss: 0.8108612298965454, acc: 0.4990234375] [gan loss: 1.587568, acc: 0.000000]\n",
            "1016: [discriminator loss: 0.7129265666007996, acc: 0.609375] [gan loss: 0.272271, acc: 0.994141]\n",
            "1017: [discriminator loss: 0.7940810322761536, acc: 0.513671875] [gan loss: 1.481849, acc: 0.005859]\n",
            "1018: [discriminator loss: 0.6967586278915405, acc: 0.61328125] [gan loss: 0.285914, acc: 0.988281]\n",
            "1019: [discriminator loss: 0.7833703756332397, acc: 0.521484375] [gan loss: 1.418702, acc: 0.011719]\n",
            "1020: [discriminator loss: 0.6975762248039246, acc: 0.63671875] [gan loss: 0.293421, acc: 0.992188]\n",
            "1021: [discriminator loss: 0.7530602216720581, acc: 0.5166015625] [gan loss: 1.381904, acc: 0.029297]\n",
            "1022: [discriminator loss: 0.6865740418434143, acc: 0.623046875] [gan loss: 0.292778, acc: 0.980469]\n",
            "1023: [discriminator loss: 0.7686872482299805, acc: 0.5029296875] [gan loss: 1.344327, acc: 0.041016]\n",
            "1024: [discriminator loss: 0.6764053106307983, acc: 0.6533203125] [gan loss: 0.279778, acc: 0.992188]\n",
            "1025: [discriminator loss: 0.7801200747489929, acc: 0.5068359375] [gan loss: 1.440140, acc: 0.007812]\n",
            "1026: [discriminator loss: 0.6906152367591858, acc: 0.642578125] [gan loss: 0.250828, acc: 0.994141]\n",
            "1027: [discriminator loss: 0.767004132270813, acc: 0.505859375] [gan loss: 1.402469, acc: 0.005859]\n",
            "1028: [discriminator loss: 0.6701566576957703, acc: 0.646484375] [gan loss: 0.284676, acc: 0.986328]\n",
            "1029: [discriminator loss: 0.7881112694740295, acc: 0.501953125] [gan loss: 1.513428, acc: 0.000000]\n",
            "1030: [discriminator loss: 0.6603103280067444, acc: 0.6748046875] [gan loss: 0.306421, acc: 0.984375]\n",
            "1031: [discriminator loss: 0.7497774362564087, acc: 0.5087890625] [gan loss: 1.524083, acc: 0.000000]\n",
            "1032: [discriminator loss: 0.6631038784980774, acc: 0.6630859375] [gan loss: 0.308961, acc: 0.986328]\n",
            "1033: [discriminator loss: 0.7509055733680725, acc: 0.5185546875] [gan loss: 1.617545, acc: 0.000000]\n",
            "1034: [discriminator loss: 0.6776872277259827, acc: 0.669921875] [gan loss: 0.302494, acc: 0.982422]\n",
            "1035: [discriminator loss: 0.7592771649360657, acc: 0.509765625] [gan loss: 1.728738, acc: 0.000000]\n",
            "1036: [discriminator loss: 0.655206561088562, acc: 0.6767578125] [gan loss: 0.317234, acc: 0.986328]\n",
            "1037: [discriminator loss: 0.7885446548461914, acc: 0.4892578125] [gan loss: 1.649890, acc: 0.000000]\n",
            "1038: [discriminator loss: 0.6837813854217529, acc: 0.66796875] [gan loss: 0.349117, acc: 0.960938]\n",
            "1039: [discriminator loss: 0.762516438961029, acc: 0.517578125] [gan loss: 1.643588, acc: 0.000000]\n",
            "1040: [discriminator loss: 0.663420557975769, acc: 0.6728515625] [gan loss: 0.358280, acc: 0.953125]\n",
            "1041: [discriminator loss: 0.7608984708786011, acc: 0.490234375] [gan loss: 1.630244, acc: 0.000000]\n",
            "1042: [discriminator loss: 0.6909323930740356, acc: 0.6650390625] [gan loss: 0.377287, acc: 0.949219]\n",
            "1043: [discriminator loss: 0.7783656120300293, acc: 0.4990234375] [gan loss: 1.749887, acc: 0.000000]\n",
            "1044: [discriminator loss: 0.6919833421707153, acc: 0.642578125] [gan loss: 0.332535, acc: 0.962891]\n",
            "1045: [discriminator loss: 0.7892168164253235, acc: 0.5048828125] [gan loss: 1.702470, acc: 0.000000]\n",
            "1046: [discriminator loss: 0.7073570489883423, acc: 0.623046875] [gan loss: 0.333583, acc: 0.960938]\n",
            "1047: [discriminator loss: 0.8050789833068848, acc: 0.5] [gan loss: 1.676663, acc: 0.000000]\n",
            "1048: [discriminator loss: 0.664971113204956, acc: 0.65234375] [gan loss: 0.367744, acc: 0.931641]\n",
            "1049: [discriminator loss: 0.7634862065315247, acc: 0.5029296875] [gan loss: 1.498240, acc: 0.023438]\n",
            "1050: [discriminator loss: 0.6610167622566223, acc: 0.6513671875] [gan loss: 0.422578, acc: 0.906250]\n",
            "1051: [discriminator loss: 0.7445735335350037, acc: 0.521484375] [gan loss: 1.582375, acc: 0.000000]\n",
            "1052: [discriminator loss: 0.6653528213500977, acc: 0.626953125] [gan loss: 0.367681, acc: 0.939453]\n",
            "1053: [discriminator loss: 0.7425367832183838, acc: 0.5361328125] [gan loss: 1.528880, acc: 0.000000]\n",
            "1054: [discriminator loss: 0.6821575164794922, acc: 0.62890625] [gan loss: 0.391438, acc: 0.900391]\n",
            "1055: [discriminator loss: 0.7507656812667847, acc: 0.51953125] [gan loss: 1.534072, acc: 0.000000]\n",
            "1056: [discriminator loss: 0.6781898736953735, acc: 0.6220703125] [gan loss: 0.347342, acc: 0.951172]\n",
            "1057: [discriminator loss: 0.7521367073059082, acc: 0.5185546875] [gan loss: 1.470990, acc: 0.000000]\n",
            "1058: [discriminator loss: 0.658493161201477, acc: 0.662109375] [gan loss: 0.362718, acc: 0.937500]\n",
            "1059: [discriminator loss: 0.7594847679138184, acc: 0.509765625] [gan loss: 1.430868, acc: 0.000000]\n",
            "1060: [discriminator loss: 0.6568104028701782, acc: 0.650390625] [gan loss: 0.366247, acc: 0.929688]\n",
            "1061: [discriminator loss: 0.7338173985481262, acc: 0.52734375] [gan loss: 1.483334, acc: 0.000000]\n",
            "1062: [discriminator loss: 0.6626253128051758, acc: 0.6083984375] [gan loss: 0.321140, acc: 0.966797]\n",
            "1063: [discriminator loss: 0.7564257979393005, acc: 0.517578125] [gan loss: 1.447460, acc: 0.005859]\n",
            "1064: [discriminator loss: 0.6639382839202881, acc: 0.62890625] [gan loss: 0.376568, acc: 0.957031]\n",
            "1065: [discriminator loss: 0.7366938591003418, acc: 0.5078125] [gan loss: 1.468225, acc: 0.001953]\n",
            "1066: [discriminator loss: 0.6755464673042297, acc: 0.61328125] [gan loss: 0.350659, acc: 0.962891]\n",
            "1067: [discriminator loss: 0.7376970052719116, acc: 0.51171875] [gan loss: 1.455650, acc: 0.003906]\n",
            "1068: [discriminator loss: 0.6846702098846436, acc: 0.58984375] [gan loss: 0.354452, acc: 0.958984]\n",
            "1069: [discriminator loss: 0.7483618855476379, acc: 0.51953125] [gan loss: 1.424404, acc: 0.001953]\n",
            "1070: [discriminator loss: 0.667785108089447, acc: 0.6201171875] [gan loss: 0.378554, acc: 0.937500]\n",
            "1071: [discriminator loss: 0.7382658123970032, acc: 0.50390625] [gan loss: 1.378538, acc: 0.000000]\n",
            "1072: [discriminator loss: 0.6727988123893738, acc: 0.6318359375] [gan loss: 0.357834, acc: 0.964844]\n",
            "1073: [discriminator loss: 0.7399625182151794, acc: 0.498046875] [gan loss: 1.370589, acc: 0.000000]\n",
            "1074: [discriminator loss: 0.6605234146118164, acc: 0.6376953125] [gan loss: 0.361060, acc: 0.976562]\n",
            "1075: [discriminator loss: 0.730947732925415, acc: 0.505859375] [gan loss: 1.421139, acc: 0.000000]\n",
            "1076: [discriminator loss: 0.6786210536956787, acc: 0.64453125] [gan loss: 0.339600, acc: 0.992188]\n",
            "1077: [discriminator loss: 0.7422063946723938, acc: 0.51171875] [gan loss: 1.431655, acc: 0.000000]\n",
            "1078: [discriminator loss: 0.6508950591087341, acc: 0.685546875] [gan loss: 0.376915, acc: 0.984375]\n",
            "1079: [discriminator loss: 0.728310227394104, acc: 0.4970703125] [gan loss: 1.473554, acc: 0.000000]\n",
            "1080: [discriminator loss: 0.640870988368988, acc: 0.6923828125] [gan loss: 0.379686, acc: 0.980469]\n",
            "1081: [discriminator loss: 0.7277176976203918, acc: 0.4921875] [gan loss: 1.454266, acc: 0.000000]\n",
            "1082: [discriminator loss: 0.6648644208908081, acc: 0.662109375] [gan loss: 0.369825, acc: 0.988281]\n",
            "1083: [discriminator loss: 0.7194660305976868, acc: 0.4970703125] [gan loss: 1.523606, acc: 0.000000]\n",
            "1084: [discriminator loss: 0.6442225575447083, acc: 0.673828125] [gan loss: 0.387459, acc: 0.990234]\n",
            "1085: [discriminator loss: 0.736386239528656, acc: 0.4873046875] [gan loss: 1.490635, acc: 0.000000]\n",
            "1086: [discriminator loss: 0.6613948345184326, acc: 0.6708984375] [gan loss: 0.405139, acc: 0.970703]\n",
            "1087: [discriminator loss: 0.7043992877006531, acc: 0.4873046875] [gan loss: 1.524699, acc: 0.000000]\n",
            "1088: [discriminator loss: 0.604987382888794, acc: 0.7109375] [gan loss: 0.465336, acc: 0.929688]\n",
            "1089: [discriminator loss: 0.689241886138916, acc: 0.5] [gan loss: 1.511912, acc: 0.000000]\n",
            "1090: [discriminator loss: 0.6312788128852844, acc: 0.6865234375] [gan loss: 0.427740, acc: 0.943359]\n",
            "1091: [discriminator loss: 0.7239528298377991, acc: 0.4765625] [gan loss: 1.608336, acc: 0.000000]\n",
            "1092: [discriminator loss: 0.6461855173110962, acc: 0.666015625] [gan loss: 0.400884, acc: 0.964844]\n",
            "1093: [discriminator loss: 0.722353458404541, acc: 0.501953125] [gan loss: 1.664328, acc: 0.000000]\n",
            "1094: [discriminator loss: 0.6576038599014282, acc: 0.6513671875] [gan loss: 0.391297, acc: 0.970703]\n",
            "1095: [discriminator loss: 0.7393787503242493, acc: 0.49609375] [gan loss: 1.610067, acc: 0.000000]\n",
            "1096: [discriminator loss: 0.6325122117996216, acc: 0.677734375] [gan loss: 0.461630, acc: 0.902344]\n",
            "1097: [discriminator loss: 0.712570071220398, acc: 0.494140625] [gan loss: 1.563921, acc: 0.000000]\n",
            "1098: [discriminator loss: 0.6365182399749756, acc: 0.6787109375] [gan loss: 0.515735, acc: 0.888672]\n",
            "1099: [discriminator loss: 0.7045888900756836, acc: 0.4931640625] [gan loss: 1.527389, acc: 0.000000]\n",
            "1100: [discriminator loss: 0.6296802163124084, acc: 0.70703125] [gan loss: 0.516862, acc: 0.900391]\n",
            "1101: [discriminator loss: 0.7193570733070374, acc: 0.4775390625] [gan loss: 1.520128, acc: 0.000000]\n",
            "1102: [discriminator loss: 0.6949470639228821, acc: 0.6650390625] [gan loss: 0.451902, acc: 0.947266]\n",
            "1103: [discriminator loss: 0.7574042081832886, acc: 0.4619140625] [gan loss: 1.575111, acc: 0.000000]\n",
            "1104: [discriminator loss: 0.6983602046966553, acc: 0.64453125] [gan loss: 0.399528, acc: 0.976562]\n",
            "1105: [discriminator loss: 0.7809311747550964, acc: 0.4716796875] [gan loss: 1.602088, acc: 0.000000]\n",
            "1106: [discriminator loss: 0.715252697467804, acc: 0.630859375] [gan loss: 0.372344, acc: 0.980469]\n",
            "1107: [discriminator loss: 0.7935047149658203, acc: 0.478515625] [gan loss: 1.504545, acc: 0.000000]\n",
            "1108: [discriminator loss: 0.6997451782226562, acc: 0.6376953125] [gan loss: 0.401414, acc: 0.980469]\n",
            "1109: [discriminator loss: 0.7824136018753052, acc: 0.4638671875] [gan loss: 1.465138, acc: 0.000000]\n",
            "1110: [discriminator loss: 0.7199538350105286, acc: 0.6357421875] [gan loss: 0.395636, acc: 0.968750]\n",
            "1111: [discriminator loss: 0.7973688840866089, acc: 0.4658203125] [gan loss: 1.462965, acc: 0.000000]\n",
            "1112: [discriminator loss: 0.6853784322738647, acc: 0.62890625] [gan loss: 0.418074, acc: 0.966797]\n",
            "1113: [discriminator loss: 0.7753292322158813, acc: 0.462890625] [gan loss: 1.418549, acc: 0.000000]\n",
            "1114: [discriminator loss: 0.7055493593215942, acc: 0.6298828125] [gan loss: 0.408697, acc: 0.957031]\n",
            "1115: [discriminator loss: 0.7794668674468994, acc: 0.46875] [gan loss: 1.413567, acc: 0.000000]\n",
            "1116: [discriminator loss: 0.7027356028556824, acc: 0.6259765625] [gan loss: 0.420959, acc: 0.943359]\n",
            "1117: [discriminator loss: 0.7905818819999695, acc: 0.466796875] [gan loss: 1.441489, acc: 0.000000]\n",
            "1118: [discriminator loss: 0.7427820563316345, acc: 0.583984375] [gan loss: 0.370493, acc: 0.974609]\n",
            "1119: [discriminator loss: 0.8238687515258789, acc: 0.466796875] [gan loss: 1.410259, acc: 0.000000]\n",
            "1120: [discriminator loss: 0.7093133926391602, acc: 0.6337890625] [gan loss: 0.404506, acc: 0.972656]\n",
            "1121: [discriminator loss: 0.7856812477111816, acc: 0.470703125] [gan loss: 1.398576, acc: 0.000000]\n",
            "1122: [discriminator loss: 0.6970327496528625, acc: 0.6640625] [gan loss: 0.416912, acc: 0.982422]\n",
            "1123: [discriminator loss: 0.7680270075798035, acc: 0.4755859375] [gan loss: 1.479498, acc: 0.000000]\n",
            "1124: [discriminator loss: 0.7042755484580994, acc: 0.65234375] [gan loss: 0.394381, acc: 0.982422]\n",
            "1125: [discriminator loss: 0.7657814621925354, acc: 0.4755859375] [gan loss: 1.526240, acc: 0.000000]\n",
            "1126: [discriminator loss: 0.7043271064758301, acc: 0.6533203125] [gan loss: 0.399115, acc: 0.976562]\n",
            "1127: [discriminator loss: 0.7465916872024536, acc: 0.486328125] [gan loss: 1.506453, acc: 0.000000]\n",
            "1128: [discriminator loss: 0.6681610345840454, acc: 0.6611328125] [gan loss: 0.457625, acc: 0.955078]\n",
            "1129: [discriminator loss: 0.7658244371414185, acc: 0.4697265625] [gan loss: 1.532855, acc: 0.000000]\n",
            "1130: [discriminator loss: 0.7036978602409363, acc: 0.6484375] [gan loss: 0.452570, acc: 0.951172]\n",
            "1131: [discriminator loss: 0.7636643648147583, acc: 0.48046875] [gan loss: 1.393806, acc: 0.000000]\n",
            "1132: [discriminator loss: 0.6818016767501831, acc: 0.6552734375] [gan loss: 0.496832, acc: 0.919922]\n",
            "1133: [discriminator loss: 0.7711902856826782, acc: 0.43359375] [gan loss: 1.389562, acc: 0.000000]\n",
            "1134: [discriminator loss: 0.6965157985687256, acc: 0.615234375] [gan loss: 0.496383, acc: 0.935547]\n",
            "1135: [discriminator loss: 0.7740498781204224, acc: 0.451171875] [gan loss: 1.532676, acc: 0.000000]\n",
            "1136: [discriminator loss: 0.7281129360198975, acc: 0.6181640625] [gan loss: 0.383965, acc: 0.998047]\n",
            "1137: [discriminator loss: 0.816307783126831, acc: 0.470703125] [gan loss: 1.662610, acc: 0.000000]\n",
            "1138: [discriminator loss: 0.7400745749473572, acc: 0.599609375] [gan loss: 0.365084, acc: 0.998047]\n",
            "1139: [discriminator loss: 0.8358139395713806, acc: 0.4755859375] [gan loss: 1.687880, acc: 0.000000]\n",
            "1140: [discriminator loss: 0.7293813824653625, acc: 0.603515625] [gan loss: 0.425610, acc: 0.949219]\n",
            "1141: [discriminator loss: 0.8045239448547363, acc: 0.4755859375] [gan loss: 1.503626, acc: 0.000000]\n",
            "1142: [discriminator loss: 0.6885712146759033, acc: 0.625] [gan loss: 0.515389, acc: 0.863281]\n",
            "1143: [discriminator loss: 0.7663335204124451, acc: 0.482421875] [gan loss: 1.531701, acc: 0.000000]\n",
            "1144: [discriminator loss: 0.6824340224266052, acc: 0.6396484375] [gan loss: 0.563004, acc: 0.751953]\n",
            "1145: [discriminator loss: 0.7368284463882446, acc: 0.4912109375] [gan loss: 1.507740, acc: 0.000000]\n",
            "1146: [discriminator loss: 0.681939959526062, acc: 0.646484375] [gan loss: 0.502930, acc: 0.861328]\n",
            "1147: [discriminator loss: 0.762946367263794, acc: 0.49609375] [gan loss: 1.584681, acc: 0.000000]\n",
            "1148: [discriminator loss: 0.6856237649917603, acc: 0.640625] [gan loss: 0.491664, acc: 0.894531]\n",
            "1149: [discriminator loss: 0.757389485836029, acc: 0.4921875] [gan loss: 1.569434, acc: 0.000000]\n",
            "1150: [discriminator loss: 0.6926400661468506, acc: 0.6259765625] [gan loss: 0.480237, acc: 0.898438]\n",
            "1151: [discriminator loss: 0.7471776604652405, acc: 0.4951171875] [gan loss: 1.432015, acc: 0.000000]\n",
            "1152: [discriminator loss: 0.6730359196662903, acc: 0.6494140625] [gan loss: 0.531191, acc: 0.839844]\n",
            "1153: [discriminator loss: 0.7229997515678406, acc: 0.49609375] [gan loss: 1.369649, acc: 0.000000]\n",
            "1154: [discriminator loss: 0.6575915217399597, acc: 0.625] [gan loss: 0.581108, acc: 0.767578]\n",
            "1155: [discriminator loss: 0.713132381439209, acc: 0.51171875] [gan loss: 1.387352, acc: 0.000000]\n",
            "1156: [discriminator loss: 0.6639505624771118, acc: 0.6318359375] [gan loss: 0.520419, acc: 0.839844]\n",
            "1157: [discriminator loss: 0.7319557070732117, acc: 0.50390625] [gan loss: 1.441498, acc: 0.000000]\n",
            "1158: [discriminator loss: 0.6711118817329407, acc: 0.650390625] [gan loss: 0.491608, acc: 0.900391]\n",
            "1159: [discriminator loss: 0.7405571937561035, acc: 0.494140625] [gan loss: 1.485502, acc: 0.000000]\n",
            "1160: [discriminator loss: 0.6724928617477417, acc: 0.6123046875] [gan loss: 0.477055, acc: 0.900391]\n",
            "1161: [discriminator loss: 0.7413269281387329, acc: 0.4873046875] [gan loss: 1.437370, acc: 0.000000]\n",
            "1162: [discriminator loss: 0.6612051129341125, acc: 0.638671875] [gan loss: 0.495313, acc: 0.892578]\n",
            "1163: [discriminator loss: 0.7357867360115051, acc: 0.494140625] [gan loss: 1.399617, acc: 0.000000]\n",
            "1164: [discriminator loss: 0.6605700254440308, acc: 0.638671875] [gan loss: 0.530626, acc: 0.824219]\n",
            "1165: [discriminator loss: 0.7060377597808838, acc: 0.4970703125] [gan loss: 1.360610, acc: 0.000000]\n",
            "1166: [discriminator loss: 0.6526064276695251, acc: 0.654296875] [gan loss: 0.536738, acc: 0.800781]\n",
            "1167: [discriminator loss: 0.7129214406013489, acc: 0.484375] [gan loss: 1.396134, acc: 0.000000]\n",
            "1168: [discriminator loss: 0.6454953551292419, acc: 0.642578125] [gan loss: 0.523108, acc: 0.816406]\n",
            "1169: [discriminator loss: 0.7157458662986755, acc: 0.4951171875] [gan loss: 1.442345, acc: 0.000000]\n",
            "1170: [discriminator loss: 0.6639383435249329, acc: 0.6142578125] [gan loss: 0.475328, acc: 0.914062]\n",
            "1171: [discriminator loss: 0.7192319631576538, acc: 0.4990234375] [gan loss: 1.443372, acc: 0.000000]\n",
            "1172: [discriminator loss: 0.6677083969116211, acc: 0.6259765625] [gan loss: 0.466834, acc: 0.917969]\n",
            "1173: [discriminator loss: 0.7346510291099548, acc: 0.494140625] [gan loss: 1.451396, acc: 0.000000]\n",
            "1174: [discriminator loss: 0.6446122527122498, acc: 0.65625] [gan loss: 0.545457, acc: 0.814453]\n",
            "1175: [discriminator loss: 0.7044984102249146, acc: 0.4921875] [gan loss: 1.370077, acc: 0.000000]\n",
            "1176: [discriminator loss: 0.6395896077156067, acc: 0.677734375] [gan loss: 0.556603, acc: 0.783203]\n",
            "1177: [discriminator loss: 0.6942594647407532, acc: 0.517578125] [gan loss: 1.320695, acc: 0.000000]\n",
            "1178: [discriminator loss: 0.6494258642196655, acc: 0.6728515625] [gan loss: 0.548292, acc: 0.818359]\n",
            "1179: [discriminator loss: 0.691452145576477, acc: 0.5029296875] [gan loss: 1.317708, acc: 0.000000]\n",
            "1180: [discriminator loss: 0.6594679355621338, acc: 0.6533203125] [gan loss: 0.528276, acc: 0.880859]\n",
            "1181: [discriminator loss: 0.6997084617614746, acc: 0.498046875] [gan loss: 1.343512, acc: 0.000000]\n",
            "1182: [discriminator loss: 0.6537221670150757, acc: 0.64453125] [gan loss: 0.507117, acc: 0.921875]\n",
            "1183: [discriminator loss: 0.7177762985229492, acc: 0.5] [gan loss: 1.411557, acc: 0.000000]\n",
            "1184: [discriminator loss: 0.6735339164733887, acc: 0.625] [gan loss: 0.486378, acc: 0.939453]\n",
            "1185: [discriminator loss: 0.7419071793556213, acc: 0.48046875] [gan loss: 1.382879, acc: 0.000000]\n",
            "1186: [discriminator loss: 0.676682710647583, acc: 0.6298828125] [gan loss: 0.463767, acc: 0.974609]\n",
            "1187: [discriminator loss: 0.7466462850570679, acc: 0.478515625] [gan loss: 1.385954, acc: 0.000000]\n",
            "1188: [discriminator loss: 0.6923751831054688, acc: 0.6171875] [gan loss: 0.477217, acc: 0.962891]\n",
            "1189: [discriminator loss: 0.7452524900436401, acc: 0.4794921875] [gan loss: 1.365586, acc: 0.000000]\n",
            "1190: [discriminator loss: 0.6896401047706604, acc: 0.623046875] [gan loss: 0.514583, acc: 0.927734]\n",
            "1191: [discriminator loss: 0.7367706298828125, acc: 0.4677734375] [gan loss: 1.285426, acc: 0.000000]\n",
            "1192: [discriminator loss: 0.6774877309799194, acc: 0.6376953125] [gan loss: 0.543567, acc: 0.900391]\n",
            "1193: [discriminator loss: 0.7121396660804749, acc: 0.490234375] [gan loss: 1.337065, acc: 0.000000]\n",
            "1194: [discriminator loss: 0.6785058975219727, acc: 0.6298828125] [gan loss: 0.500718, acc: 0.953125]\n",
            "1195: [discriminator loss: 0.7164968848228455, acc: 0.48046875] [gan loss: 1.368537, acc: 0.000000]\n",
            "1196: [discriminator loss: 0.6678742170333862, acc: 0.654296875] [gan loss: 0.530955, acc: 0.914062]\n",
            "1197: [discriminator loss: 0.7041463851928711, acc: 0.498046875] [gan loss: 1.398157, acc: 0.000000]\n",
            "1198: [discriminator loss: 0.6392710208892822, acc: 0.681640625] [gan loss: 0.560343, acc: 0.896484]\n",
            "1199: [discriminator loss: 0.6823701858520508, acc: 0.4853515625] [gan loss: 1.349387, acc: 0.000000]\n",
            "1200: [discriminator loss: 0.6269195675849915, acc: 0.693359375] [gan loss: 0.573420, acc: 0.888672]\n",
            "1201: [discriminator loss: 0.6668707132339478, acc: 0.49609375] [gan loss: 1.372939, acc: 0.000000]\n",
            "1202: [discriminator loss: 0.620200514793396, acc: 0.697265625] [gan loss: 0.557620, acc: 0.898438]\n",
            "1203: [discriminator loss: 0.6579752564430237, acc: 0.5126953125] [gan loss: 1.405426, acc: 0.000000]\n",
            "1204: [discriminator loss: 0.6066803336143494, acc: 0.72265625] [gan loss: 0.598936, acc: 0.863281]\n",
            "1205: [discriminator loss: 0.6460040211677551, acc: 0.517578125] [gan loss: 1.382421, acc: 0.000000]\n",
            "1206: [discriminator loss: 0.5968025922775269, acc: 0.7197265625] [gan loss: 0.590915, acc: 0.869141]\n",
            "1207: [discriminator loss: 0.6463584303855896, acc: 0.5126953125] [gan loss: 1.382596, acc: 0.000000]\n",
            "1208: [discriminator loss: 0.5892210602760315, acc: 0.7197265625] [gan loss: 0.604362, acc: 0.824219]\n",
            "1209: [discriminator loss: 0.6414785385131836, acc: 0.5107421875] [gan loss: 1.407312, acc: 0.000000]\n",
            "1210: [discriminator loss: 0.5870135426521301, acc: 0.724609375] [gan loss: 0.571647, acc: 0.859375]\n",
            "1211: [discriminator loss: 0.642881453037262, acc: 0.51171875] [gan loss: 1.434547, acc: 0.000000]\n",
            "1212: [discriminator loss: 0.57863450050354, acc: 0.71484375] [gan loss: 0.585674, acc: 0.847656]\n",
            "1213: [discriminator loss: 0.641947865486145, acc: 0.5107421875] [gan loss: 1.412612, acc: 0.000000]\n",
            "1214: [discriminator loss: 0.6070677042007446, acc: 0.7109375] [gan loss: 0.570173, acc: 0.859375]\n",
            "1215: [discriminator loss: 0.642849862575531, acc: 0.5263671875] [gan loss: 1.368371, acc: 0.000000]\n",
            "1216: [discriminator loss: 0.5912982821464539, acc: 0.724609375] [gan loss: 0.616267, acc: 0.716797]\n",
            "1217: [discriminator loss: 0.6253714561462402, acc: 0.5478515625] [gan loss: 1.331992, acc: 0.000000]\n",
            "1218: [discriminator loss: 0.5777780413627625, acc: 0.7265625] [gan loss: 0.615545, acc: 0.746094]\n",
            "1219: [discriminator loss: 0.645795464515686, acc: 0.5263671875] [gan loss: 1.359048, acc: 0.000000]\n",
            "1220: [discriminator loss: 0.589263916015625, acc: 0.7060546875] [gan loss: 0.660499, acc: 0.585938]\n",
            "1221: [discriminator loss: 0.6422275900840759, acc: 0.5556640625] [gan loss: 1.304059, acc: 0.000000]\n",
            "1222: [discriminator loss: 0.5918902158737183, acc: 0.7099609375] [gan loss: 0.652641, acc: 0.652344]\n",
            "1223: [discriminator loss: 0.6392369270324707, acc: 0.5380859375] [gan loss: 1.361070, acc: 0.000000]\n",
            "1224: [discriminator loss: 0.6098613142967224, acc: 0.7041015625] [gan loss: 0.581994, acc: 0.769531]\n",
            "1225: [discriminator loss: 0.6641958355903625, acc: 0.5205078125] [gan loss: 1.487971, acc: 0.000000]\n",
            "1226: [discriminator loss: 0.6291612982749939, acc: 0.6796875] [gan loss: 0.529012, acc: 0.849609]\n",
            "1227: [discriminator loss: 0.7110298871994019, acc: 0.494140625] [gan loss: 1.518496, acc: 0.000000]\n",
            "1228: [discriminator loss: 0.6503105163574219, acc: 0.6572265625] [gan loss: 0.503810, acc: 0.894531]\n",
            "1229: [discriminator loss: 0.706739604473114, acc: 0.486328125] [gan loss: 1.461874, acc: 0.000000]\n",
            "1230: [discriminator loss: 0.633324146270752, acc: 0.6748046875] [gan loss: 0.543782, acc: 0.900391]\n",
            "1231: [discriminator loss: 0.6841800212860107, acc: 0.482421875] [gan loss: 1.351018, acc: 0.000000]\n",
            "1232: [discriminator loss: 0.6197107434272766, acc: 0.7138671875] [gan loss: 0.602421, acc: 0.806641]\n",
            "1233: [discriminator loss: 0.6649932265281677, acc: 0.4833984375] [gan loss: 1.310551, acc: 0.000000]\n",
            "1234: [discriminator loss: 0.6221021413803101, acc: 0.7099609375] [gan loss: 0.593795, acc: 0.837891]\n",
            "1235: [discriminator loss: 0.6651744246482849, acc: 0.4736328125] [gan loss: 1.389573, acc: 0.000000]\n",
            "1236: [discriminator loss: 0.6311105489730835, acc: 0.685546875] [gan loss: 0.550960, acc: 0.929688]\n",
            "1237: [discriminator loss: 0.6772988438606262, acc: 0.474609375] [gan loss: 1.370512, acc: 0.000000]\n",
            "1238: [discriminator loss: 0.6221827864646912, acc: 0.69921875] [gan loss: 0.558204, acc: 0.892578]\n",
            "1239: [discriminator loss: 0.6651046276092529, acc: 0.486328125] [gan loss: 1.353765, acc: 0.000000]\n",
            "1240: [discriminator loss: 0.6440015435218811, acc: 0.6669921875] [gan loss: 0.515150, acc: 0.923828]\n",
            "1241: [discriminator loss: 0.7014144659042358, acc: 0.4814453125] [gan loss: 1.311913, acc: 0.000000]\n",
            "1242: [discriminator loss: 0.6617647409439087, acc: 0.6689453125] [gan loss: 0.426031, acc: 0.937500]\n",
            "1243: [discriminator loss: 0.7465868592262268, acc: 0.4755859375] [gan loss: 1.101455, acc: 0.085938]\n",
            "1244: [discriminator loss: 0.7369553446769714, acc: 0.5224609375] [gan loss: 0.448033, acc: 0.960938]\n",
            "1245: [discriminator loss: 0.8068893551826477, acc: 0.40625] [gan loss: 0.989131, acc: 0.089844]\n",
            "1246: [discriminator loss: 0.7554008364677429, acc: 0.541015625] [gan loss: 0.487170, acc: 0.988281]\n",
            "1247: [discriminator loss: 0.7800437211990356, acc: 0.375] [gan loss: 1.012145, acc: 0.000000]\n",
            "1248: [discriminator loss: 0.7339587211608887, acc: 0.615234375] [gan loss: 0.419452, acc: 1.000000]\n",
            "1249: [discriminator loss: 0.7771273851394653, acc: 0.42578125] [gan loss: 1.133275, acc: 0.000000]\n",
            "1250: [discriminator loss: 0.7594893574714661, acc: 0.5791015625] [gan loss: 0.370878, acc: 1.000000]\n",
            "1251: [discriminator loss: 0.8154888153076172, acc: 0.439453125] [gan loss: 1.294733, acc: 0.000000]\n",
            "1252: [discriminator loss: 0.7463043332099915, acc: 0.5908203125] [gan loss: 0.417756, acc: 1.000000]\n",
            "1253: [discriminator loss: 0.8060688376426697, acc: 0.4365234375] [gan loss: 1.374005, acc: 0.000000]\n",
            "1254: [discriminator loss: 0.7198586463928223, acc: 0.61328125] [gan loss: 0.493325, acc: 0.992188]\n",
            "1255: [discriminator loss: 0.786506712436676, acc: 0.4326171875] [gan loss: 1.393895, acc: 0.000000]\n",
            "1256: [discriminator loss: 0.7093504667282104, acc: 0.6123046875] [gan loss: 0.503959, acc: 0.968750]\n",
            "1257: [discriminator loss: 0.7565532326698303, acc: 0.4521484375] [gan loss: 1.411251, acc: 0.000000]\n",
            "1258: [discriminator loss: 0.7100749611854553, acc: 0.6474609375] [gan loss: 0.518783, acc: 0.933594]\n",
            "1259: [discriminator loss: 0.7521677017211914, acc: 0.45703125] [gan loss: 1.455764, acc: 0.000000]\n",
            "1260: [discriminator loss: 0.7026718854904175, acc: 0.654296875] [gan loss: 0.517357, acc: 0.939453]\n",
            "1261: [discriminator loss: 0.7573164701461792, acc: 0.44921875] [gan loss: 1.437340, acc: 0.000000]\n",
            "1262: [discriminator loss: 0.6924607753753662, acc: 0.6865234375] [gan loss: 0.560729, acc: 0.882812]\n",
            "1263: [discriminator loss: 0.7340660691261292, acc: 0.4482421875] [gan loss: 1.338061, acc: 0.000000]\n",
            "1264: [discriminator loss: 0.6841089725494385, acc: 0.6923828125] [gan loss: 0.582331, acc: 0.888672]\n",
            "1265: [discriminator loss: 0.7245622277259827, acc: 0.4462890625] [gan loss: 1.262812, acc: 0.000000]\n",
            "1266: [discriminator loss: 0.674707293510437, acc: 0.71875] [gan loss: 0.618568, acc: 0.787109]\n",
            "1267: [discriminator loss: 0.7133525013923645, acc: 0.443359375] [gan loss: 1.219204, acc: 0.000000]\n",
            "1268: [discriminator loss: 0.6792365312576294, acc: 0.6865234375] [gan loss: 0.623040, acc: 0.763672]\n",
            "1269: [discriminator loss: 0.7188469171524048, acc: 0.4443359375] [gan loss: 1.241254, acc: 0.000000]\n",
            "1270: [discriminator loss: 0.682490885257721, acc: 0.69921875] [gan loss: 0.616039, acc: 0.791016]\n",
            "1271: [discriminator loss: 0.7044801115989685, acc: 0.447265625] [gan loss: 1.237610, acc: 0.000000]\n",
            "1272: [discriminator loss: 0.6659489870071411, acc: 0.6689453125] [gan loss: 0.578659, acc: 0.851562]\n",
            "1273: [discriminator loss: 0.6998953223228455, acc: 0.4833984375] [gan loss: 1.350913, acc: 0.000000]\n",
            "1274: [discriminator loss: 0.6445861458778381, acc: 0.69140625] [gan loss: 0.552736, acc: 0.896484]\n",
            "1275: [discriminator loss: 0.69975745677948, acc: 0.4755859375] [gan loss: 1.304896, acc: 0.000000]\n",
            "1276: [discriminator loss: 0.6308485269546509, acc: 0.6982421875] [gan loss: 0.556517, acc: 0.878906]\n",
            "1277: [discriminator loss: 0.6822198629379272, acc: 0.482421875] [gan loss: 1.350898, acc: 0.000000]\n",
            "1278: [discriminator loss: 0.6122713685035706, acc: 0.71484375] [gan loss: 0.595283, acc: 0.792969]\n",
            "1279: [discriminator loss: 0.6514737606048584, acc: 0.494140625] [gan loss: 1.474055, acc: 0.000000]\n",
            "1280: [discriminator loss: 0.5848837494850159, acc: 0.708984375] [gan loss: 0.594443, acc: 0.830078]\n",
            "1281: [discriminator loss: 0.6408435702323914, acc: 0.51171875] [gan loss: 1.468476, acc: 0.000000]\n",
            "1282: [discriminator loss: 0.582107663154602, acc: 0.7255859375] [gan loss: 0.640037, acc: 0.726562]\n",
            "1283: [discriminator loss: 0.631117582321167, acc: 0.5029296875] [gan loss: 1.494126, acc: 0.000000]\n",
            "1284: [discriminator loss: 0.5908808708190918, acc: 0.7373046875] [gan loss: 0.654535, acc: 0.695312]\n",
            "1285: [discriminator loss: 0.6485222578048706, acc: 0.4931640625] [gan loss: 1.387768, acc: 0.000000]\n",
            "1286: [discriminator loss: 0.6197152733802795, acc: 0.72265625] [gan loss: 0.653269, acc: 0.673828]\n",
            "1287: [discriminator loss: 0.6678212881088257, acc: 0.4853515625] [gan loss: 1.291747, acc: 0.000000]\n",
            "1288: [discriminator loss: 0.649196207523346, acc: 0.662109375] [gan loss: 0.617930, acc: 0.822266]\n",
            "1289: [discriminator loss: 0.6862961053848267, acc: 0.4951171875] [gan loss: 1.285138, acc: 0.000000]\n",
            "1290: [discriminator loss: 0.6674407720565796, acc: 0.6240234375] [gan loss: 0.573479, acc: 0.841797]\n",
            "1291: [discriminator loss: 0.7145527601242065, acc: 0.4814453125] [gan loss: 1.266564, acc: 0.000000]\n",
            "1292: [discriminator loss: 0.6889450550079346, acc: 0.58203125] [gan loss: 0.478854, acc: 0.937500]\n",
            "1293: [discriminator loss: 0.7334907054901123, acc: 0.482421875] [gan loss: 1.286892, acc: 0.000000]\n",
            "1294: [discriminator loss: 0.6788138151168823, acc: 0.607421875] [gan loss: 0.511343, acc: 0.931641]\n",
            "1295: [discriminator loss: 0.721178412437439, acc: 0.470703125] [gan loss: 1.142095, acc: 0.000000]\n",
            "1296: [discriminator loss: 0.674744725227356, acc: 0.599609375] [gan loss: 0.585147, acc: 0.789062]\n",
            "1297: [discriminator loss: 0.6981536746025085, acc: 0.4990234375] [gan loss: 1.163823, acc: 0.000000]\n",
            "1298: [discriminator loss: 0.6709716320037842, acc: 0.59765625] [gan loss: 0.602960, acc: 0.724609]\n",
            "1299: [discriminator loss: 0.6979517936706543, acc: 0.498046875] [gan loss: 1.173955, acc: 0.000000]\n",
            "1300: [discriminator loss: 0.6612911224365234, acc: 0.6015625] [gan loss: 0.563132, acc: 0.791016]\n",
            "1301: [discriminator loss: 0.7043074369430542, acc: 0.4931640625] [gan loss: 1.209301, acc: 0.000000]\n",
            "1302: [discriminator loss: 0.6847244501113892, acc: 0.591796875] [gan loss: 0.515252, acc: 0.892578]\n",
            "1303: [discriminator loss: 0.7287167906761169, acc: 0.4931640625] [gan loss: 1.258080, acc: 0.000000]\n",
            "1304: [discriminator loss: 0.6733411550521851, acc: 0.5966796875] [gan loss: 0.494110, acc: 0.919922]\n",
            "1305: [discriminator loss: 0.7214800119400024, acc: 0.490234375] [gan loss: 1.217875, acc: 0.000000]\n",
            "1306: [discriminator loss: 0.6805329918861389, acc: 0.599609375] [gan loss: 0.511646, acc: 0.931641]\n",
            "1307: [discriminator loss: 0.7164855599403381, acc: 0.490234375] [gan loss: 1.169597, acc: 0.000000]\n",
            "1308: [discriminator loss: 0.6812641620635986, acc: 0.6279296875] [gan loss: 0.524632, acc: 0.919922]\n",
            "1309: [discriminator loss: 0.7095245718955994, acc: 0.4853515625] [gan loss: 1.168455, acc: 0.000000]\n",
            "1310: [discriminator loss: 0.6670362949371338, acc: 0.6357421875] [gan loss: 0.521792, acc: 0.951172]\n",
            "1311: [discriminator loss: 0.7193684577941895, acc: 0.4833984375] [gan loss: 1.147261, acc: 0.000000]\n",
            "1312: [discriminator loss: 0.6734601259231567, acc: 0.615234375] [gan loss: 0.531590, acc: 0.933594]\n",
            "1313: [discriminator loss: 0.7208434343338013, acc: 0.470703125] [gan loss: 1.168476, acc: 0.000000]\n",
            "1314: [discriminator loss: 0.6764925718307495, acc: 0.6240234375] [gan loss: 0.501092, acc: 0.982422]\n",
            "1315: [discriminator loss: 0.7289620637893677, acc: 0.4794921875] [gan loss: 1.183593, acc: 0.000000]\n",
            "1316: [discriminator loss: 0.6835643649101257, acc: 0.5927734375] [gan loss: 0.530124, acc: 0.904297]\n",
            "1317: [discriminator loss: 0.7200668454170227, acc: 0.45703125] [gan loss: 1.172516, acc: 0.000000]\n",
            "1318: [discriminator loss: 0.6868619918823242, acc: 0.6123046875] [gan loss: 0.521483, acc: 0.923828]\n",
            "1319: [discriminator loss: 0.7260136008262634, acc: 0.4736328125] [gan loss: 1.162482, acc: 0.000000]\n",
            "1320: [discriminator loss: 0.6908854246139526, acc: 0.6259765625] [gan loss: 0.529637, acc: 0.929688]\n",
            "1321: [discriminator loss: 0.7240037322044373, acc: 0.466796875] [gan loss: 1.149518, acc: 0.000000]\n",
            "1322: [discriminator loss: 0.6824808716773987, acc: 0.6201171875] [gan loss: 0.568324, acc: 0.878906]\n",
            "1323: [discriminator loss: 0.7221094369888306, acc: 0.4501953125] [gan loss: 1.146329, acc: 0.000000]\n",
            "1324: [discriminator loss: 0.6804758906364441, acc: 0.6328125] [gan loss: 0.573608, acc: 0.876953]\n",
            "1325: [discriminator loss: 0.7122747898101807, acc: 0.4736328125] [gan loss: 1.237070, acc: 0.000000]\n",
            "1326: [discriminator loss: 0.6922785639762878, acc: 0.6025390625] [gan loss: 0.510896, acc: 0.962891]\n",
            "1327: [discriminator loss: 0.7324246764183044, acc: 0.4775390625] [gan loss: 1.233250, acc: 0.000000]\n",
            "1328: [discriminator loss: 0.6871744394302368, acc: 0.6171875] [gan loss: 0.544461, acc: 0.929688]\n",
            "1329: [discriminator loss: 0.728840708732605, acc: 0.4765625] [gan loss: 1.209075, acc: 0.000000]\n",
            "1330: [discriminator loss: 0.683259904384613, acc: 0.6318359375] [gan loss: 0.545232, acc: 0.945312]\n",
            "1331: [discriminator loss: 0.7261064648628235, acc: 0.4716796875] [gan loss: 1.151217, acc: 0.000000]\n",
            "1332: [discriminator loss: 0.6761671304702759, acc: 0.634765625] [gan loss: 0.562530, acc: 0.923828]\n",
            "1333: [discriminator loss: 0.7258448004722595, acc: 0.462890625] [gan loss: 1.116080, acc: 0.000000]\n",
            "1334: [discriminator loss: 0.6954067945480347, acc: 0.6015625] [gan loss: 0.561604, acc: 0.925781]\n",
            "1335: [discriminator loss: 0.734389066696167, acc: 0.4658203125] [gan loss: 1.115601, acc: 0.000000]\n",
            "1336: [discriminator loss: 0.7001550197601318, acc: 0.5498046875] [gan loss: 0.574801, acc: 0.880859]\n",
            "1337: [discriminator loss: 0.7215996980667114, acc: 0.4638671875] [gan loss: 1.163397, acc: 0.000000]\n",
            "1338: [discriminator loss: 0.7061721086502075, acc: 0.607421875] [gan loss: 0.512826, acc: 0.976562]\n",
            "1339: [discriminator loss: 0.733203649520874, acc: 0.4794921875] [gan loss: 1.208566, acc: 0.000000]\n",
            "1340: [discriminator loss: 0.6890380382537842, acc: 0.6123046875] [gan loss: 0.527083, acc: 0.974609]\n",
            "1341: [discriminator loss: 0.7342199683189392, acc: 0.482421875] [gan loss: 1.143735, acc: 0.000000]\n",
            "1342: [discriminator loss: 0.6910587549209595, acc: 0.615234375] [gan loss: 0.546082, acc: 0.945312]\n",
            "1343: [discriminator loss: 0.7322679162025452, acc: 0.466796875] [gan loss: 1.071120, acc: 0.000000]\n",
            "1344: [discriminator loss: 0.6892471313476562, acc: 0.57421875] [gan loss: 0.563826, acc: 0.914062]\n",
            "1345: [discriminator loss: 0.7279375195503235, acc: 0.4677734375] [gan loss: 1.107257, acc: 0.000000]\n",
            "1346: [discriminator loss: 0.6930993795394897, acc: 0.6240234375] [gan loss: 0.519597, acc: 0.953125]\n",
            "1347: [discriminator loss: 0.7328153252601624, acc: 0.4765625] [gan loss: 1.166813, acc: 0.000000]\n",
            "1348: [discriminator loss: 0.6766999959945679, acc: 0.6103515625] [gan loss: 0.503524, acc: 0.992188]\n",
            "1349: [discriminator loss: 0.7373905777931213, acc: 0.4736328125] [gan loss: 1.184276, acc: 0.000000]\n",
            "1350: [discriminator loss: 0.6763179302215576, acc: 0.626953125] [gan loss: 0.540152, acc: 0.958984]\n",
            "1351: [discriminator loss: 0.719574511051178, acc: 0.4853515625] [gan loss: 1.198313, acc: 0.000000]\n",
            "1352: [discriminator loss: 0.6792849898338318, acc: 0.6357421875] [gan loss: 0.528768, acc: 0.953125]\n",
            "1353: [discriminator loss: 0.7131226658821106, acc: 0.486328125] [gan loss: 1.247732, acc: 0.000000]\n",
            "1354: [discriminator loss: 0.6794174909591675, acc: 0.6162109375] [gan loss: 0.512313, acc: 0.951172]\n",
            "1355: [discriminator loss: 0.7093520760536194, acc: 0.4912109375] [gan loss: 1.188044, acc: 0.000000]\n",
            "1356: [discriminator loss: 0.6538550853729248, acc: 0.63671875] [gan loss: 0.573949, acc: 0.828125]\n",
            "1357: [discriminator loss: 0.686372697353363, acc: 0.5048828125] [gan loss: 1.134531, acc: 0.000000]\n",
            "1358: [discriminator loss: 0.6564284563064575, acc: 0.630859375] [gan loss: 0.602064, acc: 0.730469]\n",
            "1359: [discriminator loss: 0.6855167746543884, acc: 0.5068359375] [gan loss: 1.176136, acc: 0.000000]\n",
            "1360: [discriminator loss: 0.6557172536849976, acc: 0.5927734375] [gan loss: 0.592272, acc: 0.759766]\n",
            "1361: [discriminator loss: 0.6941834688186646, acc: 0.515625] [gan loss: 1.199551, acc: 0.000000]\n",
            "1362: [discriminator loss: 0.6622176170349121, acc: 0.595703125] [gan loss: 0.531000, acc: 0.892578]\n",
            "1363: [discriminator loss: 0.7070128321647644, acc: 0.5048828125] [gan loss: 1.231690, acc: 0.000000]\n",
            "1364: [discriminator loss: 0.6626514792442322, acc: 0.5986328125] [gan loss: 0.524577, acc: 0.892578]\n",
            "1365: [discriminator loss: 0.7030451893806458, acc: 0.5048828125] [gan loss: 1.213112, acc: 0.000000]\n",
            "1366: [discriminator loss: 0.6482939124107361, acc: 0.6083984375] [gan loss: 0.526245, acc: 0.931641]\n",
            "1367: [discriminator loss: 0.7142931222915649, acc: 0.4931640625] [gan loss: 1.149766, acc: 0.000000]\n",
            "1368: [discriminator loss: 0.6577968597412109, acc: 0.5947265625] [gan loss: 0.556162, acc: 0.857422]\n",
            "1369: [discriminator loss: 0.7012521028518677, acc: 0.4931640625] [gan loss: 1.123999, acc: 0.000000]\n",
            "1370: [discriminator loss: 0.6752365827560425, acc: 0.564453125] [gan loss: 0.532938, acc: 0.941406]\n",
            "1371: [discriminator loss: 0.7264535427093506, acc: 0.4970703125] [gan loss: 1.095028, acc: 0.042969]\n",
            "1372: [discriminator loss: 0.6917544007301331, acc: 0.5439453125] [gan loss: 0.536573, acc: 0.914062]\n",
            "1373: [discriminator loss: 0.7271966338157654, acc: 0.462890625] [gan loss: 1.084142, acc: 0.064453]\n",
            "1374: [discriminator loss: 0.6972821354866028, acc: 0.5361328125] [gan loss: 0.474448, acc: 0.982422]\n",
            "1375: [discriminator loss: 0.7420800924301147, acc: 0.4892578125] [gan loss: 1.111030, acc: 0.025391]\n",
            "1376: [discriminator loss: 0.7107473611831665, acc: 0.5341796875] [gan loss: 0.452869, acc: 1.000000]\n",
            "1377: [discriminator loss: 0.7702479958534241, acc: 0.4755859375] [gan loss: 1.069882, acc: 0.013672]\n",
            "1378: [discriminator loss: 0.7234319448471069, acc: 0.521484375] [gan loss: 0.459554, acc: 0.994141]\n",
            "1379: [discriminator loss: 0.7730486989021301, acc: 0.47265625] [gan loss: 1.056299, acc: 0.041016]\n",
            "1380: [discriminator loss: 0.7305361032485962, acc: 0.4921875] [gan loss: 0.455920, acc: 0.992188]\n",
            "1381: [discriminator loss: 0.7761105298995972, acc: 0.474609375] [gan loss: 1.035382, acc: 0.003906]\n",
            "1382: [discriminator loss: 0.7386932373046875, acc: 0.48828125] [gan loss: 0.473909, acc: 0.988281]\n",
            "1383: [discriminator loss: 0.7768159508705139, acc: 0.4541015625] [gan loss: 1.005576, acc: 0.000000]\n",
            "1384: [discriminator loss: 0.7315306663513184, acc: 0.4814453125] [gan loss: 0.467785, acc: 0.988281]\n",
            "1385: [discriminator loss: 0.774440586566925, acc: 0.4677734375] [gan loss: 1.053990, acc: 0.005859]\n",
            "1386: [discriminator loss: 0.7350260615348816, acc: 0.5078125] [gan loss: 0.453440, acc: 1.000000]\n",
            "1387: [discriminator loss: 0.7751372456550598, acc: 0.4775390625] [gan loss: 1.046585, acc: 0.015625]\n",
            "1388: [discriminator loss: 0.722510039806366, acc: 0.5380859375] [gan loss: 0.474698, acc: 1.000000]\n",
            "1389: [discriminator loss: 0.7680428624153137, acc: 0.47265625] [gan loss: 1.035147, acc: 0.007812]\n",
            "1390: [discriminator loss: 0.7142549753189087, acc: 0.525390625] [gan loss: 0.490524, acc: 0.980469]\n",
            "1391: [discriminator loss: 0.7528415322303772, acc: 0.4775390625] [gan loss: 1.048532, acc: 0.000000]\n",
            "1392: [discriminator loss: 0.7165629863739014, acc: 0.548828125] [gan loss: 0.474018, acc: 0.980469]\n",
            "1393: [discriminator loss: 0.7533653378486633, acc: 0.4833984375] [gan loss: 1.074384, acc: 0.000000]\n",
            "1394: [discriminator loss: 0.7113109827041626, acc: 0.5341796875] [gan loss: 0.495017, acc: 0.953125]\n",
            "1395: [discriminator loss: 0.7448617815971375, acc: 0.4931640625] [gan loss: 1.066920, acc: 0.000000]\n",
            "1396: [discriminator loss: 0.7141075134277344, acc: 0.552734375] [gan loss: 0.527907, acc: 0.968750]\n",
            "1397: [discriminator loss: 0.7418139576911926, acc: 0.4853515625] [gan loss: 1.086717, acc: 0.000000]\n",
            "1398: [discriminator loss: 0.6989003419876099, acc: 0.5673828125] [gan loss: 0.529005, acc: 0.949219]\n",
            "1399: [discriminator loss: 0.7415928840637207, acc: 0.4794921875] [gan loss: 1.103830, acc: 0.000000]\n",
            "1400: [discriminator loss: 0.7139459848403931, acc: 0.541015625] [gan loss: 0.510255, acc: 0.919922]\n",
            "1401: [discriminator loss: 0.7482020258903503, acc: 0.4970703125] [gan loss: 1.108745, acc: 0.000000]\n",
            "1402: [discriminator loss: 0.7109853625297546, acc: 0.5322265625] [gan loss: 0.503260, acc: 0.927734]\n",
            "1403: [discriminator loss: 0.7423085570335388, acc: 0.5068359375] [gan loss: 1.091058, acc: 0.000000]\n",
            "1404: [discriminator loss: 0.699201762676239, acc: 0.576171875] [gan loss: 0.536378, acc: 0.876953]\n",
            "1405: [discriminator loss: 0.7340660691261292, acc: 0.51953125] [gan loss: 1.048889, acc: 0.000000]\n",
            "1406: [discriminator loss: 0.7001032829284668, acc: 0.5947265625] [gan loss: 0.549892, acc: 0.830078]\n",
            "1407: [discriminator loss: 0.7331343293190002, acc: 0.5234375] [gan loss: 1.070709, acc: 0.000000]\n",
            "1408: [discriminator loss: 0.6918603777885437, acc: 0.5849609375] [gan loss: 0.547715, acc: 0.865234]\n",
            "1409: [discriminator loss: 0.719706654548645, acc: 0.548828125] [gan loss: 1.112642, acc: 0.000000]\n",
            "1410: [discriminator loss: 0.6942225694656372, acc: 0.59765625] [gan loss: 0.553257, acc: 0.849609]\n",
            "1411: [discriminator loss: 0.7307328581809998, acc: 0.5302734375] [gan loss: 1.070188, acc: 0.000000]\n",
            "1412: [discriminator loss: 0.7104010581970215, acc: 0.556640625] [gan loss: 0.540571, acc: 0.824219]\n",
            "1413: [discriminator loss: 0.756345272064209, acc: 0.505859375] [gan loss: 1.093993, acc: 0.000000]\n",
            "1414: [discriminator loss: 0.7052367329597473, acc: 0.57421875] [gan loss: 0.556626, acc: 0.826172]\n",
            "1415: [discriminator loss: 0.7480787038803101, acc: 0.48046875] [gan loss: 1.084256, acc: 0.000000]\n",
            "1416: [discriminator loss: 0.6999333500862122, acc: 0.5869140625] [gan loss: 0.542749, acc: 0.892578]\n",
            "1417: [discriminator loss: 0.7441815137863159, acc: 0.4814453125] [gan loss: 1.063242, acc: 0.000000]\n",
            "1418: [discriminator loss: 0.7097674012184143, acc: 0.5537109375] [gan loss: 0.570430, acc: 0.843750]\n",
            "1419: [discriminator loss: 0.7339767217636108, acc: 0.4912109375] [gan loss: 1.075127, acc: 0.000000]\n",
            "1420: [discriminator loss: 0.7011500000953674, acc: 0.5615234375] [gan loss: 0.554278, acc: 0.890625]\n",
            "1421: [discriminator loss: 0.7404518127441406, acc: 0.482421875] [gan loss: 1.124061, acc: 0.000000]\n",
            "1422: [discriminator loss: 0.7070316076278687, acc: 0.591796875] [gan loss: 0.551225, acc: 0.964844]\n",
            "1423: [discriminator loss: 0.7313529849052429, acc: 0.4892578125] [gan loss: 1.095576, acc: 0.000000]\n",
            "1424: [discriminator loss: 0.7028518915176392, acc: 0.5888671875] [gan loss: 0.551060, acc: 0.968750]\n",
            "1425: [discriminator loss: 0.7284336686134338, acc: 0.4931640625] [gan loss: 1.093744, acc: 0.000000]\n",
            "1426: [discriminator loss: 0.688788652420044, acc: 0.5947265625] [gan loss: 0.576243, acc: 0.974609]\n",
            "1427: [discriminator loss: 0.7110068798065186, acc: 0.4892578125] [gan loss: 1.062163, acc: 0.000000]\n",
            "1428: [discriminator loss: 0.6773720383644104, acc: 0.6279296875] [gan loss: 0.585861, acc: 0.912109]\n",
            "1429: [discriminator loss: 0.7059974074363708, acc: 0.4794921875] [gan loss: 1.073614, acc: 0.000000]\n",
            "1430: [discriminator loss: 0.6812711358070374, acc: 0.619140625] [gan loss: 0.566294, acc: 0.947266]\n",
            "1431: [discriminator loss: 0.7019330263137817, acc: 0.4912109375] [gan loss: 1.096002, acc: 0.000000]\n",
            "1432: [discriminator loss: 0.6756797432899475, acc: 0.626953125] [gan loss: 0.559617, acc: 0.935547]\n",
            "1433: [discriminator loss: 0.7049963474273682, acc: 0.484375] [gan loss: 1.125017, acc: 0.000000]\n",
            "1434: [discriminator loss: 0.6747053861618042, acc: 0.619140625] [gan loss: 0.546147, acc: 0.939453]\n",
            "1435: [discriminator loss: 0.7094345092773438, acc: 0.49609375] [gan loss: 1.102601, acc: 0.000000]\n",
            "1436: [discriminator loss: 0.6743183135986328, acc: 0.6083984375] [gan loss: 0.548552, acc: 0.945312]\n",
            "1437: [discriminator loss: 0.7095338106155396, acc: 0.482421875] [gan loss: 1.049945, acc: 0.000000]\n",
            "1438: [discriminator loss: 0.6874635219573975, acc: 0.5888671875] [gan loss: 0.545478, acc: 0.958984]\n",
            "1439: [discriminator loss: 0.7206605076789856, acc: 0.4814453125] [gan loss: 1.014818, acc: 0.000000]\n",
            "1440: [discriminator loss: 0.6895618438720703, acc: 0.6015625] [gan loss: 0.576806, acc: 0.914062]\n",
            "1441: [discriminator loss: 0.7170585989952087, acc: 0.48046875] [gan loss: 1.019090, acc: 0.000000]\n",
            "1442: [discriminator loss: 0.6994650363922119, acc: 0.576171875] [gan loss: 0.545469, acc: 0.955078]\n",
            "1443: [discriminator loss: 0.7462453246116638, acc: 0.4599609375] [gan loss: 0.997286, acc: 0.013672]\n",
            "1444: [discriminator loss: 0.7132567763328552, acc: 0.556640625] [gan loss: 0.516210, acc: 0.958984]\n",
            "1445: [discriminator loss: 0.7496968507766724, acc: 0.4716796875] [gan loss: 0.992353, acc: 0.017578]\n",
            "1446: [discriminator loss: 0.7177172899246216, acc: 0.55078125] [gan loss: 0.492565, acc: 0.990234]\n",
            "1447: [discriminator loss: 0.7603363394737244, acc: 0.4736328125] [gan loss: 0.981943, acc: 0.000000]\n",
            "1448: [discriminator loss: 0.7293205261230469, acc: 0.525390625] [gan loss: 0.493314, acc: 0.994141]\n",
            "1449: [discriminator loss: 0.7732637524604797, acc: 0.4658203125] [gan loss: 0.966186, acc: 0.009766]\n",
            "1450: [discriminator loss: 0.7303609251976013, acc: 0.5400390625] [gan loss: 0.512022, acc: 1.000000]\n",
            "1451: [discriminator loss: 0.7664151191711426, acc: 0.458984375] [gan loss: 0.961839, acc: 0.068359]\n",
            "1452: [discriminator loss: 0.7367008924484253, acc: 0.5322265625] [gan loss: 0.532108, acc: 0.996094]\n",
            "1453: [discriminator loss: 0.7688995599746704, acc: 0.4404296875] [gan loss: 0.961842, acc: 0.058594]\n",
            "1454: [discriminator loss: 0.7380456924438477, acc: 0.50390625] [gan loss: 0.520330, acc: 0.996094]\n",
            "1455: [discriminator loss: 0.7833707332611084, acc: 0.455078125] [gan loss: 0.993509, acc: 0.031250]\n",
            "1456: [discriminator loss: 0.7343921661376953, acc: 0.50390625] [gan loss: 0.506727, acc: 0.994141]\n",
            "1457: [discriminator loss: 0.7755148410797119, acc: 0.474609375] [gan loss: 1.016939, acc: 0.041016]\n",
            "1458: [discriminator loss: 0.7293788194656372, acc: 0.4970703125] [gan loss: 0.464464, acc: 1.000000]\n",
            "1459: [discriminator loss: 0.7837317585945129, acc: 0.484375] [gan loss: 1.027602, acc: 0.000000]\n",
            "1460: [discriminator loss: 0.7323949933052063, acc: 0.50390625] [gan loss: 0.489806, acc: 0.996094]\n",
            "1461: [discriminator loss: 0.776120126247406, acc: 0.48828125] [gan loss: 0.996395, acc: 0.000000]\n",
            "1462: [discriminator loss: 0.7302372455596924, acc: 0.5087890625] [gan loss: 0.522430, acc: 0.986328]\n",
            "1463: [discriminator loss: 0.7658097147941589, acc: 0.478515625] [gan loss: 0.975105, acc: 0.000000]\n",
            "1464: [discriminator loss: 0.7235659956932068, acc: 0.4990234375] [gan loss: 0.561843, acc: 0.935547]\n",
            "1465: [discriminator loss: 0.7414199113845825, acc: 0.486328125] [gan loss: 0.989724, acc: 0.001953]\n",
            "1466: [discriminator loss: 0.6946496963500977, acc: 0.529296875] [gan loss: 0.569047, acc: 0.871094]\n",
            "1467: [discriminator loss: 0.7283558249473572, acc: 0.5048828125] [gan loss: 1.013925, acc: 0.000000]\n",
            "1468: [discriminator loss: 0.6830236315727234, acc: 0.5712890625] [gan loss: 0.588021, acc: 0.876953]\n",
            "1469: [discriminator loss: 0.7125252485275269, acc: 0.509765625] [gan loss: 1.068925, acc: 0.000000]\n",
            "1470: [discriminator loss: 0.6799702644348145, acc: 0.5556640625] [gan loss: 0.589774, acc: 0.886719]\n",
            "1471: [discriminator loss: 0.7049439549446106, acc: 0.5185546875] [gan loss: 1.099868, acc: 0.000000]\n",
            "1472: [discriminator loss: 0.6727867126464844, acc: 0.572265625] [gan loss: 0.563155, acc: 0.917969]\n",
            "1473: [discriminator loss: 0.7088297009468079, acc: 0.5009765625] [gan loss: 1.137911, acc: 0.000000]\n",
            "1474: [discriminator loss: 0.6577707529067993, acc: 0.576171875] [gan loss: 0.546524, acc: 0.949219]\n",
            "1475: [discriminator loss: 0.7029023170471191, acc: 0.509765625] [gan loss: 1.128401, acc: 0.000000]\n",
            "1476: [discriminator loss: 0.6579959392547607, acc: 0.591796875] [gan loss: 0.578964, acc: 0.876953]\n",
            "1477: [discriminator loss: 0.6995775699615479, acc: 0.5107421875] [gan loss: 1.105823, acc: 0.000000]\n",
            "1478: [discriminator loss: 0.6594633460044861, acc: 0.6064453125] [gan loss: 0.612849, acc: 0.775391]\n",
            "1479: [discriminator loss: 0.6865132451057434, acc: 0.51953125] [gan loss: 1.066438, acc: 0.000000]\n",
            "1480: [discriminator loss: 0.6598203778266907, acc: 0.62890625] [gan loss: 0.582233, acc: 0.882812]\n",
            "1481: [discriminator loss: 0.6899335384368896, acc: 0.5] [gan loss: 1.068662, acc: 0.000000]\n",
            "1482: [discriminator loss: 0.6693680286407471, acc: 0.609375] [gan loss: 0.604243, acc: 0.791016]\n",
            "1483: [discriminator loss: 0.6960105895996094, acc: 0.5107421875] [gan loss: 1.092808, acc: 0.000000]\n",
            "1484: [discriminator loss: 0.669532299041748, acc: 0.6142578125] [gan loss: 0.563301, acc: 0.865234]\n",
            "1485: [discriminator loss: 0.7105756402015686, acc: 0.50390625] [gan loss: 1.081037, acc: 0.000000]\n",
            "1486: [discriminator loss: 0.6845666766166687, acc: 0.5849609375] [gan loss: 0.569560, acc: 0.859375]\n",
            "1487: [discriminator loss: 0.7085117101669312, acc: 0.49609375] [gan loss: 1.062342, acc: 0.000000]\n",
            "1488: [discriminator loss: 0.6738308668136597, acc: 0.6220703125] [gan loss: 0.590580, acc: 0.824219]\n",
            "1489: [discriminator loss: 0.7071547508239746, acc: 0.4990234375] [gan loss: 1.074965, acc: 0.000000]\n",
            "1490: [discriminator loss: 0.6831152439117432, acc: 0.5859375] [gan loss: 0.563401, acc: 0.917969]\n",
            "1491: [discriminator loss: 0.712295413017273, acc: 0.4990234375] [gan loss: 1.013103, acc: 0.003906]\n",
            "1492: [discriminator loss: 0.6800009608268738, acc: 0.5498046875] [gan loss: 0.581546, acc: 0.916016]\n",
            "1493: [discriminator loss: 0.717040479183197, acc: 0.484375] [gan loss: 0.990076, acc: 0.021484]\n",
            "1494: [discriminator loss: 0.6824571490287781, acc: 0.5615234375] [gan loss: 0.588827, acc: 0.908203]\n",
            "1495: [discriminator loss: 0.7261742353439331, acc: 0.4619140625] [gan loss: 1.000676, acc: 0.000000]\n",
            "1496: [discriminator loss: 0.6862308979034424, acc: 0.5546875] [gan loss: 0.563343, acc: 0.953125]\n",
            "1497: [discriminator loss: 0.7213618159294128, acc: 0.4775390625] [gan loss: 1.065234, acc: 0.000000]\n",
            "1498: [discriminator loss: 0.6888760328292847, acc: 0.5986328125] [gan loss: 0.534764, acc: 0.998047]\n",
            "1499: [discriminator loss: 0.7230798602104187, acc: 0.4736328125] [gan loss: 1.076957, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "1500: [discriminator loss: 0.6821397542953491, acc: 0.60546875] [gan loss: 0.526917, acc: 1.000000]\n",
            "1501: [discriminator loss: 0.7318180799484253, acc: 0.4814453125] [gan loss: 1.069261, acc: 0.000000]\n",
            "1502: [discriminator loss: 0.6786901950836182, acc: 0.5966796875] [gan loss: 0.537652, acc: 1.000000]\n",
            "1503: [discriminator loss: 0.7150289416313171, acc: 0.4912109375] [gan loss: 1.072121, acc: 0.000000]\n",
            "1504: [discriminator loss: 0.6677330732345581, acc: 0.6279296875] [gan loss: 0.556236, acc: 1.000000]\n",
            "1505: [discriminator loss: 0.7139490842819214, acc: 0.4736328125] [gan loss: 1.048339, acc: 0.000000]\n",
            "1506: [discriminator loss: 0.6734851002693176, acc: 0.6318359375] [gan loss: 0.580075, acc: 0.976562]\n",
            "1507: [discriminator loss: 0.6978667974472046, acc: 0.4775390625] [gan loss: 1.005544, acc: 0.000000]\n",
            "1508: [discriminator loss: 0.6595736145973206, acc: 0.6767578125] [gan loss: 0.580340, acc: 0.929688]\n",
            "1509: [discriminator loss: 0.7021800875663757, acc: 0.4814453125] [gan loss: 1.021106, acc: 0.000000]\n",
            "1510: [discriminator loss: 0.6669400930404663, acc: 0.634765625] [gan loss: 0.555704, acc: 0.996094]\n",
            "1511: [discriminator loss: 0.7076377272605896, acc: 0.4833984375] [gan loss: 0.998374, acc: 0.000000]\n",
            "1512: [discriminator loss: 0.6648527979850769, acc: 0.654296875] [gan loss: 0.566452, acc: 0.958984]\n",
            "1513: [discriminator loss: 0.7025146484375, acc: 0.4873046875] [gan loss: 0.998991, acc: 0.000000]\n",
            "1514: [discriminator loss: 0.6623908281326294, acc: 0.65234375] [gan loss: 0.546284, acc: 0.988281]\n",
            "1515: [discriminator loss: 0.697068452835083, acc: 0.486328125] [gan loss: 0.991830, acc: 0.000000]\n",
            "1516: [discriminator loss: 0.667718768119812, acc: 0.62890625] [gan loss: 0.518885, acc: 0.994141]\n",
            "1517: [discriminator loss: 0.7117743492126465, acc: 0.4892578125] [gan loss: 1.005924, acc: 0.000000]\n",
            "1518: [discriminator loss: 0.6643664240837097, acc: 0.6533203125] [gan loss: 0.540392, acc: 0.994141]\n",
            "1519: [discriminator loss: 0.7066972255706787, acc: 0.4833984375] [gan loss: 1.011364, acc: 0.000000]\n",
            "1520: [discriminator loss: 0.6616035103797913, acc: 0.6474609375] [gan loss: 0.518812, acc: 0.990234]\n",
            "1521: [discriminator loss: 0.7234461307525635, acc: 0.4853515625] [gan loss: 1.063622, acc: 0.000000]\n",
            "1522: [discriminator loss: 0.6624912619590759, acc: 0.615234375] [gan loss: 0.489986, acc: 1.000000]\n",
            "1523: [discriminator loss: 0.7168188095092773, acc: 0.49609375] [gan loss: 1.012570, acc: 0.000000]\n",
            "1524: [discriminator loss: 0.6726830005645752, acc: 0.5859375] [gan loss: 0.547334, acc: 0.978516]\n",
            "1525: [discriminator loss: 0.6989114284515381, acc: 0.478515625] [gan loss: 0.933408, acc: 0.082031]\n",
            "1526: [discriminator loss: 0.6715473532676697, acc: 0.5615234375] [gan loss: 0.569707, acc: 0.888672]\n",
            "1527: [discriminator loss: 0.7099994421005249, acc: 0.50390625] [gan loss: 0.989166, acc: 0.064453]\n",
            "1528: [discriminator loss: 0.6680033206939697, acc: 0.5693359375] [gan loss: 0.514923, acc: 0.943359]\n",
            "1529: [discriminator loss: 0.7195885181427002, acc: 0.494140625] [gan loss: 0.925670, acc: 0.134766]\n",
            "1530: [discriminator loss: 0.6948091983795166, acc: 0.498046875] [gan loss: 0.483678, acc: 0.958984]\n",
            "1531: [discriminator loss: 0.75230473279953, acc: 0.4794921875] [gan loss: 0.946660, acc: 0.085938]\n",
            "1532: [discriminator loss: 0.7015511989593506, acc: 0.509765625] [gan loss: 0.470741, acc: 1.000000]\n",
            "1533: [discriminator loss: 0.7627317309379578, acc: 0.46875] [gan loss: 0.978805, acc: 0.029297]\n",
            "1534: [discriminator loss: 0.7018028497695923, acc: 0.5283203125] [gan loss: 0.449619, acc: 1.000000]\n",
            "1535: [discriminator loss: 0.7694188952445984, acc: 0.482421875] [gan loss: 1.024896, acc: 0.000000]\n",
            "1536: [discriminator loss: 0.7005280256271362, acc: 0.525390625] [gan loss: 0.480108, acc: 0.986328]\n",
            "1537: [discriminator loss: 0.7518364787101746, acc: 0.47265625] [gan loss: 0.974950, acc: 0.019531]\n",
            "1538: [discriminator loss: 0.6975444555282593, acc: 0.5283203125] [gan loss: 0.537713, acc: 0.937500]\n",
            "1539: [discriminator loss: 0.7337584495544434, acc: 0.4599609375] [gan loss: 1.030611, acc: 0.031250]\n",
            "1540: [discriminator loss: 0.6972790956497192, acc: 0.525390625] [gan loss: 0.532807, acc: 0.931641]\n",
            "1541: [discriminator loss: 0.7317625284194946, acc: 0.4638671875] [gan loss: 0.979938, acc: 0.048828]\n",
            "1542: [discriminator loss: 0.6894288659095764, acc: 0.5791015625] [gan loss: 0.559936, acc: 0.875000]\n",
            "1543: [discriminator loss: 0.7257648706436157, acc: 0.45703125] [gan loss: 0.974186, acc: 0.042969]\n",
            "1544: [discriminator loss: 0.6972076892852783, acc: 0.5224609375] [gan loss: 0.528927, acc: 0.974609]\n",
            "1545: [discriminator loss: 0.71458899974823, acc: 0.474609375] [gan loss: 0.998913, acc: 0.033203]\n",
            "1546: [discriminator loss: 0.6951347589492798, acc: 0.5322265625] [gan loss: 0.548293, acc: 0.968750]\n",
            "1547: [discriminator loss: 0.7279190421104431, acc: 0.46484375] [gan loss: 1.040290, acc: 0.005859]\n",
            "1548: [discriminator loss: 0.6909517049789429, acc: 0.5419921875] [gan loss: 0.532852, acc: 0.990234]\n",
            "1549: [discriminator loss: 0.7116683125495911, acc: 0.4755859375] [gan loss: 1.010172, acc: 0.005859]\n",
            "1550: [discriminator loss: 0.6805866360664368, acc: 0.548828125] [gan loss: 0.537456, acc: 0.988281]\n",
            "1551: [discriminator loss: 0.7271307110786438, acc: 0.4892578125] [gan loss: 0.989890, acc: 0.015625]\n",
            "1552: [discriminator loss: 0.6931938529014587, acc: 0.51171875] [gan loss: 0.519363, acc: 1.000000]\n",
            "1553: [discriminator loss: 0.7070273160934448, acc: 0.486328125] [gan loss: 0.954865, acc: 0.070312]\n",
            "1554: [discriminator loss: 0.6817364692687988, acc: 0.5498046875] [gan loss: 0.540925, acc: 0.996094]\n",
            "1555: [discriminator loss: 0.7100270986557007, acc: 0.46875] [gan loss: 1.004206, acc: 0.068359]\n",
            "1556: [discriminator loss: 0.672505795955658, acc: 0.5615234375] [gan loss: 0.517073, acc: 0.996094]\n",
            "1557: [discriminator loss: 0.7140579223632812, acc: 0.4892578125] [gan loss: 0.981302, acc: 0.041016]\n",
            "1558: [discriminator loss: 0.6685677766799927, acc: 0.5673828125] [gan loss: 0.504023, acc: 0.998047]\n",
            "1559: [discriminator loss: 0.7124535441398621, acc: 0.4892578125] [gan loss: 0.976803, acc: 0.056641]\n",
            "1560: [discriminator loss: 0.6739038228988647, acc: 0.564453125] [gan loss: 0.548327, acc: 0.945312]\n",
            "1561: [discriminator loss: 0.7092353105545044, acc: 0.48828125] [gan loss: 0.994958, acc: 0.021484]\n",
            "1562: [discriminator loss: 0.6766624450683594, acc: 0.5576171875] [gan loss: 0.527613, acc: 0.980469]\n",
            "1563: [discriminator loss: 0.7320511341094971, acc: 0.482421875] [gan loss: 1.050613, acc: 0.000000]\n",
            "1564: [discriminator loss: 0.6752089858055115, acc: 0.5546875] [gan loss: 0.521667, acc: 0.982422]\n",
            "1565: [discriminator loss: 0.7194846868515015, acc: 0.48828125] [gan loss: 0.999249, acc: 0.000000]\n",
            "1566: [discriminator loss: 0.6957212090492249, acc: 0.5224609375] [gan loss: 0.568178, acc: 0.955078]\n",
            "1567: [discriminator loss: 0.7347944378852844, acc: 0.4716796875] [gan loss: 1.003169, acc: 0.000000]\n",
            "1568: [discriminator loss: 0.7001668810844421, acc: 0.560546875] [gan loss: 0.557048, acc: 0.982422]\n",
            "1569: [discriminator loss: 0.7272156476974487, acc: 0.470703125] [gan loss: 0.987028, acc: 0.000000]\n",
            "1570: [discriminator loss: 0.6899510025978088, acc: 0.6181640625] [gan loss: 0.582884, acc: 0.955078]\n",
            "1571: [discriminator loss: 0.7244756817817688, acc: 0.4541015625] [gan loss: 0.993525, acc: 0.000000]\n",
            "1572: [discriminator loss: 0.6914230585098267, acc: 0.599609375] [gan loss: 0.599264, acc: 0.902344]\n",
            "1573: [discriminator loss: 0.7205345630645752, acc: 0.45703125] [gan loss: 1.000782, acc: 0.000000]\n",
            "1574: [discriminator loss: 0.6853085160255432, acc: 0.638671875] [gan loss: 0.594013, acc: 0.869141]\n",
            "1575: [discriminator loss: 0.7286150455474854, acc: 0.45703125] [gan loss: 1.040294, acc: 0.000000]\n",
            "1576: [discriminator loss: 0.6961129903793335, acc: 0.599609375] [gan loss: 0.561318, acc: 0.949219]\n",
            "1577: [discriminator loss: 0.7240936756134033, acc: 0.46875] [gan loss: 1.048202, acc: 0.000000]\n",
            "1578: [discriminator loss: 0.699156641960144, acc: 0.5927734375] [gan loss: 0.560347, acc: 0.949219]\n",
            "1579: [discriminator loss: 0.7203560471534729, acc: 0.45703125] [gan loss: 1.008396, acc: 0.000000]\n",
            "1580: [discriminator loss: 0.6915717124938965, acc: 0.595703125] [gan loss: 0.524843, acc: 0.998047]\n",
            "1581: [discriminator loss: 0.7285362482070923, acc: 0.4765625] [gan loss: 1.059586, acc: 0.000000]\n",
            "1582: [discriminator loss: 0.6927070617675781, acc: 0.619140625] [gan loss: 0.513229, acc: 0.986328]\n",
            "1583: [discriminator loss: 0.7279448509216309, acc: 0.478515625] [gan loss: 0.990683, acc: 0.000000]\n",
            "1584: [discriminator loss: 0.6874953508377075, acc: 0.61328125] [gan loss: 0.585222, acc: 0.939453]\n",
            "1585: [discriminator loss: 0.7159478664398193, acc: 0.4482421875] [gan loss: 0.950147, acc: 0.000000]\n",
            "1586: [discriminator loss: 0.6768594980239868, acc: 0.638671875] [gan loss: 0.559022, acc: 0.966797]\n",
            "1587: [discriminator loss: 0.7140976786613464, acc: 0.4736328125] [gan loss: 0.971044, acc: 0.000000]\n",
            "1588: [discriminator loss: 0.6906223893165588, acc: 0.5908203125] [gan loss: 0.550895, acc: 0.970703]\n",
            "1589: [discriminator loss: 0.7110018134117126, acc: 0.4755859375] [gan loss: 0.967468, acc: 0.000000]\n",
            "1590: [discriminator loss: 0.68867427110672, acc: 0.609375] [gan loss: 0.550591, acc: 0.982422]\n",
            "1591: [discriminator loss: 0.7232848405838013, acc: 0.4716796875] [gan loss: 0.990739, acc: 0.000000]\n",
            "1592: [discriminator loss: 0.6791368722915649, acc: 0.6171875] [gan loss: 0.539586, acc: 0.978516]\n",
            "1593: [discriminator loss: 0.7207329869270325, acc: 0.4873046875] [gan loss: 0.997661, acc: 0.000000]\n",
            "1594: [discriminator loss: 0.6750897169113159, acc: 0.60546875] [gan loss: 0.542293, acc: 0.986328]\n",
            "1595: [discriminator loss: 0.7209868431091309, acc: 0.478515625] [gan loss: 0.982676, acc: 0.000000]\n",
            "1596: [discriminator loss: 0.6777657270431519, acc: 0.583984375] [gan loss: 0.567370, acc: 0.978516]\n",
            "1597: [discriminator loss: 0.7155693173408508, acc: 0.482421875] [gan loss: 0.957351, acc: 0.000000]\n",
            "1598: [discriminator loss: 0.6820875406265259, acc: 0.60546875] [gan loss: 0.585539, acc: 0.941406]\n",
            "1599: [discriminator loss: 0.7079411745071411, acc: 0.486328125] [gan loss: 0.973171, acc: 0.000000]\n",
            "1600: [discriminator loss: 0.6766787171363831, acc: 0.6123046875] [gan loss: 0.545987, acc: 0.990234]\n",
            "1601: [discriminator loss: 0.7138361930847168, acc: 0.49609375] [gan loss: 0.996889, acc: 0.000000]\n",
            "1602: [discriminator loss: 0.6750398874282837, acc: 0.5986328125] [gan loss: 0.531941, acc: 1.000000]\n",
            "1603: [discriminator loss: 0.7298961281776428, acc: 0.4853515625] [gan loss: 1.062971, acc: 0.000000]\n",
            "1604: [discriminator loss: 0.6869494915008545, acc: 0.5439453125] [gan loss: 0.518986, acc: 0.994141]\n",
            "1605: [discriminator loss: 0.7253198027610779, acc: 0.49609375] [gan loss: 0.998712, acc: 0.000000]\n",
            "1606: [discriminator loss: 0.6805238127708435, acc: 0.58203125] [gan loss: 0.581535, acc: 0.970703]\n",
            "1607: [discriminator loss: 0.7022632956504822, acc: 0.4833984375] [gan loss: 0.935731, acc: 0.000000]\n",
            "1608: [discriminator loss: 0.672022819519043, acc: 0.6298828125] [gan loss: 0.585880, acc: 0.970703]\n",
            "1609: [discriminator loss: 0.7069176435470581, acc: 0.48046875] [gan loss: 0.959215, acc: 0.000000]\n",
            "1610: [discriminator loss: 0.6686822175979614, acc: 0.591796875] [gan loss: 0.562824, acc: 0.968750]\n",
            "1611: [discriminator loss: 0.7175679802894592, acc: 0.48828125] [gan loss: 1.012793, acc: 0.000000]\n",
            "1612: [discriminator loss: 0.681452214717865, acc: 0.5595703125] [gan loss: 0.558299, acc: 0.994141]\n",
            "1613: [discriminator loss: 0.7109956741333008, acc: 0.4931640625] [gan loss: 0.971169, acc: 0.000000]\n",
            "1614: [discriminator loss: 0.6772375106811523, acc: 0.595703125] [gan loss: 0.578287, acc: 0.982422]\n",
            "1615: [discriminator loss: 0.7083245515823364, acc: 0.4892578125] [gan loss: 0.955561, acc: 0.001953]\n",
            "1616: [discriminator loss: 0.6792912483215332, acc: 0.59765625] [gan loss: 0.591184, acc: 0.941406]\n",
            "1617: [discriminator loss: 0.7124575972557068, acc: 0.4775390625] [gan loss: 0.976991, acc: 0.001953]\n",
            "1618: [discriminator loss: 0.6754180788993835, acc: 0.6181640625] [gan loss: 0.558708, acc: 0.984375]\n",
            "1619: [discriminator loss: 0.7330259084701538, acc: 0.48828125] [gan loss: 0.987731, acc: 0.000000]\n",
            "1620: [discriminator loss: 0.695288360118866, acc: 0.5654296875] [gan loss: 0.556933, acc: 0.984375]\n",
            "1621: [discriminator loss: 0.7332673072814941, acc: 0.478515625] [gan loss: 0.929712, acc: 0.001953]\n",
            "1622: [discriminator loss: 0.6937487721443176, acc: 0.5810546875] [gan loss: 0.571521, acc: 0.964844]\n",
            "1623: [discriminator loss: 0.7306742072105408, acc: 0.4814453125] [gan loss: 0.920886, acc: 0.000000]\n",
            "1624: [discriminator loss: 0.6951558589935303, acc: 0.533203125] [gan loss: 0.580980, acc: 0.882812]\n",
            "1625: [discriminator loss: 0.7242909669876099, acc: 0.4853515625] [gan loss: 0.922244, acc: 0.000000]\n",
            "1626: [discriminator loss: 0.7004949450492859, acc: 0.5048828125] [gan loss: 0.569808, acc: 0.875000]\n",
            "1627: [discriminator loss: 0.7284272909164429, acc: 0.4853515625] [gan loss: 0.951520, acc: 0.000000]\n",
            "1628: [discriminator loss: 0.6913295388221741, acc: 0.53125] [gan loss: 0.533808, acc: 0.986328]\n",
            "1629: [discriminator loss: 0.7347285747528076, acc: 0.490234375] [gan loss: 0.956622, acc: 0.000000]\n",
            "1630: [discriminator loss: 0.6992133259773254, acc: 0.505859375] [gan loss: 0.530197, acc: 0.994141]\n",
            "1631: [discriminator loss: 0.7384291887283325, acc: 0.4892578125] [gan loss: 0.948697, acc: 0.009766]\n",
            "1632: [discriminator loss: 0.6995460987091064, acc: 0.505859375] [gan loss: 0.542115, acc: 0.988281]\n",
            "1633: [discriminator loss: 0.7354312539100647, acc: 0.4892578125] [gan loss: 0.921741, acc: 0.009766]\n",
            "1634: [discriminator loss: 0.6926673650741577, acc: 0.4794921875] [gan loss: 0.554441, acc: 0.939453]\n",
            "1635: [discriminator loss: 0.7323095202445984, acc: 0.48828125] [gan loss: 0.894601, acc: 0.000000]\n",
            "1636: [discriminator loss: 0.7002529501914978, acc: 0.5322265625] [gan loss: 0.570381, acc: 0.945312]\n",
            "1637: [discriminator loss: 0.7307012677192688, acc: 0.46875] [gan loss: 0.932459, acc: 0.000000]\n",
            "1638: [discriminator loss: 0.6811155676841736, acc: 0.5732421875] [gan loss: 0.523006, acc: 1.000000]\n",
            "1639: [discriminator loss: 0.7430064678192139, acc: 0.4931640625] [gan loss: 0.969044, acc: 0.000000]\n",
            "1640: [discriminator loss: 0.6911113262176514, acc: 0.5439453125] [gan loss: 0.516623, acc: 0.998047]\n",
            "1641: [discriminator loss: 0.738685667514801, acc: 0.4970703125] [gan loss: 0.925051, acc: 0.000000]\n",
            "1642: [discriminator loss: 0.6975573301315308, acc: 0.5625] [gan loss: 0.556252, acc: 0.992188]\n",
            "1643: [discriminator loss: 0.7293612360954285, acc: 0.482421875] [gan loss: 0.891589, acc: 0.000000]\n",
            "1644: [discriminator loss: 0.6988352537155151, acc: 0.5654296875] [gan loss: 0.564642, acc: 0.976562]\n",
            "1645: [discriminator loss: 0.7395637631416321, acc: 0.4814453125] [gan loss: 0.915471, acc: 0.000000]\n",
            "1646: [discriminator loss: 0.695052444934845, acc: 0.5537109375] [gan loss: 0.560028, acc: 0.986328]\n",
            "1647: [discriminator loss: 0.7242779731750488, acc: 0.482421875] [gan loss: 0.888327, acc: 0.000000]\n",
            "1648: [discriminator loss: 0.7034636735916138, acc: 0.5703125] [gan loss: 0.579179, acc: 0.968750]\n",
            "1649: [discriminator loss: 0.7193074226379395, acc: 0.4833984375] [gan loss: 0.881838, acc: 0.000000]\n",
            "1650: [discriminator loss: 0.6887224912643433, acc: 0.6025390625] [gan loss: 0.574629, acc: 0.990234]\n",
            "1651: [discriminator loss: 0.7304244041442871, acc: 0.4873046875] [gan loss: 0.921636, acc: 0.000000]\n",
            "1652: [discriminator loss: 0.6902023553848267, acc: 0.5751953125] [gan loss: 0.528190, acc: 0.998047]\n",
            "1653: [discriminator loss: 0.7345572113990784, acc: 0.4921875] [gan loss: 0.933134, acc: 0.000000]\n",
            "1654: [discriminator loss: 0.6843507289886475, acc: 0.55859375] [gan loss: 0.519948, acc: 1.000000]\n",
            "1655: [discriminator loss: 0.7329477071762085, acc: 0.4921875] [gan loss: 0.914231, acc: 0.007812]\n",
            "1656: [discriminator loss: 0.7008488774299622, acc: 0.5224609375] [gan loss: 0.542869, acc: 0.974609]\n",
            "1657: [discriminator loss: 0.7352201342582703, acc: 0.484375] [gan loss: 0.870591, acc: 0.027344]\n",
            "1658: [discriminator loss: 0.7030184268951416, acc: 0.521484375] [gan loss: 0.586855, acc: 0.921875]\n",
            "1659: [discriminator loss: 0.7231360673904419, acc: 0.470703125] [gan loss: 0.856013, acc: 0.039062]\n",
            "1660: [discriminator loss: 0.6888063549995422, acc: 0.5634765625] [gan loss: 0.565257, acc: 0.964844]\n",
            "1661: [discriminator loss: 0.7353057861328125, acc: 0.4814453125] [gan loss: 0.941448, acc: 0.003906]\n",
            "1662: [discriminator loss: 0.6825706958770752, acc: 0.560546875] [gan loss: 0.503995, acc: 0.988281]\n",
            "1663: [discriminator loss: 0.7453590631484985, acc: 0.49609375] [gan loss: 0.963311, acc: 0.000000]\n",
            "1664: [discriminator loss: 0.697577953338623, acc: 0.5380859375] [gan loss: 0.538227, acc: 0.982422]\n",
            "1665: [discriminator loss: 0.7297212481498718, acc: 0.4931640625] [gan loss: 0.888519, acc: 0.007812]\n",
            "1666: [discriminator loss: 0.6935915946960449, acc: 0.5498046875] [gan loss: 0.596697, acc: 0.935547]\n",
            "1667: [discriminator loss: 0.707808256149292, acc: 0.494140625] [gan loss: 0.823402, acc: 0.048828]\n",
            "1668: [discriminator loss: 0.6907450556755066, acc: 0.55078125] [gan loss: 0.636828, acc: 0.773438]\n",
            "1669: [discriminator loss: 0.7042666077613831, acc: 0.474609375] [gan loss: 0.821991, acc: 0.054688]\n",
            "1670: [discriminator loss: 0.6709299683570862, acc: 0.6181640625] [gan loss: 0.602677, acc: 0.935547]\n",
            "1671: [discriminator loss: 0.7144328355789185, acc: 0.482421875] [gan loss: 0.922058, acc: 0.003906]\n",
            "1672: [discriminator loss: 0.6727275848388672, acc: 0.5791015625] [gan loss: 0.569721, acc: 0.980469]\n",
            "1673: [discriminator loss: 0.7131118178367615, acc: 0.4921875] [gan loss: 0.955694, acc: 0.000000]\n",
            "1674: [discriminator loss: 0.6694931387901306, acc: 0.5869140625] [gan loss: 0.564471, acc: 0.992188]\n",
            "1675: [discriminator loss: 0.7168401479721069, acc: 0.494140625] [gan loss: 1.000601, acc: 0.000000]\n",
            "1676: [discriminator loss: 0.6681286096572876, acc: 0.5908203125] [gan loss: 0.555082, acc: 1.000000]\n",
            "1677: [discriminator loss: 0.7071205973625183, acc: 0.49609375] [gan loss: 0.969531, acc: 0.000000]\n",
            "1678: [discriminator loss: 0.6652234196662903, acc: 0.591796875] [gan loss: 0.585782, acc: 0.996094]\n",
            "1679: [discriminator loss: 0.6980831623077393, acc: 0.49609375] [gan loss: 0.924406, acc: 0.000000]\n",
            "1680: [discriminator loss: 0.6697683930397034, acc: 0.6123046875] [gan loss: 0.622211, acc: 0.886719]\n",
            "1681: [discriminator loss: 0.6881301999092102, acc: 0.4892578125] [gan loss: 0.890526, acc: 0.000000]\n",
            "1682: [discriminator loss: 0.6631788015365601, acc: 0.6396484375] [gan loss: 0.644757, acc: 0.751953]\n",
            "1683: [discriminator loss: 0.684410572052002, acc: 0.484375] [gan loss: 0.881509, acc: 0.000000]\n",
            "1684: [discriminator loss: 0.6647270321846008, acc: 0.6357421875] [gan loss: 0.659069, acc: 0.695312]\n",
            "1685: [discriminator loss: 0.6795563697814941, acc: 0.515625] [gan loss: 0.870275, acc: 0.000000]\n",
            "1686: [discriminator loss: 0.6680476069450378, acc: 0.630859375] [gan loss: 0.629611, acc: 0.783203]\n",
            "1687: [discriminator loss: 0.6986472606658936, acc: 0.4912109375] [gan loss: 0.956811, acc: 0.000000]\n",
            "1688: [discriminator loss: 0.662194013595581, acc: 0.6123046875] [gan loss: 0.536542, acc: 1.000000]\n",
            "1689: [discriminator loss: 0.7245018482208252, acc: 0.498046875] [gan loss: 1.029866, acc: 0.000000]\n",
            "1690: [discriminator loss: 0.6910883188247681, acc: 0.533203125] [gan loss: 0.526999, acc: 0.994141]\n",
            "1691: [discriminator loss: 0.7261909246444702, acc: 0.494140625] [gan loss: 0.914941, acc: 0.000000]\n",
            "1692: [discriminator loss: 0.6854292154312134, acc: 0.564453125] [gan loss: 0.558311, acc: 0.917969]\n",
            "1693: [discriminator loss: 0.7260525226593018, acc: 0.505859375] [gan loss: 0.900722, acc: 0.000000]\n",
            "1694: [discriminator loss: 0.687298595905304, acc: 0.5732421875] [gan loss: 0.573417, acc: 0.917969]\n",
            "1695: [discriminator loss: 0.7417343854904175, acc: 0.501953125] [gan loss: 0.896712, acc: 0.000000]\n",
            "1696: [discriminator loss: 0.6923012137413025, acc: 0.5361328125] [gan loss: 0.583749, acc: 0.931641]\n",
            "1697: [discriminator loss: 0.7307066917419434, acc: 0.48828125] [gan loss: 0.859544, acc: 0.007812]\n",
            "1698: [discriminator loss: 0.704779863357544, acc: 0.501953125] [gan loss: 0.617246, acc: 0.871094]\n",
            "1699: [discriminator loss: 0.725796639919281, acc: 0.462890625] [gan loss: 0.843257, acc: 0.001953]\n",
            "1700: [discriminator loss: 0.694590151309967, acc: 0.5625] [gan loss: 0.629198, acc: 0.882812]\n",
            "1701: [discriminator loss: 0.7175290584564209, acc: 0.47265625] [gan loss: 0.889126, acc: 0.000000]\n",
            "1702: [discriminator loss: 0.6945638060569763, acc: 0.5615234375] [gan loss: 0.611903, acc: 0.925781]\n",
            "1703: [discriminator loss: 0.7094487547874451, acc: 0.482421875] [gan loss: 0.915260, acc: 0.000000]\n",
            "1704: [discriminator loss: 0.6833741664886475, acc: 0.6103515625] [gan loss: 0.592539, acc: 0.960938]\n",
            "1705: [discriminator loss: 0.7080491781234741, acc: 0.4892578125] [gan loss: 0.983679, acc: 0.000000]\n",
            "1706: [discriminator loss: 0.6860048770904541, acc: 0.5556640625] [gan loss: 0.566266, acc: 0.992188]\n",
            "1707: [discriminator loss: 0.7147142887115479, acc: 0.49609375] [gan loss: 0.972901, acc: 0.000000]\n",
            "1708: [discriminator loss: 0.677007257938385, acc: 0.5927734375] [gan loss: 0.585222, acc: 0.984375]\n",
            "1709: [discriminator loss: 0.7103734016418457, acc: 0.4931640625] [gan loss: 0.965195, acc: 0.000000]\n",
            "1710: [discriminator loss: 0.6816747188568115, acc: 0.5791015625] [gan loss: 0.587016, acc: 0.974609]\n",
            "1711: [discriminator loss: 0.7079274654388428, acc: 0.4951171875] [gan loss: 0.976311, acc: 0.000000]\n",
            "1712: [discriminator loss: 0.669662356376648, acc: 0.60546875] [gan loss: 0.588330, acc: 0.984375]\n",
            "1713: [discriminator loss: 0.6991415619850159, acc: 0.494140625] [gan loss: 0.929949, acc: 0.000000]\n",
            "1714: [discriminator loss: 0.6758695244789124, acc: 0.6220703125] [gan loss: 0.627256, acc: 0.878906]\n",
            "1715: [discriminator loss: 0.7014724016189575, acc: 0.4775390625] [gan loss: 0.898682, acc: 0.000000]\n",
            "1716: [discriminator loss: 0.6740010380744934, acc: 0.625] [gan loss: 0.635286, acc: 0.818359]\n",
            "1717: [discriminator loss: 0.6984513998031616, acc: 0.46875] [gan loss: 0.903587, acc: 0.000000]\n",
            "1718: [discriminator loss: 0.6763622760772705, acc: 0.625] [gan loss: 0.641427, acc: 0.740234]\n",
            "1719: [discriminator loss: 0.6869522333145142, acc: 0.4775390625] [gan loss: 0.940979, acc: 0.000000]\n",
            "1720: [discriminator loss: 0.6628365516662598, acc: 0.58203125] [gan loss: 0.604827, acc: 0.898438]\n",
            "1721: [discriminator loss: 0.700944185256958, acc: 0.48828125] [gan loss: 1.018629, acc: 0.000000]\n",
            "1722: [discriminator loss: 0.6645352840423584, acc: 0.607421875] [gan loss: 0.581004, acc: 0.937500]\n",
            "1723: [discriminator loss: 0.7021052837371826, acc: 0.4951171875] [gan loss: 0.992394, acc: 0.000000]\n",
            "1724: [discriminator loss: 0.6730349659919739, acc: 0.603515625] [gan loss: 0.609600, acc: 0.861328]\n",
            "1725: [discriminator loss: 0.6964327096939087, acc: 0.494140625] [gan loss: 0.957287, acc: 0.000000]\n",
            "1726: [discriminator loss: 0.6672356128692627, acc: 0.626953125] [gan loss: 0.623565, acc: 0.906250]\n",
            "1727: [discriminator loss: 0.7005193829536438, acc: 0.4892578125] [gan loss: 0.963671, acc: 0.000000]\n",
            "1728: [discriminator loss: 0.6643304228782654, acc: 0.6416015625] [gan loss: 0.642427, acc: 0.826172]\n",
            "1729: [discriminator loss: 0.6914349794387817, acc: 0.4833984375] [gan loss: 0.933834, acc: 0.000000]\n",
            "1730: [discriminator loss: 0.6752598285675049, acc: 0.6376953125] [gan loss: 0.624720, acc: 0.855469]\n",
            "1731: [discriminator loss: 0.700029730796814, acc: 0.4833984375] [gan loss: 0.916823, acc: 0.000000]\n",
            "1732: [discriminator loss: 0.6839215755462646, acc: 0.5966796875] [gan loss: 0.650913, acc: 0.769531]\n",
            "1733: [discriminator loss: 0.6957801580429077, acc: 0.4814453125] [gan loss: 0.909841, acc: 0.000000]\n",
            "1734: [discriminator loss: 0.6859077215194702, acc: 0.5810546875] [gan loss: 0.636559, acc: 0.855469]\n",
            "1735: [discriminator loss: 0.7018119096755981, acc: 0.4765625] [gan loss: 0.933155, acc: 0.000000]\n",
            "1736: [discriminator loss: 0.6834729909896851, acc: 0.580078125] [gan loss: 0.602404, acc: 0.917969]\n",
            "1737: [discriminator loss: 0.70351243019104, acc: 0.4912109375] [gan loss: 0.957316, acc: 0.000000]\n",
            "1738: [discriminator loss: 0.6863049268722534, acc: 0.5712890625] [gan loss: 0.592168, acc: 0.951172]\n",
            "1739: [discriminator loss: 0.7062345147132874, acc: 0.486328125] [gan loss: 0.942680, acc: 0.000000]\n",
            "1740: [discriminator loss: 0.6935248970985413, acc: 0.5556640625] [gan loss: 0.587132, acc: 0.970703]\n",
            "1741: [discriminator loss: 0.7203351259231567, acc: 0.4853515625] [gan loss: 0.980125, acc: 0.000000]\n",
            "1742: [discriminator loss: 0.7009126543998718, acc: 0.5146484375] [gan loss: 0.572303, acc: 0.982422]\n",
            "1743: [discriminator loss: 0.7364494800567627, acc: 0.490234375] [gan loss: 0.912396, acc: 0.000000]\n",
            "1744: [discriminator loss: 0.7186126112937927, acc: 0.4521484375] [gan loss: 0.580818, acc: 0.947266]\n",
            "1745: [discriminator loss: 0.7413483262062073, acc: 0.4814453125] [gan loss: 0.886433, acc: 0.009766]\n",
            "1746: [discriminator loss: 0.7127178311347961, acc: 0.4765625] [gan loss: 0.594386, acc: 0.957031]\n",
            "1747: [discriminator loss: 0.7335416078567505, acc: 0.4658203125] [gan loss: 0.887238, acc: 0.023438]\n",
            "1748: [discriminator loss: 0.7125890254974365, acc: 0.45703125] [gan loss: 0.599953, acc: 0.951172]\n",
            "1749: [discriminator loss: 0.7335773706436157, acc: 0.4697265625] [gan loss: 0.898854, acc: 0.000000]\n",
            "1750: [discriminator loss: 0.703136146068573, acc: 0.4970703125] [gan loss: 0.585880, acc: 0.978516]\n",
            "1751: [discriminator loss: 0.7287706732749939, acc: 0.482421875] [gan loss: 0.936902, acc: 0.000000]\n",
            "1752: [discriminator loss: 0.6887820959091187, acc: 0.544921875] [gan loss: 0.581397, acc: 0.964844]\n",
            "1753: [discriminator loss: 0.7175719141960144, acc: 0.494140625] [gan loss: 0.941004, acc: 0.000000]\n",
            "1754: [discriminator loss: 0.6876773834228516, acc: 0.5654296875] [gan loss: 0.580263, acc: 0.964844]\n",
            "1755: [discriminator loss: 0.7191227674484253, acc: 0.4921875] [gan loss: 0.915872, acc: 0.000000]\n",
            "1756: [discriminator loss: 0.6884135007858276, acc: 0.5693359375] [gan loss: 0.602940, acc: 0.927734]\n",
            "1757: [discriminator loss: 0.7150951027870178, acc: 0.484375] [gan loss: 0.918962, acc: 0.000000]\n",
            "1758: [discriminator loss: 0.6840884685516357, acc: 0.5712890625] [gan loss: 0.606689, acc: 0.955078]\n",
            "1759: [discriminator loss: 0.7070170044898987, acc: 0.490234375] [gan loss: 0.902342, acc: 0.000000]\n",
            "1760: [discriminator loss: 0.6852226257324219, acc: 0.5966796875] [gan loss: 0.619500, acc: 0.919922]\n",
            "1761: [discriminator loss: 0.7068806290626526, acc: 0.4873046875] [gan loss: 0.907806, acc: 0.000000]\n",
            "1762: [discriminator loss: 0.6828246116638184, acc: 0.5908203125] [gan loss: 0.605192, acc: 0.949219]\n",
            "1763: [discriminator loss: 0.7166866660118103, acc: 0.4775390625] [gan loss: 0.929566, acc: 0.000000]\n",
            "1764: [discriminator loss: 0.6885994076728821, acc: 0.583984375] [gan loss: 0.595954, acc: 0.982422]\n",
            "1765: [discriminator loss: 0.7191271781921387, acc: 0.4873046875] [gan loss: 0.926947, acc: 0.000000]\n",
            "1766: [discriminator loss: 0.6939824819564819, acc: 0.5361328125] [gan loss: 0.572889, acc: 0.990234]\n",
            "1767: [discriminator loss: 0.7248989939689636, acc: 0.486328125] [gan loss: 0.932975, acc: 0.000000]\n",
            "1768: [discriminator loss: 0.7002488374710083, acc: 0.5185546875] [gan loss: 0.581619, acc: 0.998047]\n",
            "1769: [discriminator loss: 0.7271810173988342, acc: 0.4814453125] [gan loss: 0.900426, acc: 0.001953]\n",
            "1770: [discriminator loss: 0.6908270716667175, acc: 0.533203125] [gan loss: 0.577954, acc: 0.986328]\n",
            "1771: [discriminator loss: 0.7368003129959106, acc: 0.48046875] [gan loss: 0.929691, acc: 0.000000]\n",
            "1772: [discriminator loss: 0.6945748329162598, acc: 0.5126953125] [gan loss: 0.581480, acc: 0.976562]\n",
            "1773: [discriminator loss: 0.7216792702674866, acc: 0.484375] [gan loss: 0.903843, acc: 0.000000]\n",
            "1774: [discriminator loss: 0.6943262815475464, acc: 0.5380859375] [gan loss: 0.613256, acc: 0.917969]\n",
            "1775: [discriminator loss: 0.7119266390800476, acc: 0.4794921875] [gan loss: 0.869950, acc: 0.000000]\n",
            "1776: [discriminator loss: 0.6957094669342041, acc: 0.5419921875] [gan loss: 0.648018, acc: 0.796875]\n",
            "1777: [discriminator loss: 0.7024008631706238, acc: 0.4697265625] [gan loss: 0.869569, acc: 0.000000]\n",
            "1778: [discriminator loss: 0.6846631765365601, acc: 0.5830078125] [gan loss: 0.616028, acc: 0.865234]\n",
            "1779: [discriminator loss: 0.712302029132843, acc: 0.46484375] [gan loss: 0.931220, acc: 0.000000]\n",
            "1780: [discriminator loss: 0.6834270358085632, acc: 0.5791015625] [gan loss: 0.582277, acc: 0.968750]\n",
            "1781: [discriminator loss: 0.7193295955657959, acc: 0.4921875] [gan loss: 0.972133, acc: 0.000000]\n",
            "1782: [discriminator loss: 0.6831960082054138, acc: 0.55859375] [gan loss: 0.570594, acc: 0.951172]\n",
            "1783: [discriminator loss: 0.7277704477310181, acc: 0.4892578125] [gan loss: 0.905855, acc: 0.000000]\n",
            "1784: [discriminator loss: 0.706308901309967, acc: 0.5341796875] [gan loss: 0.616209, acc: 0.919922]\n",
            "1785: [discriminator loss: 0.7245456576347351, acc: 0.4658203125] [gan loss: 0.857136, acc: 0.023438]\n",
            "1786: [discriminator loss: 0.7128730416297913, acc: 0.4765625] [gan loss: 0.655443, acc: 0.761719]\n",
            "1787: [discriminator loss: 0.7191948890686035, acc: 0.42578125] [gan loss: 0.848762, acc: 0.000000]\n",
            "1788: [discriminator loss: 0.6992068290710449, acc: 0.587890625] [gan loss: 0.644723, acc: 0.865234]\n",
            "1789: [discriminator loss: 0.7187916040420532, acc: 0.4560546875] [gan loss: 0.959259, acc: 0.000000]\n",
            "1790: [discriminator loss: 0.6890305280685425, acc: 0.615234375] [gan loss: 0.589786, acc: 0.992188]\n",
            "1791: [discriminator loss: 0.719424307346344, acc: 0.482421875] [gan loss: 1.004264, acc: 0.000000]\n",
            "1792: [discriminator loss: 0.6911681294441223, acc: 0.6025390625] [gan loss: 0.607041, acc: 0.970703]\n",
            "1793: [discriminator loss: 0.7114255428314209, acc: 0.4716796875] [gan loss: 0.930173, acc: 0.000000]\n",
            "1794: [discriminator loss: 0.6892504096031189, acc: 0.625] [gan loss: 0.642558, acc: 0.882812]\n",
            "1795: [discriminator loss: 0.697509229183197, acc: 0.4775390625] [gan loss: 0.925333, acc: 0.000000]\n",
            "1796: [discriminator loss: 0.6807252168655396, acc: 0.6435546875] [gan loss: 0.657956, acc: 0.775391]\n",
            "1797: [discriminator loss: 0.6957343816757202, acc: 0.4716796875] [gan loss: 0.901763, acc: 0.000000]\n",
            "1798: [discriminator loss: 0.6838366985321045, acc: 0.587890625] [gan loss: 0.647919, acc: 0.775391]\n",
            "1799: [discriminator loss: 0.7019003033638, acc: 0.4716796875] [gan loss: 0.954704, acc: 0.000000]\n",
            "1800: [discriminator loss: 0.6853165626525879, acc: 0.603515625] [gan loss: 0.635598, acc: 0.857422]\n",
            "1801: [discriminator loss: 0.6921569108963013, acc: 0.4736328125] [gan loss: 0.949225, acc: 0.000000]\n",
            "1802: [discriminator loss: 0.6829031109809875, acc: 0.6044921875] [gan loss: 0.631842, acc: 0.851562]\n",
            "1803: [discriminator loss: 0.7059930562973022, acc: 0.4775390625] [gan loss: 0.945596, acc: 0.000000]\n",
            "1804: [discriminator loss: 0.6845129132270813, acc: 0.6162109375] [gan loss: 0.621020, acc: 0.923828]\n",
            "1805: [discriminator loss: 0.7056012749671936, acc: 0.4892578125] [gan loss: 0.963364, acc: 0.000000]\n",
            "1806: [discriminator loss: 0.6964329481124878, acc: 0.6025390625] [gan loss: 0.608823, acc: 0.955078]\n",
            "1807: [discriminator loss: 0.7146126627922058, acc: 0.4794921875] [gan loss: 0.915646, acc: 0.000000]\n",
            "1808: [discriminator loss: 0.7012689709663391, acc: 0.583984375] [gan loss: 0.639544, acc: 0.828125]\n",
            "1809: [discriminator loss: 0.7083963751792908, acc: 0.44921875] [gan loss: 0.899126, acc: 0.000000]\n",
            "1810: [discriminator loss: 0.7055574655532837, acc: 0.576171875] [gan loss: 0.634536, acc: 0.845703]\n",
            "1811: [discriminator loss: 0.721563994884491, acc: 0.46484375] [gan loss: 0.896270, acc: 0.000000]\n",
            "1812: [discriminator loss: 0.7029018998146057, acc: 0.5859375] [gan loss: 0.626179, acc: 0.849609]\n",
            "1813: [discriminator loss: 0.7181026935577393, acc: 0.4716796875] [gan loss: 0.902744, acc: 0.000000]\n",
            "1814: [discriminator loss: 0.698046863079071, acc: 0.576171875] [gan loss: 0.610006, acc: 0.857422]\n",
            "1815: [discriminator loss: 0.7196842432022095, acc: 0.4833984375] [gan loss: 0.933062, acc: 0.000000]\n",
            "1816: [discriminator loss: 0.6937946081161499, acc: 0.5869140625] [gan loss: 0.597278, acc: 0.894531]\n",
            "1817: [discriminator loss: 0.7152187824249268, acc: 0.486328125] [gan loss: 0.938393, acc: 0.000000]\n",
            "1818: [discriminator loss: 0.6884657144546509, acc: 0.5947265625] [gan loss: 0.625762, acc: 0.832031]\n",
            "1819: [discriminator loss: 0.7004952430725098, acc: 0.48828125] [gan loss: 0.950364, acc: 0.000000]\n",
            "1820: [discriminator loss: 0.6794285178184509, acc: 0.6279296875] [gan loss: 0.626118, acc: 0.845703]\n",
            "1821: [discriminator loss: 0.7036247253417969, acc: 0.48828125] [gan loss: 0.934501, acc: 0.000000]\n",
            "1822: [discriminator loss: 0.6858479380607605, acc: 0.611328125] [gan loss: 0.604703, acc: 0.898438]\n",
            "1823: [discriminator loss: 0.7159745693206787, acc: 0.482421875] [gan loss: 0.937310, acc: 0.000000]\n",
            "1824: [discriminator loss: 0.6956536173820496, acc: 0.568359375] [gan loss: 0.604685, acc: 0.851562]\n",
            "1825: [discriminator loss: 0.7222263813018799, acc: 0.4775390625] [gan loss: 0.917315, acc: 0.000000]\n",
            "1826: [discriminator loss: 0.7009229063987732, acc: 0.5556640625] [gan loss: 0.630514, acc: 0.853516]\n",
            "1827: [discriminator loss: 0.709602952003479, acc: 0.4755859375] [gan loss: 0.899053, acc: 0.000000]\n",
            "1828: [discriminator loss: 0.6901216506958008, acc: 0.576171875] [gan loss: 0.644154, acc: 0.798828]\n",
            "1829: [discriminator loss: 0.7090262174606323, acc: 0.4599609375] [gan loss: 0.915889, acc: 0.000000]\n",
            "1830: [discriminator loss: 0.6936922073364258, acc: 0.5869140625] [gan loss: 0.642003, acc: 0.814453]\n",
            "1831: [discriminator loss: 0.7000583410263062, acc: 0.470703125] [gan loss: 0.908804, acc: 0.000000]\n",
            "1832: [discriminator loss: 0.6860424876213074, acc: 0.5947265625] [gan loss: 0.661660, acc: 0.640625]\n",
            "1833: [discriminator loss: 0.7000745534896851, acc: 0.4677734375] [gan loss: 0.891014, acc: 0.001953]\n",
            "1834: [discriminator loss: 0.6832199096679688, acc: 0.6103515625] [gan loss: 0.664332, acc: 0.724609]\n",
            "1835: [discriminator loss: 0.688363254070282, acc: 0.486328125] [gan loss: 0.889260, acc: 0.013672]\n",
            "1836: [discriminator loss: 0.6912200450897217, acc: 0.607421875] [gan loss: 0.635063, acc: 0.865234]\n",
            "1837: [discriminator loss: 0.6963950991630554, acc: 0.4921875] [gan loss: 0.969144, acc: 0.000000]\n",
            "1838: [discriminator loss: 0.6945057511329651, acc: 0.580078125] [gan loss: 0.598160, acc: 0.960938]\n",
            "1839: [discriminator loss: 0.6939494013786316, acc: 0.5] [gan loss: 0.951954, acc: 0.000000]\n",
            "1840: [discriminator loss: 0.6907904744148254, acc: 0.55859375] [gan loss: 0.615058, acc: 0.935547]\n",
            "1841: [discriminator loss: 0.6909095644950867, acc: 0.5009765625] [gan loss: 0.928834, acc: 0.000000]\n",
            "1842: [discriminator loss: 0.6855855584144592, acc: 0.609375] [gan loss: 0.643493, acc: 0.775391]\n",
            "1843: [discriminator loss: 0.6865277290344238, acc: 0.5009765625] [gan loss: 0.933011, acc: 0.000000]\n",
            "1844: [discriminator loss: 0.684939980506897, acc: 0.6201171875] [gan loss: 0.639953, acc: 0.820312]\n",
            "1845: [discriminator loss: 0.683071494102478, acc: 0.498046875] [gan loss: 0.923383, acc: 0.000000]\n",
            "1846: [discriminator loss: 0.6791111826896667, acc: 0.5947265625] [gan loss: 0.643074, acc: 0.796875]\n",
            "1847: [discriminator loss: 0.6939058303833008, acc: 0.486328125] [gan loss: 0.927538, acc: 0.000000]\n",
            "1848: [discriminator loss: 0.6815899014472961, acc: 0.595703125] [gan loss: 0.639190, acc: 0.833984]\n",
            "1849: [discriminator loss: 0.6940020322799683, acc: 0.4814453125] [gan loss: 0.943973, acc: 0.000000]\n",
            "1850: [discriminator loss: 0.6865779161453247, acc: 0.564453125] [gan loss: 0.633220, acc: 0.857422]\n",
            "1851: [discriminator loss: 0.7000988125801086, acc: 0.4814453125] [gan loss: 0.954335, acc: 0.000000]\n",
            "1852: [discriminator loss: 0.6901862621307373, acc: 0.537109375] [gan loss: 0.617955, acc: 0.871094]\n",
            "1853: [discriminator loss: 0.7117912769317627, acc: 0.49609375] [gan loss: 0.942853, acc: 0.000000]\n",
            "1854: [discriminator loss: 0.6947998404502869, acc: 0.5234375] [gan loss: 0.640064, acc: 0.787109]\n",
            "1855: [discriminator loss: 0.7021828889846802, acc: 0.486328125] [gan loss: 0.901150, acc: 0.000000]\n",
            "1856: [discriminator loss: 0.6902324557304382, acc: 0.529296875] [gan loss: 0.664443, acc: 0.667969]\n",
            "1857: [discriminator loss: 0.7000262141227722, acc: 0.466796875] [gan loss: 0.926810, acc: 0.000000]\n",
            "1858: [discriminator loss: 0.6830332279205322, acc: 0.583984375] [gan loss: 0.663915, acc: 0.621094]\n",
            "1859: [discriminator loss: 0.6886174082756042, acc: 0.4921875] [gan loss: 0.912565, acc: 0.000000]\n",
            "1860: [discriminator loss: 0.6782351136207581, acc: 0.6044921875] [gan loss: 0.637083, acc: 0.828125]\n",
            "1861: [discriminator loss: 0.701854407787323, acc: 0.4873046875] [gan loss: 0.936088, acc: 0.000000]\n",
            "1862: [discriminator loss: 0.685213029384613, acc: 0.572265625] [gan loss: 0.620533, acc: 0.837891]\n",
            "1863: [discriminator loss: 0.7134412527084351, acc: 0.478515625] [gan loss: 0.969269, acc: 0.000000]\n",
            "1864: [discriminator loss: 0.6943912506103516, acc: 0.5244140625] [gan loss: 0.597626, acc: 0.927734]\n",
            "1865: [discriminator loss: 0.7061019539833069, acc: 0.4931640625] [gan loss: 0.898823, acc: 0.000000]\n",
            "1866: [discriminator loss: 0.6961963772773743, acc: 0.5458984375] [gan loss: 0.633056, acc: 0.818359]\n",
            "1867: [discriminator loss: 0.7064976692199707, acc: 0.470703125] [gan loss: 0.903221, acc: 0.000000]\n",
            "1868: [discriminator loss: 0.6942992806434631, acc: 0.548828125] [gan loss: 0.621923, acc: 0.845703]\n",
            "1869: [discriminator loss: 0.7082259058952332, acc: 0.4833984375] [gan loss: 0.935700, acc: 0.000000]\n",
            "1870: [discriminator loss: 0.6914697885513306, acc: 0.5205078125] [gan loss: 0.625355, acc: 0.804688]\n",
            "1871: [discriminator loss: 0.7076024413108826, acc: 0.5] [gan loss: 0.897799, acc: 0.000000]\n",
            "1872: [discriminator loss: 0.6930737495422363, acc: 0.5517578125] [gan loss: 0.640673, acc: 0.792969]\n",
            "1873: [discriminator loss: 0.704387903213501, acc: 0.4931640625] [gan loss: 0.908746, acc: 0.000000]\n",
            "1874: [discriminator loss: 0.6925243139266968, acc: 0.5830078125] [gan loss: 0.617275, acc: 0.851562]\n",
            "1875: [discriminator loss: 0.7103440761566162, acc: 0.4912109375] [gan loss: 0.927843, acc: 0.000000]\n",
            "1876: [discriminator loss: 0.6936297416687012, acc: 0.5419921875] [gan loss: 0.623445, acc: 0.843750]\n",
            "1877: [discriminator loss: 0.7023890018463135, acc: 0.4931640625] [gan loss: 0.904826, acc: 0.000000]\n",
            "1878: [discriminator loss: 0.6939139366149902, acc: 0.5439453125] [gan loss: 0.642669, acc: 0.777344]\n",
            "1879: [discriminator loss: 0.697222113609314, acc: 0.482421875] [gan loss: 0.875059, acc: 0.000000]\n",
            "1880: [discriminator loss: 0.691734790802002, acc: 0.5771484375] [gan loss: 0.644584, acc: 0.824219]\n",
            "1881: [discriminator loss: 0.6991342306137085, acc: 0.4873046875] [gan loss: 0.906509, acc: 0.000000]\n",
            "1882: [discriminator loss: 0.6840936541557312, acc: 0.6220703125] [gan loss: 0.618957, acc: 0.974609]\n",
            "1883: [discriminator loss: 0.6971545219421387, acc: 0.4892578125] [gan loss: 0.936689, acc: 0.000000]\n",
            "1884: [discriminator loss: 0.6851251125335693, acc: 0.5888671875] [gan loss: 0.613230, acc: 0.958984]\n",
            "1885: [discriminator loss: 0.6950681209564209, acc: 0.494140625] [gan loss: 0.935867, acc: 0.000000]\n",
            "1886: [discriminator loss: 0.6799405813217163, acc: 0.6025390625] [gan loss: 0.627558, acc: 0.882812]\n",
            "1887: [discriminator loss: 0.6869928240776062, acc: 0.498046875] [gan loss: 0.910288, acc: 0.000000]\n",
            "1888: [discriminator loss: 0.675613522529602, acc: 0.6396484375] [gan loss: 0.642442, acc: 0.759766]\n",
            "1889: [discriminator loss: 0.6878932118415833, acc: 0.4951171875] [gan loss: 0.899718, acc: 0.000000]\n",
            "1890: [discriminator loss: 0.6741657257080078, acc: 0.6318359375] [gan loss: 0.654413, acc: 0.718750]\n",
            "1891: [discriminator loss: 0.6829620599746704, acc: 0.490234375] [gan loss: 0.917130, acc: 0.000000]\n",
            "1892: [discriminator loss: 0.6642565727233887, acc: 0.646484375] [gan loss: 0.640707, acc: 0.773438]\n",
            "1893: [discriminator loss: 0.6891041994094849, acc: 0.5] [gan loss: 0.948633, acc: 0.000000]\n",
            "1894: [discriminator loss: 0.672896146774292, acc: 0.5966796875] [gan loss: 0.619189, acc: 0.853516]\n",
            "1895: [discriminator loss: 0.6863340735435486, acc: 0.5068359375] [gan loss: 0.938788, acc: 0.000000]\n",
            "1896: [discriminator loss: 0.6689417362213135, acc: 0.6162109375] [gan loss: 0.644691, acc: 0.794922]\n",
            "1897: [discriminator loss: 0.6889830231666565, acc: 0.5048828125] [gan loss: 0.933533, acc: 0.000000]\n",
            "1898: [discriminator loss: 0.6716925501823425, acc: 0.619140625] [gan loss: 0.668486, acc: 0.673828]\n",
            "1899: [discriminator loss: 0.6855645775794983, acc: 0.501953125] [gan loss: 0.919299, acc: 0.000000]\n",
            "1900: [discriminator loss: 0.6738269925117493, acc: 0.5966796875] [gan loss: 0.652958, acc: 0.712891]\n",
            "1901: [discriminator loss: 0.68917316198349, acc: 0.4873046875] [gan loss: 0.946588, acc: 0.000000]\n",
            "1902: [discriminator loss: 0.6725017428398132, acc: 0.6162109375] [gan loss: 0.636350, acc: 0.845703]\n",
            "1903: [discriminator loss: 0.6995856761932373, acc: 0.4921875] [gan loss: 0.992881, acc: 0.000000]\n",
            "1904: [discriminator loss: 0.6687232851982117, acc: 0.60546875] [gan loss: 0.599293, acc: 0.927734]\n",
            "1905: [discriminator loss: 0.7067086696624756, acc: 0.4873046875] [gan loss: 0.956472, acc: 0.000000]\n",
            "1906: [discriminator loss: 0.6790668964385986, acc: 0.603515625] [gan loss: 0.651023, acc: 0.751953]\n",
            "1907: [discriminator loss: 0.6934325098991394, acc: 0.466796875] [gan loss: 0.874738, acc: 0.000000]\n",
            "1908: [discriminator loss: 0.668697714805603, acc: 0.6416015625] [gan loss: 0.679119, acc: 0.568359]\n",
            "1909: [discriminator loss: 0.6944597959518433, acc: 0.4521484375] [gan loss: 0.883487, acc: 0.000000]\n",
            "1910: [discriminator loss: 0.6726031303405762, acc: 0.626953125] [gan loss: 0.658240, acc: 0.724609]\n",
            "1911: [discriminator loss: 0.6995368599891663, acc: 0.4677734375] [gan loss: 0.951622, acc: 0.000000]\n",
            "1912: [discriminator loss: 0.6741035580635071, acc: 0.6025390625] [gan loss: 0.641249, acc: 0.732422]\n",
            "1913: [discriminator loss: 0.7038941383361816, acc: 0.4736328125] [gan loss: 0.978161, acc: 0.000000]\n",
            "1914: [discriminator loss: 0.6748374700546265, acc: 0.583984375] [gan loss: 0.626064, acc: 0.849609]\n",
            "1915: [discriminator loss: 0.7014070153236389, acc: 0.48046875] [gan loss: 0.968490, acc: 0.000000]\n",
            "1916: [discriminator loss: 0.6790892481803894, acc: 0.5703125] [gan loss: 0.649463, acc: 0.673828]\n",
            "1917: [discriminator loss: 0.6997096538543701, acc: 0.4775390625] [gan loss: 0.932723, acc: 0.000000]\n",
            "1918: [discriminator loss: 0.6775094270706177, acc: 0.5908203125] [gan loss: 0.659603, acc: 0.638672]\n",
            "1919: [discriminator loss: 0.6927106380462646, acc: 0.482421875] [gan loss: 0.938138, acc: 0.000000]\n",
            "1920: [discriminator loss: 0.6763619184494019, acc: 0.578125] [gan loss: 0.658264, acc: 0.666016]\n",
            "1921: [discriminator loss: 0.6977408528327942, acc: 0.49609375] [gan loss: 0.927199, acc: 0.000000]\n",
            "1922: [discriminator loss: 0.6797147989273071, acc: 0.591796875] [gan loss: 0.658104, acc: 0.708984]\n",
            "1923: [discriminator loss: 0.7047512531280518, acc: 0.4951171875] [gan loss: 0.935411, acc: 0.000000]\n",
            "1924: [discriminator loss: 0.6853746771812439, acc: 0.6015625] [gan loss: 0.655827, acc: 0.714844]\n",
            "1925: [discriminator loss: 0.7004200220108032, acc: 0.4931640625] [gan loss: 0.921247, acc: 0.000000]\n",
            "1926: [discriminator loss: 0.6799090504646301, acc: 0.5947265625] [gan loss: 0.655975, acc: 0.720703]\n",
            "1927: [discriminator loss: 0.6960442662239075, acc: 0.4736328125] [gan loss: 0.953301, acc: 0.000000]\n",
            "1928: [discriminator loss: 0.6808282732963562, acc: 0.5888671875] [gan loss: 0.642366, acc: 0.751953]\n",
            "1929: [discriminator loss: 0.6952983140945435, acc: 0.4873046875] [gan loss: 0.982840, acc: 0.000000]\n",
            "1930: [discriminator loss: 0.6777958869934082, acc: 0.5712890625] [gan loss: 0.641135, acc: 0.753906]\n",
            "1931: [discriminator loss: 0.6982756853103638, acc: 0.484375] [gan loss: 0.943060, acc: 0.000000]\n",
            "1932: [discriminator loss: 0.6895183324813843, acc: 0.513671875] [gan loss: 0.663255, acc: 0.537109]\n",
            "1933: [discriminator loss: 0.7012023329734802, acc: 0.4853515625] [gan loss: 0.872679, acc: 0.076172]\n",
            "1934: [discriminator loss: 0.6882538199424744, acc: 0.5546875] [gan loss: 0.683562, acc: 0.546875]\n",
            "1935: [discriminator loss: 0.6990597248077393, acc: 0.4892578125] [gan loss: 0.906993, acc: 0.001953]\n",
            "1936: [discriminator loss: 0.6956914067268372, acc: 0.5283203125] [gan loss: 0.633120, acc: 0.835938]\n",
            "1937: [discriminator loss: 0.7101932764053345, acc: 0.4794921875] [gan loss: 0.965805, acc: 0.000000]\n",
            "1938: [discriminator loss: 0.6967746615409851, acc: 0.5556640625] [gan loss: 0.602016, acc: 0.916016]\n",
            "1939: [discriminator loss: 0.7187816500663757, acc: 0.4892578125] [gan loss: 0.969550, acc: 0.000000]\n",
            "1940: [discriminator loss: 0.6916638016700745, acc: 0.544921875] [gan loss: 0.603286, acc: 0.884766]\n",
            "1941: [discriminator loss: 0.7191106081008911, acc: 0.4921875] [gan loss: 0.935993, acc: 0.001953]\n",
            "1942: [discriminator loss: 0.6923317313194275, acc: 0.53515625] [gan loss: 0.621554, acc: 0.679688]\n",
            "1943: [discriminator loss: 0.7262732982635498, acc: 0.4892578125] [gan loss: 0.918555, acc: 0.015625]\n",
            "1944: [discriminator loss: 0.6980856657028198, acc: 0.5078125] [gan loss: 0.599914, acc: 0.837891]\n",
            "1945: [discriminator loss: 0.7263539433479309, acc: 0.482421875] [gan loss: 0.907760, acc: 0.025391]\n",
            "1946: [discriminator loss: 0.6935336589813232, acc: 0.49609375] [gan loss: 0.631518, acc: 0.646484]\n",
            "1947: [discriminator loss: 0.7094590067863464, acc: 0.5087890625] [gan loss: 0.899371, acc: 0.009766]\n",
            "1948: [discriminator loss: 0.6835991144180298, acc: 0.5341796875] [gan loss: 0.629810, acc: 0.708984]\n",
            "1949: [discriminator loss: 0.7041538953781128, acc: 0.49609375] [gan loss: 0.895572, acc: 0.003906]\n",
            "1950: [discriminator loss: 0.6671959161758423, acc: 0.6201171875] [gan loss: 0.645356, acc: 0.714844]\n",
            "1951: [discriminator loss: 0.6843665242195129, acc: 0.5] [gan loss: 0.959414, acc: 0.000000]\n",
            "1952: [discriminator loss: 0.6587515473365784, acc: 0.6181640625] [gan loss: 0.617957, acc: 0.857422]\n",
            "1953: [discriminator loss: 0.6884472966194153, acc: 0.4912109375] [gan loss: 1.012667, acc: 0.000000]\n",
            "1954: [discriminator loss: 0.6496140360832214, acc: 0.6259765625] [gan loss: 0.630067, acc: 0.853516]\n",
            "1955: [discriminator loss: 0.6699797511100769, acc: 0.4951171875] [gan loss: 0.966449, acc: 0.000000]\n",
            "1956: [discriminator loss: 0.6389129161834717, acc: 0.7109375] [gan loss: 0.685315, acc: 0.527344]\n",
            "1957: [discriminator loss: 0.664529025554657, acc: 0.494140625] [gan loss: 0.930532, acc: 0.000000]\n",
            "1958: [discriminator loss: 0.6426413059234619, acc: 0.720703125] [gan loss: 0.698143, acc: 0.435547]\n",
            "1959: [discriminator loss: 0.6676465272903442, acc: 0.5068359375] [gan loss: 0.983577, acc: 0.000000]\n",
            "1960: [discriminator loss: 0.642459511756897, acc: 0.671875] [gan loss: 0.664664, acc: 0.589844]\n",
            "1961: [discriminator loss: 0.6761975288391113, acc: 0.498046875] [gan loss: 0.992104, acc: 0.000000]\n",
            "1962: [discriminator loss: 0.6525981426239014, acc: 0.62109375] [gan loss: 0.626160, acc: 0.804688]\n",
            "1963: [discriminator loss: 0.6965184211730957, acc: 0.494140625] [gan loss: 1.040713, acc: 0.000000]\n",
            "1964: [discriminator loss: 0.6644495725631714, acc: 0.5537109375] [gan loss: 0.605191, acc: 0.800781]\n",
            "1965: [discriminator loss: 0.7052829265594482, acc: 0.4970703125] [gan loss: 1.015742, acc: 0.000000]\n",
            "1966: [discriminator loss: 0.6792501211166382, acc: 0.5107421875] [gan loss: 0.608610, acc: 0.714844]\n",
            "1967: [discriminator loss: 0.7182767391204834, acc: 0.5048828125] [gan loss: 0.957289, acc: 0.009766]\n",
            "1968: [discriminator loss: 0.6797932386398315, acc: 0.5166015625] [gan loss: 0.619960, acc: 0.623047]\n",
            "1969: [discriminator loss: 0.721053957939148, acc: 0.501953125] [gan loss: 0.908035, acc: 0.082031]\n",
            "1970: [discriminator loss: 0.6920438408851624, acc: 0.4619140625] [gan loss: 0.637175, acc: 0.646484]\n",
            "1971: [discriminator loss: 0.7137115001678467, acc: 0.48828125] [gan loss: 0.891952, acc: 0.070312]\n",
            "1972: [discriminator loss: 0.6887633800506592, acc: 0.501953125] [gan loss: 0.631352, acc: 0.722656]\n",
            "1973: [discriminator loss: 0.7158843278884888, acc: 0.46875] [gan loss: 0.903178, acc: 0.000000]\n",
            "1974: [discriminator loss: 0.6767656803131104, acc: 0.5185546875] [gan loss: 0.653282, acc: 0.619141]\n",
            "1975: [discriminator loss: 0.7042748928070068, acc: 0.484375] [gan loss: 0.943195, acc: 0.000000]\n",
            "1976: [discriminator loss: 0.6788451671600342, acc: 0.5458984375] [gan loss: 0.624558, acc: 0.839844]\n",
            "1977: [discriminator loss: 0.7081764936447144, acc: 0.482421875] [gan loss: 0.948403, acc: 0.000000]\n",
            "1978: [discriminator loss: 0.6723332405090332, acc: 0.6103515625] [gan loss: 0.607178, acc: 0.943359]\n",
            "1979: [discriminator loss: 0.7022798657417297, acc: 0.498046875] [gan loss: 0.943259, acc: 0.000000]\n",
            "1980: [discriminator loss: 0.674675464630127, acc: 0.619140625] [gan loss: 0.577523, acc: 0.986328]\n",
            "1981: [discriminator loss: 0.7193856239318848, acc: 0.49609375] [gan loss: 0.949794, acc: 0.000000]\n",
            "1982: [discriminator loss: 0.6786618232727051, acc: 0.5888671875] [gan loss: 0.599638, acc: 0.976562]\n",
            "1983: [discriminator loss: 0.7046651244163513, acc: 0.48828125] [gan loss: 0.883380, acc: 0.001953]\n",
            "1984: [discriminator loss: 0.674604594707489, acc: 0.5771484375] [gan loss: 0.609298, acc: 0.935547]\n",
            "1985: [discriminator loss: 0.7128523588180542, acc: 0.4853515625] [gan loss: 0.878566, acc: 0.023438]\n",
            "1986: [discriminator loss: 0.6833667159080505, acc: 0.578125] [gan loss: 0.647904, acc: 0.832031]\n",
            "1987: [discriminator loss: 0.7080657482147217, acc: 0.45703125] [gan loss: 0.866066, acc: 0.009766]\n",
            "1988: [discriminator loss: 0.6808929443359375, acc: 0.5986328125] [gan loss: 0.622411, acc: 0.900391]\n",
            "1989: [discriminator loss: 0.706368088722229, acc: 0.4794921875] [gan loss: 0.881850, acc: 0.000000]\n",
            "1990: [discriminator loss: 0.6871772408485413, acc: 0.5869140625] [gan loss: 0.625256, acc: 0.929688]\n",
            "1991: [discriminator loss: 0.7055380344390869, acc: 0.4794921875] [gan loss: 0.898299, acc: 0.000000]\n",
            "1992: [discriminator loss: 0.6829480528831482, acc: 0.60546875] [gan loss: 0.615675, acc: 0.962891]\n",
            "1993: [discriminator loss: 0.7055717706680298, acc: 0.4921875] [gan loss: 0.941262, acc: 0.000000]\n",
            "1994: [discriminator loss: 0.6769198775291443, acc: 0.638671875] [gan loss: 0.600201, acc: 0.964844]\n",
            "1995: [discriminator loss: 0.6990635395050049, acc: 0.4951171875] [gan loss: 0.960116, acc: 0.000000]\n",
            "1996: [discriminator loss: 0.6747201681137085, acc: 0.6328125] [gan loss: 0.597520, acc: 0.957031]\n",
            "1997: [discriminator loss: 0.6993545293807983, acc: 0.49609375] [gan loss: 0.954263, acc: 0.000000]\n",
            "1998: [discriminator loss: 0.6697559952735901, acc: 0.63671875] [gan loss: 0.629704, acc: 0.826172]\n",
            "1999: [discriminator loss: 0.6888325214385986, acc: 0.4970703125] [gan loss: 0.904583, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2000: [discriminator loss: 0.6679655313491821, acc: 0.6572265625] [gan loss: 0.679347, acc: 0.599609]\n",
            "2001: [discriminator loss: 0.6789267659187317, acc: 0.5107421875] [gan loss: 0.867967, acc: 0.000000]\n",
            "2002: [discriminator loss: 0.6603630781173706, acc: 0.6884765625] [gan loss: 0.678197, acc: 0.566406]\n",
            "2003: [discriminator loss: 0.6796872019767761, acc: 0.5166015625] [gan loss: 0.917602, acc: 0.000000]\n",
            "2004: [discriminator loss: 0.6603736877441406, acc: 0.6591796875] [gan loss: 0.635114, acc: 0.794922]\n",
            "2005: [discriminator loss: 0.6923158764839172, acc: 0.5048828125] [gan loss: 0.979412, acc: 0.000000]\n",
            "2006: [discriminator loss: 0.6690236330032349, acc: 0.634765625] [gan loss: 0.608282, acc: 0.912109]\n",
            "2007: [discriminator loss: 0.6941288709640503, acc: 0.5] [gan loss: 0.974484, acc: 0.000000]\n",
            "2008: [discriminator loss: 0.6585119962692261, acc: 0.6337890625] [gan loss: 0.605438, acc: 0.906250]\n",
            "2009: [discriminator loss: 0.6940131187438965, acc: 0.498046875] [gan loss: 0.956356, acc: 0.000000]\n",
            "2010: [discriminator loss: 0.6573347449302673, acc: 0.6220703125] [gan loss: 0.637515, acc: 0.787109]\n",
            "2011: [discriminator loss: 0.6750184893608093, acc: 0.5146484375] [gan loss: 0.918528, acc: 0.000000]\n",
            "2012: [discriminator loss: 0.6574193239212036, acc: 0.6533203125] [gan loss: 0.659275, acc: 0.703125]\n",
            "2013: [discriminator loss: 0.6834535598754883, acc: 0.521484375] [gan loss: 0.928480, acc: 0.000000]\n",
            "2014: [discriminator loss: 0.6458579301834106, acc: 0.693359375] [gan loss: 0.624131, acc: 0.787109]\n",
            "2015: [discriminator loss: 0.6950026154518127, acc: 0.4921875] [gan loss: 0.997531, acc: 0.000000]\n",
            "2016: [discriminator loss: 0.6527294516563416, acc: 0.63671875] [gan loss: 0.579508, acc: 0.949219]\n",
            "2017: [discriminator loss: 0.7076147794723511, acc: 0.4921875] [gan loss: 0.955294, acc: 0.000000]\n",
            "2018: [discriminator loss: 0.6632089614868164, acc: 0.6240234375] [gan loss: 0.612795, acc: 0.847656]\n",
            "2019: [discriminator loss: 0.7030178308486938, acc: 0.48828125] [gan loss: 0.927922, acc: 0.013672]\n",
            "2020: [discriminator loss: 0.6773608326911926, acc: 0.603515625] [gan loss: 0.644009, acc: 0.658203]\n",
            "2021: [discriminator loss: 0.7020034790039062, acc: 0.484375] [gan loss: 0.928873, acc: 0.000000]\n",
            "2022: [discriminator loss: 0.6702728271484375, acc: 0.650390625] [gan loss: 0.646740, acc: 0.716797]\n",
            "2023: [discriminator loss: 0.7036477327346802, acc: 0.4677734375] [gan loss: 0.925146, acc: 0.000000]\n",
            "2024: [discriminator loss: 0.6742677688598633, acc: 0.642578125] [gan loss: 0.642879, acc: 0.710938]\n",
            "2025: [discriminator loss: 0.7069083452224731, acc: 0.4658203125] [gan loss: 0.957976, acc: 0.000000]\n",
            "2026: [discriminator loss: 0.6787591576576233, acc: 0.5859375] [gan loss: 0.598342, acc: 0.871094]\n",
            "2027: [discriminator loss: 0.7155399918556213, acc: 0.4873046875] [gan loss: 0.953217, acc: 0.000000]\n",
            "2028: [discriminator loss: 0.6814362406730652, acc: 0.5908203125] [gan loss: 0.653345, acc: 0.708984]\n",
            "2029: [discriminator loss: 0.7080708742141724, acc: 0.47265625] [gan loss: 0.927660, acc: 0.000000]\n",
            "2030: [discriminator loss: 0.6775858402252197, acc: 0.626953125] [gan loss: 0.653911, acc: 0.746094]\n",
            "2031: [discriminator loss: 0.6949445009231567, acc: 0.4716796875] [gan loss: 0.958680, acc: 0.000000]\n",
            "2032: [discriminator loss: 0.664779543876648, acc: 0.6513671875] [gan loss: 0.637798, acc: 0.841797]\n",
            "2033: [discriminator loss: 0.6969912052154541, acc: 0.4873046875] [gan loss: 0.974059, acc: 0.000000]\n",
            "2034: [discriminator loss: 0.6734079718589783, acc: 0.63671875] [gan loss: 0.671787, acc: 0.658203]\n",
            "2035: [discriminator loss: 0.6799943447113037, acc: 0.484375] [gan loss: 0.935309, acc: 0.000000]\n",
            "2036: [discriminator loss: 0.6650992631912231, acc: 0.6796875] [gan loss: 0.667212, acc: 0.609375]\n",
            "2037: [discriminator loss: 0.6769080758094788, acc: 0.5048828125] [gan loss: 0.964471, acc: 0.000000]\n",
            "2038: [discriminator loss: 0.6609560251235962, acc: 0.6796875] [gan loss: 0.665701, acc: 0.626953]\n",
            "2039: [discriminator loss: 0.6827254891395569, acc: 0.5] [gan loss: 0.969701, acc: 0.000000]\n",
            "2040: [discriminator loss: 0.6639580130577087, acc: 0.66796875] [gan loss: 0.653872, acc: 0.667969]\n",
            "2041: [discriminator loss: 0.6857973337173462, acc: 0.501953125] [gan loss: 0.982271, acc: 0.000000]\n",
            "2042: [discriminator loss: 0.668729841709137, acc: 0.6240234375] [gan loss: 0.630214, acc: 0.726562]\n",
            "2043: [discriminator loss: 0.7037914991378784, acc: 0.509765625] [gan loss: 0.982146, acc: 0.000000]\n",
            "2044: [discriminator loss: 0.6910470724105835, acc: 0.55859375] [gan loss: 0.614731, acc: 0.796875]\n",
            "2045: [discriminator loss: 0.716507613658905, acc: 0.484375] [gan loss: 0.918099, acc: 0.017578]\n",
            "2046: [discriminator loss: 0.6946344971656799, acc: 0.525390625] [gan loss: 0.612859, acc: 0.738281]\n",
            "2047: [discriminator loss: 0.7192382216453552, acc: 0.4912109375] [gan loss: 0.898694, acc: 0.023438]\n",
            "2048: [discriminator loss: 0.6937540769577026, acc: 0.525390625] [gan loss: 0.630739, acc: 0.669922]\n",
            "2049: [discriminator loss: 0.7207511067390442, acc: 0.4814453125] [gan loss: 0.882177, acc: 0.011719]\n",
            "2050: [discriminator loss: 0.7006633281707764, acc: 0.5009765625] [gan loss: 0.628798, acc: 0.695312]\n",
            "2051: [discriminator loss: 0.7215831279754639, acc: 0.4765625] [gan loss: 0.893920, acc: 0.000000]\n",
            "2052: [discriminator loss: 0.6907370686531067, acc: 0.544921875] [gan loss: 0.622636, acc: 0.720703]\n",
            "2053: [discriminator loss: 0.7122411727905273, acc: 0.4853515625] [gan loss: 0.917877, acc: 0.000000]\n",
            "2054: [discriminator loss: 0.6914926767349243, acc: 0.5322265625] [gan loss: 0.620547, acc: 0.783203]\n",
            "2055: [discriminator loss: 0.7139322757720947, acc: 0.4794921875] [gan loss: 0.935279, acc: 0.000000]\n",
            "2056: [discriminator loss: 0.675106942653656, acc: 0.6142578125] [gan loss: 0.619682, acc: 0.841797]\n",
            "2057: [discriminator loss: 0.705316424369812, acc: 0.4853515625] [gan loss: 0.969590, acc: 0.000000]\n",
            "2058: [discriminator loss: 0.6731088161468506, acc: 0.63671875] [gan loss: 0.611766, acc: 0.894531]\n",
            "2059: [discriminator loss: 0.7132239937782288, acc: 0.4873046875] [gan loss: 0.978638, acc: 0.000000]\n",
            "2060: [discriminator loss: 0.6788458824157715, acc: 0.6142578125] [gan loss: 0.628236, acc: 0.908203]\n",
            "2061: [discriminator loss: 0.6937251687049866, acc: 0.4853515625] [gan loss: 0.941297, acc: 0.000000]\n",
            "2062: [discriminator loss: 0.6720113754272461, acc: 0.650390625] [gan loss: 0.659141, acc: 0.785156]\n",
            "2063: [discriminator loss: 0.6890735626220703, acc: 0.4775390625] [gan loss: 0.916581, acc: 0.000000]\n",
            "2064: [discriminator loss: 0.669228732585907, acc: 0.6494140625] [gan loss: 0.670061, acc: 0.681641]\n",
            "2065: [discriminator loss: 0.6862680912017822, acc: 0.47265625] [gan loss: 0.929829, acc: 0.000000]\n",
            "2066: [discriminator loss: 0.6658694744110107, acc: 0.6279296875] [gan loss: 0.638432, acc: 0.878906]\n",
            "2067: [discriminator loss: 0.696717381477356, acc: 0.48828125] [gan loss: 1.014717, acc: 0.000000]\n",
            "2068: [discriminator loss: 0.6699928045272827, acc: 0.6318359375] [gan loss: 0.613603, acc: 0.931641]\n",
            "2069: [discriminator loss: 0.6921325325965881, acc: 0.4873046875] [gan loss: 0.960814, acc: 0.000000]\n",
            "2070: [discriminator loss: 0.6732195019721985, acc: 0.603515625] [gan loss: 0.645420, acc: 0.794922]\n",
            "2071: [discriminator loss: 0.694061815738678, acc: 0.490234375] [gan loss: 0.919696, acc: 0.000000]\n",
            "2072: [discriminator loss: 0.6662260293960571, acc: 0.6474609375] [gan loss: 0.669941, acc: 0.642578]\n",
            "2073: [discriminator loss: 0.6911482810974121, acc: 0.4921875] [gan loss: 0.931181, acc: 0.000000]\n",
            "2074: [discriminator loss: 0.6745525598526001, acc: 0.6162109375] [gan loss: 0.632229, acc: 0.804688]\n",
            "2075: [discriminator loss: 0.7018029093742371, acc: 0.4912109375] [gan loss: 0.956954, acc: 0.000000]\n",
            "2076: [discriminator loss: 0.6838427782058716, acc: 0.56640625] [gan loss: 0.636213, acc: 0.792969]\n",
            "2077: [discriminator loss: 0.7116777896881104, acc: 0.4970703125] [gan loss: 0.930451, acc: 0.000000]\n",
            "2078: [discriminator loss: 0.7028595209121704, acc: 0.53515625] [gan loss: 0.616325, acc: 0.863281]\n",
            "2079: [discriminator loss: 0.7182595729827881, acc: 0.4970703125] [gan loss: 0.906259, acc: 0.000000]\n",
            "2080: [discriminator loss: 0.7017899751663208, acc: 0.53125] [gan loss: 0.634203, acc: 0.789062]\n",
            "2081: [discriminator loss: 0.71324622631073, acc: 0.49609375] [gan loss: 0.929724, acc: 0.000000]\n",
            "2082: [discriminator loss: 0.7050302028656006, acc: 0.533203125] [gan loss: 0.609514, acc: 0.908203]\n",
            "2083: [discriminator loss: 0.717384934425354, acc: 0.4921875] [gan loss: 0.892083, acc: 0.000000]\n",
            "2084: [discriminator loss: 0.7058353424072266, acc: 0.5576171875] [gan loss: 0.630791, acc: 0.833984]\n",
            "2085: [discriminator loss: 0.715059757232666, acc: 0.478515625] [gan loss: 0.877438, acc: 0.000000]\n",
            "2086: [discriminator loss: 0.7016128301620483, acc: 0.55078125] [gan loss: 0.618545, acc: 0.894531]\n",
            "2087: [discriminator loss: 0.7162617444992065, acc: 0.4814453125] [gan loss: 0.923227, acc: 0.000000]\n",
            "2088: [discriminator loss: 0.687731146812439, acc: 0.5703125] [gan loss: 0.586905, acc: 0.953125]\n",
            "2089: [discriminator loss: 0.7226834297180176, acc: 0.4853515625] [gan loss: 0.952276, acc: 0.000000]\n",
            "2090: [discriminator loss: 0.6980174779891968, acc: 0.5751953125] [gan loss: 0.625395, acc: 0.804688]\n",
            "2091: [discriminator loss: 0.7021685242652893, acc: 0.4892578125] [gan loss: 0.879111, acc: 0.011719]\n",
            "2092: [discriminator loss: 0.6920915246009827, acc: 0.548828125] [gan loss: 0.669063, acc: 0.673828]\n",
            "2093: [discriminator loss: 0.7063661813735962, acc: 0.498046875] [gan loss: 0.859784, acc: 0.013672]\n",
            "2094: [discriminator loss: 0.6965705156326294, acc: 0.5234375] [gan loss: 0.671813, acc: 0.619141]\n",
            "2095: [discriminator loss: 0.7047768235206604, acc: 0.494140625] [gan loss: 0.907843, acc: 0.000000]\n",
            "2096: [discriminator loss: 0.6811163425445557, acc: 0.6005859375] [gan loss: 0.614883, acc: 0.800781]\n",
            "2097: [discriminator loss: 0.7099642157554626, acc: 0.5029296875] [gan loss: 0.975201, acc: 0.000000]\n",
            "2098: [discriminator loss: 0.6959745287895203, acc: 0.54296875] [gan loss: 0.581015, acc: 0.873047]\n",
            "2099: [discriminator loss: 0.7195805311203003, acc: 0.4921875] [gan loss: 0.990889, acc: 0.000000]\n",
            "2100: [discriminator loss: 0.6862055063247681, acc: 0.5771484375] [gan loss: 0.605368, acc: 0.847656]\n",
            "2101: [discriminator loss: 0.7084077000617981, acc: 0.490234375] [gan loss: 0.907159, acc: 0.000000]\n",
            "2102: [discriminator loss: 0.690189778804779, acc: 0.5546875] [gan loss: 0.639393, acc: 0.781250]\n",
            "2103: [discriminator loss: 0.6972479224205017, acc: 0.4853515625] [gan loss: 0.871669, acc: 0.027344]\n",
            "2104: [discriminator loss: 0.686420738697052, acc: 0.5576171875] [gan loss: 0.651901, acc: 0.675781]\n",
            "2105: [discriminator loss: 0.7071784138679504, acc: 0.47265625] [gan loss: 0.909680, acc: 0.000000]\n",
            "2106: [discriminator loss: 0.6770431399345398, acc: 0.572265625] [gan loss: 0.614883, acc: 0.845703]\n",
            "2107: [discriminator loss: 0.7120775580406189, acc: 0.48828125] [gan loss: 0.943531, acc: 0.000000]\n",
            "2108: [discriminator loss: 0.6871651411056519, acc: 0.5517578125] [gan loss: 0.616108, acc: 0.843750]\n",
            "2109: [discriminator loss: 0.7051311731338501, acc: 0.4912109375] [gan loss: 0.939050, acc: 0.000000]\n",
            "2110: [discriminator loss: 0.6828023791313171, acc: 0.5556640625] [gan loss: 0.619030, acc: 0.867188]\n",
            "2111: [discriminator loss: 0.7029677033424377, acc: 0.4853515625] [gan loss: 0.906749, acc: 0.000000]\n",
            "2112: [discriminator loss: 0.6810548305511475, acc: 0.572265625] [gan loss: 0.630228, acc: 0.814453]\n",
            "2113: [discriminator loss: 0.7029785513877869, acc: 0.4755859375] [gan loss: 0.894254, acc: 0.000000]\n",
            "2114: [discriminator loss: 0.6798908114433289, acc: 0.5830078125] [gan loss: 0.630964, acc: 0.820312]\n",
            "2115: [discriminator loss: 0.7088088393211365, acc: 0.46484375] [gan loss: 0.945191, acc: 0.000000]\n",
            "2116: [discriminator loss: 0.6809214949607849, acc: 0.5595703125] [gan loss: 0.588609, acc: 0.988281]\n",
            "2117: [discriminator loss: 0.7207410931587219, acc: 0.490234375] [gan loss: 0.947561, acc: 0.000000]\n",
            "2118: [discriminator loss: 0.6879199147224426, acc: 0.548828125] [gan loss: 0.600628, acc: 0.986328]\n",
            "2119: [discriminator loss: 0.7245669960975647, acc: 0.48046875] [gan loss: 0.903584, acc: 0.003906]\n",
            "2120: [discriminator loss: 0.6915044784545898, acc: 0.537109375] [gan loss: 0.639805, acc: 0.865234]\n",
            "2121: [discriminator loss: 0.7116051316261292, acc: 0.46484375] [gan loss: 0.885136, acc: 0.003906]\n",
            "2122: [discriminator loss: 0.696451723575592, acc: 0.5009765625] [gan loss: 0.634518, acc: 0.863281]\n",
            "2123: [discriminator loss: 0.7165573239326477, acc: 0.4658203125] [gan loss: 0.902697, acc: 0.000000]\n",
            "2124: [discriminator loss: 0.6994000673294067, acc: 0.5244140625] [gan loss: 0.625464, acc: 0.912109]\n",
            "2125: [discriminator loss: 0.7151316404342651, acc: 0.4736328125] [gan loss: 0.909969, acc: 0.000000]\n",
            "2126: [discriminator loss: 0.6910905241966248, acc: 0.5439453125] [gan loss: 0.632723, acc: 0.892578]\n",
            "2127: [discriminator loss: 0.7117993831634521, acc: 0.474609375] [gan loss: 0.912565, acc: 0.000000]\n",
            "2128: [discriminator loss: 0.6967061161994934, acc: 0.5732421875] [gan loss: 0.643640, acc: 0.830078]\n",
            "2129: [discriminator loss: 0.7077003121376038, acc: 0.4619140625] [gan loss: 0.899557, acc: 0.000000]\n",
            "2130: [discriminator loss: 0.6925732493400574, acc: 0.5751953125] [gan loss: 0.637944, acc: 0.824219]\n",
            "2131: [discriminator loss: 0.7044398784637451, acc: 0.4677734375] [gan loss: 0.901069, acc: 0.000000]\n",
            "2132: [discriminator loss: 0.6861504316329956, acc: 0.6103515625] [gan loss: 0.621362, acc: 0.951172]\n",
            "2133: [discriminator loss: 0.7103685140609741, acc: 0.4697265625] [gan loss: 0.923087, acc: 0.000000]\n",
            "2134: [discriminator loss: 0.6804212927818298, acc: 0.6103515625] [gan loss: 0.621310, acc: 0.953125]\n",
            "2135: [discriminator loss: 0.6967933773994446, acc: 0.48828125] [gan loss: 0.940696, acc: 0.000000]\n",
            "2136: [discriminator loss: 0.6763635873794556, acc: 0.5751953125] [gan loss: 0.619759, acc: 0.941406]\n",
            "2137: [discriminator loss: 0.6993883848190308, acc: 0.4892578125] [gan loss: 0.918360, acc: 0.000000]\n",
            "2138: [discriminator loss: 0.6766992211341858, acc: 0.595703125] [gan loss: 0.640528, acc: 0.888672]\n",
            "2139: [discriminator loss: 0.6968570351600647, acc: 0.4765625] [gan loss: 0.893441, acc: 0.009766]\n",
            "2140: [discriminator loss: 0.6845564246177673, acc: 0.5634765625] [gan loss: 0.667885, acc: 0.640625]\n",
            "2141: [discriminator loss: 0.7021143436431885, acc: 0.45703125] [gan loss: 0.878668, acc: 0.048828]\n",
            "2142: [discriminator loss: 0.6901453137397766, acc: 0.548828125] [gan loss: 0.680649, acc: 0.515625]\n",
            "2143: [discriminator loss: 0.6945188641548157, acc: 0.470703125] [gan loss: 0.844458, acc: 0.208984]\n",
            "2144: [discriminator loss: 0.6878780722618103, acc: 0.5380859375] [gan loss: 0.695771, acc: 0.449219]\n",
            "2145: [discriminator loss: 0.6867948770523071, acc: 0.5146484375] [gan loss: 0.855030, acc: 0.095703]\n",
            "2146: [discriminator loss: 0.694374680519104, acc: 0.5068359375] [gan loss: 0.703165, acc: 0.460938]\n",
            "2147: [discriminator loss: 0.6931210160255432, acc: 0.494140625] [gan loss: 0.869535, acc: 0.011719]\n",
            "2148: [discriminator loss: 0.6938306093215942, acc: 0.53515625] [gan loss: 0.668045, acc: 0.703125]\n",
            "2149: [discriminator loss: 0.6980829238891602, acc: 0.466796875] [gan loss: 0.902869, acc: 0.000000]\n",
            "2150: [discriminator loss: 0.6944624185562134, acc: 0.56640625] [gan loss: 0.619596, acc: 0.927734]\n",
            "2151: [discriminator loss: 0.6979000568389893, acc: 0.4765625] [gan loss: 0.929595, acc: 0.000000]\n",
            "2152: [discriminator loss: 0.6989331245422363, acc: 0.5810546875] [gan loss: 0.592381, acc: 0.974609]\n",
            "2153: [discriminator loss: 0.7083207964897156, acc: 0.494140625] [gan loss: 0.951819, acc: 0.000000]\n",
            "2154: [discriminator loss: 0.691010594367981, acc: 0.5634765625] [gan loss: 0.596689, acc: 0.992188]\n",
            "2155: [discriminator loss: 0.6976026892662048, acc: 0.4921875] [gan loss: 0.923028, acc: 0.000000]\n",
            "2156: [discriminator loss: 0.6937914490699768, acc: 0.587890625] [gan loss: 0.638124, acc: 0.867188]\n",
            "2157: [discriminator loss: 0.6883610486984253, acc: 0.4833984375] [gan loss: 0.845527, acc: 0.000000]\n",
            "2158: [discriminator loss: 0.6899483799934387, acc: 0.6103515625] [gan loss: 0.657033, acc: 0.720703]\n",
            "2159: [discriminator loss: 0.6824876070022583, acc: 0.4736328125] [gan loss: 0.872953, acc: 0.000000]\n",
            "2160: [discriminator loss: 0.6877679228782654, acc: 0.599609375] [gan loss: 0.662188, acc: 0.707031]\n",
            "2161: [discriminator loss: 0.6898750066757202, acc: 0.4853515625] [gan loss: 0.875932, acc: 0.000000]\n",
            "2162: [discriminator loss: 0.6872029304504395, acc: 0.58984375] [gan loss: 0.667211, acc: 0.712891]\n",
            "2163: [discriminator loss: 0.6794119477272034, acc: 0.5029296875] [gan loss: 0.875485, acc: 0.000000]\n",
            "2164: [discriminator loss: 0.6860718727111816, acc: 0.6201171875] [gan loss: 0.651061, acc: 0.843750]\n",
            "2165: [discriminator loss: 0.6748331189155579, acc: 0.4931640625] [gan loss: 0.909264, acc: 0.000000]\n",
            "2166: [discriminator loss: 0.6824193000793457, acc: 0.6123046875] [gan loss: 0.613982, acc: 0.966797]\n",
            "2167: [discriminator loss: 0.6879370808601379, acc: 0.5] [gan loss: 0.950464, acc: 0.000000]\n",
            "2168: [discriminator loss: 0.6830412745475769, acc: 0.5830078125] [gan loss: 0.613750, acc: 0.955078]\n",
            "2169: [discriminator loss: 0.6858893632888794, acc: 0.4931640625] [gan loss: 0.915791, acc: 0.000000]\n",
            "2170: [discriminator loss: 0.6821590065956116, acc: 0.599609375] [gan loss: 0.629012, acc: 0.869141]\n",
            "2171: [discriminator loss: 0.6825177073478699, acc: 0.5009765625] [gan loss: 0.895621, acc: 0.011719]\n",
            "2172: [discriminator loss: 0.6901403665542603, acc: 0.564453125] [gan loss: 0.647547, acc: 0.804688]\n",
            "2173: [discriminator loss: 0.6846386790275574, acc: 0.494140625] [gan loss: 0.871270, acc: 0.000000]\n",
            "2174: [discriminator loss: 0.6885032653808594, acc: 0.57421875] [gan loss: 0.661002, acc: 0.707031]\n",
            "2175: [discriminator loss: 0.6835794448852539, acc: 0.5087890625] [gan loss: 0.843648, acc: 0.000000]\n",
            "2176: [discriminator loss: 0.6927156448364258, acc: 0.5556640625] [gan loss: 0.657377, acc: 0.726562]\n",
            "2177: [discriminator loss: 0.6758925318717957, acc: 0.51171875] [gan loss: 0.870087, acc: 0.029297]\n",
            "2178: [discriminator loss: 0.700465202331543, acc: 0.546875] [gan loss: 0.639149, acc: 0.890625]\n",
            "2179: [discriminator loss: 0.6825181841850281, acc: 0.4931640625] [gan loss: 0.923192, acc: 0.000000]\n",
            "2180: [discriminator loss: 0.6947638392448425, acc: 0.607421875] [gan loss: 0.609906, acc: 0.964844]\n",
            "2181: [discriminator loss: 0.6918743848800659, acc: 0.4833984375] [gan loss: 0.926286, acc: 0.000000]\n",
            "2182: [discriminator loss: 0.683646559715271, acc: 0.5986328125] [gan loss: 0.624443, acc: 0.921875]\n",
            "2183: [discriminator loss: 0.6881780624389648, acc: 0.4912109375] [gan loss: 0.904212, acc: 0.000000]\n",
            "2184: [discriminator loss: 0.687751293182373, acc: 0.603515625] [gan loss: 0.633645, acc: 0.863281]\n",
            "2185: [discriminator loss: 0.6840676069259644, acc: 0.4873046875] [gan loss: 0.885517, acc: 0.000000]\n",
            "2186: [discriminator loss: 0.6814396977424622, acc: 0.6083984375] [gan loss: 0.633649, acc: 0.779297]\n",
            "2187: [discriminator loss: 0.6802101135253906, acc: 0.5068359375] [gan loss: 0.901497, acc: 0.000000]\n",
            "2188: [discriminator loss: 0.6797274947166443, acc: 0.609375] [gan loss: 0.649636, acc: 0.720703]\n",
            "2189: [discriminator loss: 0.6814557313919067, acc: 0.4970703125] [gan loss: 0.877335, acc: 0.000000]\n",
            "2190: [discriminator loss: 0.6771420240402222, acc: 0.6005859375] [gan loss: 0.673275, acc: 0.576172]\n",
            "2191: [discriminator loss: 0.6731832027435303, acc: 0.5234375] [gan loss: 0.891057, acc: 0.000000]\n",
            "2192: [discriminator loss: 0.6718130707740784, acc: 0.6328125] [gan loss: 0.682801, acc: 0.582031]\n",
            "2193: [discriminator loss: 0.6717689633369446, acc: 0.5439453125] [gan loss: 0.910705, acc: 0.000000]\n",
            "2194: [discriminator loss: 0.6654685735702515, acc: 0.654296875] [gan loss: 0.663512, acc: 0.570312]\n",
            "2195: [discriminator loss: 0.6736145615577698, acc: 0.521484375] [gan loss: 0.952399, acc: 0.000000]\n",
            "2196: [discriminator loss: 0.6680648326873779, acc: 0.6142578125] [gan loss: 0.646931, acc: 0.681641]\n",
            "2197: [discriminator loss: 0.6823883652687073, acc: 0.5] [gan loss: 0.945217, acc: 0.000000]\n",
            "2198: [discriminator loss: 0.673800528049469, acc: 0.591796875] [gan loss: 0.649253, acc: 0.675781]\n",
            "2199: [discriminator loss: 0.6837319731712341, acc: 0.5] [gan loss: 0.912564, acc: 0.000000]\n",
            "2200: [discriminator loss: 0.6806955933570862, acc: 0.5771484375] [gan loss: 0.653089, acc: 0.652344]\n",
            "2201: [discriminator loss: 0.6949174404144287, acc: 0.4921875] [gan loss: 0.913087, acc: 0.000000]\n",
            "2202: [discriminator loss: 0.6891120076179504, acc: 0.5537109375] [gan loss: 0.648996, acc: 0.796875]\n",
            "2203: [discriminator loss: 0.6965392231941223, acc: 0.4892578125] [gan loss: 0.928174, acc: 0.000000]\n",
            "2204: [discriminator loss: 0.6900746822357178, acc: 0.5615234375] [gan loss: 0.630831, acc: 0.839844]\n",
            "2205: [discriminator loss: 0.7007616758346558, acc: 0.4853515625] [gan loss: 0.921254, acc: 0.000000]\n",
            "2206: [discriminator loss: 0.6897799372673035, acc: 0.564453125] [gan loss: 0.643609, acc: 0.738281]\n",
            "2207: [discriminator loss: 0.6993683576583862, acc: 0.4833984375] [gan loss: 0.898860, acc: 0.000000]\n",
            "2208: [discriminator loss: 0.6855350732803345, acc: 0.56640625] [gan loss: 0.659909, acc: 0.708984]\n",
            "2209: [discriminator loss: 0.6956769227981567, acc: 0.4716796875] [gan loss: 0.879413, acc: 0.000000]\n",
            "2210: [discriminator loss: 0.6813749074935913, acc: 0.62109375] [gan loss: 0.634330, acc: 0.802734]\n",
            "2211: [discriminator loss: 0.6986241936683655, acc: 0.4794921875] [gan loss: 0.927413, acc: 0.000000]\n",
            "2212: [discriminator loss: 0.6817379593849182, acc: 0.5830078125] [gan loss: 0.629292, acc: 0.857422]\n",
            "2213: [discriminator loss: 0.6925786137580872, acc: 0.4931640625] [gan loss: 0.930417, acc: 0.000000]\n",
            "2214: [discriminator loss: 0.6825112700462341, acc: 0.5810546875] [gan loss: 0.630310, acc: 0.824219]\n",
            "2215: [discriminator loss: 0.6900275349617004, acc: 0.490234375] [gan loss: 0.903132, acc: 0.000000]\n",
            "2216: [discriminator loss: 0.6736827492713928, acc: 0.6181640625] [gan loss: 0.665543, acc: 0.689453]\n",
            "2217: [discriminator loss: 0.6816104054450989, acc: 0.4921875] [gan loss: 0.870346, acc: 0.000000]\n",
            "2218: [discriminator loss: 0.6835281252861023, acc: 0.5927734375] [gan loss: 0.677525, acc: 0.580078]\n",
            "2219: [discriminator loss: 0.6821321845054626, acc: 0.5185546875] [gan loss: 0.873776, acc: 0.000000]\n",
            "2220: [discriminator loss: 0.6801055669784546, acc: 0.587890625] [gan loss: 0.662627, acc: 0.625000]\n",
            "2221: [discriminator loss: 0.6869542598724365, acc: 0.5029296875] [gan loss: 0.913956, acc: 0.000000]\n",
            "2222: [discriminator loss: 0.679707944393158, acc: 0.6142578125] [gan loss: 0.653792, acc: 0.710938]\n",
            "2223: [discriminator loss: 0.6905854344367981, acc: 0.490234375] [gan loss: 0.930534, acc: 0.000000]\n",
            "2224: [discriminator loss: 0.686362087726593, acc: 0.6015625] [gan loss: 0.618029, acc: 0.884766]\n",
            "2225: [discriminator loss: 0.6917979717254639, acc: 0.5009765625] [gan loss: 0.933267, acc: 0.000000]\n",
            "2226: [discriminator loss: 0.6878548860549927, acc: 0.5908203125] [gan loss: 0.614335, acc: 0.902344]\n",
            "2227: [discriminator loss: 0.6936483383178711, acc: 0.5] [gan loss: 0.928200, acc: 0.000000]\n",
            "2228: [discriminator loss: 0.6791356801986694, acc: 0.6015625] [gan loss: 0.619129, acc: 0.902344]\n",
            "2229: [discriminator loss: 0.6919111013412476, acc: 0.4931640625] [gan loss: 0.919555, acc: 0.000000]\n",
            "2230: [discriminator loss: 0.6838608980178833, acc: 0.611328125] [gan loss: 0.628509, acc: 0.865234]\n",
            "2231: [discriminator loss: 0.6977388858795166, acc: 0.4970703125] [gan loss: 0.898531, acc: 0.000000]\n",
            "2232: [discriminator loss: 0.6879233717918396, acc: 0.60546875] [gan loss: 0.646983, acc: 0.804688]\n",
            "2233: [discriminator loss: 0.6940460801124573, acc: 0.4931640625] [gan loss: 0.889883, acc: 0.007812]\n",
            "2234: [discriminator loss: 0.6831642985343933, acc: 0.5888671875] [gan loss: 0.659706, acc: 0.726562]\n",
            "2235: [discriminator loss: 0.695460319519043, acc: 0.484375] [gan loss: 0.893725, acc: 0.000000]\n",
            "2236: [discriminator loss: 0.6689260005950928, acc: 0.65234375] [gan loss: 0.652680, acc: 0.744141]\n",
            "2237: [discriminator loss: 0.682341992855072, acc: 0.509765625] [gan loss: 0.942086, acc: 0.000000]\n",
            "2238: [discriminator loss: 0.6692598462104797, acc: 0.6396484375] [gan loss: 0.604265, acc: 0.921875]\n",
            "2239: [discriminator loss: 0.6978191137313843, acc: 0.4931640625] [gan loss: 0.967185, acc: 0.000000]\n",
            "2240: [discriminator loss: 0.669626772403717, acc: 0.6259765625] [gan loss: 0.632782, acc: 0.837891]\n",
            "2241: [discriminator loss: 0.6925310492515564, acc: 0.4853515625] [gan loss: 0.915821, acc: 0.000000]\n",
            "2242: [discriminator loss: 0.6824143528938293, acc: 0.6015625] [gan loss: 0.654052, acc: 0.724609]\n",
            "2243: [discriminator loss: 0.694640040397644, acc: 0.4853515625] [gan loss: 0.909218, acc: 0.003906]\n",
            "2244: [discriminator loss: 0.6837476491928101, acc: 0.603515625] [gan loss: 0.644706, acc: 0.826172]\n",
            "2245: [discriminator loss: 0.7026256918907166, acc: 0.470703125] [gan loss: 0.912877, acc: 0.000000]\n",
            "2246: [discriminator loss: 0.6949330568313599, acc: 0.5703125] [gan loss: 0.640660, acc: 0.732422]\n",
            "2247: [discriminator loss: 0.7042722105979919, acc: 0.4765625] [gan loss: 0.913811, acc: 0.001953]\n",
            "2248: [discriminator loss: 0.6983730792999268, acc: 0.5537109375] [gan loss: 0.638917, acc: 0.847656]\n",
            "2249: [discriminator loss: 0.7053815722465515, acc: 0.46875] [gan loss: 0.877175, acc: 0.001953]\n",
            "2250: [discriminator loss: 0.6972799897193909, acc: 0.515625] [gan loss: 0.667453, acc: 0.708984]\n",
            "2251: [discriminator loss: 0.7028513550758362, acc: 0.4765625] [gan loss: 0.870635, acc: 0.000000]\n",
            "2252: [discriminator loss: 0.6946128010749817, acc: 0.5625] [gan loss: 0.660054, acc: 0.736328]\n",
            "2253: [discriminator loss: 0.6965439319610596, acc: 0.4833984375] [gan loss: 0.876024, acc: 0.000000]\n",
            "2254: [discriminator loss: 0.6932070255279541, acc: 0.5703125] [gan loss: 0.657511, acc: 0.736328]\n",
            "2255: [discriminator loss: 0.697135865688324, acc: 0.4921875] [gan loss: 0.912637, acc: 0.000000]\n",
            "2256: [discriminator loss: 0.6832993626594543, acc: 0.6181640625] [gan loss: 0.644460, acc: 0.779297]\n",
            "2257: [discriminator loss: 0.6968884468078613, acc: 0.4990234375] [gan loss: 0.935649, acc: 0.000000]\n",
            "2258: [discriminator loss: 0.690690279006958, acc: 0.533203125] [gan loss: 0.627795, acc: 0.806641]\n",
            "2259: [discriminator loss: 0.6953529119491577, acc: 0.4921875] [gan loss: 0.948120, acc: 0.000000]\n",
            "2260: [discriminator loss: 0.685157060623169, acc: 0.5419921875] [gan loss: 0.621589, acc: 0.828125]\n",
            "2261: [discriminator loss: 0.701178789138794, acc: 0.4921875] [gan loss: 0.929125, acc: 0.000000]\n",
            "2262: [discriminator loss: 0.6824546456336975, acc: 0.5927734375] [gan loss: 0.635699, acc: 0.722656]\n",
            "2263: [discriminator loss: 0.6933059096336365, acc: 0.484375] [gan loss: 0.924031, acc: 0.000000]\n",
            "2264: [discriminator loss: 0.688642680644989, acc: 0.591796875] [gan loss: 0.645061, acc: 0.710938]\n",
            "2265: [discriminator loss: 0.694290816783905, acc: 0.48046875] [gan loss: 0.911851, acc: 0.000000]\n",
            "2266: [discriminator loss: 0.6928362846374512, acc: 0.587890625] [gan loss: 0.639770, acc: 0.783203]\n",
            "2267: [discriminator loss: 0.6962780952453613, acc: 0.482421875] [gan loss: 0.919053, acc: 0.000000]\n",
            "2268: [discriminator loss: 0.6840568780899048, acc: 0.5947265625] [gan loss: 0.636243, acc: 0.845703]\n",
            "2269: [discriminator loss: 0.6957681179046631, acc: 0.4873046875] [gan loss: 0.919964, acc: 0.000000]\n",
            "2270: [discriminator loss: 0.698199450969696, acc: 0.546875] [gan loss: 0.647281, acc: 0.794922]\n",
            "2271: [discriminator loss: 0.6972061395645142, acc: 0.4658203125] [gan loss: 0.894832, acc: 0.000000]\n",
            "2272: [discriminator loss: 0.6913368105888367, acc: 0.591796875] [gan loss: 0.660330, acc: 0.744141]\n",
            "2273: [discriminator loss: 0.6927382349967957, acc: 0.4560546875] [gan loss: 0.900472, acc: 0.000000]\n",
            "2274: [discriminator loss: 0.6812765002250671, acc: 0.640625] [gan loss: 0.651770, acc: 0.767578]\n",
            "2275: [discriminator loss: 0.6900319457054138, acc: 0.47265625] [gan loss: 0.920539, acc: 0.000000]\n",
            "2276: [discriminator loss: 0.681954562664032, acc: 0.623046875] [gan loss: 0.631076, acc: 0.845703]\n",
            "2277: [discriminator loss: 0.6935451030731201, acc: 0.4892578125] [gan loss: 0.966949, acc: 0.000000]\n",
            "2278: [discriminator loss: 0.6804861426353455, acc: 0.58984375] [gan loss: 0.614619, acc: 0.912109]\n",
            "2279: [discriminator loss: 0.698603630065918, acc: 0.4873046875] [gan loss: 0.935370, acc: 0.000000]\n",
            "2280: [discriminator loss: 0.6773092746734619, acc: 0.595703125] [gan loss: 0.650010, acc: 0.732422]\n",
            "2281: [discriminator loss: 0.6901718378067017, acc: 0.47265625] [gan loss: 0.899167, acc: 0.000000]\n",
            "2282: [discriminator loss: 0.6723434329032898, acc: 0.662109375] [gan loss: 0.665889, acc: 0.621094]\n",
            "2283: [discriminator loss: 0.685861349105835, acc: 0.482421875] [gan loss: 0.897907, acc: 0.000000]\n",
            "2284: [discriminator loss: 0.6706499457359314, acc: 0.669921875] [gan loss: 0.677142, acc: 0.562500]\n",
            "2285: [discriminator loss: 0.6837714314460754, acc: 0.486328125] [gan loss: 0.896917, acc: 0.000000]\n",
            "2286: [discriminator loss: 0.6800073385238647, acc: 0.6455078125] [gan loss: 0.653609, acc: 0.691406]\n",
            "2287: [discriminator loss: 0.6891348958015442, acc: 0.494140625] [gan loss: 0.902012, acc: 0.000000]\n",
            "2288: [discriminator loss: 0.6884723901748657, acc: 0.5791015625] [gan loss: 0.660985, acc: 0.656250]\n",
            "2289: [discriminator loss: 0.6969523429870605, acc: 0.4921875] [gan loss: 0.903223, acc: 0.000000]\n",
            "2290: [discriminator loss: 0.6892303824424744, acc: 0.5556640625] [gan loss: 0.646311, acc: 0.701172]\n",
            "2291: [discriminator loss: 0.6989854574203491, acc: 0.5126953125] [gan loss: 0.923609, acc: 0.000000]\n",
            "2292: [discriminator loss: 0.6890690326690674, acc: 0.5595703125] [gan loss: 0.616603, acc: 0.802734]\n",
            "2293: [discriminator loss: 0.7049978971481323, acc: 0.4912109375] [gan loss: 0.938892, acc: 0.000000]\n",
            "2294: [discriminator loss: 0.6905418038368225, acc: 0.5556640625] [gan loss: 0.630627, acc: 0.804688]\n",
            "2295: [discriminator loss: 0.6966990232467651, acc: 0.5009765625] [gan loss: 0.889466, acc: 0.000000]\n",
            "2296: [discriminator loss: 0.6849213242530823, acc: 0.5498046875] [gan loss: 0.669766, acc: 0.640625]\n",
            "2297: [discriminator loss: 0.6869247555732727, acc: 0.5107421875] [gan loss: 0.865131, acc: 0.035156]\n",
            "2298: [discriminator loss: 0.6795920729637146, acc: 0.599609375] [gan loss: 0.672520, acc: 0.613281]\n",
            "2299: [discriminator loss: 0.689508318901062, acc: 0.4658203125] [gan loss: 0.910996, acc: 0.000000]\n",
            "2300: [discriminator loss: 0.6834473609924316, acc: 0.5888671875] [gan loss: 0.644832, acc: 0.787109]\n",
            "2301: [discriminator loss: 0.6960621476173401, acc: 0.474609375] [gan loss: 0.934336, acc: 0.000000]\n",
            "2302: [discriminator loss: 0.6742868423461914, acc: 0.6025390625] [gan loss: 0.605704, acc: 0.925781]\n",
            "2303: [discriminator loss: 0.7030642628669739, acc: 0.4921875] [gan loss: 0.929034, acc: 0.000000]\n",
            "2304: [discriminator loss: 0.6853616833686829, acc: 0.591796875] [gan loss: 0.621654, acc: 0.855469]\n",
            "2305: [discriminator loss: 0.7040796279907227, acc: 0.4736328125] [gan loss: 0.906694, acc: 0.000000]\n",
            "2306: [discriminator loss: 0.6892865896224976, acc: 0.5869140625] [gan loss: 0.628283, acc: 0.851562]\n",
            "2307: [discriminator loss: 0.7040125727653503, acc: 0.48046875] [gan loss: 0.898057, acc: 0.000000]\n",
            "2308: [discriminator loss: 0.676379919052124, acc: 0.6142578125] [gan loss: 0.634891, acc: 0.798828]\n",
            "2309: [discriminator loss: 0.7052364349365234, acc: 0.470703125] [gan loss: 0.900122, acc: 0.001953]\n",
            "2310: [discriminator loss: 0.6801561117172241, acc: 0.62890625] [gan loss: 0.648883, acc: 0.792969]\n",
            "2311: [discriminator loss: 0.6901342868804932, acc: 0.482421875] [gan loss: 0.912594, acc: 0.000000]\n",
            "2312: [discriminator loss: 0.6716116070747375, acc: 0.6171875] [gan loss: 0.635034, acc: 0.847656]\n",
            "2313: [discriminator loss: 0.6918727159500122, acc: 0.490234375] [gan loss: 0.931485, acc: 0.000000]\n",
            "2314: [discriminator loss: 0.6741780638694763, acc: 0.60546875] [gan loss: 0.621117, acc: 0.898438]\n",
            "2315: [discriminator loss: 0.7011138200759888, acc: 0.4814453125] [gan loss: 0.931182, acc: 0.000000]\n",
            "2316: [discriminator loss: 0.6764445304870605, acc: 0.599609375] [gan loss: 0.638326, acc: 0.833984]\n",
            "2317: [discriminator loss: 0.6982488036155701, acc: 0.470703125] [gan loss: 0.904099, acc: 0.000000]\n",
            "2318: [discriminator loss: 0.6746252179145813, acc: 0.6162109375] [gan loss: 0.643662, acc: 0.730469]\n",
            "2319: [discriminator loss: 0.7061984539031982, acc: 0.47265625] [gan loss: 0.905885, acc: 0.000000]\n",
            "2320: [discriminator loss: 0.6850982904434204, acc: 0.5673828125] [gan loss: 0.629334, acc: 0.878906]\n",
            "2321: [discriminator loss: 0.7119618654251099, acc: 0.462890625] [gan loss: 0.914219, acc: 0.000000]\n",
            "2322: [discriminator loss: 0.6747604012489319, acc: 0.6103515625] [gan loss: 0.607944, acc: 0.900391]\n",
            "2323: [discriminator loss: 0.7149117588996887, acc: 0.4794921875] [gan loss: 0.920997, acc: 0.000000]\n",
            "2324: [discriminator loss: 0.6726025342941284, acc: 0.5908203125] [gan loss: 0.618486, acc: 0.880859]\n",
            "2325: [discriminator loss: 0.7118845582008362, acc: 0.4833984375] [gan loss: 0.901160, acc: 0.000000]\n",
            "2326: [discriminator loss: 0.688203752040863, acc: 0.5712890625] [gan loss: 0.636718, acc: 0.798828]\n",
            "2327: [discriminator loss: 0.7076089382171631, acc: 0.482421875] [gan loss: 0.869444, acc: 0.052734]\n",
            "2328: [discriminator loss: 0.6853691339492798, acc: 0.58203125] [gan loss: 0.641816, acc: 0.703125]\n",
            "2329: [discriminator loss: 0.710038959980011, acc: 0.4775390625] [gan loss: 0.853848, acc: 0.074219]\n",
            "2330: [discriminator loss: 0.6743534803390503, acc: 0.603515625] [gan loss: 0.637474, acc: 0.720703]\n",
            "2331: [discriminator loss: 0.7091211676597595, acc: 0.4794921875] [gan loss: 0.905322, acc: 0.000000]\n",
            "2332: [discriminator loss: 0.6660295128822327, acc: 0.62109375] [gan loss: 0.606387, acc: 0.931641]\n",
            "2333: [discriminator loss: 0.7146711945533752, acc: 0.4833984375] [gan loss: 0.924066, acc: 0.000000]\n",
            "2334: [discriminator loss: 0.6838502883911133, acc: 0.5849609375] [gan loss: 0.622870, acc: 0.927734]\n",
            "2335: [discriminator loss: 0.6992534399032593, acc: 0.4814453125] [gan loss: 0.893315, acc: 0.000000]\n",
            "2336: [discriminator loss: 0.6782623529434204, acc: 0.609375] [gan loss: 0.629005, acc: 0.900391]\n",
            "2337: [discriminator loss: 0.7015109062194824, acc: 0.4794921875] [gan loss: 0.865737, acc: 0.013672]\n",
            "2338: [discriminator loss: 0.6765075922012329, acc: 0.6259765625] [gan loss: 0.629947, acc: 0.826172]\n",
            "2339: [discriminator loss: 0.7117201089859009, acc: 0.4619140625] [gan loss: 0.893363, acc: 0.000000]\n",
            "2340: [discriminator loss: 0.6741287708282471, acc: 0.58984375] [gan loss: 0.597625, acc: 0.960938]\n",
            "2341: [discriminator loss: 0.7194409370422363, acc: 0.4833984375] [gan loss: 0.912121, acc: 0.000000]\n",
            "2342: [discriminator loss: 0.6893219947814941, acc: 0.5478515625] [gan loss: 0.589613, acc: 0.976562]\n",
            "2343: [discriminator loss: 0.7242329120635986, acc: 0.4833984375] [gan loss: 0.908004, acc: 0.000000]\n",
            "2344: [discriminator loss: 0.680478572845459, acc: 0.6005859375] [gan loss: 0.600105, acc: 0.958984]\n",
            "2345: [discriminator loss: 0.7152508497238159, acc: 0.4833984375] [gan loss: 0.909919, acc: 0.000000]\n",
            "2346: [discriminator loss: 0.6777633428573608, acc: 0.625] [gan loss: 0.616076, acc: 0.949219]\n",
            "2347: [discriminator loss: 0.7134723663330078, acc: 0.470703125] [gan loss: 0.880016, acc: 0.000000]\n",
            "2348: [discriminator loss: 0.6867538094520569, acc: 0.6103515625] [gan loss: 0.638192, acc: 0.865234]\n",
            "2349: [discriminator loss: 0.706749677658081, acc: 0.4775390625] [gan loss: 0.856941, acc: 0.000000]\n",
            "2350: [discriminator loss: 0.6919727921485901, acc: 0.5673828125] [gan loss: 0.665866, acc: 0.705078]\n",
            "2351: [discriminator loss: 0.6977469325065613, acc: 0.4443359375] [gan loss: 0.852668, acc: 0.000000]\n",
            "2352: [discriminator loss: 0.679521381855011, acc: 0.634765625] [gan loss: 0.661560, acc: 0.746094]\n",
            "2353: [discriminator loss: 0.6984546780586243, acc: 0.4453125] [gan loss: 0.900176, acc: 0.000000]\n",
            "2354: [discriminator loss: 0.6801660656929016, acc: 0.59765625] [gan loss: 0.648091, acc: 0.822266]\n",
            "2355: [discriminator loss: 0.6965734958648682, acc: 0.4794921875] [gan loss: 0.912091, acc: 0.000000]\n",
            "2356: [discriminator loss: 0.6861629486083984, acc: 0.595703125] [gan loss: 0.632048, acc: 0.880859]\n",
            "2357: [discriminator loss: 0.6979081034660339, acc: 0.484375] [gan loss: 0.929422, acc: 0.000000]\n",
            "2358: [discriminator loss: 0.687553346157074, acc: 0.6123046875] [gan loss: 0.640027, acc: 0.884766]\n",
            "2359: [discriminator loss: 0.6930010914802551, acc: 0.4697265625] [gan loss: 0.934696, acc: 0.000000]\n",
            "2360: [discriminator loss: 0.6687554121017456, acc: 0.6328125] [gan loss: 0.614032, acc: 0.949219]\n",
            "2361: [discriminator loss: 0.698514461517334, acc: 0.4833984375] [gan loss: 0.950760, acc: 0.000000]\n",
            "2362: [discriminator loss: 0.6768662333488464, acc: 0.6201171875] [gan loss: 0.626868, acc: 0.890625]\n",
            "2363: [discriminator loss: 0.6956496238708496, acc: 0.4794921875] [gan loss: 0.918206, acc: 0.000000]\n",
            "2364: [discriminator loss: 0.6768186688423157, acc: 0.626953125] [gan loss: 0.630023, acc: 0.876953]\n",
            "2365: [discriminator loss: 0.7021051645278931, acc: 0.4716796875] [gan loss: 0.928829, acc: 0.000000]\n",
            "2366: [discriminator loss: 0.6699536442756653, acc: 0.6181640625] [gan loss: 0.624890, acc: 0.886719]\n",
            "2367: [discriminator loss: 0.7016034126281738, acc: 0.482421875] [gan loss: 0.895456, acc: 0.003906]\n",
            "2368: [discriminator loss: 0.6862434148788452, acc: 0.5703125] [gan loss: 0.644628, acc: 0.748047]\n",
            "2369: [discriminator loss: 0.6930775046348572, acc: 0.48046875] [gan loss: 0.892118, acc: 0.000000]\n",
            "2370: [discriminator loss: 0.6674566268920898, acc: 0.6171875] [gan loss: 0.628775, acc: 0.812500]\n",
            "2371: [discriminator loss: 0.6957563161849976, acc: 0.4794921875] [gan loss: 0.919818, acc: 0.000000]\n",
            "2372: [discriminator loss: 0.6725548505783081, acc: 0.58984375] [gan loss: 0.640405, acc: 0.775391]\n",
            "2373: [discriminator loss: 0.696523904800415, acc: 0.4794921875] [gan loss: 0.907618, acc: 0.007812]\n",
            "2374: [discriminator loss: 0.6830590963363647, acc: 0.6044921875] [gan loss: 0.630310, acc: 0.896484]\n",
            "2375: [discriminator loss: 0.6967469453811646, acc: 0.486328125] [gan loss: 0.937407, acc: 0.000000]\n",
            "2376: [discriminator loss: 0.673998236656189, acc: 0.611328125] [gan loss: 0.585690, acc: 0.994141]\n",
            "2377: [discriminator loss: 0.7206265330314636, acc: 0.4921875] [gan loss: 0.949054, acc: 0.000000]\n",
            "2378: [discriminator loss: 0.6887229084968567, acc: 0.5498046875] [gan loss: 0.608531, acc: 0.929688]\n",
            "2379: [discriminator loss: 0.7165262699127197, acc: 0.4833984375] [gan loss: 0.895752, acc: 0.000000]\n",
            "2380: [discriminator loss: 0.7067366242408752, acc: 0.4892578125] [gan loss: 0.651781, acc: 0.783203]\n",
            "2381: [discriminator loss: 0.7121412754058838, acc: 0.4521484375] [gan loss: 0.828231, acc: 0.035156]\n",
            "2382: [discriminator loss: 0.7083012461662292, acc: 0.486328125] [gan loss: 0.667252, acc: 0.726562]\n",
            "2383: [discriminator loss: 0.7039666175842285, acc: 0.4619140625] [gan loss: 0.834040, acc: 0.027344]\n",
            "2384: [discriminator loss: 0.7002381086349487, acc: 0.5390625] [gan loss: 0.679366, acc: 0.601562]\n",
            "2385: [discriminator loss: 0.709488570690155, acc: 0.4189453125] [gan loss: 0.829889, acc: 0.000000]\n",
            "2386: [discriminator loss: 0.6947517991065979, acc: 0.55078125] [gan loss: 0.660606, acc: 0.703125]\n",
            "2387: [discriminator loss: 0.7109408974647522, acc: 0.451171875] [gan loss: 0.872312, acc: 0.000000]\n",
            "2388: [discriminator loss: 0.7002899646759033, acc: 0.5439453125] [gan loss: 0.602833, acc: 0.943359]\n",
            "2389: [discriminator loss: 0.7190102338790894, acc: 0.486328125] [gan loss: 0.940304, acc: 0.000000]\n",
            "2390: [discriminator loss: 0.6909211277961731, acc: 0.5703125] [gan loss: 0.570168, acc: 0.990234]\n",
            "2391: [discriminator loss: 0.7168968915939331, acc: 0.4912109375] [gan loss: 0.941243, acc: 0.000000]\n",
            "2392: [discriminator loss: 0.697135329246521, acc: 0.5537109375] [gan loss: 0.594405, acc: 0.960938]\n",
            "2393: [discriminator loss: 0.7240052223205566, acc: 0.48828125] [gan loss: 0.897601, acc: 0.000000]\n",
            "2394: [discriminator loss: 0.7119956016540527, acc: 0.51953125] [gan loss: 0.623334, acc: 0.880859]\n",
            "2395: [discriminator loss: 0.7158179879188538, acc: 0.45703125] [gan loss: 0.842338, acc: 0.009766]\n",
            "2396: [discriminator loss: 0.7051717042922974, acc: 0.505859375] [gan loss: 0.636152, acc: 0.853516]\n",
            "2397: [discriminator loss: 0.7190971970558167, acc: 0.43359375] [gan loss: 0.847400, acc: 0.001953]\n",
            "2398: [discriminator loss: 0.6943258047103882, acc: 0.5439453125] [gan loss: 0.603090, acc: 0.882812]\n",
            "2399: [discriminator loss: 0.7318021059036255, acc: 0.4765625] [gan loss: 0.895263, acc: 0.000000]\n",
            "2400: [discriminator loss: 0.6948864459991455, acc: 0.583984375] [gan loss: 0.584894, acc: 0.990234]\n",
            "2401: [discriminator loss: 0.7318837642669678, acc: 0.4892578125] [gan loss: 0.911153, acc: 0.000000]\n",
            "2402: [discriminator loss: 0.7143188118934631, acc: 0.505859375] [gan loss: 0.615997, acc: 0.923828]\n",
            "2403: [discriminator loss: 0.7238350510597229, acc: 0.4609375] [gan loss: 0.837756, acc: 0.000000]\n",
            "2404: [discriminator loss: 0.7061522603034973, acc: 0.51953125] [gan loss: 0.662025, acc: 0.695312]\n",
            "2405: [discriminator loss: 0.7144951224327087, acc: 0.419921875] [gan loss: 0.805408, acc: 0.003906]\n",
            "2406: [discriminator loss: 0.6938227415084839, acc: 0.5546875] [gan loss: 0.660988, acc: 0.734375]\n",
            "2407: [discriminator loss: 0.7147424221038818, acc: 0.44140625] [gan loss: 0.867938, acc: 0.000000]\n",
            "2408: [discriminator loss: 0.6881207227706909, acc: 0.6298828125] [gan loss: 0.596299, acc: 0.986328]\n",
            "2409: [discriminator loss: 0.7227463722229004, acc: 0.486328125] [gan loss: 0.931205, acc: 0.000000]\n",
            "2410: [discriminator loss: 0.6901607513427734, acc: 0.568359375] [gan loss: 0.549870, acc: 1.000000]\n",
            "2411: [discriminator loss: 0.7315866947174072, acc: 0.4921875] [gan loss: 0.923418, acc: 0.000000]\n",
            "2412: [discriminator loss: 0.6905925273895264, acc: 0.544921875] [gan loss: 0.571734, acc: 0.992188]\n",
            "2413: [discriminator loss: 0.7263990044593811, acc: 0.4873046875] [gan loss: 0.878724, acc: 0.000000]\n",
            "2414: [discriminator loss: 0.6836927533149719, acc: 0.5908203125] [gan loss: 0.589663, acc: 0.962891]\n",
            "2415: [discriminator loss: 0.7211968898773193, acc: 0.482421875] [gan loss: 0.879725, acc: 0.000000]\n",
            "2416: [discriminator loss: 0.6781715154647827, acc: 0.6005859375] [gan loss: 0.594701, acc: 0.960938]\n",
            "2417: [discriminator loss: 0.7228537201881409, acc: 0.474609375] [gan loss: 0.864381, acc: 0.000000]\n",
            "2418: [discriminator loss: 0.6864769458770752, acc: 0.5712890625] [gan loss: 0.605899, acc: 0.937500]\n",
            "2419: [discriminator loss: 0.728285014629364, acc: 0.46875] [gan loss: 0.850539, acc: 0.000000]\n",
            "2420: [discriminator loss: 0.689880907535553, acc: 0.595703125] [gan loss: 0.606365, acc: 0.943359]\n",
            "2421: [discriminator loss: 0.7076507210731506, acc: 0.48046875] [gan loss: 0.838967, acc: 0.000000]\n",
            "2422: [discriminator loss: 0.6955416798591614, acc: 0.564453125] [gan loss: 0.638361, acc: 0.904297]\n",
            "2423: [discriminator loss: 0.7012926936149597, acc: 0.4619140625] [gan loss: 0.796131, acc: 0.013672]\n",
            "2424: [discriminator loss: 0.6984850764274597, acc: 0.5078125] [gan loss: 0.681648, acc: 0.595703]\n",
            "2425: [discriminator loss: 0.6934645175933838, acc: 0.474609375] [gan loss: 0.805569, acc: 0.035156]\n",
            "2426: [discriminator loss: 0.6812370419502258, acc: 0.6318359375] [gan loss: 0.670587, acc: 0.695312]\n",
            "2427: [discriminator loss: 0.6991980671882629, acc: 0.451171875] [gan loss: 0.892503, acc: 0.000000]\n",
            "2428: [discriminator loss: 0.6513782739639282, acc: 0.65625] [gan loss: 0.532996, acc: 1.000000]\n",
            "2429: [discriminator loss: 0.7360457181930542, acc: 0.4951171875] [gan loss: 1.023284, acc: 0.000000]\n",
            "2430: [discriminator loss: 0.6601343154907227, acc: 0.544921875] [gan loss: 0.501316, acc: 1.000000]\n",
            "2431: [discriminator loss: 0.7334144115447998, acc: 0.4990234375] [gan loss: 0.891704, acc: 0.000000]\n",
            "2432: [discriminator loss: 0.6956223249435425, acc: 0.5439453125] [gan loss: 0.646747, acc: 0.798828]\n",
            "2433: [discriminator loss: 0.6954711675643921, acc: 0.4775390625] [gan loss: 0.782928, acc: 0.029297]\n",
            "2434: [discriminator loss: 0.689694344997406, acc: 0.5419921875] [gan loss: 0.685232, acc: 0.568359]\n",
            "2435: [discriminator loss: 0.6957680583000183, acc: 0.46875] [gan loss: 0.760798, acc: 0.146484]\n",
            "2436: [discriminator loss: 0.6866286993026733, acc: 0.529296875] [gan loss: 0.723128, acc: 0.308594]\n",
            "2437: [discriminator loss: 0.6893793940544128, acc: 0.4873046875] [gan loss: 0.768646, acc: 0.130859]\n",
            "2438: [discriminator loss: 0.682855486869812, acc: 0.5419921875] [gan loss: 0.722160, acc: 0.337891]\n",
            "2439: [discriminator loss: 0.6898448467254639, acc: 0.470703125] [gan loss: 0.794320, acc: 0.011719]\n",
            "2440: [discriminator loss: 0.6705727577209473, acc: 0.6279296875] [gan loss: 0.676739, acc: 0.671875]\n",
            "2441: [discriminator loss: 0.6970701813697815, acc: 0.458984375] [gan loss: 0.920436, acc: 0.000000]\n",
            "2442: [discriminator loss: 0.6700953841209412, acc: 0.6083984375] [gan loss: 0.542861, acc: 1.000000]\n",
            "2443: [discriminator loss: 0.726711630821228, acc: 0.4990234375] [gan loss: 1.076690, acc: 0.000000]\n",
            "2444: [discriminator loss: 0.6720383167266846, acc: 0.5546875] [gan loss: 0.495649, acc: 1.000000]\n",
            "2445: [discriminator loss: 0.7462146282196045, acc: 0.5] [gan loss: 0.953079, acc: 0.000000]\n",
            "2446: [discriminator loss: 0.6936314105987549, acc: 0.5537109375] [gan loss: 0.631993, acc: 0.888672]\n",
            "2447: [discriminator loss: 0.6971059441566467, acc: 0.4794921875] [gan loss: 0.820016, acc: 0.000000]\n",
            "2448: [discriminator loss: 0.6775570511817932, acc: 0.6552734375] [gan loss: 0.660940, acc: 0.744141]\n",
            "2449: [discriminator loss: 0.7010510563850403, acc: 0.4619140625] [gan loss: 0.839946, acc: 0.001953]\n",
            "2450: [discriminator loss: 0.6596213579177856, acc: 0.7119140625] [gan loss: 0.639555, acc: 0.882812]\n",
            "2451: [discriminator loss: 0.7104423642158508, acc: 0.4736328125] [gan loss: 0.866838, acc: 0.001953]\n",
            "2452: [discriminator loss: 0.6704153418540955, acc: 0.6240234375] [gan loss: 0.623951, acc: 0.894531]\n",
            "2453: [discriminator loss: 0.7023182511329651, acc: 0.4853515625] [gan loss: 0.875713, acc: 0.000000]\n",
            "2454: [discriminator loss: 0.6811033487319946, acc: 0.609375] [gan loss: 0.619544, acc: 0.898438]\n",
            "2455: [discriminator loss: 0.708495557308197, acc: 0.486328125] [gan loss: 0.902149, acc: 0.000000]\n",
            "2456: [discriminator loss: 0.6795118451118469, acc: 0.607421875] [gan loss: 0.610166, acc: 0.974609]\n",
            "2457: [discriminator loss: 0.7057289481163025, acc: 0.48828125] [gan loss: 0.921735, acc: 0.000000]\n",
            "2458: [discriminator loss: 0.6657037734985352, acc: 0.6396484375] [gan loss: 0.609578, acc: 0.962891]\n",
            "2459: [discriminator loss: 0.71054607629776, acc: 0.48828125] [gan loss: 0.931824, acc: 0.000000]\n",
            "2460: [discriminator loss: 0.6928138732910156, acc: 0.5791015625] [gan loss: 0.641064, acc: 0.804688]\n",
            "2461: [discriminator loss: 0.7012876868247986, acc: 0.4736328125] [gan loss: 0.890949, acc: 0.000000]\n",
            "2462: [discriminator loss: 0.6903378367424011, acc: 0.591796875] [gan loss: 0.666539, acc: 0.638672]\n",
            "2463: [discriminator loss: 0.7003042101860046, acc: 0.4462890625] [gan loss: 0.840603, acc: 0.000000]\n",
            "2464: [discriminator loss: 0.683111310005188, acc: 0.5703125] [gan loss: 0.716587, acc: 0.416016]\n",
            "2465: [discriminator loss: 0.6888582706451416, acc: 0.4833984375] [gan loss: 0.813159, acc: 0.025391]\n",
            "2466: [discriminator loss: 0.6810563802719116, acc: 0.595703125] [gan loss: 0.750506, acc: 0.314453]\n",
            "2467: [discriminator loss: 0.6808719635009766, acc: 0.52734375] [gan loss: 0.773386, acc: 0.162109]\n",
            "2468: [discriminator loss: 0.681376039981842, acc: 0.5537109375] [gan loss: 0.768382, acc: 0.220703]\n",
            "2469: [discriminator loss: 0.6810166835784912, acc: 0.5595703125] [gan loss: 0.723333, acc: 0.320312]\n",
            "2470: [discriminator loss: 0.6860777735710144, acc: 0.482421875] [gan loss: 0.855128, acc: 0.044922]\n",
            "2471: [discriminator loss: 0.673585832118988, acc: 0.6376953125] [gan loss: 0.620571, acc: 0.902344]\n",
            "2472: [discriminator loss: 0.7001091837882996, acc: 0.490234375] [gan loss: 1.052043, acc: 0.000000]\n",
            "2473: [discriminator loss: 0.6985729932785034, acc: 0.560546875] [gan loss: 0.553588, acc: 0.974609]\n",
            "2474: [discriminator loss: 0.7142540216445923, acc: 0.4970703125] [gan loss: 0.974784, acc: 0.000000]\n",
            "2475: [discriminator loss: 0.6882975101470947, acc: 0.5791015625] [gan loss: 0.616008, acc: 0.880859]\n",
            "2476: [discriminator loss: 0.6957877278327942, acc: 0.4931640625] [gan loss: 0.892776, acc: 0.001953]\n",
            "2477: [discriminator loss: 0.6879432201385498, acc: 0.5869140625] [gan loss: 0.657822, acc: 0.769531]\n",
            "2478: [discriminator loss: 0.6897314786911011, acc: 0.4775390625] [gan loss: 0.835309, acc: 0.054688]\n",
            "2479: [discriminator loss: 0.69605553150177, acc: 0.5546875] [gan loss: 0.665706, acc: 0.691406]\n",
            "2480: [discriminator loss: 0.6879643201828003, acc: 0.48046875] [gan loss: 0.832133, acc: 0.078125]\n",
            "2481: [discriminator loss: 0.6976261138916016, acc: 0.53125] [gan loss: 0.671194, acc: 0.642578]\n",
            "2482: [discriminator loss: 0.6970607042312622, acc: 0.4873046875] [gan loss: 0.868036, acc: 0.000000]\n",
            "2483: [discriminator loss: 0.7017980217933655, acc: 0.5244140625] [gan loss: 0.651568, acc: 0.775391]\n",
            "2484: [discriminator loss: 0.7013509273529053, acc: 0.4619140625] [gan loss: 0.857531, acc: 0.000000]\n",
            "2485: [discriminator loss: 0.697409987449646, acc: 0.5107421875] [gan loss: 0.641732, acc: 0.865234]\n",
            "2486: [discriminator loss: 0.7036573886871338, acc: 0.48046875] [gan loss: 0.885629, acc: 0.000000]\n",
            "2487: [discriminator loss: 0.7036496996879578, acc: 0.541015625] [gan loss: 0.619188, acc: 0.910156]\n",
            "2488: [discriminator loss: 0.702154278755188, acc: 0.494140625] [gan loss: 0.904601, acc: 0.000000]\n",
            "2489: [discriminator loss: 0.6914485692977905, acc: 0.5966796875] [gan loss: 0.608160, acc: 0.925781]\n",
            "2490: [discriminator loss: 0.6979100704193115, acc: 0.48828125] [gan loss: 0.916170, acc: 0.000000]\n",
            "2491: [discriminator loss: 0.692608654499054, acc: 0.5849609375] [gan loss: 0.603040, acc: 0.929688]\n",
            "2492: [discriminator loss: 0.6941618919372559, acc: 0.490234375] [gan loss: 0.901244, acc: 0.000000]\n",
            "2493: [discriminator loss: 0.6908797025680542, acc: 0.6142578125] [gan loss: 0.630349, acc: 0.847656]\n",
            "2494: [discriminator loss: 0.6930681467056274, acc: 0.4853515625] [gan loss: 0.878921, acc: 0.000000]\n",
            "2495: [discriminator loss: 0.6923404932022095, acc: 0.5712890625] [gan loss: 0.647431, acc: 0.695312]\n",
            "2496: [discriminator loss: 0.6970831751823425, acc: 0.4814453125] [gan loss: 0.887402, acc: 0.000000]\n",
            "2497: [discriminator loss: 0.691499650478363, acc: 0.5732421875] [gan loss: 0.643851, acc: 0.742188]\n",
            "2498: [discriminator loss: 0.6994966268539429, acc: 0.474609375] [gan loss: 0.871282, acc: 0.000000]\n",
            "2499: [discriminator loss: 0.6913282871246338, acc: 0.5595703125] [gan loss: 0.641833, acc: 0.755859]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "2500: [discriminator loss: 0.6922791600227356, acc: 0.4775390625] [gan loss: 0.872122, acc: 0.000000]\n",
            "2501: [discriminator loss: 0.6957748532295227, acc: 0.580078125] [gan loss: 0.638099, acc: 0.771484]\n",
            "2502: [discriminator loss: 0.7002808451652527, acc: 0.48046875] [gan loss: 0.902789, acc: 0.000000]\n",
            "2503: [discriminator loss: 0.6944421529769897, acc: 0.544921875] [gan loss: 0.611838, acc: 0.880859]\n",
            "2504: [discriminator loss: 0.6971474885940552, acc: 0.4970703125] [gan loss: 0.920303, acc: 0.000000]\n",
            "2505: [discriminator loss: 0.6845366358757019, acc: 0.583984375] [gan loss: 0.611617, acc: 0.855469]\n",
            "2506: [discriminator loss: 0.7070612907409668, acc: 0.4892578125] [gan loss: 0.908902, acc: 0.000000]\n",
            "2507: [discriminator loss: 0.6881425380706787, acc: 0.580078125] [gan loss: 0.615045, acc: 0.833984]\n",
            "2508: [discriminator loss: 0.7008164525032043, acc: 0.4970703125] [gan loss: 0.895021, acc: 0.000000]\n",
            "2509: [discriminator loss: 0.6940073370933533, acc: 0.51953125] [gan loss: 0.649085, acc: 0.687500]\n",
            "2510: [discriminator loss: 0.7008175253868103, acc: 0.478515625] [gan loss: 0.867525, acc: 0.003906]\n",
            "2511: [discriminator loss: 0.6944162249565125, acc: 0.54296875] [gan loss: 0.661388, acc: 0.634766]\n",
            "2512: [discriminator loss: 0.6990799903869629, acc: 0.4931640625] [gan loss: 0.847481, acc: 0.000000]\n",
            "2513: [discriminator loss: 0.6934611201286316, acc: 0.5380859375] [gan loss: 0.662064, acc: 0.621094]\n",
            "2514: [discriminator loss: 0.6903479695320129, acc: 0.5263671875] [gan loss: 0.861246, acc: 0.000000]\n",
            "2515: [discriminator loss: 0.6858267784118652, acc: 0.5908203125] [gan loss: 0.648138, acc: 0.664062]\n",
            "2516: [discriminator loss: 0.7018547654151917, acc: 0.490234375] [gan loss: 0.900349, acc: 0.000000]\n",
            "2517: [discriminator loss: 0.6828920841217041, acc: 0.5927734375] [gan loss: 0.629709, acc: 0.814453]\n",
            "2518: [discriminator loss: 0.6983233690261841, acc: 0.494140625] [gan loss: 0.920238, acc: 0.000000]\n",
            "2519: [discriminator loss: 0.6920226216316223, acc: 0.529296875] [gan loss: 0.609724, acc: 0.880859]\n",
            "2520: [discriminator loss: 0.6969825029373169, acc: 0.5009765625] [gan loss: 0.908320, acc: 0.000000]\n",
            "2521: [discriminator loss: 0.6902025938034058, acc: 0.51171875] [gan loss: 0.626625, acc: 0.761719]\n",
            "2522: [discriminator loss: 0.6988334059715271, acc: 0.5078125] [gan loss: 0.882127, acc: 0.001953]\n",
            "2523: [discriminator loss: 0.6926573514938354, acc: 0.5009765625] [gan loss: 0.646059, acc: 0.701172]\n",
            "2524: [discriminator loss: 0.6891611814498901, acc: 0.515625] [gan loss: 0.872815, acc: 0.001953]\n",
            "2525: [discriminator loss: 0.6762431263923645, acc: 0.541015625] [gan loss: 0.656945, acc: 0.646484]\n",
            "2526: [discriminator loss: 0.6956360340118408, acc: 0.5107421875] [gan loss: 0.909523, acc: 0.000000]\n",
            "2527: [discriminator loss: 0.6795191168785095, acc: 0.5751953125] [gan loss: 0.614832, acc: 0.789062]\n",
            "2528: [discriminator loss: 0.7083615064620972, acc: 0.49609375] [gan loss: 0.931330, acc: 0.000000]\n",
            "2529: [discriminator loss: 0.6688874363899231, acc: 0.5654296875] [gan loss: 0.579687, acc: 0.847656]\n",
            "2530: [discriminator loss: 0.7112813591957092, acc: 0.498046875] [gan loss: 0.935519, acc: 0.000000]\n",
            "2531: [discriminator loss: 0.6844102144241333, acc: 0.5546875] [gan loss: 0.621636, acc: 0.783203]\n",
            "2532: [discriminator loss: 0.7038846611976624, acc: 0.5029296875] [gan loss: 0.889296, acc: 0.000000]\n",
            "2533: [discriminator loss: 0.6852470636367798, acc: 0.5556640625] [gan loss: 0.656938, acc: 0.705078]\n",
            "2534: [discriminator loss: 0.6928575038909912, acc: 0.5009765625] [gan loss: 0.853719, acc: 0.019531]\n",
            "2535: [discriminator loss: 0.6737637519836426, acc: 0.5947265625] [gan loss: 0.656924, acc: 0.617188]\n",
            "2536: [discriminator loss: 0.707573413848877, acc: 0.5185546875] [gan loss: 0.901834, acc: 0.000000]\n",
            "2537: [discriminator loss: 0.6645206212997437, acc: 0.5849609375] [gan loss: 0.597508, acc: 0.835938]\n",
            "2538: [discriminator loss: 0.7146186828613281, acc: 0.4833984375] [gan loss: 0.909568, acc: 0.000000]\n",
            "2539: [discriminator loss: 0.6863530874252319, acc: 0.533203125] [gan loss: 0.619525, acc: 0.757812]\n",
            "2540: [discriminator loss: 0.6886669993400574, acc: 0.533203125] [gan loss: 0.854185, acc: 0.113281]\n",
            "2541: [discriminator loss: 0.6938003897666931, acc: 0.5078125] [gan loss: 0.676126, acc: 0.605469]\n",
            "2542: [discriminator loss: 0.6999149918556213, acc: 0.5341796875] [gan loss: 0.854567, acc: 0.000000]\n",
            "2543: [discriminator loss: 0.6765137910842896, acc: 0.55859375] [gan loss: 0.646419, acc: 0.681641]\n",
            "2544: [discriminator loss: 0.6959612369537354, acc: 0.5205078125] [gan loss: 0.899879, acc: 0.000000]\n",
            "2545: [discriminator loss: 0.6700922250747681, acc: 0.5810546875] [gan loss: 0.600069, acc: 0.808594]\n",
            "2546: [discriminator loss: 0.7162668704986572, acc: 0.501953125] [gan loss: 0.968047, acc: 0.000000]\n",
            "2547: [discriminator loss: 0.6702736020088196, acc: 0.5888671875] [gan loss: 0.580692, acc: 0.908203]\n",
            "2548: [discriminator loss: 0.7177368998527527, acc: 0.4970703125] [gan loss: 0.947499, acc: 0.000000]\n",
            "2549: [discriminator loss: 0.6928266286849976, acc: 0.541015625] [gan loss: 0.620377, acc: 0.783203]\n",
            "2550: [discriminator loss: 0.7113615870475769, acc: 0.49609375] [gan loss: 0.868667, acc: 0.017578]\n",
            "2551: [discriminator loss: 0.6826249957084656, acc: 0.5380859375] [gan loss: 0.651059, acc: 0.646484]\n",
            "2552: [discriminator loss: 0.7123032808303833, acc: 0.49609375] [gan loss: 0.824340, acc: 0.062500]\n",
            "2553: [discriminator loss: 0.6932570934295654, acc: 0.5185546875] [gan loss: 0.645183, acc: 0.716797]\n",
            "2554: [discriminator loss: 0.7092787623405457, acc: 0.4990234375] [gan loss: 0.855829, acc: 0.003906]\n",
            "2555: [discriminator loss: 0.6856685280799866, acc: 0.52734375] [gan loss: 0.608500, acc: 0.847656]\n",
            "2556: [discriminator loss: 0.7210870981216431, acc: 0.494140625] [gan loss: 0.909877, acc: 0.000000]\n",
            "2557: [discriminator loss: 0.6777863502502441, acc: 0.578125] [gan loss: 0.592685, acc: 0.931641]\n",
            "2558: [discriminator loss: 0.7128214240074158, acc: 0.494140625] [gan loss: 0.932130, acc: 0.000000]\n",
            "2559: [discriminator loss: 0.6651399731636047, acc: 0.5556640625] [gan loss: 0.563128, acc: 0.937500]\n",
            "2560: [discriminator loss: 0.7327451705932617, acc: 0.4931640625] [gan loss: 0.939388, acc: 0.000000]\n",
            "2561: [discriminator loss: 0.6947572827339172, acc: 0.5439453125] [gan loss: 0.619441, acc: 0.908203]\n",
            "2562: [discriminator loss: 0.707893431186676, acc: 0.4775390625] [gan loss: 0.854435, acc: 0.001953]\n",
            "2563: [discriminator loss: 0.70188969373703, acc: 0.5107421875] [gan loss: 0.669712, acc: 0.642578]\n",
            "2564: [discriminator loss: 0.7093809843063354, acc: 0.458984375] [gan loss: 0.821309, acc: 0.003906]\n",
            "2565: [discriminator loss: 0.6981714963912964, acc: 0.5263671875] [gan loss: 0.685517, acc: 0.544922]\n",
            "2566: [discriminator loss: 0.7003624439239502, acc: 0.4521484375] [gan loss: 0.805696, acc: 0.060547]\n",
            "2567: [discriminator loss: 0.6989787220954895, acc: 0.501953125] [gan loss: 0.726795, acc: 0.281250]\n",
            "2568: [discriminator loss: 0.6955592632293701, acc: 0.4677734375] [gan loss: 0.789746, acc: 0.083984]\n",
            "2569: [discriminator loss: 0.6955207586288452, acc: 0.4892578125] [gan loss: 0.763885, acc: 0.134766]\n",
            "2570: [discriminator loss: 0.6914582252502441, acc: 0.51171875] [gan loss: 0.708556, acc: 0.464844]\n",
            "2571: [discriminator loss: 0.695281982421875, acc: 0.4736328125] [gan loss: 0.872405, acc: 0.000000]\n",
            "2572: [discriminator loss: 0.6876077651977539, acc: 0.568359375] [gan loss: 0.617869, acc: 0.900391]\n",
            "2573: [discriminator loss: 0.7063148617744446, acc: 0.484375] [gan loss: 0.988542, acc: 0.000000]\n",
            "2574: [discriminator loss: 0.7013090252876282, acc: 0.51953125] [gan loss: 0.548834, acc: 0.998047]\n",
            "2575: [discriminator loss: 0.7229280471801758, acc: 0.4912109375] [gan loss: 0.981234, acc: 0.000000]\n",
            "2576: [discriminator loss: 0.7047222852706909, acc: 0.5302734375] [gan loss: 0.585771, acc: 0.958984]\n",
            "2577: [discriminator loss: 0.7137550115585327, acc: 0.4873046875] [gan loss: 0.922434, acc: 0.000000]\n",
            "2578: [discriminator loss: 0.685004472732544, acc: 0.556640625] [gan loss: 0.603271, acc: 0.916016]\n",
            "2579: [discriminator loss: 0.71910560131073, acc: 0.470703125] [gan loss: 0.909269, acc: 0.000000]\n",
            "2580: [discriminator loss: 0.6895363926887512, acc: 0.5322265625] [gan loss: 0.632704, acc: 0.818359]\n",
            "2581: [discriminator loss: 0.7128169536590576, acc: 0.462890625] [gan loss: 0.881289, acc: 0.000000]\n",
            "2582: [discriminator loss: 0.6874521374702454, acc: 0.5625] [gan loss: 0.650339, acc: 0.744141]\n",
            "2583: [discriminator loss: 0.7061346173286438, acc: 0.462890625] [gan loss: 0.872544, acc: 0.013672]\n",
            "2584: [discriminator loss: 0.6893298625946045, acc: 0.6015625] [gan loss: 0.676339, acc: 0.634766]\n",
            "2585: [discriminator loss: 0.6919957399368286, acc: 0.4599609375] [gan loss: 0.864482, acc: 0.019531]\n",
            "2586: [discriminator loss: 0.6827414035797119, acc: 0.603515625] [gan loss: 0.681180, acc: 0.529297]\n",
            "2587: [discriminator loss: 0.6921838521957397, acc: 0.48046875] [gan loss: 0.896416, acc: 0.021484]\n",
            "2588: [discriminator loss: 0.6859546303749084, acc: 0.580078125] [gan loss: 0.645738, acc: 0.781250]\n",
            "2589: [discriminator loss: 0.7052254676818848, acc: 0.4794921875] [gan loss: 0.954706, acc: 0.000000]\n",
            "2590: [discriminator loss: 0.6949000954627991, acc: 0.560546875] [gan loss: 0.609109, acc: 0.876953]\n",
            "2591: [discriminator loss: 0.7109032869338989, acc: 0.5009765625] [gan loss: 0.956883, acc: 0.000000]\n",
            "2592: [discriminator loss: 0.693107008934021, acc: 0.544921875] [gan loss: 0.609658, acc: 0.888672]\n",
            "2593: [discriminator loss: 0.7074854969978333, acc: 0.4990234375] [gan loss: 0.934208, acc: 0.000000]\n",
            "2594: [discriminator loss: 0.6881411075592041, acc: 0.5517578125] [gan loss: 0.622266, acc: 0.898438]\n",
            "2595: [discriminator loss: 0.7101837992668152, acc: 0.4814453125] [gan loss: 0.925435, acc: 0.000000]\n",
            "2596: [discriminator loss: 0.6895172595977783, acc: 0.5576171875] [gan loss: 0.632990, acc: 0.847656]\n",
            "2597: [discriminator loss: 0.7131063342094421, acc: 0.482421875] [gan loss: 0.911045, acc: 0.003906]\n",
            "2598: [discriminator loss: 0.6901171803474426, acc: 0.564453125] [gan loss: 0.661325, acc: 0.695312]\n",
            "2599: [discriminator loss: 0.703370213508606, acc: 0.4755859375] [gan loss: 0.886458, acc: 0.003906]\n",
            "2600: [discriminator loss: 0.694331705570221, acc: 0.5517578125] [gan loss: 0.674687, acc: 0.638672]\n",
            "2601: [discriminator loss: 0.7027678489685059, acc: 0.501953125] [gan loss: 0.871623, acc: 0.001953]\n",
            "2602: [discriminator loss: 0.6871642470359802, acc: 0.5576171875] [gan loss: 0.690100, acc: 0.562500]\n",
            "2603: [discriminator loss: 0.6886953711509705, acc: 0.501953125] [gan loss: 0.892450, acc: 0.000000]\n",
            "2604: [discriminator loss: 0.6880391836166382, acc: 0.609375] [gan loss: 0.671250, acc: 0.625000]\n",
            "2605: [discriminator loss: 0.69552081823349, acc: 0.4765625] [gan loss: 0.894669, acc: 0.000000]\n",
            "2606: [discriminator loss: 0.6855959892272949, acc: 0.583984375] [gan loss: 0.680500, acc: 0.589844]\n",
            "2607: [discriminator loss: 0.6854127049446106, acc: 0.50390625] [gan loss: 0.901544, acc: 0.000000]\n",
            "2608: [discriminator loss: 0.6829916834831238, acc: 0.6083984375] [gan loss: 0.644509, acc: 0.763672]\n",
            "2609: [discriminator loss: 0.6873413324356079, acc: 0.4970703125] [gan loss: 0.935289, acc: 0.000000]\n",
            "2610: [discriminator loss: 0.6853981018066406, acc: 0.60546875] [gan loss: 0.639501, acc: 0.839844]\n",
            "2611: [discriminator loss: 0.6820499897003174, acc: 0.513671875] [gan loss: 0.931585, acc: 0.000000]\n",
            "2612: [discriminator loss: 0.6826128959655762, acc: 0.59375] [gan loss: 0.629374, acc: 0.841797]\n",
            "2613: [discriminator loss: 0.6856973767280579, acc: 0.5078125] [gan loss: 0.936758, acc: 0.000000]\n",
            "2614: [discriminator loss: 0.6799929141998291, acc: 0.591796875] [gan loss: 0.618078, acc: 0.853516]\n",
            "2615: [discriminator loss: 0.6872132420539856, acc: 0.5029296875] [gan loss: 0.954153, acc: 0.000000]\n",
            "2616: [discriminator loss: 0.6772445440292358, acc: 0.583984375] [gan loss: 0.617535, acc: 0.804688]\n",
            "2617: [discriminator loss: 0.6957529783248901, acc: 0.4931640625] [gan loss: 0.947190, acc: 0.000000]\n",
            "2618: [discriminator loss: 0.683993399143219, acc: 0.548828125] [gan loss: 0.626126, acc: 0.806641]\n",
            "2619: [discriminator loss: 0.6980271339416504, acc: 0.4892578125] [gan loss: 0.905348, acc: 0.000000]\n",
            "2620: [discriminator loss: 0.6762740612030029, acc: 0.591796875] [gan loss: 0.661141, acc: 0.708984]\n",
            "2621: [discriminator loss: 0.6909862160682678, acc: 0.4990234375] [gan loss: 0.905811, acc: 0.000000]\n",
            "2622: [discriminator loss: 0.6911400556564331, acc: 0.5390625] [gan loss: 0.649815, acc: 0.736328]\n",
            "2623: [discriminator loss: 0.7039723992347717, acc: 0.4814453125] [gan loss: 0.906057, acc: 0.000000]\n",
            "2624: [discriminator loss: 0.6884987354278564, acc: 0.55859375] [gan loss: 0.632140, acc: 0.781250]\n",
            "2625: [discriminator loss: 0.6981729865074158, acc: 0.4951171875] [gan loss: 0.902200, acc: 0.017578]\n",
            "2626: [discriminator loss: 0.7027637958526611, acc: 0.4873046875] [gan loss: 0.625461, acc: 0.816406]\n",
            "2627: [discriminator loss: 0.717230498790741, acc: 0.486328125] [gan loss: 0.881511, acc: 0.003906]\n",
            "2628: [discriminator loss: 0.7050907611846924, acc: 0.470703125] [gan loss: 0.632690, acc: 0.740234]\n",
            "2629: [discriminator loss: 0.7101436257362366, acc: 0.48828125] [gan loss: 0.857646, acc: 0.017578]\n",
            "2630: [discriminator loss: 0.7052586674690247, acc: 0.4892578125] [gan loss: 0.671663, acc: 0.625000]\n",
            "2631: [discriminator loss: 0.7016490697860718, acc: 0.4775390625] [gan loss: 0.814844, acc: 0.052734]\n",
            "2632: [discriminator loss: 0.6987324357032776, acc: 0.501953125] [gan loss: 0.687206, acc: 0.550781]\n",
            "2633: [discriminator loss: 0.708260715007782, acc: 0.43359375] [gan loss: 0.863788, acc: 0.000000]\n",
            "2634: [discriminator loss: 0.6894605755805969, acc: 0.5712890625] [gan loss: 0.610827, acc: 0.875000]\n",
            "2635: [discriminator loss: 0.7297542095184326, acc: 0.4697265625] [gan loss: 0.973320, acc: 0.000000]\n",
            "2636: [discriminator loss: 0.7008118629455566, acc: 0.5048828125] [gan loss: 0.556873, acc: 0.968750]\n",
            "2637: [discriminator loss: 0.7351803183555603, acc: 0.4951171875] [gan loss: 0.940429, acc: 0.000000]\n",
            "2638: [discriminator loss: 0.7091765403747559, acc: 0.49609375] [gan loss: 0.585418, acc: 0.968750]\n",
            "2639: [discriminator loss: 0.7197909951210022, acc: 0.48828125] [gan loss: 0.883748, acc: 0.048828]\n",
            "2640: [discriminator loss: 0.6955525875091553, acc: 0.533203125] [gan loss: 0.620857, acc: 0.875000]\n",
            "2641: [discriminator loss: 0.7213295698165894, acc: 0.486328125] [gan loss: 0.856991, acc: 0.021484]\n",
            "2642: [discriminator loss: 0.7014499306678772, acc: 0.5087890625] [gan loss: 0.638741, acc: 0.792969]\n",
            "2643: [discriminator loss: 0.7084577083587646, acc: 0.4794921875] [gan loss: 0.833898, acc: 0.099609]\n",
            "2644: [discriminator loss: 0.7032838463783264, acc: 0.4990234375] [gan loss: 0.661106, acc: 0.628906]\n",
            "2645: [discriminator loss: 0.7129142880439758, acc: 0.46875] [gan loss: 0.852428, acc: 0.005859]\n",
            "2646: [discriminator loss: 0.6962267756462097, acc: 0.521484375] [gan loss: 0.635566, acc: 0.824219]\n",
            "2647: [discriminator loss: 0.7053912878036499, acc: 0.46875] [gan loss: 0.892207, acc: 0.000000]\n",
            "2648: [discriminator loss: 0.687847375869751, acc: 0.5693359375] [gan loss: 0.567130, acc: 0.994141]\n",
            "2649: [discriminator loss: 0.724459171295166, acc: 0.49609375] [gan loss: 0.951484, acc: 0.000000]\n",
            "2650: [discriminator loss: 0.6917489171028137, acc: 0.544921875] [gan loss: 0.576305, acc: 0.994141]\n",
            "2651: [discriminator loss: 0.7233867049217224, acc: 0.49609375] [gan loss: 0.914763, acc: 0.000000]\n",
            "2652: [discriminator loss: 0.6913215517997742, acc: 0.53125] [gan loss: 0.618486, acc: 0.937500]\n",
            "2653: [discriminator loss: 0.7092655897140503, acc: 0.4921875] [gan loss: 0.879090, acc: 0.000000]\n",
            "2654: [discriminator loss: 0.6751124262809753, acc: 0.6337890625] [gan loss: 0.630260, acc: 0.888672]\n",
            "2655: [discriminator loss: 0.710332989692688, acc: 0.482421875] [gan loss: 0.875206, acc: 0.000000]\n",
            "2656: [discriminator loss: 0.677258312702179, acc: 0.6240234375] [gan loss: 0.642584, acc: 0.847656]\n",
            "2657: [discriminator loss: 0.7028390765190125, acc: 0.48046875] [gan loss: 0.860132, acc: 0.000000]\n",
            "2658: [discriminator loss: 0.6832143068313599, acc: 0.5732421875] [gan loss: 0.646646, acc: 0.837891]\n",
            "2659: [discriminator loss: 0.702178955078125, acc: 0.4853515625] [gan loss: 0.866658, acc: 0.000000]\n",
            "2660: [discriminator loss: 0.6800141930580139, acc: 0.583984375] [gan loss: 0.628174, acc: 0.908203]\n",
            "2661: [discriminator loss: 0.6980175375938416, acc: 0.48828125] [gan loss: 0.902262, acc: 0.000000]\n",
            "2662: [discriminator loss: 0.6777852773666382, acc: 0.6083984375] [gan loss: 0.601502, acc: 0.958984]\n",
            "2663: [discriminator loss: 0.7030189037322998, acc: 0.4951171875] [gan loss: 0.937798, acc: 0.000000]\n",
            "2664: [discriminator loss: 0.6719886660575867, acc: 0.572265625] [gan loss: 0.565021, acc: 0.986328]\n",
            "2665: [discriminator loss: 0.7284512519836426, acc: 0.4951171875] [gan loss: 0.948345, acc: 0.000000]\n",
            "2666: [discriminator loss: 0.6810495853424072, acc: 0.62890625] [gan loss: 0.611500, acc: 0.908203]\n",
            "2667: [discriminator loss: 0.7094776630401611, acc: 0.4921875] [gan loss: 0.912772, acc: 0.000000]\n",
            "2668: [discriminator loss: 0.6995730400085449, acc: 0.560546875] [gan loss: 0.641065, acc: 0.798828]\n",
            "2669: [discriminator loss: 0.7063688635826111, acc: 0.4716796875] [gan loss: 0.852383, acc: 0.000000]\n",
            "2670: [discriminator loss: 0.6872707605361938, acc: 0.5634765625] [gan loss: 0.672878, acc: 0.658203]\n",
            "2671: [discriminator loss: 0.6974526643753052, acc: 0.4833984375] [gan loss: 0.840652, acc: 0.000000]\n",
            "2672: [discriminator loss: 0.6835336685180664, acc: 0.59375] [gan loss: 0.669149, acc: 0.685547]\n",
            "2673: [discriminator loss: 0.701198399066925, acc: 0.4892578125] [gan loss: 0.881159, acc: 0.000000]\n",
            "2674: [discriminator loss: 0.6902623176574707, acc: 0.568359375] [gan loss: 0.662699, acc: 0.736328]\n",
            "2675: [discriminator loss: 0.692186713218689, acc: 0.4970703125] [gan loss: 0.907375, acc: 0.000000]\n",
            "2676: [discriminator loss: 0.6928583383560181, acc: 0.5810546875] [gan loss: 0.624100, acc: 0.828125]\n",
            "2677: [discriminator loss: 0.7031840085983276, acc: 0.4951171875] [gan loss: 0.920774, acc: 0.000000]\n",
            "2678: [discriminator loss: 0.6944283843040466, acc: 0.546875] [gan loss: 0.632836, acc: 0.787109]\n",
            "2679: [discriminator loss: 0.7019858360290527, acc: 0.4873046875] [gan loss: 0.870020, acc: 0.001953]\n",
            "2680: [discriminator loss: 0.6921701431274414, acc: 0.537109375] [gan loss: 0.670006, acc: 0.607422]\n",
            "2681: [discriminator loss: 0.6948801279067993, acc: 0.50390625] [gan loss: 0.867767, acc: 0.056641]\n",
            "2682: [discriminator loss: 0.6891387104988098, acc: 0.55078125] [gan loss: 0.663231, acc: 0.658203]\n",
            "2683: [discriminator loss: 0.7005189657211304, acc: 0.4794921875] [gan loss: 0.871701, acc: 0.021484]\n",
            "2684: [discriminator loss: 0.6904339790344238, acc: 0.5673828125] [gan loss: 0.648527, acc: 0.794922]\n",
            "2685: [discriminator loss: 0.704907238483429, acc: 0.4814453125] [gan loss: 0.940794, acc: 0.000000]\n",
            "2686: [discriminator loss: 0.6881936192512512, acc: 0.5771484375] [gan loss: 0.571066, acc: 0.988281]\n",
            "2687: [discriminator loss: 0.7161514163017273, acc: 0.4951171875] [gan loss: 1.007543, acc: 0.000000]\n",
            "2688: [discriminator loss: 0.6790764331817627, acc: 0.548828125] [gan loss: 0.543953, acc: 0.996094]\n",
            "2689: [discriminator loss: 0.723051905632019, acc: 0.5] [gan loss: 0.942051, acc: 0.000000]\n",
            "2690: [discriminator loss: 0.6870541572570801, acc: 0.5732421875] [gan loss: 0.634603, acc: 0.861328]\n",
            "2691: [discriminator loss: 0.6929987072944641, acc: 0.484375] [gan loss: 0.850958, acc: 0.000000]\n",
            "2692: [discriminator loss: 0.6792060732841492, acc: 0.5966796875] [gan loss: 0.674464, acc: 0.634766]\n",
            "2693: [discriminator loss: 0.6885105967521667, acc: 0.4716796875] [gan loss: 0.837389, acc: 0.019531]\n",
            "2694: [discriminator loss: 0.6831082105636597, acc: 0.587890625] [gan loss: 0.691783, acc: 0.546875]\n",
            "2695: [discriminator loss: 0.6839568614959717, acc: 0.4931640625] [gan loss: 0.848290, acc: 0.007812]\n",
            "2696: [discriminator loss: 0.6800867319107056, acc: 0.5986328125] [gan loss: 0.688754, acc: 0.533203]\n",
            "2697: [discriminator loss: 0.6817022562026978, acc: 0.5078125] [gan loss: 0.849171, acc: 0.015625]\n",
            "2698: [discriminator loss: 0.6781599521636963, acc: 0.5869140625] [gan loss: 0.702318, acc: 0.447266]\n",
            "2699: [discriminator loss: 0.6816185116767883, acc: 0.501953125] [gan loss: 0.878638, acc: 0.001953]\n",
            "2700: [discriminator loss: 0.6717908978462219, acc: 0.6396484375] [gan loss: 0.689820, acc: 0.498047]\n",
            "2701: [discriminator loss: 0.6840787529945374, acc: 0.486328125] [gan loss: 0.892875, acc: 0.000000]\n",
            "2702: [discriminator loss: 0.6802648901939392, acc: 0.62109375] [gan loss: 0.653620, acc: 0.710938]\n",
            "2703: [discriminator loss: 0.6861522793769836, acc: 0.4970703125] [gan loss: 0.933993, acc: 0.000000]\n",
            "2704: [discriminator loss: 0.6830529570579529, acc: 0.5908203125] [gan loss: 0.595135, acc: 0.878906]\n",
            "2705: [discriminator loss: 0.7072395086288452, acc: 0.4951171875] [gan loss: 1.003817, acc: 0.000000]\n",
            "2706: [discriminator loss: 0.6907296776771545, acc: 0.5732421875] [gan loss: 0.580420, acc: 0.933594]\n",
            "2707: [discriminator loss: 0.7072867155075073, acc: 0.49609375] [gan loss: 0.954743, acc: 0.000000]\n",
            "2708: [discriminator loss: 0.684061586856842, acc: 0.5703125] [gan loss: 0.640873, acc: 0.791016]\n",
            "2709: [discriminator loss: 0.688553512096405, acc: 0.490234375] [gan loss: 0.877574, acc: 0.000000]\n",
            "2710: [discriminator loss: 0.673359751701355, acc: 0.6171875] [gan loss: 0.680045, acc: 0.580078]\n",
            "2711: [discriminator loss: 0.6848353743553162, acc: 0.5009765625] [gan loss: 0.867085, acc: 0.000000]\n",
            "2712: [discriminator loss: 0.6846631169319153, acc: 0.5791015625] [gan loss: 0.667105, acc: 0.683594]\n",
            "2713: [discriminator loss: 0.6876979470252991, acc: 0.490234375] [gan loss: 0.864254, acc: 0.000000]\n",
            "2714: [discriminator loss: 0.6816956996917725, acc: 0.5908203125] [gan loss: 0.656798, acc: 0.705078]\n",
            "2715: [discriminator loss: 0.6917107701301575, acc: 0.4990234375] [gan loss: 0.876383, acc: 0.000000]\n",
            "2716: [discriminator loss: 0.6750817894935608, acc: 0.60546875] [gan loss: 0.632690, acc: 0.839844]\n",
            "2717: [discriminator loss: 0.6939752697944641, acc: 0.494140625] [gan loss: 0.938149, acc: 0.000000]\n",
            "2718: [discriminator loss: 0.6902318000793457, acc: 0.57421875] [gan loss: 0.622617, acc: 0.888672]\n",
            "2719: [discriminator loss: 0.6910221576690674, acc: 0.5078125] [gan loss: 0.921640, acc: 0.000000]\n",
            "2720: [discriminator loss: 0.6776041984558105, acc: 0.6181640625] [gan loss: 0.632102, acc: 0.808594]\n",
            "2721: [discriminator loss: 0.6855936050415039, acc: 0.5224609375] [gan loss: 0.892398, acc: 0.000000]\n",
            "2722: [discriminator loss: 0.6881535053253174, acc: 0.5712890625] [gan loss: 0.655328, acc: 0.742188]\n",
            "2723: [discriminator loss: 0.6762608289718628, acc: 0.5166015625] [gan loss: 0.879502, acc: 0.000000]\n",
            "2724: [discriminator loss: 0.6727532148361206, acc: 0.6513671875] [gan loss: 0.660980, acc: 0.707031]\n",
            "2725: [discriminator loss: 0.6839998960494995, acc: 0.5107421875] [gan loss: 0.918746, acc: 0.000000]\n",
            "2726: [discriminator loss: 0.6807284355163574, acc: 0.6328125] [gan loss: 0.640682, acc: 0.781250]\n",
            "2727: [discriminator loss: 0.6901599764823914, acc: 0.5048828125] [gan loss: 0.916563, acc: 0.000000]\n",
            "2728: [discriminator loss: 0.6832513809204102, acc: 0.6162109375] [gan loss: 0.622040, acc: 0.833984]\n",
            "2729: [discriminator loss: 0.6902309060096741, acc: 0.505859375] [gan loss: 0.942575, acc: 0.000000]\n",
            "2730: [discriminator loss: 0.68112713098526, acc: 0.595703125] [gan loss: 0.615023, acc: 0.843750]\n",
            "2731: [discriminator loss: 0.6879450082778931, acc: 0.5107421875] [gan loss: 0.937887, acc: 0.000000]\n",
            "2732: [discriminator loss: 0.6850515604019165, acc: 0.5947265625] [gan loss: 0.624155, acc: 0.775391]\n",
            "2733: [discriminator loss: 0.6897403001785278, acc: 0.5107421875] [gan loss: 0.904619, acc: 0.000000]\n",
            "2734: [discriminator loss: 0.6921319365501404, acc: 0.5546875] [gan loss: 0.667527, acc: 0.691406]\n",
            "2735: [discriminator loss: 0.6841721534729004, acc: 0.5166015625] [gan loss: 0.886438, acc: 0.000000]\n",
            "2736: [discriminator loss: 0.687880277633667, acc: 0.5908203125] [gan loss: 0.656475, acc: 0.753906]\n",
            "2737: [discriminator loss: 0.689606249332428, acc: 0.5009765625] [gan loss: 0.868953, acc: 0.000000]\n",
            "2738: [discriminator loss: 0.6822888851165771, acc: 0.609375] [gan loss: 0.658407, acc: 0.708984]\n",
            "2739: [discriminator loss: 0.6907206773757935, acc: 0.505859375] [gan loss: 0.897138, acc: 0.000000]\n",
            "2740: [discriminator loss: 0.6773701906204224, acc: 0.6015625] [gan loss: 0.645669, acc: 0.746094]\n",
            "2741: [discriminator loss: 0.6898095011711121, acc: 0.517578125] [gan loss: 0.937170, acc: 0.000000]\n",
            "2742: [discriminator loss: 0.6860018372535706, acc: 0.5810546875] [gan loss: 0.632980, acc: 0.755859]\n",
            "2743: [discriminator loss: 0.6875168681144714, acc: 0.525390625] [gan loss: 0.920449, acc: 0.000000]\n",
            "2744: [discriminator loss: 0.6855455040931702, acc: 0.59375] [gan loss: 0.637255, acc: 0.796875]\n",
            "2745: [discriminator loss: 0.685495138168335, acc: 0.51953125] [gan loss: 0.919337, acc: 0.000000]\n",
            "2746: [discriminator loss: 0.6863707304000854, acc: 0.5927734375] [gan loss: 0.633322, acc: 0.748047]\n",
            "2747: [discriminator loss: 0.6813176870346069, acc: 0.5283203125] [gan loss: 0.916879, acc: 0.000000]\n",
            "2748: [discriminator loss: 0.6781697869300842, acc: 0.58984375] [gan loss: 0.630645, acc: 0.773438]\n",
            "2749: [discriminator loss: 0.685655951499939, acc: 0.5244140625] [gan loss: 0.946143, acc: 0.000000]\n",
            "2750: [discriminator loss: 0.6895899176597595, acc: 0.5712890625] [gan loss: 0.623792, acc: 0.806641]\n",
            "2751: [discriminator loss: 0.6909087300300598, acc: 0.5126953125] [gan loss: 0.910670, acc: 0.000000]\n",
            "2752: [discriminator loss: 0.6831095814704895, acc: 0.60546875] [gan loss: 0.642510, acc: 0.693359]\n",
            "2753: [discriminator loss: 0.6817834973335266, acc: 0.5322265625] [gan loss: 0.868514, acc: 0.000000]\n",
            "2754: [discriminator loss: 0.6833862066268921, acc: 0.5712890625] [gan loss: 0.671024, acc: 0.568359]\n",
            "2755: [discriminator loss: 0.6801162958145142, acc: 0.564453125] [gan loss: 0.872205, acc: 0.023438]\n",
            "2756: [discriminator loss: 0.6788015365600586, acc: 0.580078125] [gan loss: 0.662529, acc: 0.564453]\n",
            "2757: [discriminator loss: 0.6843730807304382, acc: 0.544921875] [gan loss: 0.889417, acc: 0.023438]\n",
            "2758: [discriminator loss: 0.684285581111908, acc: 0.5849609375] [gan loss: 0.662359, acc: 0.597656]\n",
            "2759: [discriminator loss: 0.6764198541641235, acc: 0.5625] [gan loss: 0.915620, acc: 0.011719]\n",
            "2760: [discriminator loss: 0.6807433366775513, acc: 0.5986328125] [gan loss: 0.654128, acc: 0.638672]\n",
            "2761: [discriminator loss: 0.6774545907974243, acc: 0.53125] [gan loss: 0.945331, acc: 0.000000]\n",
            "2762: [discriminator loss: 0.6772040128707886, acc: 0.5546875] [gan loss: 0.628257, acc: 0.738281]\n",
            "2763: [discriminator loss: 0.6870250105857849, acc: 0.52734375] [gan loss: 0.961860, acc: 0.005859]\n",
            "2764: [discriminator loss: 0.6810945868492126, acc: 0.55078125] [gan loss: 0.620012, acc: 0.810547]\n",
            "2765: [discriminator loss: 0.6814648509025574, acc: 0.5390625] [gan loss: 0.917284, acc: 0.000000]\n",
            "2766: [discriminator loss: 0.672696590423584, acc: 0.591796875] [gan loss: 0.635212, acc: 0.765625]\n",
            "2767: [discriminator loss: 0.6871762275695801, acc: 0.5302734375] [gan loss: 0.932794, acc: 0.000000]\n",
            "2768: [discriminator loss: 0.6699660420417786, acc: 0.5908203125] [gan loss: 0.637451, acc: 0.734375]\n",
            "2769: [discriminator loss: 0.6856306791305542, acc: 0.5263671875] [gan loss: 0.930393, acc: 0.000000]\n",
            "2770: [discriminator loss: 0.6805969476699829, acc: 0.5810546875] [gan loss: 0.637742, acc: 0.746094]\n",
            "2771: [discriminator loss: 0.6871980428695679, acc: 0.515625] [gan loss: 0.892880, acc: 0.000000]\n",
            "2772: [discriminator loss: 0.6795337200164795, acc: 0.537109375] [gan loss: 0.658936, acc: 0.632812]\n",
            "2773: [discriminator loss: 0.6961963772773743, acc: 0.5107421875] [gan loss: 0.894720, acc: 0.000000]\n",
            "2774: [discriminator loss: 0.6840808391571045, acc: 0.55078125] [gan loss: 0.654053, acc: 0.654297]\n",
            "2775: [discriminator loss: 0.7009450793266296, acc: 0.5078125] [gan loss: 0.890155, acc: 0.005859]\n",
            "2776: [discriminator loss: 0.6821414828300476, acc: 0.55859375] [gan loss: 0.655463, acc: 0.656250]\n",
            "2777: [discriminator loss: 0.7014744281768799, acc: 0.49609375] [gan loss: 0.919677, acc: 0.000000]\n",
            "2778: [discriminator loss: 0.6926311254501343, acc: 0.533203125] [gan loss: 0.640848, acc: 0.783203]\n",
            "2779: [discriminator loss: 0.6976271867752075, acc: 0.4833984375] [gan loss: 0.925830, acc: 0.000000]\n",
            "2780: [discriminator loss: 0.6857413053512573, acc: 0.5849609375] [gan loss: 0.636112, acc: 0.824219]\n",
            "2781: [discriminator loss: 0.6955468654632568, acc: 0.4931640625] [gan loss: 0.923325, acc: 0.000000]\n",
            "2782: [discriminator loss: 0.6831358671188354, acc: 0.603515625] [gan loss: 0.637836, acc: 0.818359]\n",
            "2783: [discriminator loss: 0.694699227809906, acc: 0.5087890625] [gan loss: 0.925929, acc: 0.000000]\n",
            "2784: [discriminator loss: 0.6835492849349976, acc: 0.6005859375] [gan loss: 0.629383, acc: 0.890625]\n",
            "2785: [discriminator loss: 0.6883702874183655, acc: 0.5166015625] [gan loss: 0.929206, acc: 0.000000]\n",
            "2786: [discriminator loss: 0.6735871434211731, acc: 0.6005859375] [gan loss: 0.654195, acc: 0.761719]\n",
            "2787: [discriminator loss: 0.6929470896720886, acc: 0.4951171875] [gan loss: 0.896580, acc: 0.000000]\n",
            "2788: [discriminator loss: 0.6755453944206238, acc: 0.5927734375] [gan loss: 0.669369, acc: 0.699219]\n",
            "2789: [discriminator loss: 0.6745543479919434, acc: 0.5126953125] [gan loss: 0.919530, acc: 0.000000]\n",
            "2790: [discriminator loss: 0.6741542816162109, acc: 0.607421875] [gan loss: 0.660791, acc: 0.689453]\n",
            "2791: [discriminator loss: 0.6729894876480103, acc: 0.5029296875] [gan loss: 0.954655, acc: 0.000000]\n",
            "2792: [discriminator loss: 0.677498459815979, acc: 0.599609375] [gan loss: 0.636551, acc: 0.750000]\n",
            "2793: [discriminator loss: 0.6867513656616211, acc: 0.49609375] [gan loss: 0.970511, acc: 0.000000]\n",
            "2794: [discriminator loss: 0.6770256757736206, acc: 0.5859375] [gan loss: 0.611002, acc: 0.910156]\n",
            "2795: [discriminator loss: 0.692511796951294, acc: 0.49609375] [gan loss: 0.972719, acc: 0.000000]\n",
            "2796: [discriminator loss: 0.6848219633102417, acc: 0.58203125] [gan loss: 0.636464, acc: 0.802734]\n",
            "2797: [discriminator loss: 0.6912724375724792, acc: 0.498046875] [gan loss: 0.924156, acc: 0.000000]\n",
            "2798: [discriminator loss: 0.6836692690849304, acc: 0.5556640625] [gan loss: 0.657007, acc: 0.662109]\n",
            "2799: [discriminator loss: 0.6950706839561462, acc: 0.5009765625] [gan loss: 0.897088, acc: 0.000000]\n",
            "2800: [discriminator loss: 0.6886967420578003, acc: 0.53515625] [gan loss: 0.656539, acc: 0.658203]\n",
            "2801: [discriminator loss: 0.6915730237960815, acc: 0.5107421875] [gan loss: 0.888257, acc: 0.007812]\n",
            "2802: [discriminator loss: 0.6945883631706238, acc: 0.5390625] [gan loss: 0.648541, acc: 0.730469]\n",
            "2803: [discriminator loss: 0.6943183541297913, acc: 0.49609375] [gan loss: 0.870759, acc: 0.013672]\n",
            "2804: [discriminator loss: 0.7009983062744141, acc: 0.5380859375] [gan loss: 0.640214, acc: 0.728516]\n",
            "2805: [discriminator loss: 0.6972970366477966, acc: 0.5078125] [gan loss: 0.912680, acc: 0.000000]\n",
            "2806: [discriminator loss: 0.7055971622467041, acc: 0.525390625] [gan loss: 0.637248, acc: 0.738281]\n",
            "2807: [discriminator loss: 0.7058693766593933, acc: 0.5] [gan loss: 0.893329, acc: 0.019531]\n",
            "2808: [discriminator loss: 0.710715651512146, acc: 0.4775390625] [gan loss: 0.646485, acc: 0.755859]\n",
            "2809: [discriminator loss: 0.7023261189460754, acc: 0.4892578125] [gan loss: 0.880138, acc: 0.023438]\n",
            "2810: [discriminator loss: 0.7073307633399963, acc: 0.49609375] [gan loss: 0.633106, acc: 0.814453]\n",
            "2811: [discriminator loss: 0.7033700942993164, acc: 0.4892578125] [gan loss: 0.904233, acc: 0.000000]\n",
            "2812: [discriminator loss: 0.6914900541305542, acc: 0.5283203125] [gan loss: 0.624515, acc: 0.837891]\n",
            "2813: [discriminator loss: 0.7035700678825378, acc: 0.5] [gan loss: 0.909200, acc: 0.000000]\n",
            "2814: [discriminator loss: 0.6819543838500977, acc: 0.580078125] [gan loss: 0.618626, acc: 0.876953]\n",
            "2815: [discriminator loss: 0.7033246755599976, acc: 0.4970703125] [gan loss: 0.937800, acc: 0.000000]\n",
            "2816: [discriminator loss: 0.6810667514801025, acc: 0.591796875] [gan loss: 0.605040, acc: 0.955078]\n",
            "2817: [discriminator loss: 0.7013728618621826, acc: 0.48046875] [gan loss: 0.906407, acc: 0.001953]\n",
            "2818: [discriminator loss: 0.6946986317634583, acc: 0.5537109375] [gan loss: 0.617070, acc: 0.908203]\n",
            "2819: [discriminator loss: 0.6955090165138245, acc: 0.4921875] [gan loss: 0.908866, acc: 0.033203]\n",
            "2820: [discriminator loss: 0.6887246370315552, acc: 0.572265625] [gan loss: 0.641824, acc: 0.806641]\n",
            "2821: [discriminator loss: 0.6805061101913452, acc: 0.48828125] [gan loss: 0.882054, acc: 0.027344]\n",
            "2822: [discriminator loss: 0.6896286010742188, acc: 0.568359375] [gan loss: 0.663517, acc: 0.703125]\n",
            "2823: [discriminator loss: 0.678398072719574, acc: 0.48828125] [gan loss: 0.883394, acc: 0.017578]\n",
            "2824: [discriminator loss: 0.6917476058006287, acc: 0.5625] [gan loss: 0.662106, acc: 0.660156]\n",
            "2825: [discriminator loss: 0.6754287481307983, acc: 0.50390625] [gan loss: 0.844129, acc: 0.039062]\n",
            "2826: [discriminator loss: 0.6920449733734131, acc: 0.546875] [gan loss: 0.696238, acc: 0.462891]\n",
            "2827: [discriminator loss: 0.6714098453521729, acc: 0.529296875] [gan loss: 0.841339, acc: 0.064453]\n",
            "2828: [discriminator loss: 0.7023701071739197, acc: 0.4970703125] [gan loss: 0.733507, acc: 0.269531]\n",
            "2829: [discriminator loss: 0.6761839985847473, acc: 0.525390625] [gan loss: 0.825326, acc: 0.066406]\n",
            "2830: [discriminator loss: 0.691586434841156, acc: 0.521484375] [gan loss: 0.697033, acc: 0.460938]\n",
            "2831: [discriminator loss: 0.6857783198356628, acc: 0.5146484375] [gan loss: 0.882813, acc: 0.035156]\n",
            "2832: [discriminator loss: 0.6965379118919373, acc: 0.54296875] [gan loss: 0.642532, acc: 0.738281]\n",
            "2833: [discriminator loss: 0.6836992502212524, acc: 0.4912109375] [gan loss: 0.958057, acc: 0.000000]\n",
            "2834: [discriminator loss: 0.7030854225158691, acc: 0.5244140625] [gan loss: 0.596037, acc: 0.953125]\n",
            "2835: [discriminator loss: 0.6981222629547119, acc: 0.49609375] [gan loss: 0.989546, acc: 0.000000]\n",
            "2836: [discriminator loss: 0.7084265947341919, acc: 0.5078125] [gan loss: 0.598686, acc: 0.935547]\n",
            "2837: [discriminator loss: 0.6901170015335083, acc: 0.484375] [gan loss: 0.923784, acc: 0.000000]\n",
            "2838: [discriminator loss: 0.6939939260482788, acc: 0.55078125] [gan loss: 0.662819, acc: 0.687500]\n",
            "2839: [discriminator loss: 0.6805502772331238, acc: 0.513671875] [gan loss: 0.871675, acc: 0.011719]\n",
            "2840: [discriminator loss: 0.6911598443984985, acc: 0.578125] [gan loss: 0.699830, acc: 0.437500]\n",
            "2841: [discriminator loss: 0.6743950247764587, acc: 0.52734375] [gan loss: 0.850708, acc: 0.001953]\n",
            "2842: [discriminator loss: 0.6856617331504822, acc: 0.5654296875] [gan loss: 0.726197, acc: 0.248047]\n",
            "2843: [discriminator loss: 0.6709123253822327, acc: 0.5546875] [gan loss: 0.845092, acc: 0.027344]\n",
            "2844: [discriminator loss: 0.6754783391952515, acc: 0.6357421875] [gan loss: 0.732299, acc: 0.207031]\n",
            "2845: [discriminator loss: 0.662583589553833, acc: 0.5927734375] [gan loss: 0.867804, acc: 0.019531]\n",
            "2846: [discriminator loss: 0.6779170632362366, acc: 0.623046875] [gan loss: 0.712769, acc: 0.339844]\n",
            "2847: [discriminator loss: 0.6637136936187744, acc: 0.556640625] [gan loss: 0.936663, acc: 0.000000]\n",
            "2848: [discriminator loss: 0.6765149235725403, acc: 0.58203125] [gan loss: 0.627288, acc: 0.876953]\n",
            "2849: [discriminator loss: 0.6801232695579529, acc: 0.5068359375] [gan loss: 1.072739, acc: 0.000000]\n",
            "2850: [discriminator loss: 0.6840423345565796, acc: 0.5302734375] [gan loss: 0.568305, acc: 0.972656]\n",
            "2851: [discriminator loss: 0.6943389177322388, acc: 0.5] [gan loss: 1.048618, acc: 0.000000]\n",
            "2852: [discriminator loss: 0.6704787611961365, acc: 0.5537109375] [gan loss: 0.627519, acc: 0.853516]\n",
            "2853: [discriminator loss: 0.6812824010848999, acc: 0.498046875] [gan loss: 0.943669, acc: 0.003906]\n",
            "2854: [discriminator loss: 0.6664440035820007, acc: 0.623046875] [gan loss: 0.696182, acc: 0.515625]\n",
            "2855: [discriminator loss: 0.6702301502227783, acc: 0.54296875] [gan loss: 0.893971, acc: 0.029297]\n",
            "2856: [discriminator loss: 0.6703886985778809, acc: 0.625] [gan loss: 0.729800, acc: 0.263672]\n",
            "2857: [discriminator loss: 0.6636720299720764, acc: 0.5673828125] [gan loss: 0.875498, acc: 0.037109]\n",
            "2858: [discriminator loss: 0.6715351939201355, acc: 0.6513671875] [gan loss: 0.717697, acc: 0.349609]\n",
            "2859: [discriminator loss: 0.6668578386306763, acc: 0.5537109375] [gan loss: 0.891281, acc: 0.044922]\n",
            "2860: [discriminator loss: 0.6682320833206177, acc: 0.63671875] [gan loss: 0.699199, acc: 0.427734]\n",
            "2861: [discriminator loss: 0.6835480332374573, acc: 0.505859375] [gan loss: 0.955550, acc: 0.000000]\n",
            "2862: [discriminator loss: 0.6711300015449524, acc: 0.603515625] [gan loss: 0.633900, acc: 0.818359]\n",
            "2863: [discriminator loss: 0.6849420070648193, acc: 0.4921875] [gan loss: 1.039191, acc: 0.000000]\n",
            "2864: [discriminator loss: 0.6818915009498596, acc: 0.5751953125] [gan loss: 0.580389, acc: 0.970703]\n",
            "2865: [discriminator loss: 0.7056092023849487, acc: 0.4921875] [gan loss: 1.014890, acc: 0.000000]\n",
            "2866: [discriminator loss: 0.6816012263298035, acc: 0.5634765625] [gan loss: 0.604767, acc: 0.906250]\n",
            "2867: [discriminator loss: 0.7092024087905884, acc: 0.4921875] [gan loss: 0.946318, acc: 0.000000]\n",
            "2868: [discriminator loss: 0.6886613368988037, acc: 0.5927734375] [gan loss: 0.632318, acc: 0.882812]\n",
            "2869: [discriminator loss: 0.7005594968795776, acc: 0.48046875] [gan loss: 0.897845, acc: 0.000000]\n",
            "2870: [discriminator loss: 0.6853682994842529, acc: 0.62890625] [gan loss: 0.692620, acc: 0.582031]\n",
            "2871: [discriminator loss: 0.685867428779602, acc: 0.48046875] [gan loss: 0.881694, acc: 0.023438]\n",
            "2872: [discriminator loss: 0.6831237077713013, acc: 0.5751953125] [gan loss: 0.720178, acc: 0.396484]\n",
            "2873: [discriminator loss: 0.6864732503890991, acc: 0.455078125] [gan loss: 0.845142, acc: 0.041016]\n",
            "2874: [discriminator loss: 0.6815788745880127, acc: 0.61328125] [gan loss: 0.709077, acc: 0.371094]\n",
            "2875: [discriminator loss: 0.6856698393821716, acc: 0.4736328125] [gan loss: 0.853455, acc: 0.048828]\n",
            "2876: [discriminator loss: 0.6853641867637634, acc: 0.5888671875] [gan loss: 0.705399, acc: 0.427734]\n",
            "2877: [discriminator loss: 0.6877227425575256, acc: 0.4892578125] [gan loss: 0.865993, acc: 0.027344]\n",
            "2878: [discriminator loss: 0.687663197517395, acc: 0.572265625] [gan loss: 0.670921, acc: 0.658203]\n",
            "2879: [discriminator loss: 0.6959845423698425, acc: 0.4873046875] [gan loss: 0.929473, acc: 0.000000]\n",
            "2880: [discriminator loss: 0.6787042021751404, acc: 0.5849609375] [gan loss: 0.607775, acc: 0.876953]\n",
            "2881: [discriminator loss: 0.6986170411109924, acc: 0.4873046875] [gan loss: 1.015157, acc: 0.000000]\n",
            "2882: [discriminator loss: 0.6845887899398804, acc: 0.5703125] [gan loss: 0.563930, acc: 0.976562]\n",
            "2883: [discriminator loss: 0.7085704207420349, acc: 0.4970703125] [gan loss: 0.978173, acc: 0.000000]\n",
            "2884: [discriminator loss: 0.6861518025398254, acc: 0.5888671875] [gan loss: 0.615454, acc: 0.931641]\n",
            "2885: [discriminator loss: 0.7004377841949463, acc: 0.4853515625] [gan loss: 0.900484, acc: 0.000000]\n",
            "2886: [discriminator loss: 0.6713060140609741, acc: 0.6201171875] [gan loss: 0.632166, acc: 0.832031]\n",
            "2887: [discriminator loss: 0.6976402997970581, acc: 0.4892578125] [gan loss: 0.900889, acc: 0.000000]\n",
            "2888: [discriminator loss: 0.6547050476074219, acc: 0.6650390625] [gan loss: 0.611481, acc: 0.875000]\n",
            "2889: [discriminator loss: 0.7052081227302551, acc: 0.490234375] [gan loss: 0.935962, acc: 0.000000]\n",
            "2890: [discriminator loss: 0.6798309087753296, acc: 0.591796875] [gan loss: 0.648663, acc: 0.753906]\n",
            "2891: [discriminator loss: 0.6962683200836182, acc: 0.4599609375] [gan loss: 0.892927, acc: 0.003906]\n",
            "2892: [discriminator loss: 0.6891877055168152, acc: 0.580078125] [gan loss: 0.670274, acc: 0.632812]\n",
            "2893: [discriminator loss: 0.6846352815628052, acc: 0.482421875] [gan loss: 0.862046, acc: 0.019531]\n",
            "2894: [discriminator loss: 0.6830155849456787, acc: 0.5859375] [gan loss: 0.685795, acc: 0.560547]\n",
            "2895: [discriminator loss: 0.6790788173675537, acc: 0.48046875] [gan loss: 0.870094, acc: 0.007812]\n",
            "2896: [discriminator loss: 0.6871411800384521, acc: 0.587890625] [gan loss: 0.698856, acc: 0.505859]\n",
            "2897: [discriminator loss: 0.6769969463348389, acc: 0.505859375] [gan loss: 0.867653, acc: 0.009766]\n",
            "2898: [discriminator loss: 0.6761398315429688, acc: 0.62109375] [gan loss: 0.696595, acc: 0.449219]\n",
            "2899: [discriminator loss: 0.6749826669692993, acc: 0.5009765625] [gan loss: 0.913201, acc: 0.003906]\n",
            "2900: [discriminator loss: 0.6793789267539978, acc: 0.6123046875] [gan loss: 0.641584, acc: 0.822266]\n",
            "2901: [discriminator loss: 0.6922160387039185, acc: 0.4853515625] [gan loss: 0.959528, acc: 0.000000]\n",
            "2902: [discriminator loss: 0.676878809928894, acc: 0.58984375] [gan loss: 0.601031, acc: 0.927734]\n",
            "2903: [discriminator loss: 0.6993781328201294, acc: 0.4873046875] [gan loss: 0.968797, acc: 0.000000]\n",
            "2904: [discriminator loss: 0.6861000061035156, acc: 0.560546875] [gan loss: 0.601654, acc: 0.912109]\n",
            "2905: [discriminator loss: 0.6972119808197021, acc: 0.4892578125] [gan loss: 0.922086, acc: 0.000000]\n",
            "2906: [discriminator loss: 0.687118649482727, acc: 0.5400390625] [gan loss: 0.630943, acc: 0.761719]\n",
            "2907: [discriminator loss: 0.6823036074638367, acc: 0.5] [gan loss: 0.899965, acc: 0.005859]\n",
            "2908: [discriminator loss: 0.6827579736709595, acc: 0.5751953125] [gan loss: 0.658483, acc: 0.595703]\n",
            "2909: [discriminator loss: 0.6733246445655823, acc: 0.50390625] [gan loss: 0.887691, acc: 0.000000]\n",
            "2910: [discriminator loss: 0.6866433024406433, acc: 0.564453125] [gan loss: 0.688884, acc: 0.539062]\n",
            "2911: [discriminator loss: 0.6716968417167664, acc: 0.5341796875] [gan loss: 0.877872, acc: 0.005859]\n",
            "2912: [discriminator loss: 0.6756068468093872, acc: 0.66015625] [gan loss: 0.691736, acc: 0.537109]\n",
            "2913: [discriminator loss: 0.668070912361145, acc: 0.5205078125] [gan loss: 0.898765, acc: 0.000000]\n",
            "2914: [discriminator loss: 0.663537859916687, acc: 0.6865234375] [gan loss: 0.667162, acc: 0.673828]\n",
            "2915: [discriminator loss: 0.6615116596221924, acc: 0.517578125] [gan loss: 0.956434, acc: 0.000000]\n",
            "2916: [discriminator loss: 0.6556661128997803, acc: 0.6552734375] [gan loss: 0.633410, acc: 0.761719]\n",
            "2917: [discriminator loss: 0.6667221188545227, acc: 0.505859375] [gan loss: 1.000684, acc: 0.000000]\n",
            "2918: [discriminator loss: 0.6630063652992249, acc: 0.6064453125] [gan loss: 0.608243, acc: 0.902344]\n",
            "2919: [discriminator loss: 0.6586669683456421, acc: 0.5009765625] [gan loss: 0.994882, acc: 0.000000]\n",
            "2920: [discriminator loss: 0.6656181216239929, acc: 0.6162109375] [gan loss: 0.641225, acc: 0.765625]\n",
            "2921: [discriminator loss: 0.6603215336799622, acc: 0.494140625] [gan loss: 0.951267, acc: 0.000000]\n",
            "2922: [discriminator loss: 0.6639899611473083, acc: 0.65625] [gan loss: 0.685872, acc: 0.517578]\n",
            "2923: [discriminator loss: 0.6536135673522949, acc: 0.529296875] [gan loss: 0.910073, acc: 0.000000]\n",
            "2924: [discriminator loss: 0.6578395366668701, acc: 0.673828125] [gan loss: 0.716643, acc: 0.351562]\n",
            "2925: [discriminator loss: 0.6364704370498657, acc: 0.58984375] [gan loss: 0.886672, acc: 0.000000]\n",
            "2926: [discriminator loss: 0.6649510860443115, acc: 0.654296875] [gan loss: 0.735075, acc: 0.283203]\n",
            "2927: [discriminator loss: 0.6467298865318298, acc: 0.5927734375] [gan loss: 0.905040, acc: 0.007812]\n",
            "2928: [discriminator loss: 0.6513973474502563, acc: 0.7138671875] [gan loss: 0.701274, acc: 0.431641]\n",
            "2929: [discriminator loss: 0.6519076824188232, acc: 0.5546875] [gan loss: 0.958426, acc: 0.000000]\n",
            "2930: [discriminator loss: 0.6499117612838745, acc: 0.6640625] [gan loss: 0.684004, acc: 0.550781]\n",
            "2931: [discriminator loss: 0.6528213620185852, acc: 0.5234375] [gan loss: 1.018542, acc: 0.000000]\n",
            "2932: [discriminator loss: 0.6558930277824402, acc: 0.6142578125] [gan loss: 0.623792, acc: 0.884766]\n",
            "2933: [discriminator loss: 0.6648709774017334, acc: 0.5048828125] [gan loss: 1.047245, acc: 0.000000]\n",
            "2934: [discriminator loss: 0.6509653329849243, acc: 0.6181640625] [gan loss: 0.641144, acc: 0.771484]\n",
            "2935: [discriminator loss: 0.6577768325805664, acc: 0.5009765625] [gan loss: 0.993113, acc: 0.000000]\n",
            "2936: [discriminator loss: 0.6535882949829102, acc: 0.658203125] [gan loss: 0.665415, acc: 0.617188]\n",
            "2937: [discriminator loss: 0.648173987865448, acc: 0.546875] [gan loss: 0.928119, acc: 0.000000]\n",
            "2938: [discriminator loss: 0.647691547870636, acc: 0.705078125] [gan loss: 0.708179, acc: 0.412109]\n",
            "2939: [discriminator loss: 0.6496506929397583, acc: 0.556640625] [gan loss: 0.898081, acc: 0.000000]\n",
            "2940: [discriminator loss: 0.6585467457771301, acc: 0.64453125] [gan loss: 0.735783, acc: 0.291016]\n",
            "2941: [discriminator loss: 0.6553077697753906, acc: 0.5693359375] [gan loss: 0.872610, acc: 0.013672]\n",
            "2942: [discriminator loss: 0.6659525632858276, acc: 0.6513671875] [gan loss: 0.714602, acc: 0.369141]\n",
            "2943: [discriminator loss: 0.655379056930542, acc: 0.548828125] [gan loss: 0.923664, acc: 0.001953]\n",
            "2944: [discriminator loss: 0.6609417796134949, acc: 0.630859375] [gan loss: 0.684022, acc: 0.523438]\n",
            "2945: [discriminator loss: 0.6604018807411194, acc: 0.546875] [gan loss: 0.965235, acc: 0.000000]\n",
            "2946: [discriminator loss: 0.6747929453849792, acc: 0.611328125] [gan loss: 0.614548, acc: 0.843750]\n",
            "2947: [discriminator loss: 0.6713721752166748, acc: 0.509765625] [gan loss: 1.048881, acc: 0.000000]\n",
            "2948: [discriminator loss: 0.6735473871231079, acc: 0.5908203125] [gan loss: 0.581962, acc: 0.917969]\n",
            "2949: [discriminator loss: 0.6755031943321228, acc: 0.5107421875] [gan loss: 1.021958, acc: 0.000000]\n",
            "2950: [discriminator loss: 0.66419917345047, acc: 0.6083984375] [gan loss: 0.640181, acc: 0.712891]\n",
            "2951: [discriminator loss: 0.6645371913909912, acc: 0.513671875] [gan loss: 0.967251, acc: 0.000000]\n",
            "2952: [discriminator loss: 0.6661055684089661, acc: 0.6328125] [gan loss: 0.663069, acc: 0.660156]\n",
            "2953: [discriminator loss: 0.6614664793014526, acc: 0.515625] [gan loss: 0.947437, acc: 0.001953]\n",
            "2954: [discriminator loss: 0.6622779965400696, acc: 0.6669921875] [gan loss: 0.692530, acc: 0.570312]\n",
            "2955: [discriminator loss: 0.6589258313179016, acc: 0.546875] [gan loss: 0.936145, acc: 0.000000]\n",
            "2956: [discriminator loss: 0.6598537564277649, acc: 0.6748046875] [gan loss: 0.694705, acc: 0.515625]\n",
            "2957: [discriminator loss: 0.662818968296051, acc: 0.5390625] [gan loss: 0.961252, acc: 0.000000]\n",
            "2958: [discriminator loss: 0.6646014451980591, acc: 0.650390625] [gan loss: 0.665725, acc: 0.685547]\n",
            "2959: [discriminator loss: 0.6712453365325928, acc: 0.494140625] [gan loss: 1.023050, acc: 0.000000]\n",
            "2960: [discriminator loss: 0.6705574989318848, acc: 0.6015625] [gan loss: 0.629813, acc: 0.824219]\n",
            "2961: [discriminator loss: 0.6762309670448303, acc: 0.4921875] [gan loss: 1.021226, acc: 0.000000]\n",
            "2962: [discriminator loss: 0.67548668384552, acc: 0.5595703125] [gan loss: 0.643953, acc: 0.785156]\n",
            "2963: [discriminator loss: 0.6817223429679871, acc: 0.4921875] [gan loss: 1.012471, acc: 0.000000]\n",
            "2964: [discriminator loss: 0.6716227531433105, acc: 0.580078125] [gan loss: 0.654987, acc: 0.705078]\n",
            "2965: [discriminator loss: 0.6772500872612, acc: 0.486328125] [gan loss: 0.958240, acc: 0.007812]\n",
            "2966: [discriminator loss: 0.6692507863044739, acc: 0.599609375] [gan loss: 0.681859, acc: 0.500000]\n",
            "2967: [discriminator loss: 0.6761474609375, acc: 0.49609375] [gan loss: 0.962686, acc: 0.007812]\n",
            "2968: [discriminator loss: 0.675127387046814, acc: 0.5830078125] [gan loss: 0.670201, acc: 0.644531]\n",
            "2969: [discriminator loss: 0.6777469515800476, acc: 0.4970703125] [gan loss: 0.959499, acc: 0.000000]\n",
            "2970: [discriminator loss: 0.6827163696289062, acc: 0.583984375] [gan loss: 0.664329, acc: 0.703125]\n",
            "2971: [discriminator loss: 0.6907310485839844, acc: 0.484375] [gan loss: 0.938610, acc: 0.000000]\n",
            "2972: [discriminator loss: 0.6869490742683411, acc: 0.5859375] [gan loss: 0.655355, acc: 0.646484]\n",
            "2973: [discriminator loss: 0.6950837969779968, acc: 0.484375] [gan loss: 0.950982, acc: 0.001953]\n",
            "2974: [discriminator loss: 0.6901229619979858, acc: 0.5576171875] [gan loss: 0.617166, acc: 0.861328]\n",
            "2975: [discriminator loss: 0.6954835653305054, acc: 0.5048828125] [gan loss: 0.974696, acc: 0.000000]\n",
            "2976: [discriminator loss: 0.693220853805542, acc: 0.5625] [gan loss: 0.605531, acc: 0.875000]\n",
            "2977: [discriminator loss: 0.7011401057243347, acc: 0.4873046875] [gan loss: 0.958455, acc: 0.000000]\n",
            "2978: [discriminator loss: 0.6968614459037781, acc: 0.541015625] [gan loss: 0.614455, acc: 0.851562]\n",
            "2979: [discriminator loss: 0.6940730810165405, acc: 0.4912109375] [gan loss: 0.937445, acc: 0.000000]\n",
            "2980: [discriminator loss: 0.6880099177360535, acc: 0.5986328125] [gan loss: 0.636001, acc: 0.767578]\n",
            "2981: [discriminator loss: 0.6926015615463257, acc: 0.51171875] [gan loss: 0.913382, acc: 0.000000]\n",
            "2982: [discriminator loss: 0.6894257068634033, acc: 0.6025390625] [gan loss: 0.640854, acc: 0.750000]\n",
            "2983: [discriminator loss: 0.6896999478340149, acc: 0.52734375] [gan loss: 0.925564, acc: 0.000000]\n",
            "2984: [discriminator loss: 0.6914447546005249, acc: 0.58984375] [gan loss: 0.646146, acc: 0.748047]\n",
            "2985: [discriminator loss: 0.6868317723274231, acc: 0.5185546875] [gan loss: 0.909999, acc: 0.005859]\n",
            "2986: [discriminator loss: 0.6892826557159424, acc: 0.59375] [gan loss: 0.656942, acc: 0.671875]\n",
            "2987: [discriminator loss: 0.6808080673217773, acc: 0.51953125] [gan loss: 0.913402, acc: 0.009766]\n",
            "2988: [discriminator loss: 0.6816403865814209, acc: 0.587890625] [gan loss: 0.664708, acc: 0.621094]\n",
            "2989: [discriminator loss: 0.6795129179954529, acc: 0.5107421875] [gan loss: 0.943339, acc: 0.003906]\n",
            "2990: [discriminator loss: 0.681868314743042, acc: 0.623046875] [gan loss: 0.650576, acc: 0.755859]\n",
            "2991: [discriminator loss: 0.6841172575950623, acc: 0.5009765625] [gan loss: 0.984507, acc: 0.000000]\n",
            "2992: [discriminator loss: 0.6642177104949951, acc: 0.6240234375] [gan loss: 0.606418, acc: 0.880859]\n",
            "2993: [discriminator loss: 0.6832892894744873, acc: 0.5078125] [gan loss: 0.998415, acc: 0.000000]\n",
            "2994: [discriminator loss: 0.680350124835968, acc: 0.5712890625] [gan loss: 0.605275, acc: 0.857422]\n",
            "2995: [discriminator loss: 0.6765686869621277, acc: 0.515625] [gan loss: 0.976281, acc: 0.000000]\n",
            "2996: [discriminator loss: 0.6793025135993958, acc: 0.599609375] [gan loss: 0.638647, acc: 0.775391]\n",
            "2997: [discriminator loss: 0.6723759174346924, acc: 0.515625] [gan loss: 0.944381, acc: 0.000000]\n",
            "2998: [discriminator loss: 0.6747518181800842, acc: 0.63671875] [gan loss: 0.662648, acc: 0.642578]\n",
            "2999: [discriminator loss: 0.6765305399894714, acc: 0.5234375] [gan loss: 0.919849, acc: 0.000000]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjkAAAIuCAYAAABdOBlOAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nO3daXhVVZb/8X0hkISEIQwyTzIKgozKKFWFgLNtaZWIimC6VHws7BIcii5B28dGFGyRUqFEWxEtRSktVBRtBUtbBaGYkXkmzIEAgUAC+b/oev5PrbUPd8odzt33+3n3C+eeuyWHk+U9K2sHysvLDQAAgGsqJXsBAAAA8UCRAwAAnESRAwAAnESRAwAAnESRAwAAnESRAwAAnJQR7A8DgQC/X+6o8vLyQKLei+vIXYm6jriG3MW9CLFwvuuIT3IAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTgu5dBQBIrI4dO1pf27Jli8glJSWJWg58IhAIBM3nzp1L5HJSBp/kAAAAJ1HkAAAAJ1HkAAAAJ9GTA/xDpUp2zZ+VlSXyyZMnE7UcOKpJkyYiX3bZZSIvXLjQek3VqlVFPnPmjMjx6MfQ7+n1vogP3W9jjDGtWrUSefv27SLTk+ONT3IAAICTKHIAAICTKHIAAICTAuXl5ef/w0Dg/H+YRFWqVBG5Q4cO1jFr1qwRWT+vDPbfnQ7Ky8vth75x4tfrKCcnR+RXXnnFOmb8+PEib968Oa5rSjWJuo78eg2FQ/d6NW3aVORDhw6JXFxcHPc1eXnzzTdFHjp0qHWM7snR/4aiwb3ImIwM2R47e/Zs65gNGzaI/Pjjj4vMzzTv64hPcgAAgJMocgAAgJMocgAAgJNSsifnyJEjIY+ZOnWqyHr2xPfffy9y5cqVrXOUlpaKXFZWFu4SfY/n4Hbvw8qVK61j+vTpk6jlpCR6ckLT95bMzEyRkzV7qXr16iIfO3Ys4nPo/sho7pHci4zZunWryGvXrrWOuf7660VO9x4cjZ4cAACQVihyAACAkyhyAACAkyhyAACAk3zZeKwHI506dSron4dD/3eePXs25Dn1a/Q66tatG/TP/Swdm/3090dvvjlr1izrNXfeeWdc12SM3Zial5dnHaMHxvkFjcehXXzxxSKvW7dO5ERtrKg3fVy1apXIep1ejh8/LnKNGjUqvK50vBfpa+Ciiy4SuXv37tZr/v73v8d1TcbYm7LqzWSNsZuk/YLGYwAAkFYocgAAgJMocgAAgJN82ZOjN307ceJEMpYR0qZNm0Tu0aOHdUw0A7YSIR2eg+uNEfXmgroXJjc31zpHPDZL1L0Rel1e/WGTJ08W+aGHHor5uqJBT05o+n6WiA04db+ZMcb07t1b5E8++UTk7OzskOe96qqrRP7ss8+iWJ2UDvci/W/+8OHDIuvvV7Vq1eK+JmPsdekBuF5Dcp944gmR9UahyUJPDgAASCsUOQAAwEkUOQAAwEkp0ZNz9OhRkeMxJ8fr2aN+XqnpmRFez9q3b98uct++fUVO1IwMLR2eg9esWVPkffv2iaw3E9QbFiaKvgZCXXfG2P1Gydqsj54cSc8ZMcaYSy+9VORvv/025u/btGlTkTdu3GgdozcGDXWdec390vfmWFx36XAv0j02GzZsEHn9+vUiDxo0KO5r8hLNvSicYxKBnhwAAJBWKHIAAICTKHIAAICTIm9uSQD9/LKoqEjkOnXqhDzH6dOnRX7sscdEnjNnjsh6Hxevc3zxxRci61kH+fn51jnatm0rsl+eX6aDhg0biqz7WPT+MclyzTXXiDx//vyQr0lWDw6C87o3/fu//7vI1157rci6PzAa//qv/yqy1/UR6b1n6tSp1te47qJTq1YtkfXPuD/+8Y+JXM55PfrooyJPmjQpSSuJHT7JAQAATqLIAQAATqLIAQAATqLIAQAATkp647FuBjXGmFtuuUXk2rVrBz2H3uDQGGPatGkj8q5du4KeQw+OC4ceSjhy5EjrmIKCApFj0WQIm1dT5dtvvy2yHtS2ePHiuK4pXAsWLAh5jG6Chz/pgZPGGHPJJZeIfPPNN4v85Zdfiqx/0cIYe1ipvh7++te/ityhQwfrHPp9NT0ILi8vL+jxCN+ECRNE1o3Ie/bsSeRyzuv5558X2avxuKSkJFHLiQk+yQEAAE6iyAEAAE6iyAEAAE5Kek+O17PjBx98UOQDBw6IrJ8V6x4eY4w5dOhQDFYXXOfOnUX22jj0xIkTcV8HjLn33nutr3Xt2lVkvVnqww8/HM8lhS1Uz5kxxqxYsSIBK0FFefUYbt68WWQ97FH3OHjdR/Qxeijf8uXLRb7vvvusc9SrV09k3SukexvHjh1rnQOh6QGwxtj9ms8995zI+vuXLO3btw95zI8//piAlcQOn+QAAAAnUeQAAAAnUeQAAAAnJb0nx2uTxD59+oise3I0Pd/BGHsmSo0aNURu1qyZyGvXrrXOUaVKFZH1bAO9YafXrJajR4+KnJmZKTLzT2Jj2LBh1teOHz8u8hVXXCGyX+Y9rFy5MuQxvXv3TsBKUFF6no0xxrzwwgsi6z493V9TWlpa4XV49QLm5uaKnJOTI7Ke14PotGrVyvrasmXLRH7yySdF9sv8tL/97W8hjxkwYEACVhI7fJIDAACcRJEDAACcRJEDAACclPSeHK9+Gq/9XyKlZz5kZWWJrPsxfvazn1nn0Hsf6fk8ur/G67+lZ8+eIjdu3Fhk/Sx29+7d1jn88rzWz7x6EAoLC0Xetm1bopYTlL5uGjVqJLJXn5bu24A/6B6cl156yTpG9xS+//77cV2TMcY0bNjQ+pqei6Pvgd9++63IzPiKzsmTJ62vbdq0SWTdL5gs2dnZIus9HE+dOmW9JtXuRXySAwAAnESRAwAAnESRAwAAnESRAwAAnJT0xuN4ad26tchXXnmlyMeOHRN5ypQp1jn08D/dZPjuu++KPH36dOsct99+u8hDhgwRWTd6eTX76Q3R9OajW7ZssV7jOj148fLLL7eO0Y3geoPC/fv3x35hYfBq5vtnXpuNIjnGjx8v8pgxY0TWQ0a96EbNu+++W+R/+Zd/EXnnzp3WOfS1qn+xQjev/+Uvf7HOoa+79evXi6yHFiI6eoNpY4xp166dyPpeFGrgbbwUFxcH/fMHHnggQSuJHz7JAQAATqLIAQAATqLIAQAATgoEG+wTCARSYupPpUp2rXbTTTeJrAc0LV26VOSPPvrIOkeXLl1EfuONN0S+5557RPbaoLNJkyZBX1NUVCSy10aT27dvF3n27Nkiv/fee9ZrQikvL7cXGyeJuI7036Mxdr9EWVmZyHqT1r1790b8vvp77jWETfdM6Y1fda+E7gUzxr8DIRN1HcXjGtI9dtdff711zNy5c0U+fPiwyHXr1o34fUMNU/MahqrXpu9n77zzjshem0SuXr1aZL0Rstcw00Rw7V70P//zP9bXBg4cKLL+96w3vfz++++tc4T6/uifg9WrV7eO0f1eetNWfY/UwwLDWUeynO864pMcAADgJIocAADgJIocAADgJF/25Og+B72hYfPmzUW+4YYbrHM0bdpU5AkTJois+zX083ljjBk8eLDIM2fOFLm0tNR6jab/W/Qzzquvvlpk3fdjjL3RZMuWLUXWz1HDkerPwfXzZ73ZoDF270soXhvrzZkzR+T69euL3KtXr5DvqZ97FxQUiKxnmowbN846h1829NNSuScnJydHZK97gL5P7NmzR+RwNiusU6eOyEeOHBFZz0zRf26M3cPxwQcfiKzngHlt8tq7d2+R16xZc54VJ1aq34s0PcfMGPsaCEXPQTLGmGXLlomsZ6zpHkOv61n//Dl69KjIq1atEvmaa66xzuHXjVvpyQEAAGmFIgcAADiJIgcAADgp6XtXec2W0T0Ker8nr9/d1/TeVPqZtn5uOmjQIOscb775psi6F2br1q0iV6tWzTqH7sfQz03feustkb16OpYsWSKyX2emJJKe1eC1V88tt9wS9Bzh9FPccccdQd9Xz7jRz7iNMaZDhw4i6x4qfX2PHDnSOofeF83rmT0io/8OvXrs9H0kGnq2jqb3pfKa+zVjxgyR9b1IX5ffffeddY5169YFXQdi48UXX7S+pvdA0/S9yGt/u8suu0xk/T3XPYW6f8wYY/r16yeyvtby8/NFvvHGG61zvP322yL7/ecRn+QAAAAnUeQAAAAnUeQAAAAnUeQAAAAn+XIY4K5du0TWm1xGQzd76oYrr2a/UPRGel4Dn/T7ZmVliawbr72G2rVu3Vpkr4aySLk2gMuL3mC1f//+IutBVxs2bLDO0bZtW5HnzZsn8vz580XevXu3dQ79b0x/z2+77TaR9dBJY4y5/PLLRV68eHHQ90iUVB4G6Fd6+Kkx9jXx61//WmT9/b/44outc2zevDkGq4u9dLgX6QG2+hdM9Pfr4MGD1jn0LzB89dVXIu/YsUPkcIbE6nuRvs/o+5sx9s+jaDY2jgeGAQIAgLRCkQMAAJxEkQMAAJzky54cPeTIa2BgqtB/vzrrHhy96Zox0W3AGca6nH8Onir0JpBeG/zpwYZ6g8ZkoSen4p555hmRvYZB1q5dW2Q9uHDSpEki6w2J/Yx7kX/o4bVem8XqHkK9iXGy0JMDAADSCkUOAABwEkUOAABwki97cvScnMaNG4ucrB4dvVnf8uXLRf7f//1f6zV/+9vfRF66dKnIoTbvixeeg/tH5cqVRfbaFG/u3LkiJ2sujkZPTsUVFBSI3LBhw5CvGTdunMgTJ06M6ZoSiXuRf+j5Pfq+Y4wxN998s8h+2SyYnhwAAJBWKHIAAICTKHIAAICTfNmTEwuh+nb0XlX6WaQx9nwanfV7+KVPIhw8B/ePjIwMkfPy8qxjvPay8QN6cioumvuG3t/KL30R0eBe5B96Ts6wYcOsY/Q+anquXbLQkwMAANIKRQ4AAHASRQ4AAHASRQ4AAHCSs43HCI5mP8QCjccVd/r0aZGrVq1qHTN27FiRp0yZEtc1JRL3IsQCjccAACCtUOQAAAAnUeQAAAAnZYQ+BAAQL3oYpJft27fHfyGAg/gkBwAAOIkiBwAAOIkiBwAAOImeHADwuT179iR7CUBK4pMcAADgJIocAADgJIocAADgJHpyACCJiouLRS4rK7OOWbFiRaKWAziFT3IAAICTKHIAAICTKHIAAICTKHIAAICTaDwGgCRq2bKlyNnZ2dYxp0+fTtRyAKfwSQ4AAHASRQ4AAHASRQ4AAHBSoLy8PNlrAAAAiDk+yQEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE6iyAEAAE7KCPaHgUCgPFELQWKVl5cHEvVeXEfuStR1xDXkLu5FiIXzXUd8kgMAAJxEkQMAAJxEkQMAAJxEkQMAAJxEkQMAAJxEkQMAAJxEkQMAAJxEkQMAAJxEkQMAAJxEkQMAAJxEkQMAAJwUdO8qBFepkqwRz507l6SVwM8qV64s8tmzZ5O0EgDpLBCQ2zvpn2H6z73uVeXlqbX9F5/kAAAAJ1HkAAAAJ1HkAAAAJznRk1OvXj3ra0VFRSKfOXNGZP3sMScnxzrHjh07RK5WrZrIAwYMEHnJkiWhF4u0M3v2bJH1M+1hw4YlcjlAzOieDn1tp1r/hl/on0/GxObv8vDhwyLXrFlT5G+++Ubku+++2zrHxo0bK7yOROKTHAAA4CSKHAAA4CSKHAAA4KSU7Mlp0aKFyKtWrbKO0c809fPM4uJikfPy8qxzZGZmBj0HPTgIxy233CJyWVmZyHfccYfIiZqjw5wnBHPZZZdZX/vhhx9ELikpEblGjRoil5aWxn5hDgr18yoaf/jDH6yvef2c+2e7du0SOdX6b7zwSQ4AAHASRQ4AAHASRQ4AAHBSINizv0Ag4IshB6H20/CaKRAPf//730Xu3r17Qt43HsrLyxPzl2b8cx0lS6jn67Vq1RJZz3iKhte/iTp16ohcWFgocjQ9OYm6jtL9GkqEe+65R+Tp06eHfI2+Vhs0aCCy7tnxwr3I7v+sUqWKdYye9Va1alWR33rrLZGvv/76iNeh99lLpT69811HfJIDAACcRJEDAACcRJEDAACcRJEDAACc5MthgK1atRJ58+bNSVqJdPz48WQvASlID//TYtForF1yySXW13QTqG5kPHbsWMzXAf/Sjasvv/xyxOfQDbIM//OmfxHgggsuEPn+++8Xefjw4dY56tevL/KiRYtEHjx4cMTrWr9+vcip1GgcLj7JAQAATqLIAQAATqLIAQAATkp6T05Ghr2EunXrJmEloQ0dOjTZS4DPeQ3x0sMrFy5cKHI8NspcuXJlyGNisQkg/EEPenz++eetYzp27Ciy7tuKZqiqPkeiNpf1s3D+Hj/99FOR27RpI3JWVpb1Gj2ob8iQIRGvTd9bLr300ojPkWr4JAcAADiJIgcAADiJIgcAADgp6T05XjNENm3aJLJ+jqh7GBJl7969IidqY1D4l+5JWL58uXWMvk7atm0r8q9//WuR33nnnZDn0DNOTp8+LTL9Nm7T98Bdu3aJnJ2dHZf3nTFjhsh+mWHmJ+H825s0aZLI48ePF/miiy6yXhOPnzcLFiwQuU+fPjF/j2TjkxwAAOAkihwAAOAkihwAAOCkpPfkeCksLBS5WrVqIus9eMKhn5Pqc4TzDFv3BulnpPRBuEd/j/WsihUrVkR8Tr1vTWZmZtD3NMa+tnQPDtzh9f3X80xeeeUVkaPpwdH3M72X2apVq6zXjB49OuL3ge3dd98VuVu3biK3a9fOeo2+90RDX1sFBQUVPqff8UkOAABwEkUOAABwEkUOAABwEkUOAABwki8bjzXdhKWbMMNp1NSbfurmZq+mrvz8fJE/++yzoOsMZx1ILXpo17333lvhc+pNPOfNmycy14xb9H1BZ93wG02Dqb5mSktLrWP00EmuO//44IMPRB47dmzI1+jG8WPHjon8zTffWK/RvygxZ84ckfWQSa9rItWuEz7JAQAATqLIAQAATqLIAQAATkqJnhw9lEpv6ql7HIyxn0nXq1dPZN2T49VPc+GFF4r88ssvi5yVlSWy16Cs9evXi3z27FnrGCSH7vXSfVvGGDNmzBiR9+3bJ3L16tVDvo9+dv6LX/xC5KNHj4Y8B1KD1+bBug/ixRdfDPmaUDZs2CDyo48+KvKHH34Y8TmRPL/73e9EDmczTj0Q9IcffhC5Y8eO1mu6d+8u8rBhw0TWPxeHDx9unUP/TPM7PskBAABOosgBAABOosgBAABOCgT7nfdAIODLX4jfv3+/yHrDQy96hoDu2cnLy7NeE82zck3//Y4aNUrkGTNmVPg9olFeXh76oW+MJOI68vpe6V6tTp06ifzJJ5+IHM51pK893ZOj+2+MMebJJ58Uefr06SLrazOVJOo6Sta9SPdG6I0wMzJkW+PWrVutc9SuXVvkvXv3iqzn5Dz22GPWOd577z2R9X1FnyOVuHYvioa+B4TT6xfOvLh4OH78uMg1atRIyPuGcr7riE9yAACAkyhyAACAkyhyAACAk1JiTo5WUlIisldfUahn6bm5uSLHov/Gi16HnrWj51t07drVOod+BhrqWWw6zOLR/80tW7a0jlm0aJHIujeiWrVqEb+vflau9xkqKiqyXtOkSROR9fyllStXipxqe8O4ombNmtbX9Fwk3YOjczj3olmzZok8YsSIkGvTs8G8er+QunJyciJ+TaJ6cDR9D5w8ebLI4ey7lUh8kgMAAJxEkQMAAJxEkQMAAJxEkQMAAJyUksMAdcPViRMnrGN0U6n+79yzZ4/IujHZGLupVDcN33bbbSI3a9bsPCs+P72u3bt3W8fo/z69kWTDhg1FDqfxONUHcDVo0EDk2bNnW8fojTB1w3rVqlVF1t/vaHj9e9Lvq5tZe/XqJfKBAwcqvI5ESeVhgDfddJPIuiHYmNDN6eE0iYf6RQGdvc65c+dOkYcMGSKy3rAzlaT6vSgW9Gaaa9asCfka3Yx+5MiRkOfYtm2byI888ojIBQUFInttfq3p61U34yeqSZ5hgAAAIK1Q5AAAACdR5AAAACelZE+O5rUpnh7A1qpVK5F37doV83V4Pb/UG+dFs6mafqa5cOFCka+44opwl/jP60jp5+B6QKIe7miM/femr4H8/HyRCwsLrXOMHz9eZN0v8+KLL4q8YMEC6xxfffWVyPqayMzMFLlRo0bWOfwqlXpydM+V7mkIh9fAwH/mtdlqnTp1RP6P//gPke+55x6RvXrD9H1jzpw5Ig8dOjTouvws1e9FsaD/ze/YscM6Rl8D+jWHDh2q8Dr0UNxoBsuOGzdO5IkTJ1ZoTeGiJwcAAKQVihwAAOAkihwAAOCklOzJ0b0vn3/+uXWM7tHo2bNnXNd0Prrn5qGHHhL56aefDnq8MfacHN0XEM0cglR7Dq43sNO9D14brOqZEG3bthU5nJ4M/b7FxcUhX6PptYXabFFvJGqMd6+HH6RST87w4cNFfuONN0K+Rs/JOXXqVEWXEZV58+aJfO2114rcuHFjkffu3Rv3NcVKqt2L4qFfv34iz58/3zpGb+Tbv3//uK7JGGOmTZtmfe3+++8P+poPP/xQ5BtvvDGmazofenIAAEBaocgBAABOosgBAABOygh9SPLpPhW9F5BXP0ZRUVFc1xQu3fO0fPlykfV/m56hYowxTz31lMiJ2gvET/QcEK/vuXb69GmRo5mLEk0Pjqa/XzrruSjh/Lchcs8++2zQP9fXizHJ68HRBgwYILK+r/zud78T+eGHH477mhA+fZ/X++Z9+umnInvN/VqyZEnsFxaC7iE1JnRPzk8//RSv5USFuykAAHASRQ4AAHASRQ4AAHASRQ4AAHCSLxuPa9WqJfKRI0ciPkeNGjVE1s2ciWre1Q1nesNH3UC4YsUK6xzPPPNM7BeWYh544IGIX+O1YWowXoMYgw3LjJY+Z6jGZMRGqIZuvfGtnzz++OMiP/fccyJ7NarCP/TwxldeeUVkPXS0tLTUOsd7770X+4WFoH/Jxxj7/qXzCy+8ENc1RYpPcgAAgJMocgAAgJMocgAAgJN82ZOzcePGmJ9Tbxp2/PhxkSdMmGC9pqCgQGQ9GEw/i9RD3YwxZuvWrSI3a9Ys6Dl69eplnSMefSGpJi8vL+LX1K1bV+T27duLvGvXLpFjMfgvHHrgY1ZWlsgZGb78Z5nypk6dKvIf/vAHkW+66aZELiciF198cdA/37JlS4JWglD0pq7G2Nde/fr1g55j06ZN1td+/PHHii0sCldccYX1NT1Udd++fSIfOnQormuKFJ/kAAAAJ1HkAAAAJ1HkAAAAJ/ny4b9+plevXr2Iz3H27FmRBw8eLLKeoTJs2DDrHKHmAej5PV4b/DVs2FBkPf9A94nQf+Pt5z//ucjr1q0T2Wsmju67GjhwoMgzZ86M0eoic+DAAZGbN28usu4lMsaYwsLCuK4pHeiNbidNmiSy12ySZPCa1zR8+PCgr4nmHon48JrHVLNmTZH1LKzFixeL/NJLL1nn0D8b9HUSzc8O3f939dVXizx69GjrNbonZ9GiRUH/PNn4JAcAADiJIgcAADiJIgcAADgpEOw5XiAQ8EWDiH7GqZ9F6r4XY4xZsGCByG3bthU5mlkkoZ55eu05tHv3bpHbtWsnslcfTyKUl5fbD/7jJBbXkZ5BpJ9xv/POO9ZrOnfuLLL+u27Tpo3Ien5NvOieHN2D86c//cl6zf333y+yX557J+o68su9KBGqVq1qfS3UfeK6664T+eOPP47pmuIp1e5FoXj15Bw9elRk/T3Ws2b0/o3GGHPy5EmR9X5lmZmZIm/YsME6R4MGDUTWvX6NGzcW2Wv2mz5vjx49RNb9sIlyvuuIT3IAAICTKHIAAICTKHIAAICTKHIAAICTUqLxOBq6Oblv374if/HFFyJ7DZPTjcS6mU83benmUGOMKSkpCb3YJHCt2c+LbqJ7+umnRc7Pzxc5Xo3HderUEVk3HutGxYMHD1rn0M31yWru02g8jr0aNWpYX9u2bZvI1atXF1k30e/YsSP2C4uTdLgX6U14P/30U5F1I3LPnj2tc3j9jKooPQBT/8ybN2+e9ZoRI0aIrBuik4XGYwAAkFYocgAAgJMocgAAgJOc7clBcOnwHNwvateuLfKPP/4ocrNmzUTes2ePdQ69wezGjRtjtLqKoScn9ryGmy5cuFDkRo0aidyhQweR9RBSP+NeZPeQeg0UfO2110RetmyZyM8++6zIegNpY4z55ptvRB47dqzITZo0CfoexqRenymf5AAAACdR5AAAACdR5AAAACfRk5OmeA6eOPp5u9707re//a3I48ePt87RqlUrkfWMpmShJyf26tWrZ31N91I89dRTIusZXnl5edY5tm/fLrLXhsLJwL0IsUBPDgAASCsUOQAAwEkUOQAAwEn05KQpnoP7R25ursjZ2dnWMYcPHxY53fop0ukayszMtL6m56boWSUZGRkhz1tWViZysHt/InEvQizQkwMAANIKRQ4AAHASRQ4AAHASRQ4AAHASjcdpimY///LanM8vjcYajcexp4dHGhN5k3AszpEo3IsQCzQeAwCAtEKRAwAAnESRAwAAnBS0JwcAACBV8UkOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwEkUOAABwUkawPwwEAuWJWggSq7y8PJCo9+I6cleiriOuIXdxL0IsnO864pMcAADgJIocAADgJIocAADgJIocAADgJIocAADgJIocAADgJIocAADgJKzE8h4AAB0jSURBVIocAADgJIocAADgJIocAADgJIocAADgpKB7V6W7QEBuhVGlShWRs7OzRS4uLrbOUVZWFvuFAUgZ+j5SqZL8f8uzZ88mcjlAWuGTHAAA4CSKHAAA4CSKHAAA4CQnenL0M29jjCkvL6/weU+fPi2y7skpKCgQefDgwdY51q5dW+F1wD/0tRaL6wzu8LoXdevWTeR169aJfOrUqbiuCW7SvV3nzp1L0kr8jU9yAACAkyhyAACAkyhyAACAk1KyJycefRFevTO6B0ebPXt2yHPEA30hifPmm2+K3Lt3b5H3798v8qBBg6xz1KxZU+SOHTuK/NVXX4nMs/XUofsiXn311ZCvyc/Pj9dy4LA9e/aIrH8O/PTTTyJfc8011jnq168v8gcffCDyb37zG5GXLVsW8Tr9hk9yAACAkyhyAACAkyhyAACAk1KyJ+eCCy4QuWXLltYxu3fvFlk/v1y+fLnIderUiXgdjzzySMSviQV6cBJH7yuk+2VatGgh8qFDh6xzZGZmilxSUiKy7v364YcfrHMMGTJEZGar+MN3330n8vr1661jRo4cKTL/fhGK7vUyxpjCwkKR9X3k5MmTIuseHWOMadKkicj6Wly6dKnIXnsv6r7DRYsWWcf4CZ/kAAAAJ1HkAAAAJ1HkAAAAJ1HkAAAAJ/my8bhy5coiDx06VOSnn35a5IYNG1rn0I1belBfNI3G7777bsSvQWobMWKEyLqB/aKLLhJZN7QbYzf3ZWdnBz1nv379rHO0atVK5DVr1ngvGHH10UcfiXzppZeK/Mtf/tJ6TSIajXVze9OmTa1jNm/eHPd1IDa8BoJ26tQp6GsyMuSPc697hP6FhWrVqkV0TmPsX/zxOz7JAQAATqLIAQAATqLIAQAATgoEe14cCASSMrVKPwc8duyYyFlZWRGfU/c9hEMPQqpatarIqTzUq7y8PPK/kCgl6zpKBN0/1rp1a+sYPaRLD/vbtGmTyLr/xhhj2rVrF/ScyZKo68gv19C2bdtE1veExo0bJ2Qd+n5WWloqsr4ujTHm97//vci6tzFZuBfFh742jQk9FLe4uFjkZs2aWefQ96KioiKRk/Vz8XzXEZ/kAAAAJ1HkAAAAJ1HkAAAAJ/lyTo7uhZk/f77IerPCnJwc6xzR9OBo+tlifn6+yDNnzqzweyC16Q08vWaR6FkUr732msiDBw8WuUuXLtY52JAzOXT/34kTJ0ReuHBhIpfz/+l7k1cPjjZx4kSR/dKTg/jw2lxT34v09ayzV3+gPsbv+CQHAAA4iSIHAAA4iSIHAAA4yZc9OdrNN98s8vXXXy/yrFmzrNfUrFmzwu+rn3Nv2bKlwueEW/RMp1q1alnHDBgwQOT+/fuL/Mgjj4h8+PDhGK0OFZWbmytyjRo1RP7Tn/6UyOWcl+6vefTRR61jUnmuF0LT+5e1bNnSOuYvf/mLyHrfx6uuukrkZPXf6J5ar2tX7095PnySAwAAnESRAwAAnESRAwAAnESRAwAAnOTLDTpD0c1/+/fvt47RQ7z00LYjR46I/Pjjj1vn0JuXLV26VOQzZ86EXKtfsSledHRDnN7Abt26ddZrsrOzRd64caPIHTt2FFlfq37m+gadelPLJ554QuQ2bdqIvGPHjrivyYu+xrw2cC0pKQn6mmThXhQb+udiYWGhdYz+ZRq94eyFF14Y+4UlCBt0AgCAtEKRAwAAnESRAwAAnJQSwwC1a665RmQ9kM2L7nNYsmSJyPfee6/1Gv1ce+vWrSLrnp0pU6aEfF+kNj1wS2+2qTfA83LllVeKzDXiDw0aNLC+Nm7cOJHvvvtukXfu3BnXNYWra9euIY/5/vvvE7ASJEv37t1FDmfT1m7dusVrOb7BJzkAAMBJFDkAAMBJFDkAAMBJKTknZ/369SK3a9cuSSsJ7ejRoyLn5eUlaSUSsymi06RJE5H1XBSvTeP0v7FwN5ZLBS7NyWnfvr31tQkTJoh81113iXzq1Km4rilcRUVFIuuZKcbY151fNuzkXhQbuoc0nDlIeu5XKmNODgAASCsUOQAAwEkUOQAAwEkpOSendevWyV5C2GrVqiWy3iPLa88s+Ncll1wicjj9NePHj4/XchBDZWVl1tc2bdokst7/KVn0PCbdg+O1d5VfenAQH+H04Jw+fToBK/EXPskBAABOosgBAABOosgBAABOosgBAABOcqLxeNu2bSFfo5sKt2/fHvIcuulQb9ZXWFgocjhNqHq42BNPPCEyzYH+VlpaGvFrXn/99dgvBDGnB/0ZY8yAAQNEnjZtmsgHDx6M65rO58SJE0H//N/+7d8StBIkS25ubsSv+fjjj+OwEn/jkxwAAOAkihwAAOAkihwAAOCklNygs169eiLv27fPOubcuXMi62FZsdhYT29upt8zHP/1X/8l8oMPPlihNYWLTfHCU7lyZZF1H5a+rryugYwM2frmUt+VSxt0zpw50/pafn6+yGfPnhX5uuuuE/nzzz+3zqFfo+lePq9eC70RrL7u9HtkZWVZ54jm/pQI3IvCo3/eHDt2TORwenT0dXP8+PGKL8wn2KATAACkFYocAADgJIocAADgpJSck9O5c2eRvTaj+/LLL0WORQ+Opnsrhg8fbh0za9asoOcYNGhQTNeE2Grfvr3I+pm25jVHp2bNmiIfPXq04gtDzF111VUhj9E9WvPnzxfZ6/u/du1akXXvRIMGDUTWPVzG2D02updi8+bNIntt1lhcXGx9DaljxIgRIofqwfHqwfJrX1Y88UkOAABwEkUOAABwEkUOAABwUkrMydHPqPV8AK/nz3rvlqlTp8Z+YWEINRNl9erVIut+o3hhNkV4vvrqK5F//vOfBz3eq9+mdu3aIjMnJ3KJuIbGjBljfW3y5MlBX6O/l17ff92TpfsidK/M3r17rXP069dPZN0bNGrUKJHXrVtnnWPu3LlB15Es3IvCU1JSInJmZmbQ4/W9yxhjBg4cGNM1+QlzcgAAQFqhyAEAAE6iyAEAAE6iyAEAAE7yZePxpZdeKvKf//xnkVu2bCny6dOnrXO0adNG5N27d8dodZHRG+fpzfj0Bn8ff/xx3NdkDM1+4dID4vT3Rw+i7N+/v3WOFStWxH5hPuFS47GXRo0aiaz/PTdt2lRkr8GkF154ochLly4V+cCBAyKH0xCsN2vs1q2byAsXLrRe06JFC5H1ZrPJwr3Ipn9OGGM3wT/wwAMiz549W+SRI0da5/BLs3k80HgMAADSCkUOAABwEkUOAABwUtJ7cryePX799dci9+3bV2T9PHrmzJnWOX7zm9/EYHWR8dpYTw9w0n/fXbt2FXnNmjWxX5gHnoPbvL5/hw4dElkP4JoxY4bIXgPldB+HS1zvyUkVOTk5InsNJRw6dKjIejhgsnAvslWrVs362v79+0XW9ybdh1pWVhb7hfkYPTkAACCtUOQAAAAnUeQAAAAn2U0IPpCVlSWyfrY4bdo0kd977724r8mL7s+YPn26dYzuOTp16pTIP/30U+wXhqjomUXG2Ju/6r6d6tWri1yvXj3rHAcPHhTZ5R4dJMeZM2dE/uGHH6xj5s+fn6jloII+++wz62t6Hpz+2cJ9xRuf5AAAACdR5AAAACdR5AAAACclvSfHay+N48ePi6znA3Tp0kXk22+/3TpHlSpVRNa9FHrWzpQpU6xzdOzYUeSCggKR77rrLpH1rApj7Oek+pw8R/WP119/3fpa1apVRdbXa25ubtBsjN2TA8Sa7mP84osvrGN03w78o06dOiL36dPHOqZy5coi6330gs28S2d8kgMAAJxEkQMAAJxEkQMAAJxEkQMAAJyU9A06z/O+It92220it2vXTuQHH3zQOofXBmcVpf+u9Dr1ZpzGGDNq1CiR33jjjaDnTBQ2xbMVFxdbX9PX0bFjx0T+5JNPRL7vvvusc3htlugKNuhERXEvMqZHjx4iL1682DpG/7zRv+iiB82mGzboBAAAaYUiBwAAOIkiBwAAOMmXPTmR0s8qjTHmhRdeEHn79u0iP/300yJ7Dco6fPiwyE8++aTIV155pcgvv/yydY5FixaJrDcbTRaeg9v0wEhjjHnooYdE1pvBbt26VWSv4ZYuoycHFcW9yKY33zTGvj8x3FGiJwcAAKQVihwAAOAkihwAAOAkJ3py/MKrN8ivm6bxHByxQE8OKop7EWKBnhwAAJBWKHIAAICTKHIAAICT7MEgiJpf+28AAEhHfJIDAACcRJEDAACcRJEDAACcRJEDAACcRJEDAACcRJEDAACcRJEDAACcRJEDAACcFHSDTgAAgFTFJzkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJGcH+MBAIlCdqIUis8vLyQKLei+vIXYm6jriG3MW9CLFwvuuIT3IAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTKHIAAICTgu5dBSAy1apVs75WUlIi8rlz5xK1HDgiI0Peqs+ePStyeTlbMgFe+CQHAAA4iSIHAAA4iSIHAAA4iZ4cIAKVK1cWOTMzU+RTp05ZrwkEAnFdk5dw3pM+jtRx5MgRkfV1mJOTIzLfW+D/8EkOAABwEkUOAABwEkUOAABwUiDYs9tAIMCD3QSrWrWq9bVjx46JvG3bNpE7deokcllZWcj3KS8vT1ijiEvXke518UvvQ9u2bUVetmyZdYzuH9J9HKWlpRG/b6KuI5euoVAqVbL/31PPxdH09/LkyZMxXVM8cS+y5yC1b9/eOmbDhg0i62si3edvne864pMcAADgJIocAADgJIocAADgJObkJFmLFi1E1v02XgoLC0VOxhyWdOWXHhz9PdfP68NRo0YNkQ8fPlyhNSE2OnToEPFr9P5oSC36vp+dnW0do/vs9M+BWbNmidylSxfrHPo60a9x8R7AJzkAAMBJFDkAAMBJFDkAAMBJFDkAAMBJDANMMD3oSw/t0gPbvOzcuVPkli1bihzOUCgGcIVHf7/8MnDr9ddfF/nOO+8M+Ro9JFJfa9H8tzEMMPa+++4762u9e/cWWd+3vQYIpop0uBfpYX96wKtXo3Gk9DURzSa9zZs3F3nXrl0VXleiMAwQAACkFYocAADgJIocAADgJIYBVkCjRo1Evvrqq61jWrduLfLo0aNFDqcHR/dKdOvWLeifI3aS8Xebl5dnfe2Xv/ylyL/61a8iPu9zzz0nMteNP1SvXl3kXr16hXzNxIkT47UcxEGVKlVEjkUPjhbNUFj9mrlz54o8YMAA6zWnTp2K+H2SiU9yAACAkyhyAACAkyhyAACAk+jJiUDNmjVF1psi5ubmVvg9vOYWjRo1SmQXN1HzA69n2noTy6Kiopi/7w033CCyfi5ujDGVK1eO6JwnTpywvjZu3LjIFoa40P0ZemZKOC6//HKR9bXrl41k8X/090P3tcSjRycaek7OG2+8YR1z8OBBke+//36R/Xbt8UkOAABwEkUOAABwEkUOAABwEj05/3DBBRdYX3vrrbdE7tSpk8i6ByecZ5FnzpwRubCwUOS3337bes2rr74a8ryouJycHOtrL730ksh33HGHyLGYNfPf//3fIkcz70Jfe1OmTLGOOXv2bMTnRcXp/pn8/PwKn7Nx48Yi05Pjb/o+ofcnGzhwYMhz6O/pqlWrRF60aJHIeiablz179oisf9bo+50xxhw4cEBkv197fJIDAACcRJEDAACcRJEDAACcRJEDAACc5Gzjcd26dUWuVEnWc/v27RM5mmZP3chZXFxsHdOnTx+R161bJ7LfmrTSWUlJifW1a6+9VuTrrrtOZD0Yq6CgwDrH4MGDRW7atKnI+tr0ambWx2h6SKEeVInE8Po+PfnkkyLre0I0Vq9eLTKbrfqH18+SOnXqiNy3b9+g5/D6uTBp0iSRf//73wc9x4MPPhjyvDpXrVpV5Hvvvdc6x8aNG0X2+7XHJzkAAMBJFDkAAMBJFDkAAMBJKdmTozcrbNKkiXWM7n354x//KLLup8nIsP8q9PPKvXv3ivzII4+IPHv27POsGKkgLy/P+prePFEP3NLP3/v372+dQ/d/bdq0SeRq1aqJfPHFF4dc25YtW0SeM2eOyPPmzbPOgdjTvX/t2rWzjtH9F3pzxnA29tV9D6+88orIfh/Ilk68NtPVg2T1v9dbb71VZH2PMMbuswv1PQ/nGtBrvfHGG0X22jhUX/N+xyc5AADASRQ5AADASRQ5AADASYFgz+0CgUBSHuzq54QtW7YU+Wc/+5nIM2bMsM6h51WsXLlS5PXr14v8wgsvWOdYu3atyPrvSvdrpJLy8vLIBwNFKVnXUaR69Ohhfe2BBx4QeeTIkSKXlZVF/D56M9iePXuK7LWR5pAhQ0Q+efKkyBMnTgz658bEZ55Foq4jv1xD+t60detWkZs1axbyHKdPnxZZzybxmrOiZzi9/vrrIj/22GMiHzp0KOQ6/CId7kWh+mdq1KghcpUqVaxz6Hlb+hy33HKLyPqaMMb+2Xn48GGRp0+fLrJXT86uXbtEHjBggMi7d++2XpMI57uO+CQHAAA4iSIHAAA4iSIHAAA4Kelzctq3b299bdWqVSLr/hqvOQShvP/++yIPHDgw6HsYY/dbRNN/Af/Q10337t1Fnj9/vvWaPXv2iKz7ZcKZT6J7LvS1pt/DS+/evUWePHmyyHr2it/3k0kVeibITTfdJHI4PThaOD04oV5zww03iKy//2PGjLHOweyc5An1dx9Of+eXX34p8uOPPy6yntG0cOFC6xy1a9cWWd/PdC+Q17r1XLqHHnpI5JycHJG9eoOOHj0qsv7ZWlpaar0mWnySAwAAnESRAwAAnESRAwAAnESRAwAAnJTwYYCjRo0S+bnnnrOOycrKCnoO3VTp1bgXqkFUN396NWrqDTlDDeBKJa4N4PK6BpYsWSJyt27dRPZqNtf0vw89yE03/3k1EOrBk0eOHBFZD+jSDdHGGPP555+LrDf40+tKFJeGAXr9QsO4ceNEXrNmjcivvfaayLVq1arwOrzuyfr+pO9NekPHsWPHWufYuXOnyH4ZGOjavSheli9fLnLz5s1FLi4uFrlhw4bWOUL93Hv++edF9hoqet9994l84sQJkfWGw3p4oDHGLFiwQGT9yxcvvvii9ZpQGAYIAADSCkUOAABwEkUOAABwUtx7cjIy5LzBaIb86M3L9Jr1M0Fj7OeCgwYNElkPU9PPN42xn9HrzcyaNm0qcrL6IqKR6s/B9fPmmTNnWsdcffXVIm/btk1kvfFrOKIZ6vXqq6+KrPvFfvWrX4msn70bY8ydd94p8vHjx4OuI1FSuSenRYsWIr/55pvWMXqzwREjRog8a9Yska+88krrHLm5uSLv379fZH1v+umnn6xz6I1iV69eLfLcuXNFfuqpp6xz6IGB06ZNs45JhlS/F8WD/plnjDHffPONyLrHasKECSJPmTLFOoceGHj33XeL/Mknn4icmZlpnePZZ58V+eabbxZZ9/non8XGGHPgwAGRdc/hb3/7W+s1odCTAwAA0gpFDgAAcBJFDgAAcFLce3L0RnIffvhhyNfoXhf9XDxRJk2aJPLDDz8ssn4u3rlz57ivKVZS7Tm4noOj+xi85hytWLFC5GjmguieMr2RnH5m7dVzpje9071BerbKM888Y51Dz+Pxi1TuydF9effcc491zLJly0QePny4yEVFRSLre4Yx9uaql1xySUTr9NKjRw+Rb7zxRpHDuRddd911FV5HLKTavSge9KaWel6NMcYMHjxY5Msvv1xkfS169apWr1496GvCmUGn72e9evUSeeLEiSJ37drVOoee86Q36o6md5eeHAAAkFYocgAAgJMocgAAgJMyQh9SMVOnTg365169FMnqwdH0XkeanjmA+NF7ruj+Gt1/Y4z3tRUp3YOjhTMbyWuGzz87c+aMyC+99FLohSFieu6Vnj3jtXeV7jfQ19QvfvELkQcMGGCdY+DAgRGtMxy6V6hfv34i6/k9xth77YXqN0Ps6L47/W/81ltvDXkOfa/R82k++ugjkevXr2+d4+DBgyLXrFlTZN2j4zUnR/eU6b32evbsKbLXHoF6rdH04ISLT3IAAICTKHIAAICTKHIAAICTKHIAAICT4j4MUG/EVa9ePZF106Ux3s1OyaAHFOmN8/SGd14bkflVqg/g0s1ssWgyjpcZM2aIfNddd4msN9usW7eudQ6//vel0jDAtm3birx+/Xr9HtZr9N+7HlSqf7FCN2EaY0zr1q2DnjMW9DWjN4E1xm52ve222+K+rnCk+r0oHHrz19tvv73C5zxy5IjI+md5Xl6e9ZpQGwzrhnavTYx1g77eTFTfm3WzszH2L+0cPXo06LrCwTBAAACQVihyAACAkyhyAACAk+I+DHDevHki636Er7/+Ot5LiFqo55fHjh1L0Eqg+aVHRQ9U89qgMT8/P+g5xowZI7Jf/ttco/tpvHpwQlm8eLHIuqdw9OjR1msS8f1s1qyZyBdddJF1jB4QGOr+htjR/U+x4NVzE0qoa14P8vOi+7309b1z506RW7VqZZ0jkdcen+QAAAAnUeQAAAAnUeQAAAAnxX1Ojv6d+ssuu0zk7777rqJvETcrV64UuXPnziLrTSL1DCA/S4fZFInQsWNHkVetWmUdo+dG6Gu+b9++sV9YgqTSnBz9fdi2bZvIuq/FGLv/4MSJEyLfcMMNIierx3DJkiUh11FQUCDytGnTRE7WBp3pcC/Ss7Byc3OTsQyL7qfRG3QuX77ces0777wj8ty5c0UuLCyM0eoiw5wcAACQVihyAACAkyhyAACAk+Lek5PK9AyMKlWqiLxixQqRu3btGvc1xUo6PAdPhA0bNois90fyovvSdD9FKkmlnhxN9wvu2rXLOqZBgwZBz/Hpp5+KfOutt1rHxGOeVrdu3UTW83u8TJ48WeRx48aJnKy5OdyLbF7zbPT+ZNnZ2SI3bNgw6PHG2H1X3377rch67pfen9EY7/0m/YCeHAAAkFYocgAAgJMocgAAgJMocgAAgJPivkFnKtNNpR06dBB569atIuthY8aw2aLrNm7cKLJX4/Hu3btFTuVGY5foQX+NGjWyjnn77bdF1o3FnTp1EtlrI0Y9QFDfN3788UeRvRo7daOxfo2+93g1EX/00Uchj4E/eH1vDh48GPQ1emNM/B8+yQEAAE6iyAEAAE6iyAEAAE6iJ+cf9GAwY+wNOPWzcj2Ay+sc+tkqz8Hd0qtXr5DHrF27NgErQTwMGzZM5BEjRois+3q8/n1ffvnlIq9Zs0Zk3eu3bt066xzNmzcXecuWLSK3bt1a5NLSUuscelipnzdHBmKFT3IAAICTKHIAAICTKHIAAICT2KDzH/TGZMYY89Zbb4n8/vvvi7x69WqRvWZk/Od//qfIXhueJQOb4sXGgQMHRK5Xr551zLRp00QePXp0XNeUSKm8QadLunTpIvKsWbOsY/r37y9yUVFRXNcULu5FiAU26AQAAGmFIgcAADiJIgcAADiJnpx/CATsx3l67o2eiZGVlSWyV1/PyZMng54jWXgOHhtff/21yB07drSO0V/bv39/XNeUSPTk+IO+f3ndi7xm5/gB9yLEAj05AAAgrVDkAAAAJ1HkAAAAJ1HkAAAAJ9F4HEOVKtk147lz55KwktBo9ouNzMxMkS+44ALrmN27d4vs0iatNB6jorgXIRZoPAYAAGmFIgcAADiJIgcAADgpaE8OAABAquKTHAAA4CSKHAAA4CSKHAAA4CSKHAAA4CSKHAAA4CSKHAAA4KT/B2u0a81Rt6sQAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 720x720 with 16 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        },
        {
          "output_type": "stream",
          "text": [
            "3000: [discriminator loss: 0.6685032844543457, acc: 0.607421875] [gan loss: 0.668647, acc: 0.642578]\n",
            "3001: [discriminator loss: 0.6728441715240479, acc: 0.513671875] [gan loss: 0.928191, acc: 0.000000]\n",
            "3002: [discriminator loss: 0.6703016757965088, acc: 0.626953125] [gan loss: 0.665975, acc: 0.642578]\n",
            "3003: [discriminator loss: 0.667513370513916, acc: 0.53125] [gan loss: 0.954200, acc: 0.000000]\n",
            "3004: [discriminator loss: 0.6661546230316162, acc: 0.626953125] [gan loss: 0.631201, acc: 0.742188]\n",
            "3005: [discriminator loss: 0.682590126991272, acc: 0.50390625] [gan loss: 0.980754, acc: 0.000000]\n",
            "3006: [discriminator loss: 0.6728971600532532, acc: 0.6142578125] [gan loss: 0.636565, acc: 0.730469]\n",
            "3007: [discriminator loss: 0.6789567470550537, acc: 0.5107421875] [gan loss: 0.976744, acc: 0.000000]\n",
            "3008: [discriminator loss: 0.672558605670929, acc: 0.62890625] [gan loss: 0.639393, acc: 0.744141]\n",
            "3009: [discriminator loss: 0.6756165623664856, acc: 0.529296875] [gan loss: 0.955318, acc: 0.000000]\n",
            "3010: [discriminator loss: 0.6674797534942627, acc: 0.619140625] [gan loss: 0.645213, acc: 0.712891]\n",
            "3011: [discriminator loss: 0.6715825796127319, acc: 0.5234375] [gan loss: 0.960076, acc: 0.000000]\n",
            "3012: [discriminator loss: 0.6707107424736023, acc: 0.6416015625] [gan loss: 0.665256, acc: 0.628906]\n",
            "3013: [discriminator loss: 0.6707870364189148, acc: 0.51953125] [gan loss: 0.940109, acc: 0.000000]\n",
            "3014: [discriminator loss: 0.6617740988731384, acc: 0.669921875] [gan loss: 0.674308, acc: 0.630859]\n",
            "3015: [discriminator loss: 0.6742729544639587, acc: 0.5283203125] [gan loss: 0.964804, acc: 0.000000]\n",
            "3016: [discriminator loss: 0.6652584075927734, acc: 0.650390625] [gan loss: 0.660834, acc: 0.683594]\n",
            "3017: [discriminator loss: 0.676323413848877, acc: 0.48828125] [gan loss: 0.981397, acc: 0.000000]\n",
            "3018: [discriminator loss: 0.6660775542259216, acc: 0.63671875] [gan loss: 0.640761, acc: 0.750000]\n",
            "3019: [discriminator loss: 0.6857815384864807, acc: 0.4873046875] [gan loss: 1.017755, acc: 0.000000]\n",
            "3020: [discriminator loss: 0.6848052740097046, acc: 0.5859375] [gan loss: 0.621806, acc: 0.880859]\n",
            "3021: [discriminator loss: 0.6882681250572205, acc: 0.4931640625] [gan loss: 0.995151, acc: 0.000000]\n",
            "3022: [discriminator loss: 0.6758334636688232, acc: 0.58984375] [gan loss: 0.632223, acc: 0.802734]\n",
            "3023: [discriminator loss: 0.6964200139045715, acc: 0.4833984375] [gan loss: 0.959125, acc: 0.000000]\n",
            "3024: [discriminator loss: 0.6855168342590332, acc: 0.58984375] [gan loss: 0.652631, acc: 0.714844]\n",
            "3025: [discriminator loss: 0.697433590888977, acc: 0.4658203125] [gan loss: 0.914286, acc: 0.000000]\n",
            "3026: [discriminator loss: 0.6817885041236877, acc: 0.6171875] [gan loss: 0.694992, acc: 0.531250]\n",
            "3027: [discriminator loss: 0.6898778080940247, acc: 0.4912109375] [gan loss: 0.873106, acc: 0.009766]\n",
            "3028: [discriminator loss: 0.6853839755058289, acc: 0.560546875] [gan loss: 0.720470, acc: 0.365234]\n",
            "3029: [discriminator loss: 0.68592768907547, acc: 0.4931640625] [gan loss: 0.866218, acc: 0.000000]\n",
            "3030: [discriminator loss: 0.6698777079582214, acc: 0.6181640625] [gan loss: 0.734966, acc: 0.302734]\n",
            "3031: [discriminator loss: 0.6760191917419434, acc: 0.509765625] [gan loss: 0.904754, acc: 0.000000]\n",
            "3032: [discriminator loss: 0.6631170511245728, acc: 0.666015625] [gan loss: 0.689302, acc: 0.556641]\n",
            "3033: [discriminator loss: 0.6753796339035034, acc: 0.5048828125] [gan loss: 1.008113, acc: 0.000000]\n",
            "3034: [discriminator loss: 0.6625511646270752, acc: 0.61328125] [gan loss: 0.600738, acc: 0.894531]\n",
            "3035: [discriminator loss: 0.6918267607688904, acc: 0.4951171875] [gan loss: 1.109013, acc: 0.000000]\n",
            "3036: [discriminator loss: 0.6758237481117249, acc: 0.564453125] [gan loss: 0.541819, acc: 0.960938]\n",
            "3037: [discriminator loss: 0.7131507992744446, acc: 0.4970703125] [gan loss: 1.069795, acc: 0.000000]\n",
            "3038: [discriminator loss: 0.686697781085968, acc: 0.5771484375] [gan loss: 0.607573, acc: 0.880859]\n",
            "3039: [discriminator loss: 0.6893116235733032, acc: 0.4833984375] [gan loss: 0.943395, acc: 0.001953]\n",
            "3040: [discriminator loss: 0.6847652196884155, acc: 0.6220703125] [gan loss: 0.667110, acc: 0.701172]\n",
            "3041: [discriminator loss: 0.6690952181816101, acc: 0.482421875] [gan loss: 0.889210, acc: 0.009766]\n",
            "3042: [discriminator loss: 0.6949756741523743, acc: 0.5419921875] [gan loss: 0.683547, acc: 0.554688]\n",
            "3043: [discriminator loss: 0.6550284624099731, acc: 0.55859375] [gan loss: 0.837576, acc: 0.097656]\n",
            "3044: [discriminator loss: 0.6933134198188782, acc: 0.5205078125] [gan loss: 0.750122, acc: 0.324219]\n",
            "3045: [discriminator loss: 0.657300591468811, acc: 0.6103515625] [gan loss: 0.779725, acc: 0.189453]\n",
            "3046: [discriminator loss: 0.6864312291145325, acc: 0.478515625] [gan loss: 0.834173, acc: 0.072266]\n",
            "3047: [discriminator loss: 0.6725832223892212, acc: 0.59765625] [gan loss: 0.690734, acc: 0.537109]\n",
            "3048: [discriminator loss: 0.6905035972595215, acc: 0.4755859375] [gan loss: 0.926023, acc: 0.000000]\n",
            "3049: [discriminator loss: 0.6831270456314087, acc: 0.6181640625] [gan loss: 0.614253, acc: 0.851562]\n",
            "3050: [discriminator loss: 0.6943845748901367, acc: 0.486328125] [gan loss: 1.017100, acc: 0.000000]\n",
            "3051: [discriminator loss: 0.6804095506668091, acc: 0.6005859375] [gan loss: 0.576527, acc: 0.974609]\n",
            "3052: [discriminator loss: 0.7029236555099487, acc: 0.484375] [gan loss: 1.023421, acc: 0.000000]\n",
            "3053: [discriminator loss: 0.6750070452690125, acc: 0.619140625] [gan loss: 0.618560, acc: 0.896484]\n",
            "3054: [discriminator loss: 0.6874216794967651, acc: 0.494140625] [gan loss: 0.991523, acc: 0.000000]\n",
            "3055: [discriminator loss: 0.6775504350662231, acc: 0.6171875] [gan loss: 0.638453, acc: 0.779297]\n",
            "3056: [discriminator loss: 0.6832448840141296, acc: 0.4921875] [gan loss: 0.950119, acc: 0.000000]\n",
            "3057: [discriminator loss: 0.6675369143486023, acc: 0.640625] [gan loss: 0.675383, acc: 0.607422]\n",
            "3058: [discriminator loss: 0.6812821626663208, acc: 0.490234375] [gan loss: 0.926317, acc: 0.000000]\n",
            "3059: [discriminator loss: 0.6728670001029968, acc: 0.6337890625] [gan loss: 0.685108, acc: 0.527344]\n",
            "3060: [discriminator loss: 0.6791582107543945, acc: 0.4990234375] [gan loss: 0.939539, acc: 0.000000]\n",
            "3061: [discriminator loss: 0.6735523343086243, acc: 0.6201171875] [gan loss: 0.672431, acc: 0.605469]\n",
            "3062: [discriminator loss: 0.6807584166526794, acc: 0.4970703125] [gan loss: 0.968748, acc: 0.000000]\n",
            "3063: [discriminator loss: 0.6763005256652832, acc: 0.6259765625] [gan loss: 0.645392, acc: 0.740234]\n",
            "3064: [discriminator loss: 0.6790868043899536, acc: 0.5146484375] [gan loss: 0.982726, acc: 0.000000]\n",
            "3065: [discriminator loss: 0.6808760762214661, acc: 0.6123046875] [gan loss: 0.630026, acc: 0.869141]\n",
            "3066: [discriminator loss: 0.6779822111129761, acc: 0.4990234375] [gan loss: 0.990431, acc: 0.000000]\n",
            "3067: [discriminator loss: 0.6776875853538513, acc: 0.6083984375] [gan loss: 0.649816, acc: 0.792969]\n",
            "3068: [discriminator loss: 0.6766461133956909, acc: 0.484375] [gan loss: 0.960144, acc: 0.000000]\n",
            "3069: [discriminator loss: 0.6775882244110107, acc: 0.6328125] [gan loss: 0.669356, acc: 0.703125]\n",
            "3070: [discriminator loss: 0.6764829158782959, acc: 0.5068359375] [gan loss: 0.919607, acc: 0.000000]\n",
            "3071: [discriminator loss: 0.6784664988517761, acc: 0.630859375] [gan loss: 0.676923, acc: 0.662109]\n",
            "3072: [discriminator loss: 0.6692901849746704, acc: 0.5185546875] [gan loss: 0.908913, acc: 0.001953]\n",
            "3073: [discriminator loss: 0.6854515075683594, acc: 0.5966796875] [gan loss: 0.704192, acc: 0.505859]\n",
            "3074: [discriminator loss: 0.6703652739524841, acc: 0.5146484375] [gan loss: 0.921726, acc: 0.000000]\n",
            "3075: [discriminator loss: 0.6694280505180359, acc: 0.6376953125] [gan loss: 0.668679, acc: 0.634766]\n",
            "3076: [discriminator loss: 0.6714429259300232, acc: 0.5087890625] [gan loss: 0.996112, acc: 0.000000]\n",
            "3077: [discriminator loss: 0.6709582805633545, acc: 0.609375] [gan loss: 0.596332, acc: 0.943359]\n",
            "3078: [discriminator loss: 0.6930343508720398, acc: 0.4921875] [gan loss: 1.069904, acc: 0.000000]\n",
            "3079: [discriminator loss: 0.6882089972496033, acc: 0.5546875] [gan loss: 0.577354, acc: 0.966797]\n",
            "3080: [discriminator loss: 0.6962032318115234, acc: 0.494140625] [gan loss: 1.010147, acc: 0.000000]\n",
            "3081: [discriminator loss: 0.6777296662330627, acc: 0.5869140625] [gan loss: 0.624246, acc: 0.849609]\n",
            "3082: [discriminator loss: 0.6854292750358582, acc: 0.4833984375] [gan loss: 0.932092, acc: 0.007812]\n",
            "3083: [discriminator loss: 0.6752955913543701, acc: 0.642578125] [gan loss: 0.672532, acc: 0.648438]\n",
            "3084: [discriminator loss: 0.6683452129364014, acc: 0.5234375] [gan loss: 0.876214, acc: 0.039062]\n",
            "3085: [discriminator loss: 0.6730442047119141, acc: 0.6494140625] [gan loss: 0.716450, acc: 0.384766]\n",
            "3086: [discriminator loss: 0.6519032120704651, acc: 0.6044921875] [gan loss: 0.829948, acc: 0.076172]\n",
            "3087: [discriminator loss: 0.6787471175193787, acc: 0.57421875] [gan loss: 0.764785, acc: 0.117188]\n",
            "3088: [discriminator loss: 0.649453341960907, acc: 0.6826171875] [gan loss: 0.807090, acc: 0.109375]\n",
            "3089: [discriminator loss: 0.6778074502944946, acc: 0.56640625] [gan loss: 0.791473, acc: 0.080078]\n",
            "3090: [discriminator loss: 0.6571035981178284, acc: 0.693359375] [gan loss: 0.792327, acc: 0.101562]\n",
            "3091: [discriminator loss: 0.6712757349014282, acc: 0.57421875] [gan loss: 0.798961, acc: 0.068359]\n",
            "3092: [discriminator loss: 0.6503283977508545, acc: 0.705078125] [gan loss: 0.779230, acc: 0.158203]\n",
            "3093: [discriminator loss: 0.663985013961792, acc: 0.587890625] [gan loss: 0.840398, acc: 0.074219]\n",
            "3094: [discriminator loss: 0.6554532051086426, acc: 0.6826171875] [gan loss: 0.700631, acc: 0.447266]\n",
            "3095: [discriminator loss: 0.6672689914703369, acc: 0.5546875] [gan loss: 0.985775, acc: 0.015625]\n",
            "3096: [discriminator loss: 0.6668659448623657, acc: 0.615234375] [gan loss: 0.545543, acc: 0.958984]\n",
            "3097: [discriminator loss: 0.6910645365715027, acc: 0.5] [gan loss: 1.203189, acc: 0.000000]\n",
            "3098: [discriminator loss: 0.7214688062667847, acc: 0.517578125] [gan loss: 0.516494, acc: 1.000000]\n",
            "3099: [discriminator loss: 0.7021542191505432, acc: 0.5] [gan loss: 1.031592, acc: 0.000000]\n",
            "3100: [discriminator loss: 0.6771385669708252, acc: 0.568359375] [gan loss: 0.645390, acc: 0.767578]\n",
            "3101: [discriminator loss: 0.671278715133667, acc: 0.4990234375] [gan loss: 0.880919, acc: 0.027344]\n",
            "3102: [discriminator loss: 0.6666159629821777, acc: 0.654296875] [gan loss: 0.728552, acc: 0.328125]\n",
            "3103: [discriminator loss: 0.6595950126647949, acc: 0.5791015625] [gan loss: 0.814739, acc: 0.144531]\n",
            "3104: [discriminator loss: 0.6801354289054871, acc: 0.5634765625] [gan loss: 0.752924, acc: 0.250000]\n",
            "3105: [discriminator loss: 0.6552263498306274, acc: 0.646484375] [gan loss: 0.785554, acc: 0.140625]\n",
            "3106: [discriminator loss: 0.6883776187896729, acc: 0.4833984375] [gan loss: 0.815735, acc: 0.052734]\n",
            "3107: [discriminator loss: 0.6534079313278198, acc: 0.6728515625] [gan loss: 0.705106, acc: 0.480469]\n",
            "3108: [discriminator loss: 0.6863221526145935, acc: 0.51171875] [gan loss: 0.892919, acc: 0.007812]\n",
            "3109: [discriminator loss: 0.6563649773597717, acc: 0.66796875] [gan loss: 0.658056, acc: 0.675781]\n",
            "3110: [discriminator loss: 0.6793938875198364, acc: 0.5126953125] [gan loss: 0.990055, acc: 0.000000]\n",
            "3111: [discriminator loss: 0.669552206993103, acc: 0.6162109375] [gan loss: 0.592872, acc: 0.910156]\n",
            "3112: [discriminator loss: 0.692995011806488, acc: 0.4951171875] [gan loss: 1.084297, acc: 0.000000]\n",
            "3113: [discriminator loss: 0.6840894818305969, acc: 0.5556640625] [gan loss: 0.556999, acc: 0.960938]\n",
            "3114: [discriminator loss: 0.7036163210868835, acc: 0.4990234375] [gan loss: 1.049625, acc: 0.000000]\n",
            "3115: [discriminator loss: 0.6882374286651611, acc: 0.5439453125] [gan loss: 0.603003, acc: 0.826172]\n",
            "3116: [discriminator loss: 0.6913864612579346, acc: 0.5224609375] [gan loss: 0.973365, acc: 0.005859]\n",
            "3117: [discriminator loss: 0.6807441115379333, acc: 0.587890625] [gan loss: 0.689552, acc: 0.568359]\n",
            "3118: [discriminator loss: 0.671795129776001, acc: 0.5576171875] [gan loss: 0.886173, acc: 0.029297]\n",
            "3119: [discriminator loss: 0.6853216886520386, acc: 0.5576171875] [gan loss: 0.690756, acc: 0.544922]\n",
            "3120: [discriminator loss: 0.6705210208892822, acc: 0.537109375] [gan loss: 0.870951, acc: 0.107422]\n",
            "3121: [discriminator loss: 0.6896016001701355, acc: 0.578125] [gan loss: 0.718798, acc: 0.457031]\n",
            "3122: [discriminator loss: 0.6665530204772949, acc: 0.5751953125] [gan loss: 0.874854, acc: 0.093750]\n",
            "3123: [discriminator loss: 0.6864364147186279, acc: 0.5830078125] [gan loss: 0.715615, acc: 0.458984]\n",
            "3124: [discriminator loss: 0.6742587685585022, acc: 0.5859375] [gan loss: 0.885199, acc: 0.029297]\n",
            "3125: [discriminator loss: 0.6927027106285095, acc: 0.53515625] [gan loss: 0.655729, acc: 0.683594]\n",
            "3126: [discriminator loss: 0.6708710789680481, acc: 0.52734375] [gan loss: 0.927176, acc: 0.007812]\n",
            "3127: [discriminator loss: 0.6936818361282349, acc: 0.5693359375] [gan loss: 0.634647, acc: 0.765625]\n",
            "3128: [discriminator loss: 0.6761492490768433, acc: 0.5048828125] [gan loss: 0.973205, acc: 0.005859]\n",
            "3129: [discriminator loss: 0.6887224912643433, acc: 0.580078125] [gan loss: 0.551958, acc: 0.933594]\n",
            "3130: [discriminator loss: 0.7043413519859314, acc: 0.498046875] [gan loss: 1.058418, acc: 0.000000]\n",
            "3131: [discriminator loss: 0.7030565142631531, acc: 0.56640625] [gan loss: 0.552144, acc: 0.955078]\n",
            "3132: [discriminator loss: 0.708418071269989, acc: 0.4873046875] [gan loss: 0.959159, acc: 0.000000]\n",
            "3133: [discriminator loss: 0.7066400051116943, acc: 0.564453125] [gan loss: 0.624723, acc: 0.771484]\n",
            "3134: [discriminator loss: 0.692204475402832, acc: 0.4755859375] [gan loss: 0.859711, acc: 0.035156]\n",
            "3135: [discriminator loss: 0.6952471137046814, acc: 0.548828125] [gan loss: 0.677297, acc: 0.533203]\n",
            "3136: [discriminator loss: 0.6939956545829773, acc: 0.4501953125] [gan loss: 0.845125, acc: 0.082031]\n",
            "3137: [discriminator loss: 0.6853480339050293, acc: 0.6142578125] [gan loss: 0.657215, acc: 0.714844]\n",
            "3138: [discriminator loss: 0.7034729719161987, acc: 0.4208984375] [gan loss: 0.881245, acc: 0.035156]\n",
            "3139: [discriminator loss: 0.6830281615257263, acc: 0.5849609375] [gan loss: 0.638594, acc: 0.808594]\n",
            "3140: [discriminator loss: 0.7077104449272156, acc: 0.466796875] [gan loss: 0.928618, acc: 0.000000]\n",
            "3141: [discriminator loss: 0.6994633078575134, acc: 0.5302734375] [gan loss: 0.590701, acc: 0.845703]\n",
            "3142: [discriminator loss: 0.7200369834899902, acc: 0.494140625] [gan loss: 1.013691, acc: 0.003906]\n",
            "3143: [discriminator loss: 0.7076025605201721, acc: 0.5283203125] [gan loss: 0.531031, acc: 0.925781]\n",
            "3144: [discriminator loss: 0.7514315843582153, acc: 0.4951171875] [gan loss: 1.021232, acc: 0.000000]\n",
            "3145: [discriminator loss: 0.7181251049041748, acc: 0.4970703125] [gan loss: 0.556502, acc: 0.876953]\n",
            "3146: [discriminator loss: 0.7361361384391785, acc: 0.498046875] [gan loss: 0.929461, acc: 0.027344]\n",
            "3147: [discriminator loss: 0.707364559173584, acc: 0.48828125] [gan loss: 0.603376, acc: 0.845703]\n",
            "3148: [discriminator loss: 0.7307282090187073, acc: 0.46875] [gan loss: 0.874372, acc: 0.005859]\n",
            "3149: [discriminator loss: 0.6807265877723694, acc: 0.625] [gan loss: 0.598682, acc: 0.841797]\n",
            "3150: [discriminator loss: 0.7409353852272034, acc: 0.470703125] [gan loss: 0.935766, acc: 0.000000]\n",
            "3151: [discriminator loss: 0.6896557807922363, acc: 0.5703125] [gan loss: 0.601078, acc: 0.863281]\n",
            "3152: [discriminator loss: 0.724186897277832, acc: 0.4912109375] [gan loss: 0.947966, acc: 0.000000]\n",
            "3153: [discriminator loss: 0.7094292640686035, acc: 0.50390625] [gan loss: 0.625761, acc: 0.833984]\n",
            "3154: [discriminator loss: 0.7252155542373657, acc: 0.466796875] [gan loss: 0.925202, acc: 0.000000]\n",
            "3155: [discriminator loss: 0.6929197311401367, acc: 0.5712890625] [gan loss: 0.615175, acc: 0.853516]\n",
            "3156: [discriminator loss: 0.7197222113609314, acc: 0.4755859375] [gan loss: 0.945842, acc: 0.000000]\n",
            "3157: [discriminator loss: 0.6843715906143188, acc: 0.603515625] [gan loss: 0.581704, acc: 0.882812]\n",
            "3158: [discriminator loss: 0.723121702671051, acc: 0.48828125] [gan loss: 1.001821, acc: 0.000000]\n",
            "3159: [discriminator loss: 0.6710869669914246, acc: 0.6083984375] [gan loss: 0.556055, acc: 0.970703]\n",
            "3160: [discriminator loss: 0.7286214828491211, acc: 0.4931640625] [gan loss: 0.993958, acc: 0.000000]\n",
            "3161: [discriminator loss: 0.6814912557601929, acc: 0.609375] [gan loss: 0.600630, acc: 0.871094]\n",
            "3162: [discriminator loss: 0.7008126378059387, acc: 0.484375] [gan loss: 0.932245, acc: 0.000000]\n",
            "3163: [discriminator loss: 0.6816666126251221, acc: 0.6142578125] [gan loss: 0.645279, acc: 0.742188]\n",
            "3164: [discriminator loss: 0.6948037147521973, acc: 0.4794921875] [gan loss: 0.934548, acc: 0.000000]\n",
            "3165: [discriminator loss: 0.675459086894989, acc: 0.623046875] [gan loss: 0.665030, acc: 0.628906]\n",
            "3166: [discriminator loss: 0.6837438344955444, acc: 0.4794921875] [gan loss: 0.916931, acc: 0.000000]\n",
            "3167: [discriminator loss: 0.675693929195404, acc: 0.623046875] [gan loss: 0.695059, acc: 0.460938]\n",
            "3168: [discriminator loss: 0.6867955923080444, acc: 0.4716796875] [gan loss: 0.905377, acc: 0.005859]\n",
            "3169: [discriminator loss: 0.6666894555091858, acc: 0.6650390625] [gan loss: 0.675442, acc: 0.503906]\n",
            "3170: [discriminator loss: 0.6875448226928711, acc: 0.484375] [gan loss: 0.956088, acc: 0.000000]\n",
            "3171: [discriminator loss: 0.678910493850708, acc: 0.609375] [gan loss: 0.601204, acc: 0.923828]\n",
            "3172: [discriminator loss: 0.6990218162536621, acc: 0.48828125] [gan loss: 1.025840, acc: 0.000000]\n",
            "3173: [discriminator loss: 0.6770738363265991, acc: 0.587890625] [gan loss: 0.578290, acc: 0.964844]\n",
            "3174: [discriminator loss: 0.7093312740325928, acc: 0.484375] [gan loss: 1.005548, acc: 0.000000]\n",
            "3175: [discriminator loss: 0.687483549118042, acc: 0.5947265625] [gan loss: 0.601141, acc: 0.908203]\n",
            "3176: [discriminator loss: 0.7035887241363525, acc: 0.4853515625] [gan loss: 0.956778, acc: 0.000000]\n",
            "3177: [discriminator loss: 0.6793234944343567, acc: 0.634765625] [gan loss: 0.614465, acc: 0.890625]\n",
            "3178: [discriminator loss: 0.6939882040023804, acc: 0.484375] [gan loss: 0.957606, acc: 0.000000]\n",
            "3179: [discriminator loss: 0.6764169931411743, acc: 0.6318359375] [gan loss: 0.617889, acc: 0.894531]\n",
            "3180: [discriminator loss: 0.6999346017837524, acc: 0.482421875] [gan loss: 0.962116, acc: 0.000000]\n",
            "3181: [discriminator loss: 0.6772278547286987, acc: 0.6328125] [gan loss: 0.607040, acc: 0.906250]\n",
            "3182: [discriminator loss: 0.6984386444091797, acc: 0.4853515625] [gan loss: 0.993109, acc: 0.000000]\n",
            "3183: [discriminator loss: 0.6614905595779419, acc: 0.6259765625] [gan loss: 0.575654, acc: 0.984375]\n",
            "3184: [discriminator loss: 0.7090173959732056, acc: 0.4853515625] [gan loss: 1.024406, acc: 0.000000]\n",
            "3185: [discriminator loss: 0.6877879500389099, acc: 0.59375] [gan loss: 0.580865, acc: 0.976562]\n",
            "3186: [discriminator loss: 0.7073786854743958, acc: 0.4814453125] [gan loss: 0.980056, acc: 0.000000]\n",
            "3187: [discriminator loss: 0.6876691579818726, acc: 0.5810546875] [gan loss: 0.626370, acc: 0.867188]\n",
            "3188: [discriminator loss: 0.7054166197776794, acc: 0.4736328125] [gan loss: 0.936761, acc: 0.000000]\n",
            "3189: [discriminator loss: 0.6771107912063599, acc: 0.599609375] [gan loss: 0.624604, acc: 0.748047]\n",
            "3190: [discriminator loss: 0.7030312418937683, acc: 0.48046875] [gan loss: 0.950705, acc: 0.000000]\n",
            "3191: [discriminator loss: 0.6727582216262817, acc: 0.6064453125] [gan loss: 0.592961, acc: 0.902344]\n",
            "3192: [discriminator loss: 0.7175750136375427, acc: 0.4853515625] [gan loss: 0.978217, acc: 0.000000]\n",
            "3193: [discriminator loss: 0.6790460348129272, acc: 0.587890625] [gan loss: 0.599974, acc: 0.839844]\n",
            "3194: [discriminator loss: 0.7181233763694763, acc: 0.478515625] [gan loss: 0.957246, acc: 0.000000]\n",
            "3195: [discriminator loss: 0.6801127195358276, acc: 0.6083984375] [gan loss: 0.622510, acc: 0.841797]\n",
            "3196: [discriminator loss: 0.7050468921661377, acc: 0.4697265625] [gan loss: 0.946649, acc: 0.000000]\n",
            "3197: [discriminator loss: 0.671745240688324, acc: 0.6484375] [gan loss: 0.616300, acc: 0.861328]\n",
            "3198: [discriminator loss: 0.6937015056610107, acc: 0.4775390625] [gan loss: 0.959027, acc: 0.000000]\n",
            "3199: [discriminator loss: 0.6861854195594788, acc: 0.623046875] [gan loss: 0.641894, acc: 0.792969]\n",
            "3200: [discriminator loss: 0.7006902694702148, acc: 0.47265625] [gan loss: 0.943474, acc: 0.000000]\n",
            "3201: [discriminator loss: 0.6778661608695984, acc: 0.646484375] [gan loss: 0.631439, acc: 0.839844]\n",
            "3202: [discriminator loss: 0.7002981305122375, acc: 0.4697265625] [gan loss: 0.958401, acc: 0.000000]\n",
            "3203: [discriminator loss: 0.666439414024353, acc: 0.630859375] [gan loss: 0.597899, acc: 0.912109]\n",
            "3204: [discriminator loss: 0.7057177424430847, acc: 0.498046875] [gan loss: 1.010451, acc: 0.000000]\n",
            "3205: [discriminator loss: 0.667217493057251, acc: 0.58984375] [gan loss: 0.557570, acc: 0.947266]\n",
            "3206: [discriminator loss: 0.7231617569923401, acc: 0.498046875] [gan loss: 1.021905, acc: 0.000000]\n",
            "3207: [discriminator loss: 0.687485933303833, acc: 0.5849609375] [gan loss: 0.577819, acc: 0.957031]\n",
            "3208: [discriminator loss: 0.7129263877868652, acc: 0.5] [gan loss: 0.971124, acc: 0.000000]\n",
            "3209: [discriminator loss: 0.6777009963989258, acc: 0.5986328125] [gan loss: 0.608003, acc: 0.845703]\n",
            "3210: [discriminator loss: 0.7125539779663086, acc: 0.4814453125] [gan loss: 0.942448, acc: 0.000000]\n",
            "3211: [discriminator loss: 0.6810270547866821, acc: 0.6220703125] [gan loss: 0.644097, acc: 0.718750]\n",
            "3212: [discriminator loss: 0.7005041241645813, acc: 0.4794921875] [gan loss: 0.914289, acc: 0.003906]\n",
            "3213: [discriminator loss: 0.6735043525695801, acc: 0.6435546875] [gan loss: 0.663065, acc: 0.681641]\n",
            "3214: [discriminator loss: 0.6948530077934265, acc: 0.46875] [gan loss: 0.921524, acc: 0.000000]\n",
            "3215: [discriminator loss: 0.6706503033638, acc: 0.6484375] [gan loss: 0.655190, acc: 0.730469]\n",
            "3216: [discriminator loss: 0.6832811832427979, acc: 0.486328125] [gan loss: 0.973250, acc: 0.000000]\n",
            "3217: [discriminator loss: 0.6780313849449158, acc: 0.60546875] [gan loss: 0.626952, acc: 0.855469]\n",
            "3218: [discriminator loss: 0.6953154802322388, acc: 0.478515625] [gan loss: 0.983333, acc: 0.000000]\n",
            "3219: [discriminator loss: 0.6744875907897949, acc: 0.6025390625] [gan loss: 0.578181, acc: 0.917969]\n",
            "3220: [discriminator loss: 0.7167993783950806, acc: 0.4892578125] [gan loss: 1.051513, acc: 0.000000]\n",
            "3221: [discriminator loss: 0.6727763414382935, acc: 0.6083984375] [gan loss: 0.563727, acc: 0.960938]\n",
            "3222: [discriminator loss: 0.707779586315155, acc: 0.49609375] [gan loss: 1.026735, acc: 0.000000]\n",
            "3223: [discriminator loss: 0.6879880428314209, acc: 0.576171875] [gan loss: 0.581598, acc: 0.908203]\n",
            "3224: [discriminator loss: 0.7001230716705322, acc: 0.4892578125] [gan loss: 0.975861, acc: 0.001953]\n",
            "3225: [discriminator loss: 0.6813879013061523, acc: 0.572265625] [gan loss: 0.621952, acc: 0.777344]\n",
            "3226: [discriminator loss: 0.6932114362716675, acc: 0.494140625] [gan loss: 0.967952, acc: 0.000000]\n",
            "3227: [discriminator loss: 0.6725319623947144, acc: 0.609375] [gan loss: 0.625701, acc: 0.763672]\n",
            "3228: [discriminator loss: 0.6928832530975342, acc: 0.4755859375] [gan loss: 0.964791, acc: 0.007812]\n",
            "3229: [discriminator loss: 0.680615246295929, acc: 0.560546875] [gan loss: 0.664452, acc: 0.625000]\n",
            "3230: [discriminator loss: 0.6871668100357056, acc: 0.4970703125] [gan loss: 0.947635, acc: 0.009766]\n",
            "3231: [discriminator loss: 0.6787729263305664, acc: 0.5966796875] [gan loss: 0.665343, acc: 0.636719]\n",
            "3232: [discriminator loss: 0.6945698261260986, acc: 0.494140625] [gan loss: 0.948164, acc: 0.021484]\n",
            "3233: [discriminator loss: 0.677911102771759, acc: 0.5947265625] [gan loss: 0.656333, acc: 0.681641]\n",
            "3234: [discriminator loss: 0.6826806664466858, acc: 0.5068359375] [gan loss: 0.995217, acc: 0.023438]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-9776ad56bd27>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-19-46c5e2efb0f5>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(models, x_train)\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mnoise\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muniform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlatent_size\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgan\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_on_batch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnoise\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     28\u001b[0m         \u001b[0mlog\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"%s [gan loss: %f, acc: %f]\"\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0macc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_on_batch\u001b[0;34m(self, x, y, sample_weight, class_weight, reset_metrics, return_dict)\u001b[0m\n\u001b[1;32m   1728\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1729\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1730\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1731\u001b[0m     \u001b[0mlogs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_numpy_or_python_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlogs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1732\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mreset_metrics\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1667\u001b[0m     \"\"\"\n\u001b[1;32m   1668\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mm\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1669\u001b[0;31m       \u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset_states\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1670\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1671\u001b[0m   def train_on_batch(self,\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/metrics.py\u001b[0m in \u001b[0;36mreset_states\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    251\u001b[0m     \u001b[0mwhen\u001b[0m \u001b[0ma\u001b[0m \u001b[0mmetric\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mevaluated\u001b[0m \u001b[0mduring\u001b[0m \u001b[0mtraining\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    252\u001b[0m     \"\"\"\n\u001b[0;32m--> 253\u001b[0;31m     \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbatch_set_value\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mv\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    254\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    255\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mabc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabstractmethod\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/dispatch.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    199\u001b[0m     \u001b[0;34m\"\"\"Call target, and fall back on dispatchers if there is a TypeError.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    200\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 201\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    202\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mTypeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m       \u001b[0;31m# Note: convert_to_eager_tensor currently raises a ValueError, not a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/keras/backend.py\u001b[0m in \u001b[0;36mbatch_set_value\u001b[0;34m(tuples)\u001b[0m\n\u001b[1;32m   3704\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexecuting_eagerly_outside_functions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3705\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtuples\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3706\u001b[0;31m       \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0massign\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3707\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3708\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mget_graph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/resource_variable_ops.py\u001b[0m in \u001b[0;36massign\u001b[0;34m(self, value, use_locking, name, read_value)\u001b[0m\n\u001b[1;32m    891\u001b[0m             (tensor_name, self._shape, value_tensor.shape))\n\u001b[1;32m    892\u001b[0m       assign_op = gen_resource_variable_ops.assign_variable_op(\n\u001b[0;32m--> 893\u001b[0;31m           self.handle, value_tensor, name=name)\n\u001b[0m\u001b[1;32m    894\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mread_value\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lazy_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0massign_op\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_resource_variable_ops.py\u001b[0m in \u001b[0;36massign_variable_op\u001b[0;34m(resource, value, name)\u001b[0m\n\u001b[1;32m    140\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m       _result = pywrap_tfe.TFE_Py_FastPathExecute(\n\u001b[0;32m--> 142\u001b[0;31m         _ctx, \"AssignVariableOp\", name, resource, value)\n\u001b[0m\u001b[1;32m    143\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYqoQLDvnonz"
      },
      "source": [
        "for layer in gen.layers:\n",
        "    try:\n",
        "        print(layer.name, layer.input)\n",
        "        # print()\n",
        "        # weights = layer.get_weights()[0]\n",
        "        # print(weights.mean(), weights.std(), weights.shape)\n",
        "    except:\n",
        "        pass"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dQft0052UiAc"
      },
      "source": [
        "X = tf.random.uniform((1,6272))\n",
        "for layer in gen.layers:\n",
        "    X = layer(X)\n",
        "    print(layer.__class__.__name__, 'output shape:\\t', X.shape)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QET1-p4bT0r2"
      },
      "source": [
        "gen.model().summary()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwRUls75tSP0"
      },
      "source": [
        "imgs_in_row = 20\n",
        "l1 = np.linspace(-1, 1, imgs_in_row)\n",
        "l2 = np.linspace(-1, 1, imgs_in_row)\n",
        "noise = np.array([l1, l2]).T\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0xV7OlQ2Wp0l"
      },
      "source": [
        "i = 0\n",
        "plt.figure(figsize=(15, 15))\n",
        "for n in range(imgs_in_row):\n",
        "    for m in range(imgs_in_row):\n",
        "        i += 1\n",
        "        plt.subplot(imgs_in_row, imgs_in_row, i)\n",
        "        image = gen.predict(np.array([l1[m], l2[n]]).reshape(-1, 2)).reshape(28, 28)\n",
        "        # image = np.reshape(images[i], [image_size, image_size])\n",
        "        plt.imshow(image, cmap='gray')\n",
        "        if i % 2 == 0:\n",
        "            plt.title(f\"{round(l1[m], 2)}, {round(l2[n], 2)}\")\n",
        "        plt.axis('off')\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cZ0bg5q0r9l1"
      },
      "source": [
        "l1, l2\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8dr0p24hsAgu"
      },
      "source": [
        "plot_images(gen, noise)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bYZ9u7zosR6s"
      },
      "source": [
        "np.array([l1[m], l2[n]]).reshape(-1, 2).shape"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VPAKgkKAwKSX"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}